{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03274c5e-be51-4a05-ad24-e6ba2b2594b2",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "### Refer the below URL for document Loaders\n",
    "#### https://python.langchain.com/v0.1/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184fd01-3929-4956-bc6e-011a3fe2bc79",
   "metadata": {},
   "source": [
    "## Text Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a61429a-f954-44f3-bc77-0efcfc90a893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x24652865790>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('sample.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bf9c3-4e11-44bf-b6a1-abba835b6196",
   "metadata": {},
   "source": [
    "## Packages required to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c57219-5086-46db-aa12-60f089af6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install pypdf\n",
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b1bd32e-9e27-4b1f-89cf-5357f33a7dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='Standard Chartered PLC is a British multinational bank with operations in wealth management, corporate and investment banking, and treasury services. Despite being headquartered in the United Kingdom, it does not conduct retail banking in the UK, and around 90% of its profits come from Asia, Africa, and the Middle East.\\n\\nStandard Chartered has a primary listing on the London Stock Exchange and is a constituent of the FTSE 100 Index. It has secondary listings on the Hong Kong Stock Exchange, the National Stock Exchange of India, and OTC Markets Group Pink. Its largest shareholder is the Government of Singapore-owned Temasek Holdings.[4][5][6] The Financial Stability Board considers it a systemically important bank.\\n\\nJosÃ© ViÃ±als is the group chairman of Standard Chartered.[7] Bill Winters is the current group chief executive.[8] Diego De Giorgi is the current group chief financial officer.[9]\\n\\nThe name Standard Chartered comes from the names of the two banks that merged in 1969 to create it: The Chartered Bank of India, Australia and China, and Standard Bank of British South Africa.[10]\\n\\nThe Chartered Bank began when Queen Victoria granted a royal charter to Scotsman James Wilson in 1853. Chartered opened its first branches in Bombay, Calcutta, and Shanghai in 1858; branches in Hong Kong and Singapore followed in 1859. The bank started issuing banknotes denominated in Hong Kong dollars in 1862.[10]')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef9332-1fb2-4cb8-a50a-b4705c0e3174",
   "metadata": {},
   "source": [
    "## Reading a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a28785-c3ef-49c7-bd25-83a3ddf4ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x24655d2a8d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader_pdf = PyPDFLoader('DSML.pdf')\n",
    "loader_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b28f5f-9fab-46d8-bc78-3ac69a2eaccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 0, 'page_label': '1'}, page_content='Data Science and Machine Learning\\nMathematical and Statistical Methods\\nDirk P. Kroese, Zdravko I. Botev, Thomas Taimre, Radislav Vaisman\\n22nd August 2024'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 1, 'page_label': '2'}, page_content='To my wife and daughters: Lesley, Elise, and Jessica\\n— DPK\\nTo Sarah, Sofia, and my parents\\n— ZIB\\nTo my grandparents: Arno, Harry, Juta, and Maila\\n— TT\\nTo Valerie\\n— RV'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 2, 'page_label': 'vii'}, page_content='CONTENTS\\nPreface xiii\\nNotation xvii\\n1 Importing, Summarizing, and Visualizing Data 1\\n1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Structuring Features According to Type . . . . . . . . . . . . . . . . . . 3\\n1.3 Summary Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.4 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.5 Visualizing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.5.1 Plotting Qualitative Variables . . . . . . . . . . . . . . . . . . . . 9\\n1.5.2 Plotting Quantitative Variables . . . . . . . . . . . . . . . . . . . 9\\n1.5.3 Data Visualization in a Bivariate Setting . . . . . . . . . . . . . . 12\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2 Statistical Learning 19\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.2 Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . . 20\\n2.3 Training and Test Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.4 Tradeo ffs in Statistical Learning . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5 Estimating Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.1 In-Sample Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.5.2 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n2.6 Modeling Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.7 Multivariate Normal Models . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.8 Normal Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n2.9 Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n3 Monte Carlo Methods 67\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n3.2 Monte Carlo Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n3.2.1 Generating Random Numbers . . . . . . . . . . . . . . . . . . . 68\\n3.2.2 Simulating Random Variables . . . . . . . . . . . . . . . . . . . 69\\n3.2.3 Simulating Random Vectors and Processes . . . . . . . . . . . . . 74\\n3.2.4 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n3.2.5 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 78\\n3.3 Monte Carlo Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\nvii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 3, 'page_label': 'viii'}, page_content='viii Contents\\n3.3.1 Crude Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n3.3.2 Bootstrap Method . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n3.3.3 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n3.4 Monte Carlo for Optimization . . . . . . . . . . . . . . . . . . . . . . . . 96\\n3.4.1 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . 96\\n3.4.2 Cross-Entropy Method . . . . . . . . . . . . . . . . . . . . . . . 100\\n3.4.3 Splitting for Optimization . . . . . . . . . . . . . . . . . . . . . . 103\\n3.4.4 Noisy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 106\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n4 Unsupervised Learning 121\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n4.2 Risk and Loss in Unsupervised Learning . . . . . . . . . . . . . . . . . . 122\\n4.3 Expectation–Maximization (EM) Algorithm . . . . . . . . . . . . . . . . 128\\n4.4 Empirical Distribution and Density Estimation . . . . . . . . . . . . . . . 131\\n4.5 Clustering via Mixture Models . . . . . . . . . . . . . . . . . . . . . . . 135\\n4.5.1 Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n4.5.2 EM Algorithm for Mixture Models . . . . . . . . . . . . . . . . . 137\\n4.6 Clustering via Vector Quantization . . . . . . . . . . . . . . . . . . . . . 142\\n4.6.1 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n4.6.2 Clustering via Continuous Multiextremal Optimization . . . . . . 146\\n4.7 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\\n4.8 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . 153\\n4.8.1 Motivation: Principal Axes of an Ellipsoid . . . . . . . . . . . . . 154\\n4.8.2 PCA and Singular Value Decomposition (SVD) . . . . . . . . . . 155\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n5 Regression 167\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n5.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n5.3 Analysis via Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n5.3.1 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . 171\\n5.3.2 Model Selection and Prediction . . . . . . . . . . . . . . . . . . . 172\\n5.3.3 Cross-Validation and Predictive Residual Sum of Squares . . . . . 173\\n5.3.4 In-Sample Risk and Akaike Information Criterion . . . . . . . . . 175\\n5.3.5 Categorical Features . . . . . . . . . . . . . . . . . . . . . . . . 177\\n5.3.6 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n5.3.7 Coe fficient of Determination . . . . . . . . . . . . . . . . . . . . 181\\n5.4 Inference for Normal Linear Models . . . . . . . . . . . . . . . . . . . . 182\\n5.4.1 Comparing Two Normal Linear Models . . . . . . . . . . . . . . 183\\n5.4.2 Confidence and Prediction Intervals . . . . . . . . . . . . . . . . 186\\n5.5 Nonlinear Regression Models . . . . . . . . . . . . . . . . . . . . . . . . 188\\n5.6 Linear Models in Python . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n5.6.1 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n5.6.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\n5.6.3 Analysis of Variance (ANOV A) . . . . . . . . . . . . . . . . . . 196'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 4, 'page_label': 'ix'}, page_content='Contents ix\\n5.6.4 Confidence and Prediction Intervals . . . . . . . . . . . . . . . . 198\\n5.6.5 Model Validation . . . . . . . . . . . . . . . . . . . . . . . . . . 199\\n5.6.6 Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n5.7 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 204\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n6 Regularization and Kernel Methods 215\\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\\n6.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n6.3 Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 222\\n6.4 Construction of Reproducing Kernels . . . . . . . . . . . . . . . . . . . . 224\\n6.4.1 Reproducing Kernels via Feature Mapping . . . . . . . . . . . . . 224\\n6.4.2 Kernels from Characteristic Functions . . . . . . . . . . . . . . . 225\\n6.4.3 Reproducing Kernels Using Orthonormal Features . . . . . . . . 227\\n6.4.4 Kernels from Kernels . . . . . . . . . . . . . . . . . . . . . . . . 229\\n6.5 Representer Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\\n6.6 Smoothing Cubic Splines . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n6.7 Gaussian Process Regression . . . . . . . . . . . . . . . . . . . . . . . . 238\\n6.8 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\\n7 Classification 251\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\\n7.2 Classification Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\\n7.3 Classification via Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . 257\\n7.4 Linear and Quadratic Discriminant Analysis . . . . . . . . . . . . . . . . 259\\n7.5 Logistic Regression and Softmax Classification . . . . . . . . . . . . . . 266\\n7.6 K-Nearest Neighbors Classification . . . . . . . . . . . . . . . . . . . . . 268\\n7.7 Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\\n7.8 Classification with Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . 277\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\\n8 Decision Trees and Ensemble Methods 287\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\\n8.2 Top-Down Construction of Decision Trees . . . . . . . . . . . . . . . . . 289\\n8.2.1 Regional Prediction Functions . . . . . . . . . . . . . . . . . . . 290\\n8.2.2 Splitting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\\n8.2.3 Termination Criterion . . . . . . . . . . . . . . . . . . . . . . . . 292\\n8.2.4 Basic Implementation . . . . . . . . . . . . . . . . . . . . . . . . 294\\n8.3 Additional Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.1 Binary Versus Non-Binary Trees . . . . . . . . . . . . . . . . . . 298\\n8.3.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.3 Alternative Splitting Rules . . . . . . . . . . . . . . . . . . . . . 298\\n8.3.4 Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . . 299\\n8.3.5 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\n8.4 Controlling the Tree Shape . . . . . . . . . . . . . . . . . . . . . . . . . 300\\n8.4.1 Cost-Complexity Pruning . . . . . . . . . . . . . . . . . . . . . . 303'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 5, 'page_label': 'x'}, page_content='x Contents\\n8.4.2 Advantages and Limitations of Decision Trees . . . . . . . . . . . 304\\n8.5 Bootstrap Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\\n8.6 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\\n8.7 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\\n9 Deep Learning 323\\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n9.2 Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 326\\n9.3 Back-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\n9.4 Methods for Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.4.1 Steepest Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.4.2 Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . 336\\n9.4.3 Limited-Memory BFGS Method . . . . . . . . . . . . . . . . . . 337\\n9.4.4 Adaptive Gradient Methods . . . . . . . . . . . . . . . . . . . . . 339\\n9.5 Examples in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\\n9.5.1 Simple Polynomial Regression . . . . . . . . . . . . . . . . . . . 341\\n9.5.2 Image Classification . . . . . . . . . . . . . . . . . . . . . . . . 345\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\\nA Linear Algebra and Functional Analysis 355\\nA.1 Vector Spaces, Bases, and Matrices . . . . . . . . . . . . . . . . . . . . . 355\\nA.2 Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\\nA.3 Complex Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . 361\\nA.4 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\\nA.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . 363\\nA.5.1 Left- and Right-Eigenvectors . . . . . . . . . . . . . . . . . . . . 364\\nA.6 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\\nA.6.1 (P)LU Decomposition . . . . . . . . . . . . . . . . . . . . . . . 368\\nA.6.2 Woodbury Identity . . . . . . . . . . . . . . . . . . . . . . . . . 370\\nA.6.3 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . 373\\nA.6.4 QR Decomposition and the Gram–Schmidt Procedure . . . . . . . 375\\nA.6.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . 376\\nA.6.6 Solving Structured Matrix Equations . . . . . . . . . . . . . . . . 379\\nA.7 Functional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\nA.8 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\\nA.8.1 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . 392\\nA.8.2 Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . 394\\nB Multivariate Di fferentiation and Optimization 397\\nB.1 Multivariate Di fferentiation . . . . . . . . . . . . . . . . . . . . . . . . . 397\\nB.1.1 Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . 400\\nB.1.2 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\\nB.2 Optimization Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\\nB.2.1 Convexity and Optimization . . . . . . . . . . . . . . . . . . . . 403\\nB.2.2 Lagrangian Method . . . . . . . . . . . . . . . . . . . . . . . . . 406\\nB.2.3 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 6, 'page_label': 'xi'}, page_content='Contents xi\\nB.3 Numerical Root-Finding and Minimization . . . . . . . . . . . . . . . . . 408\\nB.3.1 Newton-Like Methods . . . . . . . . . . . . . . . . . . . . . . . 409\\nB.3.2 Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . 411\\nB.3.3 Normal Approximation Method . . . . . . . . . . . . . . . . . . 413\\nB.3.4 Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . 414\\nB.4 Constrained Minimization via Penalty Functions . . . . . . . . . . . . . . 415\\nC Probability and Statistics 421\\nC.1 Random Experiments and Probability Spaces . . . . . . . . . . . . . . . 421\\nC.2 Random Variables and Probability Distributions . . . . . . . . . . . . . . 422\\nC.3 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426\\nC.4 Joint Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\\nC.5 Conditioning and Independence . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.1 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428\\nC.5.3 Expectation and Covariance . . . . . . . . . . . . . . . . . . . . 429\\nC.5.4 Conditional Density and Conditional Expectation . . . . . . . . . 431\\nC.6 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . 431\\nC.7 Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . . . . 434\\nC.8 Convergence of Random Variables . . . . . . . . . . . . . . . . . . . . . 439\\nC.9 Law of Large Numbers and Central Limit Theorem . . . . . . . . . . . . 445\\nC.10 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\\nC.11 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\\nC.12 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\\nC.12.1 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . 455\\nC.12.2 Maximum Likelihood Method . . . . . . . . . . . . . . . . . . . 456\\nC.13 Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\\nC.14 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\\nD Python Primer 463\\nD.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463\\nD.2 Python Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\\nD.3 Types and Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466\\nD.4 Functions and Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\\nD.5 Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\\nD.6 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\\nD.7 Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472\\nD.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\\nD.9 Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476\\nD.10 NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\\nD.10.1 Creating and Shaping Arrays . . . . . . . . . . . . . . . . . . . . 478\\nD.10.2 Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480\\nD.10.3 Array Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nD.10.4 Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 483\\nD.11 Matplotlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483\\nD.11.1 Creating a Basic Plot . . . . . . . . . . . . . . . . . . . . . . . . 483'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 7, 'page_label': 'xii'}, page_content='xii Contents\\nD.12 Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486\\nD.12.1 Series and DataFrame . . . . . . . . . . . . . . . . . . . . . . . . 486\\nD.12.2 Manipulating Data Frames . . . . . . . . . . . . . . . . . . . . . 487\\nD.12.3 Extracting Information . . . . . . . . . . . . . . . . . . . . . . . 489\\nD.12.4 Plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13 Scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13.1 Partitioning the Data . . . . . . . . . . . . . . . . . . . . . . . . 491\\nD.13.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\\nD.13.3 Fitting and Prediction . . . . . . . . . . . . . . . . . . . . . . . . 493\\nD.13.4 Testing the Model . . . . . . . . . . . . . . . . . . . . . . . . . . 493\\nD.14 System Calls, URL Access, and Speed-Up . . . . . . . . . . . . . . . . . 494\\nBibliography 496\\nIndex 505'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 8, 'page_label': 'xiii'}, page_content='PREFACE\\nIn our present world of automation, cloud computing, algorithms, artificial intelligence,\\nand big data, few topics are as relevant asdata science and machine learning. Their recent\\npopularity lies not only in their applicability to real-life questions, but also in their natural\\nblending of many different disciplines, including mathematics, statistics, computer science,\\nengineering, science, and finance.\\nTo someone starting to learn these topics, the multitude of computational techniques\\nand mathematical ideas may seem overwhelming. Some may be satisfied with only learn-\\ning how to use off-the-shelf recipes to apply to practical situations. But what if the assump-\\ntions of the black-box recipe are violated? Can we still trust the results? How should the\\nalgorithm be adapted? To be able to truly understand data science and machine learning it\\nis important to appreciate the underlying mathematics and statistics, as well as the resulting\\nalgorithms.\\nThe purpose of this book is to provide an accessible, yet comprehensive, account of\\ndata science and machine learning. It is intended for anyone interested in gaining a better\\nunderstanding of the mathematics and statistics that underpin the rich variety of ideas and\\nmachine learning algorithms in data science. Our viewpoint is that computer languages\\ncome and go, but the underlying key ideas and algorithms will remain forever and will\\nform the basis for future developments.\\nBefore we turn to a description of the topics in this book, we would like to say a\\nfew words about its philosophy. This book resulted from various courses in data science\\nand machine learning at the Universities of Queensland and New South Wales, Australia.\\nWhen we taught these courses, we noticed that students were eager to learn not only how\\nto apply algorithms but also to understand how these algorithms actually work. However,\\nmany existing textbooks assumed either too much background knowledge (e.g., measure\\ntheory and functional analysis) or too little (everything is a black box), and the information\\noverload from often disjointed and contradictory internet sources made it more difficult for\\nstudents to gradually build up their knowledge and understanding. We therefore wanted to\\nwrite a book about data science and machine learning that can be read as a linear story,\\nwith a substantial “backstory” in the appendices. The main narrative starts very simply and\\nbuilds up gradually to quite an advanced level. The backstory contains all the necessary\\nxiii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 9, 'page_label': 'xiv'}, page_content='xiv Preface\\nbackground, as well as additional information, from linear algebra and functional analysis\\n(Appendix A), multivariate differentiation and optimization (Appendix B), and probability\\nand statistics (Appendix C). Moreover, to make the abstract ideas come alive, we believe\\nit is important that the reader sees actual implementations of the algorithms, directly trans-\\nlated from the theory. After some deliberation we have chosen Python as our programming\\nlanguage. It is freely available and has been adopted as the programming language of\\nchoice for many practitioners in data science and machine learning. It has many useful\\npackages for data manipulation (often ported from R) and has been designed to be easy to\\nprogram. A gentle introduction to Python is given in Appendix D.\\nTo keep the book manageable in size we had to be selective in our choice of topics.\\nImportant ideas and connections between various concepts are highlighted via keywords\\nkeywords and page references (indicated by a ☞) in the margin. Key definitions and theorems are\\nhighlighted in boxes. Whenever feasible we provide proofs of theorems. Finally, we place\\ngreat importance on notation. It is often the case that once a consistent and concise system\\nof notation is in place, seemingly di fficult ideas suddenly become obvious. We use differ-\\nent fonts to distinguish between different types of objects. Vectors are denoted by letters in\\nboldface italics, x,X, and matrices by uppercase letters in boldface roman font, A,K. We\\nalso distinguish between random vectors and their values by using upper and lower case\\nletters, e.g., X (random vector) and x (its value or outcome). Sets are usually denoted by\\ncalligraphic letters G,H. The symbols for probability and expectation arePand E, respect-\\nively. Distributions are indicated by sans serif font, as in Bin and Gamma; exceptions are\\nthe ubiquitous notations N and U for the normal and uniform distributions. A summary of\\nthe most important symbols and abbreviations is given on Pages xvii–xxi.☞xvii\\nData science provides the language and techniques necessary for understanding and\\ndealing with data. It involves the design, collection, analysis, and interpretation of nu-\\nmerical data, with the aim of extracting patterns and other useful information. Machine\\nlearning, which is closely related to data science, deals with the design of algorithms and\\ncomputer resources to learn from data. The organization of the book follows roughly the\\ntypical steps in a data science project: Gathering data to gain information about a research\\nquestion; cleaning, summarization, and visualization of the data; modeling and analysis of\\nthe data; translating decisions about the model into decisions and predictions about the re-\\nsearch question. As this is a mathematics and statistics oriented book, most emphasis will\\nbe on modeling and analysis.\\nWe start in Chapter 1 with the reading, structuring, summarization, and visualization\\nof data using the data manipulation package pandas in Python. Although the material\\ncovered in this chapter requires no mathematical knowledge, it forms an obvious starting\\npoint for data science: to better understand the nature of the available data. In Chapter 2, we\\nintroduce the main ingredients of statistical learning. We distinguish between supervised\\nand unsupervised learning techniques, and discuss how we can assess the predictive per-\\nformance of (un)supervised learning methods. An important part of statistical learning is\\nthe modeling of data. We introduce various useful models in data science including linear,\\nmultivariate Gaussian, and Bayesian models. Many algorithms in machine learning and\\ndata science make use of Monte Carlo techniques, which is the topic of Chapter 3. Monte\\nCarlo can be used for simulation, estimation, and optimization. Chapter 4 is concerned\\nwith unsupervised learning, where we discuss techniques such as density estimation, clus-\\ntering, and principal component analysis. We then turn our attention to supervised learning'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 10, 'page_label': 'xv'}, page_content='Preface xv\\nin Chapter 5, and explain the ideas behind a broad class of regression models. Therein, we\\nalso describe how Python’sstatsmodels package can be used to define and analyze linear\\nmodels. Chapter 6 builds upon the previous regression chapter by developing the power-\\nful concepts of kernel methods and regularization, which allow the fundamental ideas of\\nChapter 5 to be expanded in an elegant way, using the theory of reproducing kernel Hilbert\\nspaces. In Chapter 7, we proceed with the classification task, which also belongs to the\\nsupervised learning framework, and consider various methods for classification, including\\nBayes classification, linear and quadratic discriminant analysis, K-nearest neighbors, and\\nsupport vector machines. In Chapter 8 we consider versatile methods for regression and\\nclassification that make use of tree structures. Finally, in Chapter 9, we consider the work-\\nings of neural networks and deep learning, and show that these learning algorithms have a\\nsimple mathematical interpretation. An extensive range of exercises is provided at the end\\nof each chapter.\\nPython code and data sets for each chapter can be downloaded from the GitHub site:\\nhttps://github.com/DSML-book\\nAcknowledgments\\nSome of the Python code for Chapters 1 and 5 was adapted from [73]. We thank Benoit\\nLiquet for making this available, and Lauren Jones for translating the R code into Python.\\nWe thank all who through their comments, feedback, and suggestions have contributed\\nto this book, including Qibin Duan, Luke Taylor, Rémi Mouzayek, Harry Goodman, Bryce\\nStansfield, Ryan Tongs, Dillon Steyl, Bill Rudd, Nan Ye, Christian Hirsch, Chris van der\\nHeide, Sarat Moka, Aapeli Vuorinen, Joshua Ross, Giang Nguyen, and the anonymous\\nreferees. David Grubbs deserves a special accollade for his professionalism and attention\\nto detail in his role as Editor for this book.\\nThe book was test-run during the 2019 Summer School of the Australian Mathemat-\\nical Sciences Institute. More than 80 bright upper-undergraduate (Honours) students used\\nthe book for the course Mathematical Methods for Machine Learning, taught by Zdravko\\nBotev. We are grateful for the valuable feedback that they provided.\\nOur special thanks go out to Robert Salomone, Liam Berry, Robin Carrick, and Sam\\nDaley, who commented in great detail on earlier versions of the entire book and wrote and\\nimproved our Python code. Their enthusiasm, perceptiveness, and kind assistance have\\nbeen invaluable.\\nOf course, none of this work would have been possible without the loving support,\\npatience, and encouragement from our families, and we thank them with all our hearts.\\nThis book was financially supported by the Australian Research Council Centre of\\nExcellence for Mathematical & Statistical Frontiers, under grant number CE140100049.\\nDirk Kroese, Zdravko Botev,\\nThomas Taimre, and Radislav Vaisman\\nBrisbane and Sydney'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 11, 'page_label': 'xvi'}, page_content='xvi'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 12, 'page_label': 'xvii'}, page_content='NOTATION\\nWe could, of course, use any notation we want; do not laugh at notations;\\ninvent them, they are powerful. In fact, mathematics is, to a large extent, in-\\nvention of better notations.\\nRichard P. Feynman\\nWe have tried to use a notation system that is, in order of importance, simple, descript-\\nive, consistent, and compatible with historical choices. Achieving all of these goals all of\\nthe time would be impossible, but we hope that our notation helps to quickly recognize\\nthe type or “flavor” of certain mathematical objects (vectors, matrices, random vectors,\\nprobability measures, etc.) and clarify intricate ideas.\\nWe make use of various typographical aids, and it will be beneficial for the reader to\\nbe aware of some of these.\\n• Boldface font is used to indicate composite objects, such as column vectors x =\\n[x1,..., xn]⊤ and matrices X = [xi j]. Note also the di fference between the upright bold\\nfont for matrices and the slanted bold font for vectors.\\n• Random variables are generally specified with upper case roman letters X,Y,Z and their\\noutcomes with lower case letters x,y,z. Random vectors are thus denoted in upper case\\nslanted bold font: X = [X1,..., Xn]⊤.\\n• Sets of vectors are generally written in calligraphic font, such as X, but the set of real\\nnumbers uses the common blackboard bold font R. Expectation and probability also use\\nthe latter font.\\n• Probability distributions use a sans serif font, such as Bin and Gamma. Exceptions to\\nthis rule are the “standard” notations N and U for the normal and uniform distributions.\\n• We often omit brackets when it is clear what the argument is of a function or operator.\\nFor example, we prefer EX2 to E[X2].\\nxvii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 13, 'page_label': 'xviii'}, page_content='xviii Notation\\n• We employ color to emphasize that certain words refer to a dataset, function, or\\npackage in Python. All code is written in typewriter font. To be compatible with past\\nnotation choices, we introduced a special blue symbol X for the model (design) matrix of\\na linear model.\\n• Important notation such as T, g, g∗ is often defined in a mnemonic way, such as Tfor\\n“training”, g for “guess”, g∗for the “star” (that is, optimal) guess, and ℓfor “loss”.\\n• We will occasionally use a Bayesian notation convention in which the same symbol is\\nused to denote different (conditional) probability densities. In particular, instead of writing\\nfX(x) and fX |Y (x |y) for the probability density function (pdf) of X and the conditional pdf\\nof X given Y, we simply write f (x) and f (x |y). This particular style of notation can be of\\ngreat descriptive value, despite its apparent ambiguity.\\nGeneral font/notation rules\\nx scalar\\nx vector\\nX random vector\\nX matrix\\nX set\\nbx estimate or approximation\\nx∗ optimal\\nx average\\nCommon mathematical symbols\\n∀ for all\\n∃ there exists\\n∝ is proportional to\\n⊥ is perpendicular to\\n∼ is distributed as\\niid\\n∼, ∼iid are independent and identically distributed as\\napprox.\\n∼ is approximately distributed as\\n∇f gradient of f\\n∇2 f Hessian of f\\nf ∈Cp f has continuous derivatives of order p\\n≈ is approximately\\n≃ is asymptotically\\n≪ is much smaller than\\n⊕ direct sum'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 14, 'page_label': 'xix'}, page_content='Notation xix\\n⊙ elementwise product\\n∩ intersection\\n∪ union\\n:=, =: is defined as\\na.s.\\n−→ converges almost surely to\\nd\\n−→ converges in distribution to\\nP\\n−→ converges in probability to\\nLp\\n−→ converges in Lp-norm to\\n∥·∥ Euclidean norm\\n⌈x⌉ smallest integer larger than x\\n⌊x⌋ largest integer smaller than x\\nx+ max{x,0}\\nMatrix/vector notation\\nA⊤, x⊤ transpose of matrix A or vector x\\nA−1 inverse of matrix A\\nA+ pseudo-inverse of matrix A\\nA−⊤ inverse of matrix A⊤or transpose of A−1\\nA ≻0 matrix A is positive definite\\nA ⪰0 matrix A is positive semidefinite\\ndim(x) dimension of vector x\\ndet(A) determinant of matrix A\\n|A| absolute value of the determinant of matrix A\\ntr(A) trace of matrix A\\nReserved letters and words\\nC set of complex numbers\\nd di fferential symbol\\nE expectation\\ne the number 2 .71828 ...\\nf probability density (discrete or continuous)\\ng prediction function\\n1{A}or 1A indicator function of set A\\ni the square root of −1\\nℓ risk: expected loss'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 15, 'page_label': 'xx'}, page_content='xx Notation\\nLoss loss function\\nln (natural) logarithm\\nN set of natural numbers {0,1,... }\\nO big-O order symbol: f (x) = O(g(x)) if |f (x)|⩽αg(x) for some constant αas\\nx →a\\no little-o order symbol: f (x) = o(g(x)) if f (x)/g(x) →0 as x →a\\nP probability measure\\nπ the number 3.14159 ...\\nR set of real numbers (one-dimensional Euclidean space)\\nRn n-dimensional Euclidean space\\nR+ positive real line: [0,∞)\\nτ deterministic training set\\nT random training set\\nX model (design) matrix\\nZ set of integers {..., −1,0,1,... }\\nProbability distributions\\nBer Bernoulli\\nBeta beta\\nBin binomial\\nExp exponential\\nGeom geometric\\nGamma gamma\\nF Fisher–Snedecor F\\nN normal or Gaussian\\nPareto Pareto\\nPoi Poisson\\nt Student’s t\\nU uniform\\nAbbreviations and acronyms\\ncdf cumulative distribution function\\nCMC crude Monte Carlo\\nCE cross-entropy\\nEM expectation–maximization\\nGP Gaussian process\\nKDE Kernel density estimate /estimator'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 16, 'page_label': 'xxi'}, page_content='Notation xxi\\nKL Kullback–Leibler\\nKKT Karush–Kuhn–Tucker\\niid independent and identically distributed\\nMAP maximum a posteriori\\nMCMC Markov chain Monte Carlo\\nMLE maximum likelihood estimator /estimate\\nOOB out-of-bag\\nPCA principal component analysis\\npdf probability density function (discrete or continuous)\\nSVD singular value decomposition'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 17, 'page_label': 'xxii'}, page_content='xxii'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 18, 'page_label': '1'}, page_content='CHAPTER 1\\nIMPORTING , SUMMARIZING , AND\\nVISUALIZING DATA\\nThis chapter describes where to find useful data sets, how to load them into Python,\\nand how to (re)structure the data. We also discuss various ways in which the data can\\nbe summarized via tables and figures. Which type of plots and numerical summaries\\nare appropriate depends on the type of the variable(s) in play. Readers unfamiliar with\\nPython are advised to read Appendix D first.\\n1.1 Introduction\\nData comes in many shapes and forms, but can generally be thought of as being the result\\nof some random experiment — an experiment whose outcome cannot be determined in\\nadvance, but whose workings are still subject to analysis. Data from a random experiment\\nare often stored in a table or spreadsheet. A statistical convention is to denote variables —\\noften called features features— as columns and the individual items (or units) as rows. It is useful\\nto think of three types of columns in such a spreadsheet:\\n1. The first column is usually an identifier or index column, where each unit /row is\\ngiven a unique name or ID.\\n2. Certain columns (features) can correspond to the design of the experiment, specify-\\ning, for example, to which experimental group the unit belongs. Often the entries in\\nthese columns are deterministic; that is, they stay the same if the experiment were to\\nbe repeated.\\n3. Other columns represent the observed measurements of the experiment. Usually,\\nthese measurements exhibit variability; that is, they would change if the experiment\\nwere to be repeated.\\nThere are many data sets available from the Internet and in software packages. A well-\\nknown repository of data sets is the Machine Learning Repository maintained by the Uni-\\nversity of California at Irvine (UCI), found at https://archive.ics.uci.edu/.\\n1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 19, 'page_label': '2'}, page_content=\"2 Introduction\\nThese data sets are typically stored in a CSV (comma separated values) format, which\\ncan be easily read into Python. For example, to access theabalone data set from this web-\\nsite with Python, download the file to your working directory, import the pandas package\\nvia\\nimport pandas as pd\\nand read in the data as follows:\\nabalone = pd.read_csv( 'abalone.data ',header = None)\\nIt is important to add header = None, as this lets Python know that the first line of the\\nCSV does not contain the names of the features, as it assumes so by default. The data set\\nwas originally used to predict the age of abalone from physical measurements, such as\\nshell weight and diameter.\\nAnother useful repository of over 1000 data sets from various packages in the R pro-\\ngramming language, collected by Vincent Arel-Bundock, can be found at:\\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html.\\nFor example, to read Fisher’s famous iris data set from R’s datasets package into Py-\\nthon, type:\\nurlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\\ndataname = 'datasets/iris.csv '\\niris = pd.read_csv(urlprefix + dataname)\\nThe iris data set contains four physical measurements (sepal /petal length/width) on\\n50 specimens (each) of 3 species of iris: setosa, versicolor, and virginica. Note that in this\\ncase the headers are included. The output of read_csv is a DataFrame object, which is\\npandas’s implementation of a spreadsheet; see Section D.12.1. The DataFrame method☞486\\nhead gives the first few rows of the DataFrame, including the feature names. The number\\nof rows can be passed as an argument and is 5 by default. For the iris DataFrame, we\\nhave:\\niris.head()\\nUnnamed: 0 Sepal.Length ... Petal.Width Species\\n0 1 5.1 ... 0.2 setosa\\n1 2 4.9 ... 0.2 setosa\\n2 3 4.7 ... 0.2 setosa\\n3 4 4.6 ... 0.2 setosa\\n4 5 5.0 ... 0.2 setosa\\n[5 rows x 6 columns]\\nThe names of the features can be obtained via thecolumnsattribute of the DataFrame\\nobject, as in iris.columns. Note that the first column is a duplicate index column, whose\\nname (assigned by pandas) is 'Unnamed: 0'. We can drop this column and reassign the\\nirisobject as follows:\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 20, 'page_label': '3'}, page_content=\"Importing, Summarizing, and Visualizing Data 3\\niris = iris.drop( 'Unnamed: 0 ',1)\\nThe data for each feature (corresponding to its specific name) can be accessed by using\\nPython’s slicing notation []. For example, the object iris[’Sepal.Length’] contains\\nthe 150 sepal lengths.\\nThe first three rows of the abalone data set from the UCI repository can be found as\\nfollows:\\nabalone.head(3)\\n0 1 2 3 4 5 6 7 8\\n0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15\\n1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7\\n2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9\\nHere, the missing headers have been assigned according to the order of the natural\\nnumbers. The names should correspond to Sex, Length, Diameter, Height, Whole weight,\\nShucked weight, Viscera weight, Shell weight, and Rings, as described in the file with the\\nname abalone.nameson the UCI website. We can manually add the names of the features\\nto the DataFrameby reassigning the columns attribute, as in:\\nabalone.columns = [ 'Sex', 'Length ', 'Diameter ', 'Height ',\\n'Whole weight ','Shucked weight ', 'Viscera weight ', 'Shell weight ',\\n'Rings ']\\n1.2 Structuring Features According to Type\\nWe can generally classify features as either quantitative or qualitative.Quantitative Quantitativefeatures\\npossess “numerical quantity”, such as height, age, number of births, etc., and can either be\\ncontinuous or discrete. Continuous quantitative features take values in a continuous range\\nof possible values, such as height, voltage, or crop yield; such features capture the idea\\nthat measurements can always be made more precisely. Discrete quantitative features have\\na countable number of possibilities, such as a count.\\nIn contrast, qualitative qualitativefeatures do not have a numerical meaning, but their possible\\nvalues can be divided into a fixed number of categories, such as {M,F}for gender or {blue,\\nblack, brown, green}for eye color. For this reason such features are also calledcategorical categorical.\\nA simple rule of thumb is: if it does not make sense to average the data, it is categorical.\\nFor example, it does not make sense to average eye colors. Of course it is still possible to\\nrepresent categorical data with numbers, such as 1 = blue, 2 = black, 3 = brown, but such\\nnumbers carry no quantitative meaning. Categorical features are often called factors factors.\\nWhen manipulating, summarizing, and displaying data, it is important to correctly spe-\\ncify the type of the variables (features). We illustrate this using the nutrition_elderly\\ndata set from [73], which contains the results of a study involving nutritional measure-\\nments of thirteen features (columns) for 226 elderly individuals (rows). The data set can be\\nobtained from:\\nhttp://www.biostatisticien.eu/springeR/nutrition_elderly.xls.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 21, 'page_label': '4'}, page_content=\"4 Structuring Features According to Type\\nExcel files can be read directly into pandas via the read_excel method:\\nxls = 'http://www.biostatisticien.eu/springeR/nutrition_elderly.xls '\\nnutri = pd.read_excel(xls)\\nThis creates a DataFrameobject nutri. The first three rows are as follows:\\npd.set_option( 'display.max_columns ', 8) # to fit display\\nnutri.head(3)\\ngender situation tea ... cooked_fruit_veg chocol fat\\n0 2 1 0 ... 4 5 6\\n1 2 1 1 ... 5 1 4\\n2 2 1 0 ... 2 5 4\\n[3 rows x 13 columns]\\nYou can check the type (or structure) of the variables via theinfo method of nutri.\\nnutri.info()\\n<class 'pandas.core.frame.DataFrame '>\\nRangeIndex: 226 entries, 0 to 225\\nData columns (total 13 columns):\\ngender 226 non-null int64\\nsituation 226 non-null int64\\ntea 226 non-null int64\\ncoffee 226 non-null int64\\nheight 226 non-null int64\\nweight 226 non-null int64\\nage 226 non-null int64\\nmeat 226 non-null int64\\nfish 226 non-null int64\\nraw_fruit 226 non-null int64\\ncooked_fruit_veg 226 non-null int64\\nchocol 226 non-null int64\\nfat 226 non-null int64\\ndtypes: int64(13)\\nmemory usage: 23.0 KB\\nAll 13 features in nutri are (at the moment) interpreted by Python as quantitative\\nvariables, indeed as integers, simply because they have been entered as whole numbers.\\nThe meaning of these numbers becomes clear when we consider the description of the\\nfeatures, given in Table 1.2. Table 1.1 shows how the variable types should be classified.\\nTable 1.1: The feature types for the data frame nutri.\\nQualitative gender, situation, fat\\nmeat, fish, raw_fruit, cooked_fruit_veg, chocol\\nDiscrete quantitative tea, coffee\\nContinuous quantitative height, weight, age\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 22, 'page_label': '5'}, page_content=\"Importing, Summarizing, and Visualizing Data 5\\nTable 1.2: Description of the variables in the nutritional study [73].\\nFeature Description Unit orCoding\\ngender Gender 1 =Male; 2=Female\\nsituation Family status\\n1=Single\\n2=Living with spouse\\n3=Living with family\\n4=Living with someone else\\ntea Daily consumption of tea Number of cups\\ncoffee Daily consumption of coffee Number of cups\\nheight Height cm\\nweight Weight (actually: mass) kg\\nage Age at date of interview Years\\nmeat Consumption of meat\\n0=Never\\n1=Less than once a week\\n2=Once a week\\n3=2–3 times a week\\n4=4–6 times a week\\n5=Every day\\nfish Consumption of fish As in meat\\nraw_fruit Consumption of raw fruits As inmeat\\ncooked_fruit_veg Consumption of cookedAs inmeatfruits and vegetables\\nchocol Consumption of chocolate As inmeat\\nfat\\n1=Butter\\n2=Margarine\\n3=Peanut oil\\nType of fat used 4 =Sunflower oil\\nfor cooking 5 =Olive oil\\n6=Mix of vegetable oils (e.g., Isio4)\\n7=Colza oil\\n8=Duck or goose fat\\nNote that the categories of the qualitative features in the second row of Table 1.1,meat,\\n. . . ,chocolhave a natural order. Such qualitative features are sometimes calledordinal, in\\ncontrast to qualitative features without order, which are called nominal. We will not make\\nsuch a distinction in this book.\\nWe can modify the Python value and type for each categorical feature, using the\\nreplace and astype methods. For categorical features, such as gender, we can replace\\nthe value 1 with 'Male' and 2 with 'Female', and change the type to 'category' as\\nfollows.\\nDICT = {1: 'Male ', 2: 'Female '} # dictionary specifies replacement\\nnutri[ 'gender '] = nutri[ 'gender '].replace(DICT).astype( 'category ')\\nThe structure of the other categorical-type features can be changed in a similar way.\\nContinuous features such as heightshould have type float:\\nnutri[ 'height '] = nutri[ 'height '].astype( float )\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 23, 'page_label': '6'}, page_content=\"6 Summary Tables\\nWe can repeat this for the other variables (see Exercise 2) and save this modified data\\nframe as a CSV file, by using the pandas method to_csv.\\nnutri.to_csv( 'nutri.csv ',index=False)\\n1.3 Summary Tables\\nIt is often useful to summarize a large spreadsheet of data in a more condensed form. A\\ntable of counts or a table of frequencies makes it easier to gain insight into the underlying\\ndistribution of a variable, especially if the data are qualitative. Such tables can be obtained\\nwith the methods describe and value_counts.\\nAs a first example, we load the nutri DataFrame, which we restructured and saved\\n(see previous section) as 'nutri.csv', and then construct a summary for the feature\\n(column) 'fat'.\\nnutri = pd.read_csv( 'nutri.csv ')\\nnutri[ 'fat'].describe()\\ncount 226\\nunique 8\\ntop sunflower\\nfreq 68\\nName: fat, dtype: object\\nWe see that there are 8 di fferent types of fat used and that sunflower has the highest\\ncount, with 68 out of 226 individuals using this type of cooking fat. The method\\nvalue_counts gives the counts for the different fat types.\\nnutri[ 'fat'].value_counts()\\nsunflower 68\\npeanut 48\\nolive 40\\nmargarine 27\\nIsio4 23\\nbutter 15\\nduck 4\\ncolza 1\\nName: fat, dtype: int64\\nColumn labels are also attributes of a DataFrame, and nutri.fat, for example, is\\nexactly the same object as nutri['fat'].\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 24, 'page_label': '7'}, page_content=\"Importing, Summarizing, and Visualizing Data 7\\nIt is also possible to use crosstab to cross tabulate between two or more variables, cross tabulate\\ngiving a contingency table:\\npd.crosstab(nutri.gender, nutri.situation)\\nsituation Couple Family Single\\ngender\\nFemale 56 7 78\\nMale 63 2 20\\nWe see, for example, that the proportion of single men is substantially smaller than the\\nproportion of single women in the data set of elderly people. To add row and column totals\\nto a table, use margins=True.\\npd.crosstab(nutri.gender, nutri.situation, margins=True)\\nsituation Couple Family Single All\\ngender\\nFemale 56 7 78 141\\nMale 63 2 20 85\\nAll 119 9 98 226\\n1.4 Summary Statistics\\nIn the following, x = [x1,..., xn]⊤ is a column vector of n numbers. For our nutri data,\\nthe vector x could, for example, correspond to the heights of the n = 226 individuals.\\nThe sample mean sample meanof x, denoted by x, is simply the average of the data values:\\nx = 1\\nn\\nnX\\ni=1\\nxi.\\nUsing the mean method in Python for the nutri data, we have, for instance:\\nnutri[ 'height '].mean()\\n163.96017699115043\\nThe p-sample quantile sample quantile(0 < p <1) of x is a value x such that at least a fraction p of the\\ndata is less than or equal to x and at least a fraction 1−p of the data is greater than or equal\\nto x. The sample median sample medianis the sample 0 .5-quantile. The p-sample quantile is also called\\nthe 100 ×p percentile. The 25, 50, and 75 sample percentiles are called the first, second,\\nand third quartiles quartilesof the data. For the nutri data they are obtained as follows.\\nnutri[ 'height '].quantile(q=[0.25,0.5,0.75])\\n0.25 157.0\\n0.50 163.0\\n0.75 170.0\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 25, 'page_label': '8'}, page_content=\"8 Visualizing Data\\nThe sample mean and median give information about thelocation of the data, while the\\ndistance between sample quantiles (say the 0.1 and 0.9 quantiles) gives some indication of\\nthe dispersion (spread) of the data. Other measures for dispersion are the sample range,sample range\\nmaxi xi −mini xi, the sample variancesample variance\\ns2 = 1\\nn −1\\nnX\\ni=1\\n(xi −x )2, (1.1)\\nand the sample standard deviation s =\\n√\\ns2. For the nutri data, the range (in cm) is:sample\\nstandard\\ndeviation\\n☞455\\nnutri[ 'height '].max () - nutri[ 'height '].min ()\\n48.0\\nThe variance (in cm2) is:\\nround (nutri[ 'height '].var(), 2) # round to two decimal places\\n81.06\\nAnd the standard deviation can be found via:\\nround (nutri[ 'height '].std(), 2)\\n9.0\\nWe already encountered thedescribe method in the previous section for summarizing\\nqualitative features, via the most frequent count and the number of unique elements. When\\napplied to a quantitative feature, it returns instead the minimum, maximum, mean, and the\\nthree quartiles. For example, the 'height' feature in the nutri data has the following\\nsummary statistics.\\nnutri[ 'height '].describe()\\ncount 226.000000\\nmean 163.960177\\nstd 9.003368\\nmin 140.000000\\n25\\\\% 157.000000\\n50\\\\% 163.000000\\n75\\\\% 170.000000\\nmax 188.000000\\nName: height, dtype: float64\\n1.5 Visualizing Data\\nIn this section we describe various methods for visualizing data. The main point we would\\nlike to make is that the way in which variables are visualized should always be adapted to\\nthe variable types; for example, qualitative data should be plotted differently from quantit-\\native data.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 26, 'page_label': '9'}, page_content=\"Importing, Summarizing, and Visualizing Data 9\\nFor the rest of this section, it is assumed that matplotlib.pyplot, pandas, and\\nnumpy, have been imported in the Python code as follows.\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\n1.5.1 Plotting Qualitative Variables\\nSuppose we wish to display graphically how many elderly people are living by themselves,\\nas a couple, with family, or other. Recall that the data are given in the situationcolumn\\nof our nutri data. Assuming that we already restructured the data, as in Section 1.2, we ☞ 3\\ncan make a barplot of the number of people in each category via the plt.bar function of barplot\\nthe standard matplotlib plotting library. The inputs are the x-axis positions, heights, and\\nwidths of each bar respectively.\\nwidth = 0.35 # the width of the bars\\nx = [0, 0.8, 1.6] # the bar positions on x-axis\\nsituation_counts=nutri[ 'situation '].value_counts()\\nplt.bar(x, situation_counts, width, edgecolor = 'black ')\\nplt.xticks(x, situation_counts.index)\\nplt.show()\\nCouple Single Family\\n0\\n25\\n50\\n75\\n100\\n125\\nFigure 1.1: Barplot for the qualitative variable 'situation'.\\n1.5.2 Plotting Quantitative Variables\\nWe now present a few useful methods for visualizing quantitative data, again using the\\nnutri data set. We will first focus on continuous features (e.g.,'age') and then add some\\nspecific graphs related to discrete features (e.g., 'tea'). The aim is to describe the variab-\\nility present in a single feature. This typically involves a central tendency, where observa-\\ntions tend to gather around, with fewer observations further away. The main aspects of the\\ndistribution are the location (or center) of the variability, thespread of the variability (how\\nfar the values extend from the center), and theshape of the variability; e.g., whether or not\\nvalues are spread symmetrically on either side of the center.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 27, 'page_label': '10'}, page_content=\"10 Visualizing Data\\n1.5.2.1 Boxplot\\nA boxplot can be viewed as a graphical representation of the five-number summary ofboxplot\\nthe data consisting of the minimum, maximum, and the first, second, and third quartiles.\\nFigure 1.2 gives a boxplot for the 'age' feature of the nutri data.\\nplt.boxplot(nutri[ 'age'],widths=width,vert=False)\\nplt.xlabel( 'age')\\nplt.show()\\nThe widthsparameter determines the width of the boxplot, which is by default plotted\\nvertically. Setting vert=Falseplots the boxplot horizontally, as in Figure 1.2.\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n1\\nFigure 1.2: Boxplot for 'age'.\\nThe box is drawn from the first quartile (Q1) to the third quartile (Q3). The vertical line\\ninside the box signifies the location of the median. So-called “whiskers” extend to either\\nside of the box. The size of the box is called the interquartile range: IQR = Q3 −Q1. The\\nleft whisker extends to the largest of (a) the minimum of the data and (b) Q1 −1.5 IQR.\\nSimilarly, the right whisker extends to the smallest of (a) the maximum of the data and\\n(b) Q3 + 1.5 IQR. Any data point outside the whiskers is indicated by a small hollow dot,\\nindicating a suspicious or deviant point (outlier). Note that a boxplot may also be used for\\ndiscrete quantitative features.\\n1.5.2.2 Histogram\\nA histogram is a common graphical representation of the distribution of a quantitativehistogram\\nfeature. We start by breaking the range of the values into a number of bins or classes.\\nWe tally the counts of the values falling in each bin and then make the plot by drawing\\nrectangles whose bases are the bin intervals and whose heights are the counts. In Python\\nwe can use the function plt.hist. For example, Figure 1.3 shows a histogram of the 226\\nages in nutri, constructed via the following Python code.\\nweights = np.ones_like(nutri.age)/nutri.age.count()\\nplt.hist(nutri.age,bins=9,weights=weights,facecolor= 'cyan ',\\nedgecolor= 'black ', linewidth=1)\\nplt.xlabel( 'age')\\nplt.ylabel( 'Proportion of Total ')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 28, 'page_label': '11'}, page_content=\"Importing, Summarizing, and Visualizing Data 11\\nHere 9 bins were used. Rather than using raw counts (the default), the vertical axis\\nhere gives the percentage in each class, defined by count\\ntotal . This is achieved by choosing the\\n“weights” parameter to be equal to the vector with entries 1/266, with length 226. Various\\nplotting parameters have also been changed.\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20Proportion of Total\\nFigure 1.3: Histogram of 'age'.\\nHistograms can also be used for discrete features, although it may be necessary to\\nexplicitly specify the bins and placement of the ticks on the axes.\\n1.5.2.3 Empirical Cumulative Distribution Function\\nThe empirical cumulative distribution function , denoted by Fn, is a step function which empirical\\ncumulative\\ndistribution\\nfunction\\njumps an amount k/n at observation values, where k is the number of tied observations\\nat that value. For observations x1,..., xn, Fn(x) is the fraction of observations less than or\\nequal to x, i.e.,\\nFn(x) = number of xi ⩽x\\nn = 1\\nn\\nnX\\ni=1\\n1{xi ⩽x}, (1.2)\\nwhere 1 denotes the indicator indicatorfunction; that is, 1{xi ⩽x}is equal to 1 when xi ⩽x and 0\\notherwise. To produce a plot of the empirical cumulative distribution function we can use\\nthe plt.step function. The result for the age data is shown in Figure 1.4. The empirical\\ncumulative distribution function for a discrete quantitative variable is obtained in the same\\nway.\\nx = np.sort(nutri.age)\\ny = np.linspace(0,1, len (nutri.age))\\nplt.xlabel( 'age')\\nplt.ylabel( 'Fn(x) ')\\nplt.step(x,y)\\nplt.xlim(x. min (),x. max ())\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 29, 'page_label': '12'}, page_content=\"12 Visualizing Data\\n65\\n 70\\n 75\\n 80\\n 85\\n 90\\nage\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Fn(x)\\nFigure 1.4: Plot of the empirical distribution function for the continuous quantitative fea-\\nture 'age'.\\n1.5.3 Data Visualization in a Bivariate Setting\\nIn this section, we present a few useful visual aids to explore relationships between two\\nfeatures. The graphical representation will depend on the type of the two features.\\n1.5.3.1 Two-way Plots for Two Categorical Variables\\nComparing barplots for two categorical variables involves introducing subplots to the fig-\\nure. Figure 1.5 visualizes the contingency table of Section 1.3, which cross-tabulates the\\nfamily status (situation) with the gender of the elderly people. It simply shows two barplots\\nnext to each other in the same figure.\\nCouple\\n Family\\n Single\\n0\\n20\\n40\\n60\\n80Counts\\nMale\\nFemale\\nFigure 1.5: Barplot for two categorical variables.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 30, 'page_label': '13'}, page_content=\"Importing, Summarizing, and Visualizing Data 13\\nThe figure was made using the seaborn package, which was specifically designed to\\nsimplify statistical visualization tasks.\\nimport seaborn as sns\\nsns.countplot(x= 'situation ', hue = 'gender ', data=nutri,\\nhue_order = [ 'Male ', 'Female '], palette = [ 'SkyBlue ','Pink '],\\nsaturation = 1, edgecolor= 'black ')\\nplt.legend(loc= 'upper center ')\\nplt.xlabel( '' )\\nplt.ylabel( 'Counts ')\\nplt.show()\\n1.5.3.2 Plots for Two Quantitative Variables\\nWe can visualize patterns between two quantitative features using ascatterplot scatterplot. This can be\\ndone with plt.scatter. The following code produces a scatterplot of 'weight' against\\n'height' for the nutri data.\\nplt.scatter(nutri.height, nutri.weight, s=12, marker= 'o')\\nplt.xlabel( 'height ')\\nplt.ylabel( 'weight ')\\nplt.show()\\n140\\n 150\\n 160\\n 170\\n 180\\n 190\\nheight\\n40\\n50\\n60\\n70\\n80\\n90weight\\nFigure 1.6: Scatterplot of 'weight' against 'height'.\\nThe next Python code illustrates that it is possible to produce highly sophisticated scat-\\nter plots, such as in Figure 1.7. The figure shows the birth weights (mass) of babies whose\\nmothers smoked (blue triangles) or not (red circles). In addition, straight lines were fitted to\\nthe two groups, suggesting that birth weight decreases with age when the mother smokes,\\nbut increases when the mother does not smoke! The question is whether these trends are\\nstatistically significant or due to chance. We will revisit this data set later on in the book. ☞ 200\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 31, 'page_label': '14'}, page_content=\"14 Visualizing Data\\nurlprefix = 'https://vincentarelbundock.github.io/Rdatasets/csv/ '\\ndataname = 'MASS/birthwt.csv '\\nbwt = pd.read_csv(urlprefix + dataname)\\nbwt = bwt.drop( 'Unnamed: 0 ',1) #drop unnamed column\\nstyles = {0: [ 'o','red'], 1: [ '^','blue ']}\\nfor k in styles:\\ngrp = bwt[bwt.smoke==k]\\nm,b = np.polyfit(grp.age, grp.bwt, 1) # fit a straight line\\nplt.scatter(grp.age, grp.bwt, c=styles[k][1], s=15, linewidth=0,\\nmarker = styles[k][0])\\nplt.plot(grp.age, m*grp.age + b, '-', color=styles[k][1])\\nplt.xlabel( 'age')\\nplt.ylabel( 'birth weight (g) ')\\nplt.legend([ 'non-smokers ','smokers '],prop={ 'size ':8},\\nloc=(0.5,0.8))\\nplt.show()\\n10\\n 15\\n 20\\n 25\\n 30\\n 35\\n 40\\n 45\\n 50\\nage\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000birth weight (g)\\nnon-smokers\\nsmokers\\nFigure 1.7: Birth weight against age for smoking and non-smoking mothers.\\n1.5.3.3 Plots for One Qualitative and One Quantitative Variable\\nIn this setting, it is interesting to draw boxplots of the quantitative feature for each level\\nof the categorical feature. Assuming the variables are structured correctly, the function\\nplt.boxplot can be used to produce Figure 1.8, using the following code:\\nmales = nutri[nutri.gender == 'Male ']\\nfemales = nutri[nutri.gender == 'Female ']\\nplt.boxplot([males.coffee,females.coffee],notch=True,widths\\n=(0.5,0.5))\\nplt.xlabel( 'gender ')\\nplt.ylabel( 'coffee ')\\nplt.xticks([1,2],[ 'Male ','Female '])\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 32, 'page_label': '15'}, page_content=\"Importing, Summarizing, and Visualizing Data 15\\nMale\\n Female\\ngender\\n0\\n1\\n2\\n3\\n4\\n5coffee\\nFigure 1.8: Boxplots of a quantitative feature 'coffee' as a function of the levels of a\\ncategorical feature 'gender'. Note that we used a different, “notched”, style boxplot this\\ntime.\\nFurther Reading\\nThe focus in this book is on the mathematical and statistical analysis of data, and for the\\nrest of the book we assume that the data is available in a suitable form for analysis. How-\\never, a large part of practical data science involves the cleaning of data; that is, putting\\nit into a form that is amenable to analysis with standard software packages. Standard Py-\\nthon modules such as numpy and pandas can be used to reformat rows, rename columns,\\nremove faulty outliers, merge rows, and so on. McKinney, the creator of pandas, gives\\nmany practical case studies in [84]. Effective data visualization techniques are beautifully\\nillustrated in [65].\\nExercises\\nBefore you attempt these exercises, make sure you have up-to-date versions of the relevant\\nPython packages, specifically matplotlib, pandas, and seaborn. An easy way to ensure\\nthis is to update packages via the Anaconda Navigator, as explained in Appendix D.\\n1. Visit the UCI Repository https://archive.ics.uci.edu/. Read the description of\\nthe data and download the Mushroom data setagaricus-lepiota.data. Using pandas,\\nread the data into a DataFramecalled mushroom, via read_csv.\\n(a) How many features are in this data set?\\n(b) What are the initial names and types of the features?\\n(c) Rename the first feature (index 0) to 'edibility' and the sixth feature (index 5) to\\n'odor' [Hint: the column names in pandas are immutable; so individual columns\\ncannot be modified directly. However it is possible to assign the entire column names\\nlist via mushroom.columns = newcols. ]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 33, 'page_label': '16'}, page_content=\"16 Exercises\\n(d) The 6th column lists the various odors of the mushrooms: encoded as 'a', 'c', . . . .\\nReplace these with the names 'almond', 'creosote', etc. (categories correspond-\\ning to each letter can be found on the website). Also replace the 'edibility' cat-\\negories 'e' and 'p' with 'edible' and 'poisonous'.\\n(e) Make a contingency table cross-tabulating 'edibility' and 'odor'.\\n(f) Which mushroom odors should be avoided, when gathering mushrooms for consump-\\ntion?\\n(g) What proportion of odorless mushroom samples were safe to eat?\\n2. Change the type and value of variables in the nutri data set according to Table 1.2 and\\nsave the data as a CSV file. The modified data should have eight categorical features, three\\nfloats, and two integer features.\\n3. It frequently happens that a table with data needs to be restructured before the data can\\nbe analyzed using standard statistical software. As an example, consider the test scores in\\nTable 1.3 of 5 students before and after specialized tuition.\\nTable 1.3: Student scores.\\nStudent Before After\\n1 75 85\\n2 30 50\\n3 100 100\\n4 50 52\\n5 60 65\\nThis is not in the standard format described in Section 1.1. In particular, the student scores\\nare divided over two columns, whereas the standard format requires that they are collected\\nin one column, e.g., labelled 'Score'. Reformat (by hand) the table in standard format,\\nusing three features:\\n• 'Score', taking continuous values,\\n• 'Time', taking values 'Before' and 'After',\\n• 'Student', taking values from 1 to 5.\\nUseful methods for reshaping tables in pandas are melt, stack, and unstack.\\n4. Create a similar barplot as in Figure 1.5, but now plot the corresponding proportions of\\nmales and females in each of the three situation categories. That is, the heights of the bars\\nshould sum up to 1 for both barplots with the same ’gender’value. [Hint: seaborn does\\nnot have this functionality built in, instead you need to first create a contingency table and\\nuse matplotlib.pyplot to produce the figure.]\\n5. The iris data set, mentioned in Section 1.1, contains various features, including☞2\\n'Petal.Length' and 'Sepal.Length', of three species of iris: setosa, versicolor, and\\nvirginica.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 34, 'page_label': '17'}, page_content=\"Importing, Summarizing, and Visualizing Data 17\\n(a) Load the data set into a pandas DataFrameobject.\\n(b) Using matplotlib.pyplot, produce boxplots of 'Petal.Length' for each the\\nthree species, in one figure.\\n(c) Make a histogram with 20 bins for 'Petal.Length'.\\n(d) Produce a similar scatterplot for 'Sepal.Length' against 'Petal.Length' to that\\nof the left plot in Figure 1.9. Note that the points should be colored according to the\\n’Species’feature as per the legend in the right plot of the figure.\\n(e) Using the kdeplot method of the seaborn package, reproduce the right plot of\\nFigure 1.9, where kernel density plots for 'Petal.Length' are given. ☞ 131\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\nPetal.Length\\n5\\n6\\n7\\n8Sepal.Length\\n2\\n 4\\n 6\\n 8\\nPetal.Length\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5Density\\nsetosa\\nversicolor\\nvirginica\\nFigure 1.9: Left: scatterplot of 'Sepal.Length' against 'Petal.Length'. Right: kernel\\ndensity estimates of 'Petal.Length' for the three species of iris.\\n6. Import the data set EuStockMarkets from the same website as theiris data set above.\\nThe data set contains the daily closing prices of four European stock indices during the\\n1990s, for 260 working days per year.\\n(a) Create a vector of times (working days) for the stock prices, between 1991.496 and\\n1998.646 with increments of 1/260.\\n(b) Reproduce Figure 1.10. [Hint: Use a dictionary to map column names (stock indices)\\nto colors.]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 35, 'page_label': '18'}, page_content=\"18 Exercises\\n1991\\n 1992\\n 1993\\n 1994\\n 1995\\n 1996\\n 1997\\n 1998\\n 1999\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\n7000\\n8000\\n9000\\nDAX\\nSMI\\nCAC\\nFTSE\\nFigure 1.10: Closing stock indices for various European stock markets.\\n7. Consider the KASANDRdata set from the UCI Machine Learning Repository, which can\\nbe downloaded from\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/00385/de\\n.tar.bz2.\\nThis archive file has a size of 900Mb, so it may take a while to download. Uncompressing\\nthe file (e.g., via 7-Zip) yields a directorydecontaining two large CSV files:test_de.csv\\nand train_de.csv, with sizes 372Mb and 3Gb, respectively. Such large data files can still\\nbe processed e fficiently in pandas, provided there is enough memory. The files contain\\nrecords of user information from Kelkoo web logs in Germany as well as meta-data on\\nusers, offers, and merchants. The data sets have 7 attributes and 1919561 and 15844717\\nrows, respectively. The data sets are anonymized via hex strings.\\n(a) Load train_de.csvinto a pandas DataFrameobject de, using\\nread_csv('train_de.csv',␣delimiter␣=␣'\\\\t').\\nIf not enough memory is available, load test_de.csv instead. Note that entries are\\nseparated here by tabs, not commas. Time how long it takes for the file to load, using\\nthe time package. (It took 38 seconds for train_de.csv to load on one of our\\ncomputers.)\\n(b) How many unique users and merchants are in this data set?\\n8. Visualizing data involving more than two features requires careful design, which is often\\nmore of an art than a science.\\n(a) Go to Vincent Arel-Bundocks’s website (URL given in Section 1.1) and read the\\nOrangedata set into a pandas DataFrame objectcalled orange. Remove its first\\n(unnamed) column.\\n(b) The data set contains the circumferences of 5 orange trees at various stages in their\\ndevelopment. Find the names of the features.\\n(c) In Python, import seaborn and visualize the growth curves (circumference against\\nage) of the trees, using the regplot and FacetGrid methods.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 36, 'page_label': '19'}, page_content='CHAPTER 2\\nSTATISTICAL LEARNING\\nThe purpose of this chapter is to introduce the reader to some common concepts\\nand themes in statistical learning. We discuss the di fference between supervised and\\nunsupervised learning, and how we can assess the predictive performance of supervised\\nlearning. We also examine the central role that the linear and Gaussian properties play\\nin the modeling of data. We conclude with a section on Bayesian learning. The required\\nprobability and statistics background is given in Appendix C.\\n2.1 Introduction\\nAlthough structuring and visualizing data are important aspects of data science, the main\\nchallenge lies in the mathematical analysis of the data. When the goal is to interpret the\\nmodel and quantify the uncertainty in the data, this analysis is usually referred to as stat-\\nistical learning. In contrast, when the emphasis is on making predictions using large-scale statistical\\nlearningdata, then it is common to speak about machine learning or data mining.\\nmachine\\nlearning\\ndata mining\\nThere are two major goals for modeling data: 1) to accurately predict some future\\nquantity of interest, given some observed data, and 2) to discover unusual or interesting\\npatterns in the data. To achieve these goals, one must rely on knowledge from three im-\\nportant pillars of the mathematical sciences.\\nFunction approximation. Building a mathematical model for data usually means under-\\nstanding how one data variable depends on another data variable. The most natural\\nway to represent the relationship between variables is via a mathematical function or\\nmap. We usually assume that this mathematical function is not completely known,\\nbut can be approximated well given enough computing power and data. Thus, data\\nscientists have to understand how best to approximate and represent functions using\\nthe least amount of computer processing and memory.\\nOptimization. Given a class of mathematical models, we wish to find the best possible\\nmodel in that class. This requires some kind of e fficient search or optimization pro-\\ncedure. The optimization step can be viewed as a process of fitting or calibrating\\na function to observed data. This step usually requires knowledge of optimization\\nalgorithms and efficient computer coding or programming.\\n19'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 37, 'page_label': '20'}, page_content='20 Supervised and Unsupervised Learning\\nProbability and Statistics. In general, the data used to fit the model is viewed as a realiz-\\nation of a random process or numerical vector, whose probability law determines the\\naccuracy with which we can predict future observations. Thus, in order to quantify\\nthe uncertainty inherent in making predictions about the future, and the sources of er-\\nror in the model, data scientists need a firm grasp of probability theory and statistical\\ninference.\\n2.2 Supervised and Unsupervised Learning\\nGiven an input or featurefeature vector x, one of the main goals of machine learning is to predict\\nan output or responseresponse variable y. For example, x could be a digitized signature and y a\\nbinary variable that indicates whether the signature is genuine or false. Another example is\\nwhere x represents the weight and smoking habits of an expecting mother and y the birth\\nweight of the baby. The data science attempt at this prediction is encoded in a mathematical\\nfunction g, called theprediction functionprediction\\nfunction\\n, which takes as an inputx and outputs aguess g(x)\\nfor y (denoted by by, for example). In a sense, g encompasses all the information about the\\nrelationship between the variables x and y, excluding the effects of chance and randomness\\nin nature.\\nIn regression problems, the response variable y can take any real value. In contrast,regression\\nwhen y can only lie in a finite set, say y ∈{0,..., c −1}, then predicting y is conceptually\\nthe same as classifying the input x into one of c categories, and so prediction becomes a\\nclassificationclassification problem.\\nWe can measure the accuracy of a prediction by with respect to a given response y by\\nusing some loss functionloss function Loss(y,by). In a regression setting the usual choice is the squared-\\nerror loss (y−by)2. In the case of classification, the zero–one (also written 0–1) loss function\\nLoss(y,by) = 1{y , by}is often used, which incurs a loss of 1 whenever the predicted class\\nby is not equal to the class y. Later on in this book, we will encounter various other useful\\nloss functions, such as the cross-entropy and hinge loss functions (see, e.g., Chapter 7).\\nThe word error is often used as a measure of distance between a “true” object y and\\nsome approximation by thereof. If y is real-valued, the absolute error |y −by|and the\\nsquared error (y−by)2 are both well-established error concepts, as are the norm∥y−by∥\\nand squared norm ∥y −by∥2 for vectors. The squared error (y −by)2 is just one example\\nof a loss function.\\nIt is unlikely that any mathematical functiong will be able to make accurate predictions\\nfor all possible pairs ( x,y) one may encounter in Nature. One reason for this is that, even\\nwith the same input x, the output y may be different, depending on chance circumstances\\nor randomness. For this reason, we adopt a probabilistic approach and assume that each\\npair (x,y) is the outcome of a random pair ( X,Y) that has some joint probability density\\nf (x,y). We then assess the predictive performance via the expected loss, usually called the\\nriskrisk , for g:\\nℓ(g) = ELoss(Y,g(X)). (2.1)\\nFor example, in the classification case with zero–one loss function the risk is equal to the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 38, 'page_label': '21'}, page_content='Statistical Learning 21\\nprobability of incorrect classification: ℓ(g) = P[Y , g(X)]. In this context, the prediction\\nfunction g is called a classifier classifier. Given the distribution of (X,Y) and any loss function, we\\ncan in principle find the best possibleg∗:= argming ELoss(Y,g(X)) that yields the smallest\\nrisk ℓ∗:= ℓ(g∗). We will see in Chapter 7 that in the classification case withy ∈{0,..., c−1} ☞ 251\\nand ℓ(g) = P[Y , g(X)], we have\\ng∗(x) = argmax\\ny∈{0,...,c−1}\\nf (y |x),\\nwhere f (y |x) = P[Y = y |X = x] is the conditional probability of Y = y given X = x.\\nAs already mentioned, for regression the most widely-used loss function is the squared-\\nerror loss. In this setting, the optimal prediction function g∗ is often called the regression\\nfunction. The following theorem specifies its exact form. regression\\nfunction\\nTheorem 2.1: Optimal Prediction Function for Squared-Error Loss\\nFor the squared-error loss Loss(y,by) = (y −by)2, the optimal prediction function g∗is\\nequal to the conditional expectation of Y given X = x:\\ng∗(x) = E[Y |X = x].\\nProof: Let g∗(x) = E[Y |X = x]. For any function g, the squared-error risk satisfies\\nE(Y −g(X))2 = E[(Y −g∗(X) + g∗(X) −g(X))2]\\n= E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))] + E(g∗(X) −g(X))2\\n⩾E(Y −g∗(X))2 + 2E[(Y −g∗(X))(g∗(X) −g(X))]\\n= E(Y −g∗(X))2 + 2E{(g∗(X) −g(X))E[Y −g∗(X) |X]}.\\nIn the last equation we used the tower property. By the definition of the conditional expect- ☞ 431\\nation, we have E[Y −g∗(X) |X] = 0. It follows that E(Y −g(X))2 ⩾E(Y −g∗(X))2, showing\\nthat g∗yields the smallest squared-error risk. □\\nOne consequence of Theorem 2.1 is that, conditional on X = x, the (random) response\\nY can be written as\\nY = g∗(x) + ε(x), (2.2)\\nwhere ε(x) can be viewed as the random deviation of the response from its conditional\\nmean at x. This random deviation satisfies Eε(x) = 0. Further, the conditional variance of\\nthe response Y at x can be written as Var ε(x) = v2(x) for some unknown positive function\\nv. Note that, in general, the probability distribution of ε(x) is unspecified.\\nSince, the optimal prediction functiong∗depends on the typically unknown joint distri-\\nbution of (X,Y), it is not available in practice. Instead, all that we have available is a finite\\nnumber of (usually) independent realizations from the joint density f (x,y). We denote this\\nsample by T = {(X1,Y1),..., (Xn,Yn)}and call it the training set training set(Tis a mnemonic for\\ntraining) with n examples. It will be important to distinguish between a random training\\nset Tand its (deterministic) outcome {(x1,y1),..., (xn,yn)}. We will use the notation τfor\\nthe latter. We will also add the subscriptn in τn when we wish to emphasize the size of the\\ntraining set.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 39, 'page_label': '22'}, page_content='22 Supervised and Unsupervised Learning\\nOur goal is thus to “learn” the unknown g∗using the n examples in the training set T.\\nLet us denote bygT the best (by some criterion) approximation forg∗that we can construct\\nfrom T. Note that gT is a random function. A particular outcome is denoted by gτ. It is\\noften useful to think of a teacher–learner metaphor, whereby the function gT is a learnerlearner\\nwho learns the unknown functional relationship g∗: x 7→y from the training data T. We\\ncan imagine a “teacher” who provides n examples of the true relationship between the\\noutput Yi and the input Xi for i = 1,..., n, and thus “trains” the learner gT to predict the\\noutput of a new input X, for which the correct output Y is not provided by the teacher (is\\nunknown).\\nThe above setting is calledsupervised learningsupervised\\nlearning\\n, because one tries to learn the functional\\nrelationship between the feature vector x and response y in the presence of a teacher who\\nprovides n examples. It is common to speak of “explaining” or predicting y on the basis of\\nx, where x is a vector of explanatory variablesexplanatory\\nvariables\\n.\\nAn example of supervised learning is email spam detection. The goal is to train the\\nlearner gT to accurately predict whether any future email, as represented by the feature\\nvector x, is spam or not. The training data consists of the feature vectors of a number\\nof different email examples as well as the corresponding labels (spam or not spam). For\\ninstance, a feature vector could consist of the number of times sales-pitch words like “free”,\\n“sale”, or “miss out” occur within a given email.\\nAs seen from the above discussion, most questions of interest in supervised learning\\ncan be answered if we know the conditional pdf f (y |x), because we can then in principle\\nwork out the function value g∗(x).\\nIn contrast, unsupervised learningunsupervised\\nlearning\\nmakes no distinction between response and explan-\\natory variables, and the objective is simply to learn the structure of the unknown distribu-\\ntion of the data. In other words, we need to learn f (x). In this case the guess g(x) is an\\napproximation of f (x) and the risk is of the form\\nℓ(g) = ELoss( f (X),g(X)).\\nAn example of unsupervised learning is when we wish to analyze the purchasing be-\\nhaviors of the customers of a grocery shop that has a total of, say, a hundred items on sale.\\nA feature vector here could be a binary vector x ∈{0,1}100 representing the items bought\\nby a customer on a visit to the shop (a 1 in the k-th position if a customer bought item\\nk ∈{1,..., 100}and a 0 otherwise). Based on a training set τ = {x1,..., xn}, we wish to\\nfind any interesting or unusual purchasing patterns. In general, it is di fficult to know if an\\nunsupervised learner is doing a good job, because there is no teacher to provide examples\\nof accurate predictions.\\nThe main methodologies for unsupervised learning include clustering, principal com-\\nponent analysis, and kernel density estimation, which will be discussed in Chapter 4.☞121\\nIn the next three sections we will focus on supervised learning. The main super-\\nvised learning methodologies are regression and classification, to be discussed in detail in\\nChapters 5 and 7. More advanced supervised learning techniques, including reproducing☞167\\n☞251 kernel Hilbert spaces, tree methods, and deep learning, will be discussed in Chapters 6, 8,\\nand 9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 40, 'page_label': '23'}, page_content='Statistical Learning 23\\n2.3 Training and Test Loss\\nGiven an arbitrary prediction function g, it is typically not possible to compute its riskℓ(g)\\nin (2.1). However, using the training sample T, we can approximate ℓ(g) via the empirical\\n(sample average) risk\\nℓT(g) = 1\\nn\\nnX\\ni=1\\nLoss(Yi,g(Xi)), (2.3)\\nwhich we call the training loss training loss. The training loss is thus an unbiased estimator of the risk\\n(the expected loss) for a prediction function g, based on the training data.\\nTo approximate the optimal prediction function g∗ (the minimizer of the risk ℓ(g)) we\\nfirst select a suitable collection of approximating functions Gand then take our learner to\\nbe the function in Gthat minimizes the training loss; that is,\\ngG\\nT = argmin\\ng∈G\\nℓT(g). (2.4)\\nFor example, the simplest and most useful Gis the set of linear functions of x; that is, the\\nset of all functions g : x 7→β⊤x for some real-valued vector β.\\nWe suppress the superscript Gwhen it is clear which function class is used. Note that\\nminimizing the training loss over all possible functions g (rather than over all g ∈G) does\\nnot lead to a meaningful optimization problem, as any function g for which g(Xi) = Yi for\\nall i gives minimal training loss. In particular, for a squared-error loss, the training loss will\\nbe 0. Unfortunately, such functions have a poor ability to predict new (that is, independent\\nfrom T) pairs of data. This poor generalization performance is called overfitting overfitting.\\nBy choosing g a function that predicts the training data exactly (and is, for example,\\n0 otherwise), the squared-error training loss is zero. Minimizing the training loss is\\nnot the ultimate goal!\\nThe prediction accuracy of new pairs of data is measured by the generalization risk generalization\\nrisk\\nof\\nthe learner. For a fixed training set τit is defined as\\nℓ(gG\\nτ) = ELoss(Y,gG\\nτ(X)), (2.5)\\nwhere (X,Y) is distributed according to f (x,y). In the discrete case the generalization risk\\nis therefore: ℓ(gG\\nτ) = P\\nx,y Loss(y,gG\\nτ(x)) f (x,y) (replace the sum with an integral for the\\ncontinuous case). The situation is illustrated in Figure 2.1, where the distribution of (X,Y)\\nis indicated by the red dots. The training set (points in the shaded regions) determines a\\nfixed prediction function shown as a straight line. Three possible outcomes of ( X,Y) are\\nshown (black dots). The amount of loss for each point is shown as the length of the dashed\\nlines. The generalization risk is the average loss over all possible pairs (x,y), weighted by\\nthe corresponding f (x,y).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 41, 'page_label': '24'}, page_content='24 Training and Test Loss\\nxx x\\ny\\ny\\ny\\nFigure 2.1: The generalization risk for a fixed training set is the weighted-average loss over\\nall possible pairs (x,y).\\nFor a random training set T, the generalization risk is thus a random variable that\\ndepends on T(and G). If we average the generalization risk over all possible instances of\\nT, we obtain the expected generalization riskexpected\\ngeneralization\\nrisk\\n:\\nEℓ(gG\\nT) = ELoss(Y,gG\\nT(X)), (2.6)\\nwhere (X,Y) in the expectation above is independent of T. In the discrete case, we have\\nEℓ(gG\\nT) = P\\nx,y,x1,y1,...,xn,yn Loss(y,gG\\nτ(x)) f (x,y) f (x1,y1) ··· f (xn,yn). Figure 2.2 gives an il-\\nlustration.\\ny\\nx\\ny\\ny\\nxx\\nFigure 2.2: The expected generalization risk is the weighted-average loss over all possible\\npairs (x,y) and over all training sets.\\nFor any outcome τof the training data, we can estimate the generalization risk without\\nbias by taking the sample average\\nℓT′(gG\\nτ) := 1\\nn′\\nn′\\nX\\ni=1\\nLoss(Y′\\ni ,gG\\nτ(X′\\ni )), (2.7)\\nwhere {(X′\\n1,Y′\\n1),..., (X′\\nn′,Y′\\nn′)}=: T′ is a so-called test sampletest sample . The test sample is com-\\npletely separate from T, but is drawn in the same way asT; that is, via independent draws\\nfrom f (x,y), for some sample size n′. We call the estimator (2.7) the test losstest loss . For a ran-\\ndom training set Twe can define ℓT′(gG\\nT) similarly. It is then crucial to assume that Tis\\nindependent of T′. Table 2.1 summarizes the main definitions and notation for supervised\\nlearning.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 42, 'page_label': '25'}, page_content='Statistical Learning 25\\nTable 2.1: Summary of definitions for supervised learning.\\nx Fixed explanatory (feature) vector.\\nX Random explanatory (feature) vector.\\ny Fixed (real-valued) response.\\nY Random response.\\nf (x,y) Joint pdf of X and Y, evaluated at (x,y).\\nf (y |x) Conditional pdf of Y given X = x, evaluated at y.\\nτor τn Fixed training data {(xi,yi),i = 1,..., n}.\\nTor Tn Random training data {(Xi,Yi),i = 1,..., n}.\\nX Matrix of explanatory variables, with n rows x⊤\\ni ,i = 1,..., n\\nand dim( x) feature columns; one of the features may be the\\nconstant 1.\\ny Vector of response variables (y1,..., yn)⊤.\\ng Prediction (guess) function.\\nLoss(y,by) Loss incurred when predicting response y with by.\\nℓ(g) Risk for prediction function g; that is, ELoss(Y,g(X)).\\ng∗ Optimal prediction function; that is, argming ℓ(g).\\ngG Optimal prediction function in function class G; that is,\\nargming∈Gℓ(g).\\nℓτ(g) Training loss for prediction function g; that is, the sample av-\\nerage estimate of ℓ(g) based on a fixed training sample τ.\\nℓT(g) The same as ℓτ(g), but now for a random training sample T.\\ngG\\nτ or gτ The learner: argmin g∈Gℓτ(g). That is, the optimal prediction\\nfunction based on a fixed training set τ and function class G.\\nWe suppress the superscript Gif the function class is implicit.\\ngG\\nT or gT The learner, where we have replaced τwith a random training\\nset T.\\nTo compare the predictive performance of various learners in the function class G, as\\nmeasured by the test loss, we can use the same fixed training set τ and test set τ′ for all\\nlearners. When there is an abundance of data, the “overall” data set is usually (randomly)\\ndivided into a training and test set, as depicted in Figure 2.3. We then use the training data\\nto construct various learners gG1\\nτ ,gG2\\nτ ,... , and use the test data to select the best (with the\\nsmallest test loss) among these learners. In this context the test set is called the validation\\nset validation set. Once the best learner has been chosen, a third “test” set can be used to assess the\\npredictive performance of the best learner. The training, validation, and test sets can again\\nbe obtained from the overall data set via a random allocation. When the overall data set\\nis of modest size, it is customary to perform the validation phase (model selection) on the\\ntraining set only, using cross-validation. This is the topic of Section 2.5.2. ☞ 38'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 43, 'page_label': '26'}, page_content='26 Training and Test Loss\\n\\x01\\x02\\x03\\x04\\x05\\x04\\x05\\x06\\n\\x01\\x07\\x08\\t\\n\\n\\x0b\\x0c\\r\\x03\\x05\\x03\\t\\x0e\\x02\\x0f\\x10\\x07\\x08\\x0c\\x0e\\x05\\x08\\x07 \\n\\x0b\\x0c\\r\\x03\\x05\\x03\\t\\x0e\\x02\\x0f\\x10\\x07\\x08\\x0c\\x0e\\x05\\x08\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x04\\x05\\x06\\n\\x11\\x03\\r\\x04\\x12\\x03\\t\\x04\\x0e\\x05\\n\\x01\\x07\\x08\\t\\nFigure 2.3: Statistical learning algorithms often require the data to be divided into training\\nand test data. If the latter is used for model selection, a third set is needed for testing the\\nperformance of the selected model.\\nWe next consider a concrete example that illustrates the concepts introduced so far.\\nExample 2.1 (Polynomial Regression) In what follows, it will appear that we have ar-\\nbitrarily replaced the symbols x,g,Gwith u,h,H, respectively. The reason for this switch\\nof notation will become clear at the end of the example.\\nThe data (depicted as dots) in Figure 2.4 are n = 100 points (ui,yi),i = 1,..., n drawn\\nfrom iid random points ( Ui,Yi),i = 1,..., n, where the {Ui}are uniformly distributed on\\nthe interval (0,1) and, given Ui = ui, the random variable Yi has a normal distribution with\\nexpectation 10 −140ui + 400u2\\ni −250u3\\ni and variance ℓ∗ = 25. This is an example of a\\npolynomial regression modelpolynomial\\nregression\\nmodel\\n. Using a squared-error loss, the optimal prediction function\\nh∗(u) = E[Y |U = u] is thus\\nh∗(u) = 10 −140u + 400u2 −250u3,\\nwhich is depicted by the dashed curve in Figure 2.4.\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\nu\\n10\\n0\\n10\\n20\\n30\\n40\\nh * (u)\\ndata points\\ntrue\\nFigure 2.4: Training data and the optimal polynomial prediction function h∗.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 44, 'page_label': '27'}, page_content='Statistical Learning 27\\nTo obtain a good estimate of h∗(u) based on the training set τ = {(ui,yi),i = 1,..., n},\\nwe minimize the outcome of the training loss (2.3):\\nℓτ(h) = 1\\nn\\nnX\\ni=1\\n(yi −h(ui))2, (2.8)\\nover a suitable setHof candidate functions. Let us take the setHp of polynomial functions\\nin u of order p −1:\\nh(u) := β1 + β2u + β3u2 + ··· + βpup−1 (2.9)\\nfor p = 1,2,... and parameter vector β= [β1,β2,...,β p]⊤. This function class contains the\\nbest possible h∗(u) = E[Y |U = u] for p ⩾4. Note that optimization overHp is a parametric\\noptimization problem, in that we need to find the best β. Optimization of (2.8) over Hp is\\nnot straightforward, unless we notice that (2.9) is alinear function in β. In particular, if we\\nmap each feature u to a feature vector x = [1,u,u2,..., up−1]⊤, then the right-hand side of\\n(2.9) can be written as the function\\ng(x) = x⊤β,\\nwhich is linear in x (as well as β). The optimal h∗(u) in Hp for p ⩾4 then corresponds\\nto the function g∗(x) = x⊤β∗ in the set Gp of linear functions from Rp to R, where β∗ =\\n[10,−140,400,−250,0,..., 0]⊤. Thus, instead of working with the set Hp of polynomial\\nfunctions we may prefer to work with the set Gp of linear functions. This brings us to a\\nvery important idea in statistical learning:\\nExpand the feature space to obtain a linear prediction function.\\nLet us now reformulate the learning problem in terms of the new explanatory (feature)\\nvariables xi = [1,ui,u2\\ni ,..., up−1\\ni ]⊤, i = 1,..., n. It will be convenient to arrange these\\nfeature vectors into a matrix X with rows x⊤\\n1 ,..., x⊤\\nn :\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 u1 u2\\n1 ··· up−1\\n1\\n1 u2 u2\\n2 ··· up−1\\n2\\n... ... ... ... ...\\n1 un u2\\nn ··· up−1\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (2.10)\\nCollecting the responses {yi}into a column vector y, the training loss (2.3) can now be\\nwritten compactly as\\n1\\nn ∥y −Xβ∥2. (2.11)\\nTo find the optimal learner (2.4) in the class Gp we need to find the minimizer of (2.11):\\nbβ= argmin\\nβ\\n∥y −Xβ∥2, (2.12)\\nwhich is called theordinary least-squares ordinary\\nleast-squares\\nsolution. As is illustrated in Figure 2.5, to findbβ,\\nwe choose Xbβto be equal to the orthogonal projection of y onto the linear space spanned\\nby the columns of the matrix X; that is, Xbβ= Py, where P is the projection matrix projection\\nmatrix\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 45, 'page_label': '28'}, page_content='28 Training and Test Loss\\nSpan(X)\\nXβ\\nXˆβ\\ny\\nFigure 2.5: Xbβ is the orthogonal projection of y onto the linear space spanned by the\\ncolumns of the matrix X.\\nAccording to Theorem A.4, the projection matrix is given by☞362\\nP = X X+, (2.13)\\nwhere the p ×n matrix X+ in (2.13) is the pseudo-inverse of X. If X happens to be of full☞360\\npseudo-inverse column rank (so that none of the columns can be expressed as a linear combination of the\\n☞356 other columns), then X+ = (X⊤X)−1X⊤.\\nIn any case, from Xbβ = Py and PX = X, we can see that bβ satisfies the normal\\nequationsnormal\\nequations\\n:\\nX⊤Xβ= X⊤Py = (PX)⊤y = X⊤y. (2.14)\\nThis is a set of linear equations, which can be solved very fast and whose solution can be\\nwritten explicitly as:\\nbβ= X+y. (2.15)\\nFigure 2.6 shows the trained learners for various values of p:\\nh\\nHp\\nτ (u) = g\\nGp\\nτ (x) = x⊤bβ\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\nu\\n10\\n0\\n10\\n20\\n30\\n40\\nh p(u)\\ndata points\\ntrue\\np = 2, underfit\\np = 4, correct\\np = 16, overfit\\nFigure 2.6: Training data with fitted curves forp = 2,4, and 16. The true cubic polynomial\\ncurve for p = 4 is also plotted (dashed line).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 46, 'page_label': '29'}, page_content='Statistical Learning 29\\nWe see that for p = 16 the fitted curve lies closer to the data points, but is further away\\nfrom the dashed true polynomial curve, indicating that we overfit. The choice p = 4 (the\\ntrue cubic polynomial) is much better than p = 16, or indeed p = 2 (straight line).\\nEach function class Gp gives a different learner g\\nGp\\nτ , p = 1,2,... . To assess which is\\nbetter, we should not simply take the one that gives the smallest training loss. We can\\nalways get a zero training loss by taking p = n, because for any set of n points there exists\\na polynomial of degree n −1 that interpolates all points!\\nInstead, we assess the predictive performance of the learners using the test loss (2.7),\\ncomputed from a test data set. If we collect all n′ test feature vectors in a matrix X′ and\\nthe corresponding test responses in a vector y′, then, similar to (2.11), the test loss can be\\nwritten compactly as\\nℓτ′(g\\nGp\\nτ ) = 1\\nn′∥y′−X′bβ∥2,\\nwhere bβis given by (2.15), using the training data.\\nFigure 2.7 shows a plot of the test loss against the number of parameters in the vector\\nβ; that is, p. The graph has a characteristic “bath-tub” shape and is at its lowest for p = 4,\\ncorrectly identifying the polynomial order 3 for the true model. Note that the test loss, as\\nan estimate for the generalization risk (2.7), becomes numerically unreliable after p = 16\\n(the graph goes down, where it should go up). The reader may check that the graph for\\nthe training loss exhibits a similar numerical instability for large p, and in fact fails to\\nnumerically decrease to 0 for largep, contrary to what it should do in theory. The numerical\\nproblems arise from the fact that for large p the columns of the (Vandermonde) matrix X\\nare of vastly different magnitudes and so floating point errors quickly become very large.\\nFinally, observe that the lower bound for the test loss is here around 21, which corres-\\nponds to an estimate of the minimal (squared-error) risk ℓ∗= 25.\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n 11\\n 12\\n 13\\n 14\\n 15\\n 16\\n 17\\n 18\\nNumber of parameters p\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160Test loss\\nFigure 2.7: Test loss as function of the number of parameters p of the model.\\nThis script shows how the training data were generated and plotted in Python:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 47, 'page_label': '30'}, page_content='30 Training and Test Loss\\npolyreg1.py\\nimport numpy as np\\nfrom numpy.random import rand , randn\\nfrom numpy.linalg import norm , solve\\nimport matplotlib.pyplot as plt\\ndef generate_data(beta , sig, n):\\nu = np.random.rand(n, 1)\\ny = (u ** np.arange(0, 4)) @ beta + sig * np.random.randn(n, 1)\\nreturn u, y\\nnp.random.seed(12)\\nbeta = np.array([[10, -140, 400, -250]]).T\\nn = 100\\nsig = 5\\nu, y = generate_data(beta , sig, n)\\nxx = np.arange(np. min (u), np. max (u)+5e-3, 5e-3)\\nyy = np.polyval(np.flip(beta), xx)\\nplt.plot(u, y, \\'.\\', markersize=8)\\nplt.plot(xx, yy, \\'--\\',linewidth=3)\\nplt.xlabel(r \\'$u$\\')\\nplt.ylabel(r \\'$h^*(u)$ \\')\\nplt.legend([ \\'data points \\',\\'true \\'])\\nplt.show()\\nThe following code, which imports the code above, fits polynomial models with p =\\n1,..., K = 18 parameters to the training data and plots a selection of fitted curves, as\\nshown in Figure 2.6.\\npolyreg2.py\\nfrom polyreg1 import *\\nmax_p = 18\\np_range = np.arange(1, max_p + 1, 1)\\nX = np.ones((n, 1))\\nbetahat, trainloss = {}, {}\\nfor p in p_range: # p is the number of parameters\\nif p > 1:\\nX = np.hstack((X, u**(p-1))) # add column to matrix\\nbetahat[p] = solve(X.T @ X, X.T @ y)\\ntrainloss[p] = (norm(y - X @ betahat[p])**2/n)\\np = [2, 4, 16] # select three curves\\n#replot the points and true line and store in the list \"plots\"\\nplots = [plt.plot(u, y, \\'k.\\', markersize=8)[0],\\nplt.plot(xx, yy, \\'k--\\',linewidth=3)[0]]\\n# add the three curves\\nfor i in p:\\nyy = np.polyval(np.flip(betahat[i]), xx)\\nplots.append(plt.plot(xx, yy)[0])'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 48, 'page_label': '31'}, page_content=\"Statistical Learning 31\\nplt.xlabel(r '$u$')\\nplt.ylabel(r '$h^{\\\\mathcal{H}_p}_{\\\\tau}(u)$ ')\\nplt.legend(plots,( 'data points ', 'true ','$p=2$, underfit ',\\n'$p=4$, correct ','$p=16$, overfit '))\\nplt.savefig( 'polyfitpy.pdf ',format ='pdf')\\nplt.show()\\nThe last code snippet which imports the previous code, generates the test data and plots the\\ngraph of the test loss, as shown in Figure 2.7.\\npolyreg3.py\\nfrom polyreg2 import *\\n# generate test data\\nu_test, y_test = generate_data(beta, sig, n)\\nMSE = []\\nX_test = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX_test = np.hstack((X_test, u_test**(p-1)))\\ny_hat = X_test @ betahat[p] # predictions\\nMSE.append(np. sum ((y_test - y_hat)**2/n))\\nplt.plot(p_range, MSE, 'b', p_range, MSE, 'bo')\\nplt.xticks(ticks=p_range)\\nplt.xlabel( 'Number of parameters $p$ ')\\nplt.ylabel( 'Test loss ')\\n2.4 Tradeoffs in Statistical Learning\\nThe art of machine learning in the supervised case is to make the generalization risk (2.5)\\nor expected generalization risk (2.6) as small as possible, while using as few computational\\nresources as possible. In pursuing this goal, a suitable class Gof prediction functions has\\nto be chosen. This choice is driven by various factors, such as\\n• the complexity of the class (e.g., is it rich enough to adequately approximate, or even\\ncontain, the optimal prediction function g∗?),\\n• the ease of training the learner via the optimization program (2.4),\\n• how accurately the training loss (2.3) estimates the risk (2.1) within class G,\\n• the feature types (categorical, continuous, etc.).\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 49, 'page_label': '32'}, page_content='32 Tradeoffs in Statistical Learning\\nAs a result, the choice of a suitable function class Gusually involves a tradeoff between\\nconflicting factors. For example, a learner from a simple class Gcan be trained very\\nquickly, but may not approximate g∗ very well, whereas a learner from a rich class G\\nthat contains g∗may require a lot of computing resources to train.\\nTo better understand the relation between model complexity, computational simplicity,\\nand estimation accuracy, it is useful to decompose the generalization risk into several parts,\\nso that the tradeoffs between these parts can be studied. We will consider two such decom-\\npositions: the approximation–estimation tradeoff and the bias–variance tradeoff.\\nWe can decompose the generalization risk (2.5) into the following three components:\\nℓ(gG\\nτ) = ℓ∗\\n|{z}\\nirreducible risk\\n+ ℓ(gG) −ℓ∗\\n|     {z     }\\napproximation error\\n+ ℓ(gG\\nτ) −ℓ(gG)|          {z          }\\nstatistical error\\n, (2.16)\\nwhere ℓ∗:= ℓ(g∗) is the irreducible riskirreducible risk and gG:= argming∈Gℓ(g) is the best learner within\\nclass G. No learner can predict a new response with a smaller risk than ℓ∗.\\nThe second component is the approximation errorapproximation\\nerror\\n; it measures the difference between\\nthe irreducible risk and the best possible risk that can be obtained by selecting the best\\nprediction function in the selected class of functionsG. Determining a suitable class Gand\\nminimizing ℓ(g) over this class is purely a problem of numerical and functional analysis,\\nas the training data τare not present. For a fixedGthat does not contain the optimal g∗, the\\napproximation error cannot be made arbitrarily small and may be the dominant component\\nin the generalization risk. The only way to reduce the approximation error is by expanding\\nthe class Gto include a larger set of possible functions.\\nThe third component is the statistical (estimation) errorstatistical\\n(estimation)\\nerror\\n. It depends on the training\\nset τand, in particular, on how well the learner gG\\nτ estimates the best possible prediction\\nfunction, gG, within class G. For any sensible estimator this error should decay to zero (in\\nprobability or expectation) as the training size tends to infinity.☞439\\nThe approximation–estimation tradeo ffapproximation–\\nestimation\\ntradeoff\\npits two competing demands against each\\nother. The first is that the class Ghas to be simple enough so that the statistical error is\\nnot too large. The second is that the classGhas to be rich enough to ensure a small approx-\\nimation error. Thus, there is a tradeoff between the approximation and estimation errors.\\nFor the special case of the squared-error loss, the generalization risk is equal toℓ(gG\\nτ) =\\nE(Y −gG\\nτ(X))2; that is, the expected squared error 1 between the predicted value gG\\nτ(X)\\nand the response Y. Recall that in this case the optimal prediction function is given by\\ng∗(x) = E[Y |X = x]. The decomposition (2.16) can now be interpreted as follows.\\n1. The first component, ℓ∗ = E(Y −g∗(X))2, is the irreducible error, as no prediction\\nfunction will yield a smaller expected squared error.\\n2. The second component, the approximation error ℓ(gG) −ℓ(g∗), is equal to E(gG(X) −\\ng∗(X))2. We leave the proof (which is similar to that of Theorem 2.1) as an exercise;\\nsee Exercise 2. Thus, the approximation error (defined as a risk difference) can here\\nbe interpreted as the expected squared error between the optimal predicted value and\\nthe optimal predicted value within the class G.\\n3. For the third component, the statistical error, ℓ(gG\\nτ) −ℓ(gG) there is no direct inter-\\npretation as an expected squared error unless Gis the class of linear functions; that\\n1Colloquially called mean squared error.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 50, 'page_label': '33'}, page_content='Statistical Learning 33\\nis, g(x) = x⊤β for some vector β. In this case we can write (see Exercise 3) the\\nstatistical error as ℓ(gG\\nτ) −ℓ(gG) = E(gG\\nτ(X) −gG(X))2.\\nThus, when using a squared-error loss, the generalization risk for a linear class Gcan\\nbe decomposed as:\\nℓ(gG\\nτ) = E(gG\\nτ(X) −Y)2 = ℓ∗+ E(gG(X) −g∗(X))2\\n|                 {z                 }\\napproximation error\\n+ E(gG\\nτ(X) −gG(X))2\\n|                  {z                  }\\nstatistical error\\n. (2.17)\\nNote that in this decomposition the statistical error is the only term that depends on the\\ntraining set.\\nExample 2.2 (Polynomial Regression (cont.)) We continue Example 2.1. Here G =\\nGp is the class of linear functions of x = [1,u,u2,..., up−1]⊤, and g∗(x) = x⊤β∗. Condi-\\ntional on X = x we have that Y = g∗(x) + ε(x), with ε(x) ∼N(0,ℓ∗), where ℓ∗ = E(Y −\\ng∗(X))2 = 25 is the irreducible error. We wish to understand how the approximation and\\nstatistical errors behave as we change the complexity parameter p.\\nFirst, we consider the approximation error. Any function g ∈Gp can be written as\\ng(x) = h(u) = β1 + β2u + ··· + βpup−1 = [1,u,..., up−1] β,\\nand so g(X) is distributed as [1 ,U,..., Up−1]β, where U ∼ U(0,1). Similarly, g∗(X) is\\ndistributed as [1 ,U,U2,U3]β∗. It follows that an expression for the approximation error\\nis:\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x112\\ndu.To minimize this error, we set the gradient\\nwith respect to βto zero and obtain the p linear equations ☞ 397\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x11\\ndu = 0,\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3 ]β∗\\x11\\nu du = 0,\\n...\\nR 1\\n0\\n\\x10\\n[1,u,..., up−1] β−[1,u,u2,u3] β∗\\x11\\nup−1du = 0.\\nLet\\nHp =\\nZ 1\\n0\\n[1,u,..., up−1]⊤[1,u,..., up−1] du\\nbe the p ×p Hilbert matrix Hilbert matrix, which has (i, j)-th entry given by\\nR 1\\n0 ui+ j−2 du = 1/(i + j −1).\\nThen, the above system of linear equations can be written as Hpβ = eHβ∗, where eH is the\\np ×4 upper left sub-block of Hep and ep = max{p,4}. The solution, which we denote by βp,\\nis:\\nβp =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n65\\n6 , p = 1,\\n[−20\\n3 ,35]⊤, p = 2,\\n[−5\\n2 ,10,25]⊤, p = 3,\\n[10,−140,400,−250,0,..., 0]⊤, p ⩾4.\\n(2.18)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 51, 'page_label': '34'}, page_content='34 Tradeoffs in Statistical Learning\\nHence, the approximation error E\\n\\x10\\ngGp (X) −g∗(X)\\n\\x112\\nis given by\\nZ 1\\n0\\n\\x10\\n[1,u,..., up−1] βp −[1,u,u2,u3] β∗\\x112\\ndu =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n32225\\n252 ≈127.9, p = 1,\\n1625\\n63 ≈25.8, p = 2,\\n625\\n28 ≈22.3, p = 3,\\n0, p ⩾4.\\n(2.19)\\nNotice how the approximation error becomes smaller as p increases. In this particular\\nexample the approximation error is in fact zero for p ⩾4. In general, as the class of ap-\\nproximating functions Gbecomes more complex, the approximation error goes down.\\nNext, we illustrate the typical behavior of the statistical error. Since gτ(x) = x⊤bβ, the\\nstatistical error can be written as\\nZ 1\\n0\\n\\x10\\n[1,..., up−1](bβ−βp)\\n\\x112\\ndu = (bβ−βp)⊤Hp(bβ−βp). (2.20)\\nFigure 2.8 illustrates the decomposition (2.17) of the generalization risk for thesame train-\\ning set that was used to compute the test loss in Figure 2.7. Recall that test loss gives an\\nestimate of the generalization risk, using independent test data. Comparing the two figures,\\nwe see that in this case the two match closely. The global minimum of the statistical error is\\napproximately 0.28, with minimizer p = 4. Since the approximation error is monotonically\\ndecreasing to zero, p = 4 is also the global minimizer of the generalization risk.\\n0 2 4 6 8 10 12 14 16 18\\n0\\n50\\n100\\n150 approximation error\\nstatistical error\\nirreducible error\\ngeneralization risk\\nFigure 2.8: The generalization risk for a particular training set is the sum of the irreducible\\nerror, the approximation error, and the statistical error. The approximation error decreases\\nto zero as p increases, whereas the statistical error has a tendency to increase after p = 4.\\nNote that the statistical error depends on the estimate bβ, which in its turn depends on\\nthe training set τ. We can obtain a better understanding of the statistical error by consid-\\nering its expected behavior; that is, averaged over many training sets. This is explored in\\nExercise 11.\\nUsing again a squared-error loss, a second decomposition (for general G) starts from\\nℓ(gG\\nτ) = ℓ∗+ ℓ(gG\\nτ) −ℓ(g∗),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 52, 'page_label': '35'}, page_content='Statistical Learning 35\\nwhere the statistical error and approximation error are combined. Using similar reasoning\\nas in the proof of Theorem 2.1, we have\\nℓ(gG\\nτ) = E(gG\\nτ(X) −Y)2 = ℓ∗+ E\\n\\x10\\ngG\\nτ(X) −g∗(X)\\n\\x112\\n= ℓ∗+ ED2(X,τ),\\nwhere D(x,τ) := gG\\nτ(x) −g∗(x). Now consider the random variable D(x,T) for a random\\ntraining set T. The expectation of its square is:\\nE\\n\\x10\\ngG\\nT(x) −g∗(x)\\n\\x112\\n= ED2(x,T) = (ED(x,T))2 + Var D(x,T)\\n= (EgG\\nT(x) −g∗(x))2\\n|                 {z                 }\\npointwise squared bias\\n+ Var gG\\nT(x)|     {z     }\\npointwise variance\\n. (2.21)\\nIf we view the learner gG\\nT(x) as a function of a random training set, then the pointwise\\nsquared bias pointwise\\nsquared bias\\nterm is a measure for how close gG\\nT(x) is on average to the true g∗(x),\\nwhereas the pointwise variance term measures the deviation of gG\\nT(x) from its expected pointwise\\nvariancevalue EgG\\nT(x). The squared bias can be reduced by making the class of functions Gmore\\ncomplex. However, decreasing the bias by increasing the complexity often leads to an in-\\ncrease in the variance term. We are thus seeking learners that provide an optimal balance\\nbetween the bias and variance, as expressed via a minimal generalization risk. This is called\\nthe bias–variance tradeoff bias–variance\\ntradeoff\\n.\\nNote that the expected generalization risk (2.6) can be written asℓ∗+ED2(X,T), where\\nX and Tare independent. It therefore decomposes as\\nEℓ(gG\\nT) = ℓ∗+ E(E[gG\\nT(X) |X] −g∗(X))2\\n|                           {z                           }\\nexpected squared bias\\n+ E[Var[gG\\nT(X) |X]]|                 {z                 }\\nexpected variance\\n. (2.22)\\n2.5 Estimating Risk\\nThe most straightforward way to quantify the generalization risk (2.5) is to estimate it via\\nthe test loss (2.7). However, the generalization risk depends inherently on the training set,\\nand so different training sets may yield significantly di fferent estimates. Moreover, when\\nthere is a limited amount of data available, reserving a substantial proportion of the data\\nfor testing rather than training may be uneconomical. In this section we consider di fferent\\nmethods for estimating risk measures which aim to circumvent these difficulties.\\n2.5.1 In-Sample Risk\\nWe mentioned that, due to the phenomenon of overfitting, the training loss of the learner,\\nℓτ(gτ) (for simplicity, here we omitGfrom gG\\nτ), is not a good estimate of the generalization\\nrisk ℓ(gτ) of the learner. One reason for this is that we use the same data for both training\\nthe model and assessing its risk. How should we then estimate the generalization risk or\\nexpected generalization risk?\\nTo simplify the analysis, suppose that we wish to estimate the average accuracy of the\\npredictions of the learner gτ at the n feature vectors x1,..., xn (these are part of the training'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 53, 'page_label': '36'}, page_content='36 Estimating Risk\\nset τ). In other words, we wish to estimate the in-sample riskin-sample risk of the learner gτ:\\nℓin(gτ) = 1\\nn\\nnX\\ni=1\\nELoss(Y′\\ni ,gτ(xi)), (2.23)\\nwhere each response Y′\\ni is drawn from f (y |xi), independently. Even in this simplified set-\\nting, the training loss of the learner will be a poor estimate of the in-sample risk. Instead, the\\nproper way to assess the prediction accuracy of the learner at the feature vectorsx1,..., xn,\\nis to draw new response values Y′\\ni ∼ f (y |xi), i = 1,..., n, that are independent from the\\nresponses y1,..., yn in the training data, and then estimate the in-sample risk of gτ via\\n1\\nn\\nnX\\ni=1\\nLoss(Y′\\ni ,gτ(xi)).\\nFor a fixed training set τ, we can compare the training loss of the learner with the\\nin-sample risk. Their difference,\\nopτ = ℓin(gτ) −ℓτ(gτ),\\nis called the optimism (of the training loss), because it measures how much the training\\nloss underestimates (is optimistic about) the unknown in-sample risk. Mathematically, it is\\nsimpler to work with the expected optimismexpected\\noptimism\\n:\\nE[opT|X1 = x1,..., Xn = xn] =: EX opT,\\nwhere the expectation is taken over a random training set T, conditional on Xi = xi,\\ni = 1,..., n. For ease of notation, we have abbreviated the expected optimism to EX opT,\\nwhere EX denotes the expectation operator conditional on Xi = xi,i = 1,..., n. As in Ex-\\nample 2.1, the feature vectors are stored as the rows of ann×p matrix X. It turns out that the\\nexpected optimism for various loss functions can be expressed in terms of the (conditional)\\ncovariance between the observed and predicted response.\\nTheorem 2.2: Expected Optimism\\nFor the squared-error loss and 0–1 loss with 0–1 response, the expected optimism is\\nEX opT = 2\\nn\\nnX\\ni=1\\nCovX(gT(xi),Yi). (2.24)\\nProof: In what follows, all expectations are taken conditional on X1 = x1,..., Xn = xn.\\nLet Yi be the response for xi and let bYi = gT(xi) be the predicted value. Note that the latter\\ndepends on Y1,..., Yn. Also, let Y′\\ni be an independent copy of Yi for the same xi, as in\\n(2.23). In particular, Y′\\ni has the same distribution as Yi and is statistically independent of\\nall {Yj}, including Yi, and therefore is also independent of bYi. We have\\nEX opT = 1\\nn\\nnX\\ni=1\\nEX\\nh\\n(Y′\\ni −bYi)2 −(Yi −bYi)2i\\n= 2\\nn\\nnX\\ni=1\\nEX\\nh\\n(Yi −Y′\\ni )bYi\\ni\\n= 2\\nn\\nnX\\ni=1\\n\\x10\\nEX[YibYi] −EXYi EXbYi\\n\\x11\\n= 2\\nn\\nnX\\ni=1\\nCovX(bYi,Yi).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 54, 'page_label': '37'}, page_content='Statistical Learning 37\\nThe proof for the 0–1 loss with 0–1 response is left as Exercise 4. □\\nIn summary, the expected optimism indicates how much, on average, the training loss\\ndeviates from the expected in-sample risk. Since the covariance of independent random\\nvariables is zero, the expected optimism is zero if the learnergT is statistically independent\\nfrom the responses Y1,..., Yn.\\nExample 2.3 (Polynomial Regression (cont.)) We continue Example 2.2, where the\\ncomponents of the response vectorY = [Y1,..., Yn]⊤are independent and normally distrib-\\nuted with variance ℓ∗ = 25 (the irreducible error) and expectations EXYi = g∗(xi) = x⊤\\ni β∗,\\ni = 1,..., n. Using the formula (2.15) for the least-squares estimator bβ, the expected op-\\ntimism (2.24) is\\n2\\nn\\nnX\\ni=1\\nCovX\\n\\x10\\nx⊤\\ni bβ,Yi\\n\\x11\\n= 2\\nntr\\n\\x10\\nCovX\\n\\x10\\nXbβ,Y\\n\\x11\\x11\\n= 2\\nntr \\x00CovX\\n\\x00XX+Y,Y\\x01\\x01\\n= 2tr (XX+CovX (Y,Y))\\nn = 2ℓ∗tr (XX+)\\nn = 2ℓ∗p\\nn .\\nIn the last equation we used the cyclic property of the trace (Theorem A.1): tr( XX+) = ☞ 357\\ntr(X+X) = tr(Ip), assuming that rank(X) = p. Therefore, an estimate for the in-sample risk\\n(2.23) is:\\nbℓin(gτ) = ℓτ(gτ) + 2ℓ∗p/n, (2.25)\\nwhere we have assumed that the irreducible risk ℓ∗ is known. Figure 2.9 shows that this\\nestimate is very close to the test loss from Figure 2.7. Hence, instead of computing the test\\nloss to assess the best model complexity p, we could simply have minimized the training\\nloss plus the correction term 2ℓ∗p/n. In practice, ℓ∗also has to be estimated somehow.\\n2 4 6 8 10 12 14 16 18\\n0\\n50\\n100\\n150\\nFigure 2.9: In-sample risk estimate bℓin(gτ) as a function of the number of parameters p of\\nthe model. The test loss is superimposed as a blue dashed curve.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 55, 'page_label': '38'}, page_content='38 Estimating Risk\\n2.5.2 Cross-Validation\\nIn general, for complex function classes G, it is very difficult to derive simple formulas of\\nthe approximation and statistical errors, let alone for the generalization risk or expected\\ngeneralization risk. As we saw, when there is an abundance of data, the easiest way to\\nassess the generalization risk for a given training setτis to obtain a test set τ′and evaluate\\nthe test loss (2.7). When a su fficiently large test set is not available but computational☞24\\nresources are cheap, one can instead gain direct knowledge of the expected generalization\\nrisk via a computationally intensive method called cross-validationcross-validation .\\nThe idea is to make multiple identical copies of the data set, and to partition each copy\\ninto different training and test sets, as illustrated in Figure 2.10. Here, there are four copies\\nof the data set (consisting of response and explanatory variables). Each copy is divided into\\na test set (colored blue) and training set (colored pink). For each of these sets, we estimate\\nthe model parameters using only training data and then predict the responses for the test\\nset. The average loss between the predicted and observed responses is then a measure for\\nthe predictive power of the model.\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05\\n\\t\\n\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05 \\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x03\\x05\\n\\n\\x07\\x08\\n\\n\\x07\\x08\\n\\n\\x07\\x08\\nFigure 2.10: An illustration of four-fold cross-validation, representing four copies of the\\nsame data set. The data in each copy is partitioned into a training set (pink) and a test\\nset (blue). The darker columns represent the response variable and the lighter ones the\\nexplanatory variables.\\nIn particular, suppose we partition a data setTof size n into K foldsfolds C1,..., CK of sizes\\nn1,..., nK (hence, n1 + ··· + nK = n). Typically nk ≈n/K, k = 1,..., K.\\nLet ℓCk be the test loss when using Ck as test data and all remaining data, denoted T−k,\\nas training data. Each ℓCk is an unbiased estimator of the generalization risk for training set\\nT−k; that is, for ℓ(gT−k ).\\nThe K-fold cross-validationK-fold\\ncross-validation\\nloss is the weighted average of these risk estimators:\\nCVK =\\nKX\\nk=1\\nnk\\nn ℓCk (gT−k )\\n= 1\\nn\\nKX\\nk=1\\nX\\ni∈Ck\\nLoss(gT−k (xi),yi)\\n= 1\\nn\\nnX\\ni=1\\nLoss(gT−κ(i) (xi),yi),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 56, 'page_label': '39'}, page_content=\"Statistical Learning 39\\nwhere the function κ : {1,..., n} 7→ {1,..., K}indicates to which of the K folds each\\nof the n observations belongs. As the average is taken over varying training sets {T−k}, it\\nestimates the expected generalization risk Eℓ(gT), rather than the generalization risk ℓ(gτ)\\nfor the particular training set τ.\\nExample 2.4 (Polynomial Regression (cont.)) For the polynomial regression ex-\\nample, we can calculate aK-fold cross-validation loss with a nonrandom partitioning of the\\ntraining set using the following code, which imports the previous code for the polynomial\\nregression example. We omit the full plotting code.\\npolyregCV.py\\nfrom polyreg3 import *\\nK_vals = [5, 10, 100] # number of folds\\ncv = np.zeros(( len (K_vals), max_p)) # cv loss\\nX = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX = np.hstack((X, u**(p-1)))\\nj = 0\\nfor K in K_vals:\\nloss = []\\nfor k in range (1, K+1):\\n# integer indices of test samples\\ntest_ind = ((n/K)*(k-1) + np.arange(1,n/K+1)-1).astype( 'int')\\ntrain_ind = np.setdiff1d(np.arange(n), test_ind)\\nX_train, y_train = X[train_ind, :], y[train_ind, :]\\nX_test, y_test = X[test_ind, :], y[test_ind]\\n# fit model and evaluate test loss\\nbetahat = solve(X_train.T @ X_train, X_train.T @ y_train)\\nloss.append(norm(y_test - X_test @ betahat) ** 2)\\ncv[j, p-1] = sum (loss)/n\\nj += 1\\n# basic plotting\\nplt.plot(p_range, cv[0, :], 'k-.')\\nplt.plot(p_range, cv[1, :], 'r')\\nplt.plot(p_range, cv[2, :], 'b--')\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 57, 'page_label': '40'}, page_content='40 Modeling Data\\n2\\n 4\\n 6\\n 8\\n 10\\n 12\\n 14\\n 16\\n 18\\nNumber of parameters p\\n50\\n100\\n150\\n200\\n250\\n300K-fold cross-validation loss\\nK=5\\nK=10\\nK=100\\nFigure 2.11: K-fold cross-validation for the polynomial regression example.\\nFigure 2.11 shows the cross-validation loss forK ∈{5,10,100}. The case K = 100 cor-\\nresponds to the leave-one-out cross-validation, which can be computed more e fficientlyleave-one-out\\ncross-validation using the formula in Theorem 5.1.\\n☞174\\n2.6 Modeling Data\\nThe first step in any data analysis is tomodelmodel the data in one form or another. For example,\\nin an unsupervised learning setting with data represented by a vector x = [x1,..., xp]⊤, a\\nvery general model is to assume thatx is the outcome of a random vectorX = [X1,..., Xp]⊤\\nwith some unknown pdf f . The model can then be refined by assuming a specific form of\\nf .\\nWhen given a sequence of such data vectorsx1,..., xn, one of the simplest models is to\\nassume that the corresponding random vectors X1,..., Xn are independent and identically\\ndistributed (iid). We write☞429\\nX1,..., Xn\\niid\\n∼f or X1,..., Xn\\niid\\n∼Dist,\\nto indicate that the random vectors form an iid sample from a sampling pdf f or sampling\\ndistribution Dist. This model formalizes the notion that the knowledge about one variable\\ndoes not provide extra information about another variable. The main theoretical use of\\nindependent data models is that the joint density of the random vectorsX1,..., Xn is simply\\nthe product of the marginal ones; see Theorem C.1. Specifically,☞429\\nfX1,..., Xn (x1,..., xn) = f (x1) ··· f (xn).\\nIn most models of this kind, our approximation or model for the sampling distribution is\\nspecified up to a small number of parameters. That is, g(x) is of the form g(x |β) which\\nis known up to some parameter vector β. Examples for the one-dimensional case ( p = 1)\\ninclude the N(µ,σ2),Bin(n,p), and Exp(λ) distributions. See Tables C.1 and C.2 for other☞425'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 58, 'page_label': '41'}, page_content='Statistical Learning 41\\ncommon sampling distributions.\\nTypically, the parameters are unknown and must be estimated from the data. In a non-\\nparametric setting the whole sampling distribution would be unknown. To visualize the\\nunderlying sampling distribution from outcomes x1,..., xn one can use graphical repres-\\nentations such as histograms, density plots, and empirical cumulative distribution func-\\ntions, as discussed in Chapter 1. ☞ 11\\nIf the order in which the data were collected (or their labeling) is not informative or\\nrelevant, then the joint pdf of X1,..., Xn satisfies the symmetry:\\nfX1,...,Xn (x1,..., xn) = fXπ1 ,...,Xπn (xπ1 ,..., xπn ) (2.26)\\nfor any permutation π1,...,π n of the integers 1 ,..., n. We say that the infinite sequence\\nX1,X2,... is exchangeable exchangeableif this permutational invariance (2.26) holds for any finite subset\\nof the sequence. As we shall see in Section 2.9 on Bayesian learning, it is common to\\nassume that the random vectors X1,..., Xn are a subset of an exchangeable sequence and\\nthus satisfy (2.26). Note that while iid random variables are exchangeable, the converse is\\nnot necessarily true. Thus, the assumption of an exchangeable sequence of random vectors\\nis weaker than the assumption of iid random vectors.\\nFigure 2.12 illustrates the modeling tradeoffs. The keywords within the triangle repres-\\nent various modeling paradigms. A few keywords have been highlighted, symbolizing their\\nimportance in modeling. The specific meaning of the keywords does not concern us here,\\nbut the point is there are many models to choose from, depending on what assumptions are\\nmade about the data.\\nFigure 2.12: Illustration of the modeling dilemma. Complex models are more generally\\napplicable, but may be di fficult to analyze. Simple models may be highly tractable, but\\nmay not describe the data accurately. The triangular shape signifies that there are a great\\nmany specific models but not so many generic ones.\\nOn the one hand, models that make few assumptions are more widely applicable, but at\\nthe same time may not be very mathematically tractable or provide insight into the nature\\nof the data. On the other hand, very specific models may be easy to handle and interpret, but'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 59, 'page_label': '42'}, page_content='42 Modeling Data\\nmay not match the data very well. This tradeoff between the tractability and applicability of\\nthe model is very similar to the approximation–estimation tradeoff described in Section 2.4.\\nIn the typical unsupervised setting we have a training setτ= {x1,..., xn}that is viewed\\nas the outcome of n iid random variables X1,..., Xn from some unknown pdf f . The ob-\\njective is then to learn or estimate f from the finite training data. To put the learning in\\na similar framework as for supervised learning discussed in the preceding Sections 2.3–\\n2.5, we begin by specifying a class of probability density functions Gp := {g(·|θ),θ∈Θ},\\nwhere θis a parameter in some subset Θ of Rp. We now seek the best g in Gp to minimize\\nsome risk. Note that Gp may not necessarily contain the true f even for very large p.\\nWe stress that our notation g(x) has a different meaning in the supervised and unsu-\\npervised case. In the supervised case, g is interpreted as a prediction function for a\\nresponse y; in the unsupervised setting, g is an approximation of a density f .\\nFor each x we measure the discrepancy between the true model f (x) and the hypothes-\\nized model g(x |θ) using the loss function\\nLoss( f (x),g(x |θ)) = ln f (x)\\ng(x |θ) = ln f (x) −ln g(x |θ).\\nThe expected value of this loss (that is, the risk) is thus\\nℓ(g) = Eln f (X)\\ng(X |θ) =\\nZ\\nf (x) ln f (x)\\ng(x |θ) dx. (2.27)\\nThe integral in (2.27) provides a fundamental way to measure the distance between two\\ndensities and is called the Kullback–Leibler (KL) divergence2Kullback–\\nLeibler\\ndivergence\\nbetween f and g(·|θ). Note\\nthat the KL divergence is not symmetric in f and g(·|θ). Moreover, it is always greater\\nthan or equal to 0 (see Exercise 15) and equal to 0 when f = g(·|θ).\\nUsing similar notation as for the supervised learning setting in Table 2.1, define gGp as\\nthe global minimizer of the risk in the class Gp; that is, gGp = argming∈Gp ℓ(g). If we define\\nθ∗= argmin\\nθ\\nELoss( f (X),g(X |θ)) = argmin\\nθ\\nZ \\x00ln f (x) −ln g(x |θ)\\x01f (x) dx\\n= argmax\\nθ\\nZ\\nf (x) lng(x |θ) dx = argmax\\nθ\\nEln g(X |θ),\\nthen gGp = g(·|θ∗) and learning gGp is equivalent to learning (or estimating) θ∗. To learn θ∗\\nfrom a training set τ= {x1,..., xn}we then minimize the training loss,\\n1\\nn\\nnX\\ni=1\\nLoss( f (xi),g(xi |θ)) = −1\\nn\\nnX\\ni=1\\nln g(xi |θ) + 1\\nn\\nnX\\ni=1\\nln f (xi),\\ngiving:\\nbθn := argmax\\nθ\\n1\\nn\\nnX\\ni=1\\nln g(xi |θ). (2.28)\\n2Sometimes called cross-entropy distance.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 60, 'page_label': '43'}, page_content='Statistical Learning 43\\nAs the logarithm is an increasing function, this is equivalent to\\nbθn := argmax\\nθ\\nnY\\ni=1\\ng(xi |θ),\\nwhere Qn\\ni=1 g(xi |θ) is the likelihood of the data; that is, the joint density of the {Xi}eval-\\nuated at the points {xi}. We therefore have recovered the classical maximum likelihood\\nestimate of θ∗. maximum\\nlikelihood\\nestimate\\n☞ 456\\nWhen the risk ℓ(g(·|θ)) is convex in θover a convex set Θ, we can find the maximum\\nlikelihood estimator by setting the gradient of the training loss to zero; that is, we solve\\n−1\\nn\\nnX\\ni=1\\nS(xi |θ) = 0,\\nwhere S(x |θ) := ∂ln g(x |θ)\\n∂θ is the gradient of ln g(x |θ) with respect to θand is often called\\nthe score score.\\nExample 2.5 (Exponential Model) Suppose we have the training dataτn = {x1,..., xn},\\nwhich is modeled as a realization of n positive iid random variables: X1,..., Xn ∼iid f (x).\\nWe select the class of approximating functions Gto be the parametric class {g : g(x |θ) =\\nθexp(−x θ),x > 0,θ > 0}. In other words, we look for the best gG within the family of\\nexponential distributions with unknown parameter θ> 0. The likelihood of the data is\\nnY\\ni=1\\ng(xi |θ) =\\nnY\\ni=1\\nθexp(−θxi) = exp(−θn xn + n ln θ)\\nand the score isS (x |θ) = −x+θ−1. Thus, maximizing the likelihood with respect toθis the\\nsame as maximizing −θn xn + n ln θor solving −Pn\\ni=1 S (xi |θ)/n = xn −θ−1 = 0. In other\\nwords, the solution to (2.28) is the maximum likelihood estimate bθn = 1/xn.\\nIn a supervised setting, where the data is represented by a vector x of explanatory\\nvariables and a response y, the general model is that ( x,y) is an outcome of ( X,Y) ∼f\\nfor some unknown f . And for a training sequence ( x1,y1),..., (xn,yn) the default model\\nassumption is that ( X1,Y1),..., (Xn,Yn) ∼iid f . As explained in Section 2.2, the analysis\\nprimarily involves the conditional pdf f (y |x) and in particular (when using the squared-\\nerror loss) the conditional expectation g∗(x) = E[Y |X = x]. The resulting representation\\n(2.2) allows us to then write the response at X = x as a function of the feature x plus an\\nerror term: Y = g∗(x) + ε(x).\\nThis leads to the simplest and most important model for supervised learning, where we\\nchoose a linear class Gof prediction or guess functions and assume that it is rich enough\\nto contain the true g∗. If we further assume that, conditional on X = x, the error term ε\\ndoes not depend on x, that is, Eε= 0 and Var ε= σ2, then we obtain the following model.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 61, 'page_label': '44'}, page_content='44 Modeling Data\\nDefinition 2.1: Linear Model\\nIn a linear modellinear model the response Y depends on a p-dimensional explanatory variable\\nx = [x1,..., xp]⊤via the linear relationship\\nY = x⊤β+ ε, (2.29)\\nwhere Eε= 0 and Var ε= σ2.\\nNote that (2.29) is a model for a single pair ( x,Y). The model for the training set\\n{(xi,Yi)}is simply that each Yi satisfies (2.29) (with x = xi) and that the {Yi}are independ-\\nent. Gathering all responses in the vector Y = [Y1,..., Yn]⊤, we can write\\nY = Xβ+ ε, (2.30)\\nwhere ε = [ε1,...,ε n]⊤is a vector of iid copies of εand X is the so-called model matrixmodel matrix ,\\nwith rows x⊤\\n1 ,..., x⊤\\nn . Linear models are fundamental building blocks of statistical learning\\nalgorithms. For this reason, a large part of Chapter 5 is devoted to linear regression models.☞167\\nExample 2.6 (Polynomial Regression (cont.)) For our running Example 2.1, we see☞26\\nthat the data is described by a linear model of the form (2.30), with model matrix X given\\nin (2.10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 62, 'page_label': '45'}, page_content='Statistical Learning 45\\nBefore we discuss a few other models in the following sections, we would like to em-\\nphasize a number of points about modeling.\\n• Any model for data is likely to be wrong. For example, real data (as opposed to\\ncomputer-generated data) are often assumed to come from a normal distribution,\\nwhich is never exactly true. However, an important advantage of using a normal\\ndistribution is that it has many nice mathematical properties, as we will see in Sec-\\ntion 2.7.\\n• Most data models depend on a number of unknown parameters, which need to be\\nestimated from the observed data.\\n• Any model for real-life data needs to be checked for suitability. An important cri-\\nterion is that data simulated from the model should resemble the observed data, at\\nleast for a certain choice of model parameters.\\nHere are some guidelines for choosing a model. Think of the data as a spreadsheet or\\ndata frame, as in Chapter 1, where rows represent the data units and the columns the data\\nfeatures (variables, groups).\\n• First establish the type of the features (quantitative, qualitative, discrete, continuous,\\netc.).\\n• Assess whether the data can be assumed to be independent across rows or columns.\\n• Decide on the level of generality of the model. For example, should we use a simple\\nmodel with a few unknown parameters or a more generic model that has a large\\nnumber of parameters? Simple specific models are easier to fit to the data (low es-\\ntimation error) than more general models, but the fit itself may not be accurate (high\\napproximation error). The tradeoffs discussed in Section 2.4 play an important role\\nhere.\\n• Decide on using a classical (frequentist) or Bayesian model. Section 2.9 gives a short\\nintroduction to Bayesian learning. ☞ 48\\n2.7 Multivariate Normal Models\\nA standard model for numerical observations x1,..., xn (forming, e.g., a column in a\\nspreadsheet or data frame) is that they are the outcomes of iid normal random variables\\nX1,..., Xn\\niid\\n∼N(µ,σ2).\\nIt is helpful to view a normally distributed random variable as a simple transformation\\nof a standard normal random variable. To wit, ifZ has a standard normal distribution, then\\nX = µ+ σZ has a N(µ,σ2) distribution. The generalization to n dimensions is discussed\\nin Appendix C.7. We summarize the main points: Let Z1,..., Zn\\niid\\n∼ N(0,1). The pdf of ☞ 434\\nZ = [Z1,..., Zn]⊤(that is, the joint pdf of Z1,..., Zn) is given by\\nfZ(z) =\\nnY\\ni=1\\n1√\\n2π\\ne−1\\n2 z2\\ni = (2π)−n\\n2 e−1\\n2 z⊤z, z ∈Rn. (2.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 63, 'page_label': '46'}, page_content='46 Multivariate Normal Models\\nWe write Z ∼N(0,In) and say that Z has a standard normal distribution in Rn. Let\\nX = µ+ B Z (2.32)\\nfor some m ×n matrix B and m-dimensional vector µ. Then X has expectation vector µand\\ncovariance matrix Σ = BB⊤; see (C.20) and (C.21). This leads to the following definition.☞432\\nDefinition 2.2: Multivariate Normal Distribution\\nAn m-dimensional random vector X that can be written in the form (2.32) for some\\nm-dimensional vector µ and m ×n matrix B, with Z ∼N(0,In), is said to have a\\nmultivariate normalmultivariate\\nnormal\\nor multivariate Gaussian distribution with mean vector µand\\ncovariance matrix Σ = BB⊤. We write X ∼N(µ,Σ).\\nThe m-dimensional density of a multivariate normal distribution has a very similar form\\nto the density of the one-dimensional normal distribution and is given in the next theorem.\\nWe leave the proof as an exercise; see Exercise 5.☞60\\nTheorem 2.3: Density of a Multivariate Random Vector\\nLet X ∼N(µ,Σ), where the m ×m covariance matrix Σ is invertible. Then X has pdf\\nfX(x) = 1√(2π)m |Σ|\\ne−1\\n2 (x−µ)⊤Σ−1(x−µ), x ∈Rm. (2.33)\\nFigure 2.13 shows the pdfs of two bivariate (that is, two-dimensional) normal distribu-\\ntions. In both cases the mean vector isµ= [0,0]⊤and the variances (the diagonal elements\\nof Σ) are 1. The correlation coefficients (or, equivalently here, the covariances) are respect-\\nively ϱ= 0 and ϱ= 0.8.\\n0\\n0.1\\n2\\n0.2\\n0\\n-2 20-2\\n0\\n0.1\\n2\\n0.2\\n0\\n-2 20-2\\nFigure 2.13: Pdfs of bivariate normal distributions with means zero, variances 1, and cor-\\nrelation coefficients 0 (left) and 0.8 (right).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 64, 'page_label': '47'}, page_content='Statistical Learning 47\\nThe main reason why the multivariate normal distribution plays an important role in\\ndata science and machine learning is that it satisfies the following properties, the details\\nand proofs of which can be found in Appendix C.7: ☞ 434\\n1. A ffine combinations are normal.\\n2. Marginal distributions are normal.\\n3. Conditional distributions are normal.\\n2.8 Normal Linear Models\\nNormal linear models combine the simplicity of the linear model with the tractability of\\nthe Gaussian distribution. They are the principal model for traditional statistics, and include\\nthe classic linear regression and analysis of variance models.\\nDefinition 2.3: Normal Linear Model\\nIn a normal linear model normal linear\\nmodel\\nthe response Y depends on a p-dimensional explanatory\\nvariable x = [x1,..., xp]⊤, via the linear relationship\\nY = x⊤β+ ε, (2.34)\\nwhere ε∼N(0,σ2).\\nThus, a normal linear model is a linear model (in the sense of Definition 2.1) with\\nnormal error terms. Similar to (2.30), the corresponding normal linear model for the whole\\ntraining set {(xi,Yi)}has the form\\nY = Xβ+ ε, (2.35)\\nwhere X is the model matrix comprised of rows x⊤\\n1 ,..., x⊤\\nn and ε∼N(0,σ2In). Con-\\nsequently, Y can be written asY = Xβ+ σZ, where Z ∼N(0,In), so thatY ∼N(Xβ,σ2In).\\nIt follows from (2.33) that its joint density is given by ☞ 46\\ng(y |β,σ2,X) = (2πσ2)−n\\n2 e− 1\\n2σ2 ||y−Xβ||2\\n. (2.36)\\nEstimation of the parameter βcan be performed via the least-squares method, as discussed\\nin Example 2.1. An estimate can also be obtained via the maximum likelihood method.\\nThis simply means finding the parameters σ2 and β that maximize the likelihood of the\\noutcome y, given by the right-hand side of (2.36). It is clear that for every value of σ2\\nthe likelihood is maximal when ∥y −Xβ∥2 is minimal. As a consequence, the maximum\\nlikelihood estimate for βis the same as the least-squares estimate (2.15). We leave it as an\\nexercise (see Exercise 18) to show that the maximum likelihood estimate of σ2 is equal to ☞ 64\\ncσ2 = ∥y −Xbβ∥2\\nn , (2.37)\\nwhere bβis the maximum likelihood estimate (least squares estimate in this case) of β.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 65, 'page_label': '48'}, page_content='48 Bayesian Learning\\n2.9 Bayesian Learning\\nIn Bayesian unsupervised learning, we seek to approximate the unknown joint density\\nf (x1,..., xn) of the training data Tn = {X1,..., Xn}via a joint pdf of the form\\nZ \\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnY\\ni=1\\ng(xi |θ)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8w(θ) dθ, (2.38)\\nwhere g(·|θ) belongs to a family of parametric densities Gp := {g(·|θ), θ ∈Θ}(viewed\\nas a family of pdfs conditional on a parameter θ in some set Θ ⊂Rp) and w(θ) is a pdf\\nthat belongs to a (possibly different) family of densities Wp. Note how the joint pdf (2.38)\\nsatisfies the permutational invariance (2.26) and can thus be useful as a model for training\\ndata which is part of an exchangeable sequence of random variables.\\nFollowing standard practice in a Bayesian context, instead of writing fX(x) and\\nfX |Y (x |y) for the pdf of X and the conditional pdf of X given Y, one simply writes\\nf (x) and f (x |y). If Y is a different random variable, its pdf (at y) is thus denoted by\\nf (y).\\nThus, we will use the same symbol g for different (conditional) approximating probab-\\nility densities and f for the different (conditional) true and unknown probability densities.\\nUsing Bayesian notation, we can write g(τ|θ) = Qn\\ni=1 g(xi |θ) and thus the approximating\\njoint pdf (2.38) can then be written as\\nR\\ng(τ|θ) w(θ) dθand the true unknown joint pdf as\\nf (τ) = f (x1,..., xn).\\nOnce Gp and Wp are specified, selecting an approximating function g(x) of the form\\ng(x) =\\nZ\\ng(x |θ) w(θ) dθ\\nis equivalent to selecting a suitablew from Wp. Similar to (2.27), we can use the Kullback–\\nLeibler risk to measure the discrepancy between the proposed approximation (2.38) and the\\ntrue f (τ):\\nℓ(g) = Eln f (T)R\\ng(T| θ) w(θ) dθ\\n=\\nZ\\nf (τ) ln f (τ)R\\ng(τ|θ) w(θ) dθ\\ndτ. (2.39)\\nThe main difference with (2.27) is that since the training data is not necessarily iid (it may\\nbe exchangeable, for example), the expectation must be with respect to the joint density of☞41\\nT, not with respect to the marginal f (x) (as in the iid case).\\nMinimizing the training loss is equivalent to maximizing the likelihood of the training\\ndata τ; that is, solving the optimization problem\\nmax\\nw∈Wp\\nZ\\ng(τ|θ) w(θ) dθ,\\nwhere the maximization is over an appropriate class Wp of density functions that is be-\\nlieved to result in the smallest KL risk.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 66, 'page_label': '49'}, page_content='Statistical Learning 49\\nSuppose that we have a rough guess, denoted w0(θ), for the best w ∈Wp that min-\\nimizes the Kullback–Leibler risk. We can always increase the resulting likelihood L0 :=R\\ng(τ|θ) w0(θ) dθ by instead using the density w1(θ) := w0(θ) g(τ|θ)/L0, giving a likeli-\\nhood L1 :=\\nR\\ng(τ|θ) w1(θ) dθ. To see this, write L0 and L1 as expectations with respect to\\nw0. In particular, we can write\\nL0 = Ew0 g(τ|θ) and L1 = Ew1 g(τ|θ) = Ew0 g2(τ|θ)/L0.\\nIt follows that\\nL1 −L0 = 1\\nL0\\nEw0\\nh\\ng2(τ|θ) −L2\\n0\\ni\\n= 1\\nL0\\nVarw0 [g(τ|θ)] ⩾0. (2.40)\\nWe may thus expect to obtain better predictions usingw1 instead of w0, because w1 has\\ntaken into account the observed data τand increased the likelihood of the model. In fact,\\nif we iterate this process (see Exercise 20) and create a sequence of densities w1,w2,...\\nsuch that wt(θ) ∝wt−1(θ) g(τ|θ), then wt(θ) concentrates more and more of its probability\\nmass at the maximum likelihood estimator bθ(see (2.28)) and in the limit equals a (degen-\\nerate) point-mass pdf atbθ. In other words, in the limit we recover the maximum likelihood\\nmethod: gτ(x) = g(x |bθ). Thus, unless the class of densities Wp is restricted to be non-\\ndegenerate, maximizing the likelihood as much as possible leads to a degenerate choice\\nfor w(θ).\\nIn many situations, the maximum likelihood estimate g(τ|bθ) is either not an ap-\\npropriate approximation to f (τ) (see Example 2.9), or simply fails to exist (see Exer-\\ncise 10 in Chapter 4). In such cases, given an initial non-degenerate guess w0(θ) = g(θ), ☞ 162\\none can obtain a more appropriate and non-degenerate approximation to f (τ) by taking\\nw(θ) = w1(θ) ∝g(τ|θ) g(θ) in (2.38), giving the following Bayesian learner of f (x):\\ngτ(x) :=\\nZ\\ng(x |θ) g(τ|θ) g(θ)R\\ng(τ|ϑ) g(ϑ) dϑ\\ndθ, (2.41)\\nwhere\\nR\\ng(τ|ϑ) g(ϑ) dϑ= g(τ). Using Bayes’ formula for probability densities, ☞ 428\\ng(θ|τ) = g(τ|θ) g(θ)\\ng(τ) , (2.42)\\nwe can write w1(θ) = g(θ|τ). With this notation, we have the following definitions.\\nDefinition 2.4: Prior, Likelihood, and Posterior\\nLet τ and Gp := {g(·|θ),θ ∈Θ}be the training set and family of approximating\\nfunctions.\\n• A pdf g(θ) that reflects our a priori beliefs about θis called the prior priorpdf.\\n• The conditional pdf g(τ|θ) is called the likelihood likelihood.\\n• Inference about θis given by the posterior posteriorpdf g(θ|τ), which is proportional\\nto the product of the prior and the likelihood:\\ng(θ|τ) ∝g(τ|θ) g(θ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 67, 'page_label': '50'}, page_content='50 Bayesian Learning\\nRemark 2.1 (Early Stopping) Bayes iteration is an example of an “early stopping”\\nheuristic for maximum likelihood optimization, where we exit after only one step. As ob-\\nserved above, if we keep iterating, we obtain the maximum likelihood estimate (MLE). In\\na sense the Bayes rule provides a regularization of the MLE. Regularization is discussed in\\nmore detail in Chapter 6; see also Example 2.9. The early stopping rule is also of benefit\\nin regularization; see Exercise 20 in Chapter 6.\\nOn the one hand, the initial guess g(θ) conveys the a priori (prior to training the\\nBayesian learner) information about the optimal density inWp that minimizes the KL risk.\\nUsing this prior g(θ), the Bayesian approximation to f (x) is the prior predictive densityprior predictive\\ndensity\\n:\\ng(x) =\\nZ\\ng(x |θ) g(θ) dθ.\\nOn the other hand, the posterior pdf conveys improved knowledge about this optimal dens-\\nity in Wp after training with τ. Using the posterior g(θ|τ), the Bayesian learner of f (x) is\\nthe posterior predictive densityposterior\\npredictive\\ndensity\\n:\\ngτ(x) = g(x |τ) =\\nZ\\ng(x |θ) g(θ|τ) dθ,\\nwhere we have assumed that g(x |θ,τ) = g(x |θ); that is, the likelihood depends on τonly\\nthrough the parameter θ.\\nThe choice of the prior is typically governed by two considerations:\\n1. the prior should be simple enough to facilitate the computation or simulation of the\\nposterior pdf;\\n2. the prior should be general enough to model ignorance of the parameter of interest.\\nPriors that do not convey much knowledge of the parameter are said to be uninformat-\\nive. The uniform or flat prior in Example 2.9 (to follow) is frequently used.uninformative\\nprior\\nFor the purpose of analytical and numerical computations, we can view θas a ran-\\ndom vector with prior density g(θ), which after training is updated to the posterior\\ndensity g(θ|τ).\\nThe above thinking allows us to write g(x |τ) ∝\\nR\\ng(x |θ) g(τ|θ) g(θ) dθ, for example,\\nthus ignoring any constants that do not depend on the argument of the densities.\\nExample 2.7 (Normal Model) Suppose that the training data T= {X1,..., Xn} is\\nmodeled using the likelihood g(x |θ) that is the pdf of\\nX |θ∼N(µ,σ2),\\nwhere θ := [µ,σ2]⊤. Next, we need to specify the prior distribution of θ to complete\\nthe model. We can specify prior distributions for µand σ2 separately and then take their'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 68, 'page_label': '51'}, page_content='Statistical Learning 51\\nproduct to obtain the prior for vector θ(assuming independence). A possible prior distri-\\nbution for µis\\nµ∼N(ν,ϕ2). (2.43)\\nIt is typical to refer to any parameters of the prior density as hyperparameters hyperparamet-\\ners\\nof the\\nBayesian model. Instead of giving directly a prior for σ2 (or σ), it turns out to be con-\\nvenient to give the following prior distribution to 1/σ2:\\n1\\nσ2 ∼Gamma(α,β). (2.44)\\nThe smaller αand βare, the less informative is the prior. Under this prior,σ2 is said to have\\nan inverse gamma inverse gamma3 distribution. If 1/Z ∼Gamma(α,β), then the pdf of Z is proportional\\nto exp (−β/z) /zα+1 (Exercise 19). The Bayesian posterior is then given by: ☞ 64\\ng(µ,σ2 |τ) ∝g(µ) ×g(σ2) ×g(τ|µ,σ2)\\n∝exp\\n(\\n−(µ−ν)2\\n2ϕ2\\n)\\n×\\nexp\\nn\\n−β/σ2\\no\\n(σ2)α+1 ×\\nexp\\nn\\n−P\\ni(xi −µ)2/(2σ2)\\no\\n(σ2)n/2\\n∝(σ2)−n/2−α−1 exp\\n(\\n−(µ−ν)2\\n2ϕ2 − β\\nσ2 −(µ−xn)2 + S 2\\nn\\n2σ2/n\\n)\\n,\\nwhere S 2\\nn := 1\\nn\\nP\\ni x2\\ni −x2\\nn = 1\\nn\\nP\\ni(xi −xn)2 is the (scaled) sample variance. All inference\\nabout (µ,σ2) is then represented by the posterior pdf. To facilitate computations it is helpful\\nto find out if the posterior belongs to a recognizable family of distributions. For example,\\nthe conditional pdf of µgiven σ2 and τis\\ng(µ|σ2,τ) ∝exp\\n(\\n−(µ−ν)2\\n2ϕ2 −(µ−xn)2\\n2σ2/n\\n)\\n,\\nwhich after simplification can be recognized as the pdf of\\n(µ|σ2,τ) ∼N\\n\\x10\\nγn xn + (1 −γn)ν, γn σ2/n\\n\\x11\\n, (2.45)\\nwhere we have defined the weight parameter: γn := n\\nσ2\\n.\\x10\\n1\\nϕ2 + n\\nσ2\\n\\x11\\n.We can then see that the\\nposterior mean E[µ|σ2,τ] = γn xn + (1 −γn)νis a weighted linear combination of the prior\\nmean ν and the sample average xn. Further, as n →∞, the weight γn →1 and thus the\\nposterior mean approaches the maximum likelihood estimate xn.\\nIt is sometimes possible to use a priorg(θ) that is not a bona fide probability density, in the\\nsense that\\nR\\ng(θ) dθ= ∞, as long as the resulting posteriorg(θ|τ) ∝g(τ|θ)g(θ) is a proper\\npdf. Such a prior is called an improper prior improper prior.\\nExample 2.8 (Normal Model (cont.)) An example of an improper prior is obtained\\nfrom (2.43) when we let ϕ→∞ (the larger ϕ is, the more uninformative is the prior).\\n3Reciprocal gamma distribution would have been a better name.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 69, 'page_label': '52'}, page_content='52 Bayesian Learning\\nThen, g(µ) ∝1 is a flat prior, but\\nR\\ng(µ) dµ= ∞, making it an improper prior. Neverthe-\\nless, the posterior is a proper density, and in particular the conditional posterior of (µ|σ2,τ)\\nsimplifies to\\n(µ|σ2,τ) ∼N\\n\\x10\\nxn,σ2/n\\n\\x11\\n,\\nbecause the weight parameter γn goes to 1 as ϕ →∞. The improper prior g(µ) ∝1 also\\nallows us to simplify the posterior marginal for σ2:\\ng(σ2 |τ) =\\nZ\\ng(µ,σ2 |τ) dµ∝(σ2)−(n−1)/2−α−1 exp\\n(\\n−β+ nS 2\\nn/2\\nσ2\\n)\\n,\\nwhich we recognize as the density corresponding to\\n1\\nσ2\\n\\x0c\\x0c\\x0c\\x0cτ∼Gamma\\n \\nα+ n −1\\n2 , β+ n\\n2S 2\\nn\\n!\\n.\\nIn addition tog(µ) ∝1, we can also use an improper prior forσ2. If we take the limitα→0\\nand β→0 in (2.44), then we also obtain the improper prior g(σ2) ∝1/σ2 (or equivalently\\ng(1/σ2) ∝1/σ2). In this case, the posterior marginal density for σ2 implies that:\\nnS 2\\nn\\nσ2\\n\\x0c\\x0c\\x0c\\x0cτ∼χ2\\nn−1\\nand the posterior marginal density for µimplies that:\\nµ−xn\\nS n/\\n√\\nn −1\\n\\x0c\\x0c\\x0c\\x0cτ∼tn−1. (2.46)\\nIn general, deriving a simple formula for the posterior density of θ is either impossible\\nor too tedious. Instead, the Monte Carlo methods in Chapter 3 can be used to simulate\\n(approximately) from the posterior for the purposes of inference and prediction.\\nOne way in which a distributional result such as (2.46) can be useful is in the construc-\\ntion of a 95% credible intervalcredible\\ninterval\\nIfor the parameter µ; that is, an interval Isuch that the\\nprobability P[µ∈I| τ] is equal to 0.95. For example, the symmetric 95% credible interval\\nis\\nI=\\n\"\\nxn − S n\\n√\\nn −1\\nγ, xn + S n\\n√\\nn −1\\nγ\\n#\\n,\\nwhere γ is the 0 .975-quantile of the tn−1 distribution. Note that the credible interval is\\nnot a random object and that the parameter µ is interpreted as a random variable with a\\ndistribution. This is unlike the case of classical confidence intervals, where the parameter\\nis nonrandom, but the interval is (the outcome of) a random object.☞457\\nAs a generalization of the 95% Bayesian credible interval we can define a 1−αcredible\\nregioncredible region , which is any set Rsatisfying\\nP[θ∈R| τ] =\\nZ\\nθ∈R\\ng(θ|τ) dθ⩾1 −α. (2.47)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 70, 'page_label': '53'}, page_content='Statistical Learning 53\\nExample 2.9 (Bayesian Regularization of Maximum Likelihood) Consider model-\\ning the number of deaths during birth in a maternity ward. Suppose that the hospital data\\nconsists of τ = {x1,..., xn}, with xi = 1 if the i-th baby has died during birth and xi = 0\\notherwise, for i = 1,..., n. A possible Bayesian model for the data is θ∼U(0,1) (uniform\\nprior) with (X1,..., Xn |θ)\\niid\\n∼Ber(θ). The likelihood is therefore\\ng(τ|θ) =\\nnY\\ni=1\\nθxi (1 −θ)1−xi = θs (1 −θ)n−s,\\nwhere s = x1 + ··· + xn is the total number of deaths. Since g(θ) = 1, the posterior pdf is\\ng(θ|τ) ∝θs (1 −θ)n−s, θ ∈[0,1],\\nwhich is the pdf of the Beta(s + 1,n −s + 1) distribution. The normalization constant is\\n(n + 1)\\n\\x10n\\ns\\n\\x11\\n. The posterior pdf is shown in Figure 2.14 for ( s,n) = (0,100). It is not difficult\\nFigure 2.14: Posterior pdf for θ, with n = 100 and s = 0.\\nto see that the maximum a posteriori maximum a\\nposteriori\\n(MAP) estimate of θ(the mode or maximizer of the\\nposterior density) is\\nargmax\\nθ\\ng(θ|τ) = s\\nn,\\nwhich agrees with the maximum likelihood estimate. Figure 2.14 also shows that the left\\none-sided 95% credible interval for θ is [0 ,0.0292], where 0 .0292 is the 0.95 quantile\\n(rounded) of the Beta(1,101) distribution.\\nObserve that when (s,n) = (0,100) the maximum likelihood estimate bθ = 0 infers that\\ndeaths at birth are not possible. We know that this inference is wrong — the probability of\\ndeath can never be zero, it is simply (and fortunately) too small to be inferred accurately\\nfrom a sample size of n = 100. In contrast to the maximum likelihood estimate, the pos-\\nterior mean E[θ|τ] = (s + 1)/(n + 2) is not zero for (s,n) = (0,100) and provides the more\\nreasonable point estimate of 0.0098 for the probability of death.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 71, 'page_label': '54'}, page_content='54 Bayesian Learning\\nIn addition, while computing a Bayesian credible interval poses no conceptual di ffi-\\nculties, it is not simple to derive a confidence interval for the maximum likelihood estimate\\nof bθ, because the likelihood as a function of θis not differentiable at θ = 0. As a result of\\nthis lack of smoothness, the usual confidence intervals based on the normal approximation\\ncannot be used.\\nWe now return to the unsupervised learning setting of Section 2.6, but consider this\\nfrom a Bayesian perspective. Recall from (2.39) that the Kullback–Leibler risk for an ap-\\nproximating function g is\\nℓ(g) =\\nZ\\nf (τ′\\nn)[ln f (τ′\\nn) −ln g(τ′\\nn)] dτ′\\nn,\\nwhere τ′\\nn denotes the test data. Since\\nR\\nf (τ′\\nn) ln f (τ′\\nn) dτ′\\nn plays no role in minimizing the\\nrisk, we consider instead the cross-entropy risk, defined as☞122\\nℓ(g) = −\\nZ\\nf (τ′\\nn) lng(τ′\\nn) dτ′\\nn.\\nNote that the smallest possible cross-entropy risk isℓ∗\\nn = −\\nR\\nf (τ′\\nn) ln f (τ′\\nn) dτ′\\nn. The expec-\\nted generalization risk of the Bayesian learner can then be decomposed as\\nEℓ(gTn ) = ℓ∗\\nn +\\nZ\\nf (τ′\\nn) ln f (τ′\\nn)\\nEg(τ′\\nn |Tn) dτ′\\nn\\n|                            {z                            }\\n“bias” component\\n+ E\\nZ\\nf (τ′\\nn) ln Eg(τ′\\nn |Tn)\\ng(τ′\\nn |Tn) dτ′\\nn\\n|                               {z                               }\\n“variance” component\\n,\\nwhere gTn (τ′\\nn) = g(τ′\\nn |Tn) =\\nR\\ng(τ′\\nn |θ) g(θ|Tn) dθis the posterior predictive density after\\nobserving Tn.\\nAssuming that the setsTn and T′\\nn are comprised of 2n iid random variables with density\\nf , we can show (Exercise 23) that the expected generalization risk simplifies to\\nEℓ(gTn ) = Eln g(Tn) −Eln g(T2n), (2.48)\\nwhere g(τn) and g(τ2n) are the prior predictive densities of τn and τ2n, respectively.\\nLet θn = argmaxθ g(θ|Tn) be the MAP estimator of θ∗ := argmaxθ Eln g(X |θ). As-\\nsuming that θn converges to θ∗(with probability one) and 1\\nn Eln g(Tn |θn) = Eln g(X |θ∗) +\\nO(1/n), we can use the following large-sample approximation of the expected generaliza-\\ntion risk.\\nTheorem 2.4: Approximating the Bayesian Cross-Entropy Risk\\nFor n →∞, the expected cross-entropy generalization risk satisfies:\\nEℓ(gTn ) ≃−Eln g(Tn) −p\\n2 ln n, (2.49)\\nwhere (with p the dimension of the parameter vector θand θn the MAP estimator):\\nEln g(Tn) ≃Eln g(Tn |θn) −p\\n2 ln n. (2.50)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 72, 'page_label': '55'}, page_content='Statistical Learning 55\\nProof: To show (2.50), we apply Theorem C.21 to ln\\nR\\ne−nrn(θ)g(θ) dθ, where ☞ 450\\nrn(θ) := −1\\nn ln g(Tn |θ) = −1\\nn\\nnX\\ni=1\\nln g(Xi |θ)\\na.s.\\n−→− Eln g(X |θ) =: r(θ) <∞.\\nThis gives (with probability one)\\nln\\nZ\\ng(Tn |θ) g(θ) dθ≃−nr(θ∗) −p\\n2 ln(n).\\nTaking expectations on both sides and usingnr(θ∗) = nE[rn(θn)] + O(1), we deduce (2.50).\\nTo demonstrate (2.49), we derive the asymptotic approximation ofEln g(T2n) by repeating\\nthe argument for (2.50), but replacing n with 2n, where necessary. Thus, we obtain:\\nEln g(T2n) ≃−2nr(θ∗) −p\\n2 ln(2n).\\nThen, (2.49) follows from the identity (2.48). □\\nThe results of Theorem 2.4 have two major implications for model selection and assess-\\nment. First, (2.49) suggests that −ln g(Tn) can be used as a crude (leading-order) asymp-\\ntotic approximation to the expected generalization risk for large n and fixed p. In this\\ncontext, the prior predictive densityg(Tn) is usually called the model evidence model evidenceor marginal\\nlikelihood for the class Gp. Since the integral\\nR\\ng(Tn |θ) g(θ) dθis rarely available in closed\\nform, the exact computation of the model evidence is typically not feasible and may require\\nMonte Carlo estimation methods. ☞ 78\\nSecond, when the model evidence is di fficult to compute via Monte Carlo methods or\\notherwise, (2.50) suggests that we can use the following large-sample approximation:\\n−2Eln g(Tn) ≃−2 lng(Tn |θn) + p ln(n). (2.51)\\nThe asymptotic approximation on the right-hand side of (2.51) is called the Bayesian in-\\nformation criterion Bayesian\\ninformation\\ncriterion\\n(BIC). We prefer the classGp with the smallest BIC. The BIC is typic-\\nally used when the model evidence is di fficult to compute and n is sufficiently larger than\\np. For a fixed p, and as n becomes larger and larger, the BIC becomes a more and more\\naccurate estimator of −2Eln g(Tn). Note that the BIC approximation is valid even when the\\ntrue density f < Gp. The BIC provides an alternative to the Akaike information criterion\\n(AIC) for model selection. However, while the BIC approximation does not assume that ☞ 126\\nthe true model f belongs to the parametric class under consideration, the AIC assumes\\nthat f ∈Gp. Thus, the AIC is merely a heuristic approximation based on the asymptotic\\napproximations in Theorem 4.1.\\nAlthough the above Bayesian theory has been presented in an unsupervised learn-\\ning setting, it can be readily extended to the supervised case. We only need to relabel\\nthe training set Tn. In particular, when (as is typical for regression models) the train-\\ning responses Y1,..., Yn are considered as random variables but the corresponding fea-\\nture vectors x1,..., xn are viewed as being fixed, then Tn is the collection of random re-\\nsponses {Y1,..., Yn}. Alternatively, we can simply identify Tn with the response vector\\nY = [Y1,..., Yn]⊤. We will adopt this notation in the next example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 73, 'page_label': '56'}, page_content='56 Bayesian Learning\\nExample 2.10 (Polynomial Regression (cont.)) Consider Example 2.2 once again, but\\nnow in a Bayesian framework, where the prior knowledge on ( σ2,β) is specified by\\ng(σ2) = 1/σ2 and β|σ2 ∼N(0,σ2D), and D is a (matrix) hyperparameter. Let Σ :=\\n(X⊤X + D−1)−1. Then the posterior can be written as:\\ng(β,σ2 |y) =\\nexp\\n\\x10\\n−∥y−Xβ∥2\\n2σ2\\n\\x11\\n(2πσ2)n/2 ×\\nexp\\n\\x10\\n−β⊤D−1β\\n2σ2\\n\\x11\\n(2πσ2)p/2 |D|1/2 × 1\\nσ2\\n,\\ng(y)\\n= (σ2)−(n+p)/2−1\\n(2π)(n+p)/2 |D|1/2 exp\\n \\n−∥Σ−1/2(β−β)∥2\\n2σ2 −(n + p + 2) σ2\\n2σ2\\n!,\\ng(y),\\nwhere β:= ΣX⊤y and σ2 := y⊤(I −XΣX⊤)y/(n + p + 2) are the MAP estimates of βand\\nσ2, and g(y) is the model evidence for Gp:\\ng(y) =\\n\"\\ng(β,σ2,y) dβdσ2\\n= |Σ|1/2\\n(2π)n/2|D|1/2\\nZ ∞\\n0\\nexp\\n\\x12\\n−(n+p+2) σ\\n2\\n2σ2\\n\\x13\\n(σ2)n/2+1 dσ2\\n= |Σ|1/2Γ(n/2)\\n|D|1/2(π(n + p + 2) σ2)n/2 .\\nTherefore, based on (2.49), we have\\n2Eℓ(gTn ) ≃−2 lng(y) = n ln\\nh\\nπ(n + p + 2) σ2i\\n−2 lnΓ(n/2) + ln |D|−ln |Σ|.\\nOn the other hand, the minus of the log-likelihood of Y can be written as\\n−ln g(y |β,σ2) = ∥y −Xβ∥2\\n2σ2 + n\\n2 ln(2πσ2)\\n= ∥Σ−1/2(β−β)∥2\\n2σ2 + (n + p + 2) σ2\\n2σ2 + n\\n2 ln(2πσ2).\\nTherefore, the BIC approximation (2.51) is\\n−2 lng(y |β,σ2) + (p + 1) ln(n) = n[ln(2πσ2) + 1] + (p + 1) ln(n) + (p + 2), (2.52)\\nwhere the extra ln( n) term in ( p + 1) ln(n) is due to the inclusion of σ2 in θ = (σ2,β).\\nFigure 2.15 shows the model evidence and its BIC approximation, where we used a hyper-\\nparameter D = 104 ×Ip for the prior density of β. We can see that both approximations\\nexhibit a pronounced minimum at p = 4, thus identifying the true polynomial regression\\nmodel. Compare the overall qualitative shape of the cross-entropy risk estimate with the\\nshape of the square-error risk estimate in Figure 2.11.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 74, 'page_label': '57'}, page_content='Statistical Learning 57\\n123456789 1 0\\n600\\n650\\n700\\n750\\n800\\nFigure 2.15: The BIC and marginal likelihood used for model selection.\\nIt is possible to give the model complexity parameter p a Bayesian treatment, in which\\nwe define a prior density on the set of all models under consideration. For example, let\\ng(p), p = 1,..., m be a prior density on m candidate models. Treating the model com-\\nplexity index p as an additional parameter to θ ∈Rp, and applying Bayes’ formula, the\\nposterior for (θ,p) can be written as:\\ng(θ,p |τ) = g(θ|p,τ) ×g(p |τ)\\n= g(τ|θ,p) g(θ|p)\\ng(τ|p)|               {z               }\\nposterior of θgiven model p\\n× g(τ|p) g(p)\\ng(τ)|        {z        }\\nposterior of model p\\n.\\nThe model evidence for a fixed p is now interpreted as the prior predictive density of τ,\\nconditional on the model p:\\ng(τ|p) =\\nZ\\ng(τ|θ,p) g(θ|p) dθ,\\nand the quantity g(τ) = Pm\\np=1 g(τ|p) g(p) is interpreted as the marginal likelihood of all the\\nm candidate models. Finally, a simple method for model selection is to pick the index bp\\nwith the largest posterior probability:\\nbp = argmax\\np\\ng(p |τ) = argmax\\np\\ng(τ|p) g(p).\\nExample 2.11 (Polynomial Regression (cont.)) Let us revisit Example 2.10 by giving\\nthe parameter p = 1,..., m, with m = 10, a Bayesian treatment. Recall that we used the\\nnotation τ= y in that example. We assume that the prior g(p) = 1/m is flat and uninform-\\native so that the posterior is given by\\ng(p |y) ∝g(y |p) = |Σ|1/2 Γ(n/2)\\n|D|1/2(π(n + p + 2) σ2)n/2 ,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 75, 'page_label': '58'}, page_content='58 Bayesian Learning\\nwhere all quantities in g(y |p) are computed using the first p columns of X. Figure 2.16\\nshows the resulting posterior density g(p |y). The figure also shows the posterior density\\nbg(y |p)\\x0eP10\\np=1 bg(y |p),where\\nbg(y |p) := exp\\n \\n−n[ln(2πσ2) + 1] + (p + 1) ln(n) + (p + 2)\\n2\\n!\\nis derived from the BIC approximation (2.52). In both cases, there is a clear maximum at\\np = 4, suggesting that a third-degree polynomial is the most appropriate model for the\\ndata.\\n1 2 3 4 5 6 7 8 9 10\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 2.16: Posterior probabilities for each polynomial model of degree p −1.\\nSuppose that we wish to compare two models, say model p = 1 and model p = 2.\\nInstead of computing the posterior g(p |τ) explicitly, we can compare the posterior odds\\nratio:\\ng(p = 1 |τ)\\ng(p = 2 |τ) = g(p = 1)\\ng(p = 2) ×g(τ|p = 1)\\ng(τ|p = 2)|        {z        }\\nBayes factor B1 |2\\n.\\nThis gives rise to the Bayes factorBayes factor Bi |j, whose value signifies the strength of the evidence\\nin favor of model i over model j. In particular Bi |j >1 means that the evidence in favor for\\nmodel i is larger.\\nExample 2.12 (Savage–Dickey Ratio) Suppose that we have two models. Model p =\\n2 has a likelihood g(τ|µ,ν, p = 2), depending on two parameters. Model p = 1 has the\\nsame functional form for the likelihood but now ν is fixed to some (known) ν0; that\\nis, g(τ|µ,p = 1) = g(τ|µ,ν = ν0,p = 2). We also assume that the prior information on µ'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 76, 'page_label': '59'}, page_content='Statistical Learning 59\\nfor model 1 is the same as that for model 2, conditioned on ν= ν0. That is, we assume\\ng(µ|p = 1) = g(µ|ν= ν0,p = 2). As model 2 contains model 1 as a special case, the latter\\nis said to be nested inside model 2. We can formally write (see also Exercise 26):\\ng(τ|p = 1) =\\nZ\\ng(τ|µ,p = 1) g(µ|p = 1) dµ\\n=\\nZ\\ng(τ|µ,ν = ν0,p = 2) g(µ|ν= ν0,p = 2) dµ\\n= g(τ|ν= ν0,p = 2) = g(τ,ν = ν0 |p = 2)\\ng(ν= ν0 |p = 2) .\\nHence, the Bayes factor simplifies to\\nB1 |2 = g(τ|p = 1)\\ng(τ|p = 2) = g(τ,ν = ν0 |p = 2)\\ng(ν= ν0 |p = 2)\\n\\x1e\\ng(τ|p = 2) = g(ν= ν0 |τ,p = 2)\\ng(ν= ν0 |p = 2) .\\nIn other words, B1 |2 is the ratio of the posterior density to the prior density ofν, evaluated at\\nν= ν0 and both under the unrestricted modelp = 2. This ratio of posterior to prior densities\\nis called the Savage–Dickey density ratio Savage–Dickey\\ndensity ratio\\n.\\nWhether to use a classical (frequentist) or Bayesian model is largely a question of con-\\nvenience. Classical inference is useful because it comes with a huge repository of ready-\\nto-use results, and requires no (subjective) prior information on the parameters. Bayesian\\nmodels are useful because the whole theory is based on the elegant Bayes’ formula, and\\nuncertainty in the inference (e.g., confidence intervals) can be quantified much more nat-\\nurally (e.g., credible intervals). A usual practice is to “Bayesify” a classical model, simply\\nby adding some prior information on the parameters.\\nFurther Reading\\nA popular textbook on statistical learning is [55]. Accessible treatments of mathematical\\nstatistics can be found, for example, in [69], [74], and [124]. More advanced treatments\\nare given in [10], [25], and [78]. A good overview of modern-day statistical inference\\nis given in [36]. Classical references on pattern classification and machine learning are\\n[12] and [35]. For advanced learning theory including information theory and Rademacher\\ncomplexity, we refer to [28] and [109]. An applied reference for Bayesian inference is [46].\\nFor a survey of numerical techniques relevant to computational statistics, see [90].\\nExercises\\n1. Suppose that the loss function is the piecewise linear function\\nLoss(y,by) = α(by −y)+ + β(y −by)+, α,β> 0,\\nwhere c+ is equal to c if c > 0, and zero otherwise. Show that the minimizer of the risk\\nℓ(g) = ELoss(Y,g(X)) satisfies\\nP[Y <g∗(x) |X = x] = β\\nα+ β.\\nIn other words, g∗(x) is the β/(α+ β) quantile of Y, conditional on X = x.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 77, 'page_label': '60'}, page_content='60 Exercises\\n2. Show that, for the squared-error loss, the approximation error ℓ(gG) −ℓ(g∗) in (2.16), is\\nequal to E(gG(X) −g∗(X))2. [Hint: expand ℓ(gG) = E(Y −g∗(X) + g∗(X) −gG(X))2.]\\n3. Suppose Gis the class of linear functions. A linear function evaluated at a featurex can\\nbe described as g(x) = β⊤x for some parameter vector βof appropriate dimension. Denote\\ngG(x) = x⊤βGand gG\\nτ(x) = x⊤bβ. Show that\\nE\\n\\x10\\ngG\\nτ(X) −g∗(X)\\n\\x112\\n= E\\n\\x10\\nX⊤bβ−X⊤βG\\x112\\n+ E\\n\\x10\\nX⊤βG−g∗(X)\\n\\x112\\n.\\nHence, deduce that the statistical error in (2.16) is ℓ(gG\\nτ) −ℓ(gG) = E(gG\\nτ(X) −gG(X))2.\\n4. Show that formula (2.24) holds for the 0–1 loss with 0–1 response.\\n5. Let X be an n-dimensional normal random vector with mean vector µand covariance\\nmatrix Σ, where the determinant ofΣ is non-zero. Show thatX has joint probability density\\nfX(x) = 1√(2π)n |Σ|\\ne−1\\n2 (x−µ)⊤Σ−1(x−µ), x ∈Rn.\\n6. Let bβ = A+y. Using the defining properties of the pseudo-inverse, show that for any☞360\\nβ∈Rp,\\n∥Abβ−y∥⩽∥Aβ−y∥.\\n7. Suppose that in the polynomial regression Example 2.1 we select the linear class of\\nfunctions Gp with p ⩾4. Then, g∗∈Gp and the approximation error is zero, because\\ngGp (x) = g∗(x) = x⊤β, where β = [10,−140,400,−250,0,..., 0]⊤ ∈Rp. Use the tower\\nproperty to show that the learner gτ(x) = x⊤bβwith bβ = X+y, assuming rank( X) ⩾4, is☞431\\nunbiasedunbiased :\\nEgT(x) = g∗(x).\\n8. (Exercise 7 continued.) Observe that the learner gT can be written as a linear combina-\\ntion of the response variable:gT(x) = x⊤X+Y. Prove that for any learner of the formx⊤Ay,\\nwhere A ∈Rp×n is some matrix and that satisfies EX[x⊤AY] = g∗(x), we have\\nVarX[x⊤X+Y] ⩽VarX[x⊤AY],\\nwhere the equality is achieved for A = X+. This is called the Gauss–Markov inequalityGauss–Markov\\ninequality\\n.\\nHence, using the Gauss–Markov inequality deduce that for the unconditional variance:\\nVar gT(x) ⩽Var[x⊤AY].\\nDeduce that A = X+ also minimizes the expected generalization risk.\\n9. Consider again the polynomial regression Example 2.1. Use the fact thatEX bβ= X+h∗(u),\\nwhere h∗(u) = E[Y |U = u] = [h∗(u1),..., h∗(un)]⊤, to show that the expected in-sample\\nrisk is:\\nEX ℓin(gT) = ℓ∗+ ∥h∗(u)∥2 −∥XX+h∗(u)∥2\\nn + ℓ∗p\\nn .\\nAlso, use Theorem C.2 to show that the expected statistical error is:☞430\\nEX (bβ−β)⊤Hp(bβ−β) = ℓ∗tr(X+(X+)⊤Hp) + (X+h∗(u) −β)⊤Hp(X+h∗(u) −β).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 78, 'page_label': '61'}, page_content='Statistical Learning 61\\n10. Consider the setting of the polynomial regression in Example 2.2. Use Theorem C.19\\nto prove that ☞ 449\\n√n (bβn −βp)\\nd\\n−→N\\n\\x10\\n0, ℓ∗H−1\\np + H−1\\np MpH−1\\np\\n\\x11\\n, (2.53)\\nwhere Mp := E[XX⊤(g∗(X) −gGp (X))2] is the matrix with (i, j)-th entry:\\nZ 1\\n0\\nui+ j−2(hHp (u) −h∗(u))2 du,\\nand H−1\\np is the p ×p inverse Hilbert matrix inverse Hilbert\\nmatrix\\nwith (i, j)-th entry:\\n(−1)i+ j(i + j −1)\\n p + i −1\\np −j\\n! p + j −1\\np −i\\n! i + j −2\\ni −1\\n!2\\n.\\nObserve that Mp = 0 for p ⩾4, so that the matrix Mp term is due to choosing a restrictive\\nclass Gp that does not contain the true prediction function.\\n11. In Example 2.2 we saw that the statistical error can be expressed (see (2.20)) as\\nZ 1\\n0\\n\\x10\\n[1,..., up−1](bβ−βp)\\n\\x112\\ndu = (bβ−βp)⊤Hp(bβ−βp).\\nBy Exercise 10 the random vector Zn := √n(bβn −βp) has asymptotically a multivariate\\nnormal distribution with mean vector 0 and covariance matrix V := ℓ∗H−1\\np + H−1\\np MpH−1\\np .\\nUse Theorem C.2 to show that the expected statistical error is asymptotically ☞ 430\\nE(bβ−βp)⊤Hp(bβ−βp) ≃ℓ∗p\\nn +\\ntr(MpH−1\\np )\\nn , n →∞. (2.54)\\nPlot this large-sample approximation of the expected statistical error and compare it with\\nthe outcome of the statistical error.\\nWe note a subtle technical detail: In general, convergence in distribution does not imply\\nconvergence in Lp-norm (see Example C.6), and so here we have implicitly assumed that ☞ 442\\n∥Zn∥\\nd\\n−→Dist.⇒∥Zn∥\\nL2\\n−→constant := limn↑∞E∥Zn∥.\\n12. Consider again Example 2.2. The result in (2.53) suggests that Ebβ →βp as n →∞,\\nwhere βp is the solution in the class Gp given in (2.18). Thus, the large-sample approxim-\\nation of the pointwise bias of the learner g\\nGp\\nT (x) = x⊤bβat x = [1,..., up−1]⊤is\\nEg\\nGp\\nT (x) −g∗(x) ≃[1,..., up−1] βp −[1,u,u2,u3] β∗, n →∞.\\nUse Python to reproduce Figure 2.17, which shows the (large-sample) pointwise squared\\nbias of the learner for p ∈{1,2,3}. Note how the bias is larger near the endpoints u = 0\\nand u = 1. Explain why the areas under the curves correspond to the approximation errors.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 79, 'page_label': '62'}, page_content='62 Exercises\\n0 0.2 0.4 0.6 0.8 1\\n0\\n50\\n100\\n150\\n200\\n250\\nFigure 2.17: The large-sample pointwise squared bias of the learner for p = 1,2,3. The\\nbias is zero for p ⩾4.\\n13. For our running Example 2.2 we can use (2.53) to derive a large-sample approximation\\nof the pointwise variance of the learner gT(x) = x⊤bβn. In particular, show that for large n\\nVar gT(x) ≃\\nℓ∗x⊤H−1\\np x\\nn +\\nx⊤H−1\\np MpH−1\\np x\\nn , n →∞. (2.55)\\nFigure 2.18 shows this (large-sample) variance of the learner for di fferent values of the\\npredictor u and model index p. Observe that the variance ultimately increases in p and that\\nit is smaller at u = 1/2 than closer to the endpoints u = 0 or u = 1. Since the bias is also\\n9\\n7\\n5\\n1\\n2\\n0.05\\n3\\n3\\n4\\n0.5 10.95\\nFigure 2.18: The pointwise variance of the learner for various pairs of p and u.\\nlarger near the endpoints, we deduce that the pointwise mean squared error (2.21) is larger\\nnear the endpoints of the interval [0 ,1] than near its middle. In other words, the error is\\nmuch smaller in the center of the data cloud than near its periphery.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 80, 'page_label': '63'}, page_content='Statistical Learning 63\\n14. Let h : x 7→Rbe a convex function and let X be a random variable. Use the subgradi-\\nent definition of convexity to prove Jensen’s inequality: ☞ 403\\nJensen’s\\ninequalityEh(X) ⩾h(EX). (2.56)\\n15. Using Jensen’s inequality, show that the Kullback–Leibler divergence between prob-\\nability densities f and g is always positive; that is,\\nEln f (X)\\ng(X) ⩾0,\\nwhere X ∼f .\\n16. The purpose of this exercise is to prove the followingVapnik–Chernovenkis bound Vapnik–\\nChernovenkis\\nbound\\n: for\\nany finite class G(containing only a finite number |G|of possible functions) and a general\\nbounded loss function, l ⩽Loss ⩽u, the expected statistical error is bounded from above\\naccording to:\\nEℓ(gG\\nTn\\n) −ℓ(gG) ⩽(u −l) √2 ln(2|G|)√n . (2.57)\\nNote how this bound conveniently does not depend on the distribution of the training set\\nTn (which is typically unknown), but only on the complexity (i.e., cardinality) of the class\\nG. We can break up the proof of (2.57) into the following four parts:\\n(a) For a general function class G, training set T, risk function ℓ, and training loss ℓT,\\nwe have, by definition, ℓ(gG) ⩽ℓ(g) and ℓT(gG\\nT) ⩽ℓT(g) for all g ∈G. Show that\\nℓ(gG\\nT) −ℓ(gG) ⩽sup\\ng∈G\\n|ℓT(g) −ℓ(g)|+ ℓT(gG) −ℓ(gG),\\nwhere we used the notation sup (supremum) for the least upper bound. Since\\nEℓT(g) = Eℓ(g), we obtain, after taking expectations on both sides of the inequal-\\nity above:\\nEℓ(gG\\nT) −ℓ(gG) ⩽Esup\\ng∈G\\n|ℓT(g) −ℓ(g)|.\\n(b) If X is a zero-mean random variable taking values in the interval [ l,u], then the fol-\\nlowing Hoeffding’s inequality Hoeffding’s\\ninequality\\nstates that the moment generating function satisfies\\nEetX ⩽exp\\n t2(u −l)2\\n8\\n!\\n, t ∈R. (2.58)\\nProve this result by using the fact that the line segment joining points ( l,exp(tl)) and\\n(u,exp(tu)) bounds the convex function x 7→exp(tx) for x ∈[l,u]; that is:\\netx ⩽etl u −x\\nu −l + etu x −l\\nu −l, x ∈[l,u].\\n(c) Let Z1,..., Zn be (possibly dependent and non-identically distributed) zero-mean ran-\\ndom variables with moment generating functions that satisfyEexp(tZk) ⩽exp(t2η2/2)\\nfor all k and some parameter η. Use Jensen’s inequality (2.56) to prove that for any ☞ 427'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 81, 'page_label': '64'}, page_content='64 Exercises\\nt >0,\\nEmax\\nk\\nZk = 1\\nt Eln max\\nk\\netZk ⩽1\\nt ln n + tη2\\n2 .\\nFrom this derive that\\nEmax\\nk\\nZk ⩽η\\n√\\n2 lnn.\\nFinally, show that this last inequality implies that\\nEmax\\nk\\n|Zk|⩽η\\np\\n2 ln(2n). (2.59)\\n(d) Returning to the objective of this exercise, denote the elements of Gby g1,..., g|G|,\\nand let Zk = ℓTn (gk) −ℓ(gk). By part (a) it is sufficient to bound Emaxk |Zk|. Show that\\nthe {Zk}satisfy the conditions of (c) with η = (u −l)/√n. For this you will need to\\napply part (b) to the random variable Loss(g(X),Y) −ℓ(g), where (X,Y) is a generic\\ndata point. Now complete the proof of (2.57).\\n17. Consider the problem in Exercise 16a above. Show that\\n|ℓT(gG\\nT) −ℓ(gG)|⩽2 sup\\ng∈G\\n|ℓT(g) −ℓ(g)|+ ℓT(gG) −ℓ(gG).\\nFrom this, conclude:\\nE|ℓT(gG\\nT) −ℓ(gG)|⩽2Esup\\ng∈G\\n|ℓT(g) −ℓ(g)|.\\nThe last bound allows us to assess how close the training loss ℓT(gG\\nT) is to the optimal risk\\nℓ(gG) within class G.\\n18. Show that for the normal linear model Y ∼N(Xβ,σ2In), the maximum likelihood es-\\ntimator of σ2 is identical to the method of moments estimator (2.37).\\n19. Let X ∼Gamma(α,λ). Show that the pdf of Z = 1/X is equal to\\nλα(z)−α−1e−λ(z)−1\\nΓ(α) , z >0.\\n20. Consider the sequence w0,w1,... , where w0 = g(θ) is a non-degenerate initial guess\\nand wt(θ) ∝wt−1(θ)g(τ|θ), t >1. We assume thatg(τ|θ) is not the constant function (with\\nrespect to θ) and that the maximum likelihood value\\ng(τ|bθ) = max\\nθ\\ng(τ|θ) <∞\\nexists (is bounded). Let\\nlt :=\\nZ\\ng(τ|θ)wt(θ) dθ.\\nShow that {lt}is a strictly increasing and bounded sequence. Hence, conclude that its limit\\nis g(τ|bθ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 82, 'page_label': '65'}, page_content='Statistical Learning 65\\n21. Consider the Bayesian model for τ= {x1,..., xn}with likelihood g(τ|µ) such that\\n(X1,..., Xn |µ) ∼iid N(µ,1) and prior pdf g(µ) such that µ∼N(ν,1) for some hyperpara-\\nmeter ν. Define a sequence of densities wt(µ),t ⩾2 via wt(µ) ∝wt−1(µ) g(τ|µ), start-\\ning with w1(µ) = g(µ). Let at and bt denote the mean and precision 4 of µ under the\\nposterior gt(µ|τ) ∝g(τ|µ)wt(µ). Show that gt(µ|τ) is a normal density with precision\\nbt = bt−1 + n, b0 = 1 and mean at = (1 −γt)at−1 + γt xn, a0 = ν, where γt := n/(bt−1 + n).\\nHence, deduce that gt(µ|τ) converges to a degenerate density with a point-mass at xn.\\n22. Consider again Example 2.8, where we have a normal model with improper prior\\ng(θ) = g(µ,σ2) ∝1/σ2. Show that the prior predictive pdf is an improper densityg(x) ∝1,\\nbut that the posterior predictive density is\\ng(x |τ) ∝\\n \\n1 + (x −xn)2\\n(n + 1)S 2\\nn\\n!−n/2\\n.\\nDeduce that X−xn\\nS n\\n√(n+1)/(n−1) ∼tn−1.\\n23. Assuming that X1,..., Xn\\niid\\n∼f , show that (2.48) holds and that ℓ∗\\nn = −n Eln f (X).\\n24. Suppose that τ = {x1,..., xn}are observations of iid continuous and strictly positive\\nrandom variables, and that there are two possible models for their pdf. The first model\\np = 1 is\\ng(x |θ,p = 1) = θexp (−θx)\\nand the second p = 2 is\\ng(x |θ,p = 2) =\\n 2θ\\nπ\\n!1/2\\nexp\\n \\n−θx2\\n2\\n!\\n.\\nFor both models, assume that the prior for θis a gamma density\\ng(θ) = bt\\nΓ(t)θt−1 exp (−bθ) ,\\nwith the same hyperparameters b and t. Find a formula for the Bayes factor, g(τ|p =\\n1)/g(τ|p = 2), for comparing these models.\\n25. Suppose that we have a total of m possible models with prior probabilities g(p),p =\\n1,..., m. Show that the posterior probability of model g(p |τ) can be expressed in terms of\\nall the p(p −1) Bayes factors:\\ng(p = i |τ) =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 +\\nX\\nj,i\\ng(p = j)\\ng(p = i) Bj |i\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n−1\\n.\\n4The precision is the reciprocal of the variance.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 83, 'page_label': '66'}, page_content='66 Exercises\\n26. Given the data τ = {x1,..., xn}, suppose that we use the likelihood ( X |θ) ∼N(µ,σ2)\\nwith parameter θ= (µ,σ2)⊤and wish to compare the following two nested models.\\n(a) Model p = 1, where σ2 = σ2\\n0 is known and this is incorporated via the prior\\ng(θ|p = 1) = g(µ|σ2,p = 1) g(σ2 |p = 1) = 1√\\n2πσ\\ne−(µ−x0)2\\n2σ2 ×δ(σ2 −σ2\\n0).\\n(b) Model p = 2, where both mean and variance are unknown with prior\\ng(θ|p = 2) = g(µ|σ2) g(σ2) = 1√\\n2πσ\\ne−(µ−x0)2\\n2σ2 ×bt(σ2)−t−1e−b/σ2\\nΓ(t) .\\nShow that the prior g(θ|p = 1) can be viewed as the limit of the prior g(θ|p = 2) when\\nt →∞ and b = tσ2\\n0. Hence, conclude that\\ng(τ|p = 1) = lim\\nt→∞\\nb=tσ2\\n0\\ng(τ|p = 2)\\nand use this result to calculateB1 |2. Check that the formula forB1 |2 agrees with the Savage–\\nDickey density ratio:\\ng(τ|p = 1)\\ng(τ|p = 2) = g(σ2 = σ2\\n0 |τ)\\ng(σ2 = σ2\\n0) ,\\nwhere g(σ2 |τ) and g(σ2) are the posterior and prior, respectively, under model p = 2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 84, 'page_label': '67'}, page_content='CHAPTER 3\\nMONTE CARLO METHODS\\nMany algorithms in machine learning and data science make use of Monte Carlo\\ntechniques. This chapter gives an introduction to the three main uses of Monte Carlo\\nsimulation: to (1) simulate random objects and processes in order to observe their beha-\\nvior, (2) estimate numerical quantities by repeated sampling, and (3) solve complicated\\noptimization problems through randomized algorithms.\\n3.1 Introduction\\nBriefly put, Monte Carlo simulation Monte Carlo\\nsimulation\\nis the generation of random data by means of a com-\\nputer. These data could arise from simple models, such as those described in Chapter 2,\\nor from very complicated models describing real-life systems, such as the positions of\\nvehicles on a complex road network, or the evolution of security prices in the stock mar-\\nket. In many cases, Monte Carlo simulation simply involves random sampling from certain\\nprobability distributions. The idea is to repeat the random experiment that is described by\\nthe model many times to obtain a large quantity of data that can be used to answer questions\\nabout the model. The three main uses of Monte Carlo simulation are:\\nSampling. Here the objective is to gather information about a random object by observing\\nmany realizations of it. For instance, this could be a random process that mimics the\\nbehavior of some real-life system such as a production line or telecommunications\\nnetwork. Another usage is found in Bayesian statistics, where Markov chains are\\noften used to sample from a posterior distribution. ☞ 49\\nEstimation. In this case the emphasis is on estimating certain numerical quantities related\\nto a simulation model. An example is the evaluation of multidimensional integrals\\nvia Monte Carlo techniques. This is achieved by writing the integral as the expecta-\\ntion of a random variable, which is then approximated by the sample mean. Appeal-\\ning to the Law of Large Numbers guarantees that this approximation will eventually ☞ 446\\nconverge when the sample size becomes large.\\nOptimization. Monte Carlo simulation is a powerful tool for the optimization of complic-\\nated objective functions. In many applications these functions are deterministic and\\n67'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 85, 'page_label': '68'}, page_content='68 Monte Carlo Sampling\\nrandomness is introduced artificially in order to more efficiently search the domain of\\nthe objective function. Monte Carlo techniques are also used to optimize noisy func-\\ntions, where the function itself is random; for example, when the objective function\\nis the output of a Monte Carlo simulation.\\nThe Monte Carlo method dramatically changed the way in which statistics is used in\\ntoday’s analysis of data. The ever-increasing complexity of data requires radically different\\nstatistical models and analysis techniques from those that were used 20 to 100 years ago.\\nBy using Monte Carlo techniques, the data analyst is no longer restricted to using basic\\n(and often inappropriate) models to describe data. Now, any probabilistic model that can\\nbe simulated on a computer can serve as the basis for statistical analysis. This Monte Carlo\\nrevolution has had an impact on both Bayesian and frequentist statistics. In particular, in\\nfrequentist statistics, Monte Carlo methods are often referred to as resampling techniques.\\nAn important example is the well-known bootstrap method [37], where statistical quantit-\\nies such as confidence intervals and P-values for statistical tests can simply be determined\\nby simulation without the need of a sophisticated analysis of the underlying probability\\ndistributions; see, for example, [69] for basic applications. The impact on Bayesian statist-\\nics has been even more profound, through the use of Markov chain Monte Carlo (MCMC)\\ntechniques [87, 48]. MCMC samplers construct a Markov process which converges in dis-\\ntribution to a desired (often high-dimensional) density. This convergence in distribution\\njustifies using a finite run of the Markov process as an approximate random realization\\nfrom the target density. The MCMC approach has rapidly gained popularity as a versat-\\nile heuristic approximation, partly due to its simple computer implementation and inbuilt\\nmechanism to tradeoff between computational cost and accuracy; namely, the longer one\\nruns the Markov process, the better the approximation. Nowadays, MCMC methods are\\nindispensable for analyzing posterior distributions for inference and model selection; see\\nalso [50, 99].\\nThe following three sections elaborate on these three uses of Monte Carlo simulation\\nin turn.\\n3.2 Monte Carlo Sampling\\nIn this section we describe a variety of Monte Carlo sampling methods, from the building\\nblock of simulating uniform random numbers to MCMC samplers.\\n3.2.1 Generating Random Numbers\\nAt the heart of any Monte Carlo method is a random number generator: a procedure thatrandom number\\ngenerator produces a stream of uniform random numbers on the interval (0,1). Since such numbers\\nare usually produced via deterministic algorithms, they are not truly random. However, for\\nmost applications all that is required is that such pseudo-random numbers are statistically\\nindistinguishable from genuine random numbers U1,U2,... that are uniformly distributed\\non the interval (0,1) and are independent of each other; we write U1,U2,... ∼iid U(0,1).\\nFor example, in Python the rand method of the numpy.random module is widely used for\\nthis purpose.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 86, 'page_label': '69'}, page_content='Monte Carlo Methods 69\\nMost random number generators at present are based on linear recurrence relations.\\nOne of the most important random number generators is the multiple-recursive generator multiple-\\nrecursive\\ngenerator(MRG) of order k, which generates a sequence of integers Xk,Xk+1,... via the linear recur-\\nrence\\nXt = (a1Xt−1 + ··· + akXt−k) mod m, t = k,k + 1,... (3.1)\\nfor some modulus m and multipliers {ai,i = 1,..., k}. Here “mod” refers to the modulo op- modulus\\nmultiplierseration: n mod m is the remainder when n is divided by m. The recurrence is initialized by\\nspecifying k “seeds”, X0,..., Xk−1. To yield fast algorithms, all but a few of the multipliers\\nshould be 0. When m is a large integer, one can obtain a stream of pseudo-random numbers\\nUk,Uk+1,... between 0 and 1 from the sequence Xk,Xk+1,... , simply by setting Ut = Xt/m.\\nIt is also possible to set a small modulus, in particular m = 2. The output function for such\\nmodulo 2 generators is then typically of the form modulo 2\\ngenerators\\nUt =\\nwX\\ni=1\\nXtw+i−12−i\\nfor some w ⩽k, e.g., w = 32 or 64. Examples of modulo 2 generators are thefeedback shift\\nregister generators, the most popular of which are theMersenne twisters; see, for example, feedback shift\\nregister\\nMersenne\\ntwisters\\n[79] and [83]. MRGs with excellent statistical properties can be implemented e fficiently\\nby combining several simpler MRGs and carefully choosing their respective moduli and\\nmultipliers. One of the most successful is L’Ecuyer’sMRG32k3agenerator; see [77]. From\\nnow on, we assume that the reader has a sound random number generator available.\\n3.2.2 Simulating Random Variables\\nSimulating a random variable X from an arbitrary (that is, not necessarily uniform) distri-\\nbution invariably involves the following two steps:\\n1. Simulate uniform random numbers U1,..., Uk on (0,1) for some k = 1,2,... .\\n2. Return X = g(U1,..., Uk), where g is some real-valued function.\\nThe construction of suitable functions g is as much of an art as a science. Many\\nsimulation methods may be found, for example, in [71] and the accompanying website\\nwww.montecarlohandbook.org. Two of the most useful general procedures for gen-\\nerating random variables are the inverse-transform method and the acceptance–rejection\\nmethod. Before we discuss these, we show one possible way to simulate standard normal\\nrandom variables. In Python we can generate standard normal random variables via the\\nrandn method of the numpy.random module.\\nExample 3.1 (Simulating Standard Normal Random Variables) If X and Y are in-\\ndependent standard normally distributed random variables (that is, X,Y ∼iid N(0,1)), then\\ntheir joint pdf is\\nf (x,y) = 1\\n2πe−1\\n2 (x2+y2), (x,y) ∈R2,\\nwhich is a radially symmetric function. In Example C.2 we see that, in polar coordin- ☞ 433\\nates, the angle Θ that the random vector [X,Y]⊤makes with the positive x-axis is U(0,2π)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 87, 'page_label': '70'}, page_content='70 Monte Carlo Sampling\\ndistributed (as would be expected from the radial symmetry) and the radius R has pdf\\nfR(r) = r e−r2/2,r >0. Moreover, R and Θ are independent. We will see shortly, in Ex-\\nample 3.4, that R has the same distribution as\\n√\\n−2 lnU with U ∼U(0,1). So, to sim-☞72\\nulate X,Y ∼iid N(0,1), the idea is to first simulate R and Θ independently and then return\\nX = R cos(Θ) and Y = R sin(Θ) as a pair of independent standard normal random variables.\\nThis leads to the Box–Muller approach for generating standard normal random variables.\\nAlgorithm 3.2.1: Normal Random Variable Simulation: Box–Muller Approach\\noutput: Independent standard normal random variables X and Y.\\n1 Simulate two independent random variables, U1 and U2, from U(0,1).\\n2 X ←(−2 lnU1)1/2 cos(2πU2)\\n3 Y ←(−2 lnU1)1/2 sin(2πU2)\\n4 return X,Y\\nOnce a standard normal number generator is available, simulation from any n-\\ndimensional normal distribution N(µ,Σ) is relatively straightforward. The first step is to\\nfind an n ×n matrix B that decomposes Σ into the matrix product BB⊤. In fact there exist\\nmany such decompositions. One of the more important ones is theCholesky decomposition,Cholesky\\ndecomposition which is a special case of the LU decomposition; see Section A.6.1 for more information\\n☞368 on such decompositions. In Python, the functioncholesky of numpy.linalg can be used\\nto produce such a matrix B.\\nOnce the Cholesky factorization is determined, it is easy to simulate X ∼N(µ,Σ) as,\\nby definition, it is the a ffine transformation µ+ BZ of an n-dimensional standard normal\\nrandom vector.\\nAlgorithm 3.2.2: Normal Random Vector Simulation\\ninput: µ,Σ\\noutput: X ∼N(µ,Σ)\\n1 Determine the Cholesky factorization Σ = BB⊤.\\n2 Simulate Z = [Z1,..., Zn]⊤by drawing Z1,..., Zn ∼iid N(0,1).\\n3 X ←µ+ BZ\\n4 return X\\nExample 3.2 (Simulating from a Bivariate Normal Distribution) The Python code\\nbelow draws N = 1000 iid samples from the two bivariate ( n = 2) normal pdfs in Fig-\\nure 2.13. The resulting point clouds are given in Figure 3.1.☞46\\nbvnormal.py\\nimport numpy as np\\nfrom numpy.random import randn\\nimport matplotlib.pyplot as plt\\nN = 1000\\nr = 0.0 #change to 0.8 for other plot\\nSigma = np.array([[1, r], [r, 1]])'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 88, 'page_label': '71'}, page_content='Monte Carlo Methods 71\\nB = np.linalg.cholesky(Sigma)\\nx = B @ randn(2,N)\\nplt.scatter([x[0,:]],[x[1,:]], alpha =0.4, s = 4)\\n2\\n 0\\n 2\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n2\\n 0\\n 2\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nFigure 3.1: 1000 realizations of bivariate normal distributions with means zero, variances\\n1, and correlation coefficients 0 (left) and 0.8 (right).\\nIn some cases, the covariance matrix Σ has special structure which can be exploited to\\ncreate even faster generation algorithms, as illustrated in the following example.\\nExample 3.3 (Simulating Normal Vectors in O(n2) Time) Suppose that the random\\nvector X = [X1,..., Xn]⊤represents the values at times t0 + kδ, k = 0,..., n −1 of a zero-\\nmean Gaussian process(X(t),t ⩾0) that isweakly stationary, meaning thatCov(X(s),X(t)) ☞ 238\\ndepends only ont−s. Then clearly the covariance matrix ofX, say An, is a symmetric Toep-\\nlitz matrix. Suppose for simplicity that Var X(t) = 1. Then the covariance matrix is in fact ☞ 379\\na correlation matrix, and will have the following structure:\\nAn :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 a1 ... an−2 an−1\\na1 1 ... an−2\\n... ... ... ... ...\\nan−2\\n... ... a1\\nan−1 an−2 ··· a1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nUsing the Levinson–Durbin algorithm we can compute a lower diagonal matrix Ln and\\na diagonal matrix Dn in O(n2) time such that Ln An L⊤\\nn = Dn; see Theorem A.14. If we ☞ 383\\nsimulate Zn ∼N(0,In), then the solution X of the linear system:\\nLn X = D1/2\\nn Zn\\nhas the desired distribution N(0,An). The linear system is solved inO(n2) time via forward\\nsubstitution.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 89, 'page_label': '72'}, page_content='72 Monte Carlo Sampling\\n3.2.2.1 Inverse-Transform Method\\nLet X be a random variable with cumulative distribution function (cdf) F. Let F−1 denote\\nthe inverse1 of F and U ∼U(0,1). Then,\\nP[F−1(U) ⩽x] = P[U ⩽F(x)] = F(x). (3.2)\\nThis leads to the following method to simulate a random variable X with cdf F:\\nAlgorithm 3.2.3: Inverse-Transform Method\\ninput: Cumulative distribution function F.\\noutput: Random variable X distributed according to F.\\n1 Generate U from U(0,1).\\n2 X ←F−1(U)\\n3 return X\\nThe inverse-transform method works both for continuous and discrete distribu-\\ntions. After importing numpy as np, simulating numbers 0 ,..., k −1 according to\\nprobabilities p0,..., pk−1 can be done via np.min(np.where(np.cumsum(p) >\\nnp.random.rand())), where pis the vector of the probabilities.\\nExample 3.4 (Example 3.1 (cont.)) One remaining issue in Example 3.1 was how to\\nsimulate the radius R when we only know its density fR(r) = r e−r2/2,r >0. We can use the\\ninverse-transform method for this, but first we need to determine its cdf. The cdf of R is,\\nby integration of the pdf,\\nFR(r) = 1 −e−1\\n2 r2\\n, r >0,\\nand its inverse is found by solving u = FR(r) in terms of r, giving\\nF−1\\nR (u) =\\np\\n−2 ln(1 −u), u ∈(0,1).\\nThus R has the same distribution as √−2 ln(1 −U), with U ∼U(0,1). Since 1−U also has\\na U(0,1) distribution, R has also the same distribution as\\n√\\n−2 lnU.\\n3.2.2.2 Acceptance–Rejection Method\\nThe acceptance–rejection method is used to sample from a “di fficult” probability density\\nfunction (pdf) f (x) by generating instead from an “easy” pdf g(x) satisfying f (x) ⩽C g(x)\\nfor some constant C ⩾1 (for example, via the inverse-transform method), and then ac-\\ncepting or rejecting the drawn sample with a certain probability. Algorithm 3.2.4 gives the\\npseudo-code.\\nThe idea of the algorithm is to generate uniformly a point (X,Y) under the graph of the\\nfunction Cg, by first drawing X ∼g and then Y ∼U(0,Cg(X)). If this point lies under the\\ngraph of f , then we accept X as a sample from f ; otherwise, we try again. The e fficiency\\nof the acceptance–rejection method is usually expressed in terms of the probability of\\nacceptance, which is 1/C.\\n1Every cdf has a unique inverse function defined by F−1(u) = inf{x : F(x) ⩾u}. If, for each u, the\\nequation F(x) = u has a unique solution x, this definition coincides with the usual interpretation of the\\ninverse function.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 90, 'page_label': '73'}, page_content='Monte Carlo Methods 73\\nAlgorithm 3.2.4: Acceptance–Rejection Method\\ninput: Pdf g and constant C such that Cg(x) ⩾f (x) for all x.\\noutput: Random variable X distributed according to pdf f .\\n1 found←false\\n2 while not founddo\\n3 Generate X from g.\\n4 Generate U from U(0,1) independently of X.\\n5 Y ←UCg(X)\\n6 if Y ⩽f (X) then found←true\\n7 return X\\nExample 3.5 (Simulating Gamma Random Variables) Simulating random variables\\nfrom a Gamma(α,λ) distribution is generally done via the acceptance–rejection method.\\nConsider, for example, the Gamma distribution with α= 1.3 and λ= 5.6. Its pdf, ☞ 425\\nf (x) = λαxα−1e−λx\\nΓ(α) , x ⩾0,\\nwhere Γ is the gamma function Γ(α) :=\\nR ∞\\n0 e−x xα−1 dx, α> 0, is depicted by the blue solid\\ncurve in Figure 3.2.\\n0 0.5 1 1.5 2\\n0\\n1\\n2\\n3\\n4\\n5\\nFigure 3.2: The pdf g of the Exp(4) distribution multiplied by C = 1.2 dominates the pdf f\\nof the Gamma(1.3,5.6) distribution.\\nThis pdf happens to lie completely under the graph of Cg(x), where C = 1.2 and\\ng(x) = 4 exp(−4x),x ⩾0 is the pdf of the exponential distribution Exp(4). Hence, we\\ncan simulate from this particular Gamma distribution by accepting or rejecting a sample\\nfrom the Exp(4) distribution according to Step 6 of Algorithm 3.2.4. Simulating from the ☞ 425\\nExp(4) distribution can be done via the inverse-transform method: simulate U ∼U(0,1)\\nand return X = −ln(U)/4. The following Python code implements Algorithm 3.2.4 for this\\nexample.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 91, 'page_label': '74'}, page_content='74 Monte Carlo Sampling\\naccrejgamma.py\\nfrom math import exp, gamma, log\\nfrom numpy.random import rand\\nalpha = 1.3\\nlam = 5.6\\nf = lambda x: lam**alpha * x**(alpha-1) * exp(-lam*x)/gamma(alpha)\\ng = lambda x: 4*exp(-4*x)\\nC = 1.2\\nfound = False\\nwhile not found:\\nx = - log(rand())/4\\nif C*g(x)*rand() <= f(x):\\nfound = True\\nprint (x)\\n3.2.3 Simulating Random Vectors and Processes\\nTechniques for generating random vectors and processes are as diverse as the class of\\nrandom processes themselves; see, for example, [71]. We highlight a few general scenarios.\\nWhen X1,..., Xn are independent random variables with pdfs fi, i = 1,..., n, so that\\ntheir joint pdf is f (x) = f1(x1) ··· fn(xn), the random vector X = [X1,..., Xn]⊤ can be☞429\\nsimply simulated by drawing each component Xi ∼fi individually — for example, via the\\ninverse-transform method or acceptance–rejection.\\nFor dependent components X1,..., Xn, we can, as a consequence of the product rule of\\nprobability, represent the joint pdf f (x) as☞431\\nf (x) = f (x1,..., xn) = f1(x1) f2(x2 |x1) ··· fn(xn |x1,..., xn−1), (3.3)\\nwhere f1(x1) is the marginal pdf of X1 and fk(xk |x1,..., xk−1) is the conditional pdf of Xk\\ngiven X1 = x1,X2 = x2,..., Xk−1 = xk−1. Provided the conditional pdfs are known, one can\\ngenerate X by first generating X1, then, given X1 = x1, generate X2 from f2(x2 |x1), and so\\non, until generating Xn from fn(xn |x1,..., xn−1).\\nThe latter method is particularly applicable for generating Markov chains. Recall from\\nSection C.10 that a Markov chain is a stochastic process {Xt,t = 0,1,2,... }that satisfies☞451\\nMarkov chain the Markov property; meaning that for all t and s the conditional distribution of Xt+s given\\nXu,u ⩽t, is the same as that of Xt+s given only Xt. As a result, each conditional density\\nft(xt |x1,..., xt−1) can be written as a one-step transition density q t(xt |xt−1); that is, the\\nprobability density to go from state xt−1 to state xt in one step. In many cases of interest\\nthe chain is time-homogeneous, meaning that the transition density qt does not depend on\\nt. Such Markov chains can be generated sequentially, as given in Algorithm 3.2.5.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 92, 'page_label': '75'}, page_content='Monte Carlo Methods 75\\nAlgorithm 3.2.5: Simulate a Markov Chain\\ninput: Number of steps N, initial pdf f0, transition density q.\\n1 Draw X0 from the initial pdf f0.\\n2 for t = 1 to N do\\n3 Draw Xt from the distribution corresponding to the density q(·|Xt−1)\\n4 return X0,..., XN\\nExample 3.6 (Markov Chain Simulation) For time-homogeneous Markov chains\\nwith a discrete state space, we can visualize the one-step transitions by means of a trans-\\nition graph transition\\ngraph\\n, where arrows indicate possible transitions between states and the labels de-\\nscribe the corresponding probabilities. Figure 3.3 shows (on the left) the transition graph\\nof the Markov chain {Xt,t = 0,1,2,... }with state space {1,2,3,4}and one-step transition\\nmatrix\\nP =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 .2 0 .5 0 .3\\n0.5 0 0 .5 0\\n0.3 0 .7 0 0\\n0.1 0 0 0 .9\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n1 2\\n3 4\\n0.2\\n0.7\\n0.5\\n0.9\\n0.50.3\\n0.50.3\\n0.1\\n0 20 40 60 80 100\\n1\\n2\\n3\\n4\\nFigure 3.3: The transition graph (left) and a typical path (right) of the Markov chain.\\nIn the same figure (on the right) a typical outcome (path) of the Markov chain is\\nshown. The path was simulated using the Python program below. In this implementation\\nthe Markov chain always starts in state 1. We will revisit Markov chains, and in particular\\nMarkov chains with continuous state spaces, in Section 3.2.5. ☞ 78\\nMCsim.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nn = 101\\nP = np.array([[0, 0.2, 0.5, 0.3],\\n[0.5, 0, 0.5, 0],\\n[0.3, 0.7, 0, 0],\\n[0.1, 0, 0, 0.9]])\\nx = np.array(np.ones(n, dtype= int ))\\nx[0] = 0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 93, 'page_label': '76'}, page_content=\"76 Monte Carlo Sampling\\nfor t in range (0,n-1):\\nx[t+1] = np. min (np.where(np.cumsum(P[x[t],:]) >\\nnp.random.rand()))\\nx = x + 1 #add 1 to all elements of the vector x\\nplt.plot(np.array( range (0,n)),x, 'o')\\nplt.plot(np.array( range (0,n)),x, '--')\\nplt.show()\\n3.2.4 Resampling\\nThe idea behind resampling is very simple: an iid sample τ := {x1,..., xn}from someresampling\\nunknown cdf F represents our best knowledge of F if we make no further a priori as-\\nsumptions about it. If it is not possible to simulate more samples from F, the best way to\\n“repeat” the experiment is to resample from the original data by drawing from the empir-\\nical cdf Fn; see (1.2). That is, we draw each xi with equal probability and repeat this N☞11\\ntimes, according to Algorithm 3.2.6 below. As we draw here “with replacement”, multiple\\ninstances of the original data points may occur in the resampled data.\\nAlgorithm 3.2.6: Sampling from an Empirical Cdf.\\ninput: Original iid sample x1,..., xn and sample size N.\\noutput: Iid sample X∗\\n1,..., X∗\\nN from the empirical cdf.\\n1 for t = 1 to N do\\n2 Draw U ∼U(0,1)\\n3 Set I ←⌈nU⌉\\n4 Set X∗\\nt ←xI\\n5 return X∗\\n1,..., X∗\\nN\\nIn Step 3, ⌈nU⌉returns the ceiling of nU; that is, it is the smallest integer larger than\\nor equal to nU. Consequently, I is drawn uniformly at random from the set of indices\\n{1,..., n}.\\nBy sampling from the empirical cdf we can thus (approximately) repeat the experiment\\nthat gave us the original data as many times as we like. This is useful if we want to assess\\nthe properties of certain statistics obtained from the data. For example, suppose that the\\noriginal data τ gave the statistic t(τ). By resampling we can gain information about the\\ndistribution of the corresponding random variable t(T).\\nExample 3.7 (Quotient of Uniforms) Let U1,..., Un,V1,..., Vn be iid U(0,1) random\\nvariables and define Xi = Ui/Vi, i = 1,..., n. Suppose we wish to investigate the distribu-\\ntion of the sample median eX and sample mean X of the (random) data T := {X1,..., Xn}.\\nSince we know the model for Texactly, we can generate a large number, N say, of inde-\\npendent copies of it, and for each of these copies evaluate the sample medians eX1,..., eXN\\nand sample means X1,..., XN. For n = 100 and N = 1000 the empirical cdfs might look\\nlike the left and right curves in Figure 3.4, respectively. Contrary to what you might have\\nexpected, the distributions of the sample median and sample mean do not match at all. The\\nsample median is quite concentrated around 1, whereas the distribution of the sample mean\\nis much more spread out.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 94, 'page_label': '77'}, page_content='Monte Carlo Methods 77\\n0 1 2 3 4 5 6 7\\n0\\n0.5\\n1\\nFigure 3.4: Empirical cdfs of the medians of the resampled data (left curve) and sample\\nmeans (right curve) of the resampled data.\\nInstead of sampling completely new data, we could also reuse the original data by\\nresampling them via Algorithm 3.2.6. This gives independent copies eX∗\\n1,..., eX∗\\nN and\\nX\\n∗\\n1,..., X\\n∗\\nN, for which we can again plot the empirical cdf. The results will be similar\\nto the previous case. In fact, in Figure 3.4 the cdf of the resampled sample medians and\\nsample means are plotted. The corresponding Python code is given below. The essential\\npoint of this example is that resampling of data can greatly add to the understanding of the\\nprobabilistic properties of certain measurements on the data, even if the underlying model\\nis not known. See Exercise 12 for a further investigation of this example. ☞ 117\\nquotunif.py\\nimport numpy as np\\nfrom numpy.random import rand, choice\\nimport matplotlib.pyplot as plt\\nfrom statsmodels.distributions.empirical_distribution import ECDF\\nn = 100\\nN = 1000\\nx = rand(n)/rand(n) # data\\nmed = np.zeros(N)\\nave = np.zeros(N)\\nfor i in range (0,N):\\ns = choice(x, n, replace=True) # resampled data\\nmed[i] = np.median(s)\\nave[i] = np.mean(s)\\nmed_cdf = ECDF(med)\\nave_cdf = ECDF(ave)\\nplt.plot(med_cdf.x, med_cdf.y)\\nplt.plot(ave_cdf.x, ave_cdf.y)\\nplt.show()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 95, 'page_label': '78'}, page_content='78 Monte Carlo Sampling\\n3.2.5 Markov Chain Monte Carlo\\nMarkov chain Monte Carlo (MCMC) is a Monte Carlo sampling technique for (approxim-Markov chain\\nMonte Carlo ately) generating samples from an arbitrary distribution — often referred to as the target\\ntarget distribution. The basic idea is to run a Markov chain long enough such that its limiting\\ndistribution is close to the target distribution. Often such a Markov chain is constructed to\\nbe reversible, so that the detailed balance equations (C.43) can be used. Depending on the☞453\\nstarting position of the Markov chain, the initial random variables in the Markov chain may\\nhave a distribution that is significantly different from the target (limiting) distribution. The\\nrandom variables that are generated during this burn-in periodburn-in period are often discarded. The\\nremaining random variables form an approximate and dependent sample from the target\\ndistribution.\\nIn the next two sections we discuss two popular MCMC samplers: the Metropolis–\\nHastings sampler and the Gibbs sampler.\\n3.2.5.1 Metropolis–Hastings Sampler\\nThe Metropolis–Hastings sampler [87] is similar to the acceptance–rejection method in☞72\\nthat it simulates a trial state, which is then accepted or rejected according to some random\\nmechanism. Specifically, suppose we wish to sample from a target pdf f (x), where x takes\\nvalues in some d-dimensional set. The aim is to construct a Markov chain{Xt,t = 0,1,... }\\nin such a way that its limiting pdf is f . Suppose the Markov chain is in state x at time t. A\\ntransition of the Markov chain from state x is carried out in two phases. First a proposalproposal\\nstate Y is drawn from a transition density q(·|x). This state is accepted as the new state,\\nwith acceptance probabilityacceptance\\nprobability\\nα(x,y) = min\\n( f (y) q(x |y)\\nf (x) q(y |x),1\\n)\\n, (3.4)\\nor rejected otherwise. In the latter case the chain remains in state x. The algorithm just\\ndescribed can be summarized as follows.\\nAlgorithm 3.2.7: Metropolis–Hastings Sampler\\ninput: Initial state X0, sample size N, target pdf f (x), proposal function q(y |x).\\noutput: X1,..., XN (dependent), approximately distributed according to f (x).\\n1 for t = 0 to N −1 do\\n2 Draw Y ∼q(y |Xt) // draw a proposal\\n3 α←α(Xt,Y) // acceptance probability as in (3.4)\\n4 Draw U ∼U(0,1)\\n5 if U ⩽αthen Xt+1 ←Y\\n6 else Xt+1 ←Xt\\n7 return X1,..., XN\\nThe fact that the limiting distribution of the Metropolis–Hastings Markov chain is equal\\nto the target distribution (under general conditions) is a consequence of the following result.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 96, 'page_label': '79'}, page_content='Monte Carlo Methods 79\\nTheorem 3.1: Local Balance for the Metropolis–Hastings Sampler\\nThe transition density of the Metropolis–Hastings Markov chain satisfies ☞ 453 the de-\\ntailed balance equations.\\nProof: We prove the theorem for the discrete case only. Because a transition of the\\nMetropolis–Hastings Markov chain consists of two steps, the one-step transition probabil-\\nity to go from x to y is not q(y |x) but\\neq(y |x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nq(y |x) α(x,y), if y , x,\\n1 −P\\nz,x q(z |x) α(x,z), if y = x. (3.5)\\nWe thus need to show that\\nf (x)eq(y |x) = f (y)eq(x |y) for all x,y. (3.6)\\nWith the acceptance probability as in (3.4), we need to check (3.6) for three cases:\\n(a) x = y,\\n(b) x , y and f (y)q(x |y) ⩽f (x)q(y |x), and\\n(c) x , y and f (y)q(x |y) > f (x)q(y |x).\\nCase (a) holds trivially. For case (b), α(x,y) = f (y)q(x |y)/( f (x)q(y |x)) and α(y,x) = 1.\\nConsequently,\\neq(y |x) = f (y)q(x |y)/f (x) and eq(x |y) = q(x |y),\\nso that (3.6) holds. Similarly, for case (c) we have α(x,y) = 1 and α(y,x) = f (x)q(y |x)/\\n( f (y)q(x |y)). It follows that,\\neq(y |x) = q(y |x) and eq(x |y) = f (x)q(y |x)/f (y),\\nso that (3.6) holds again. □\\nThus if the Metropolis–Hastings Markov chain is ergodic, then its limiting pdf is f (x). ☞ 452\\nA fortunate property of the algorithm, which is important in many applications, is that in\\norder to evaluate the acceptance probability α(x,y) in (3.4), one only needs to know the\\ntarget pdf f (x) up to a constant ; that is f (x) = c f (x) for some known function f (x) but\\nunknown constant c.\\nThe efficiency of the algorithm depends of course on the choice of the proposal trans-\\nition density q(y |x). Ideally, we would like q(y |x) to be “close” to the target f (y), irre-\\nspective of x. We discuss two common approaches.\\n1. Choose the proposal transition density q(y |x) independent of x; that is, q(y |x) =\\ng(y) for some pdf g(y). An MCMC sampler of this type is called an independence\\nsampler independence\\nsampler\\n. The acceptance probability is thus\\nα(x,y) = min\\n( f (y) g(x)\\nf (x) g(y), 1\\n)\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 97, 'page_label': '80'}, page_content='80 Monte Carlo Sampling\\n2. If the proposal transition density is symmetric (that is, q(y |x) = q(x |y)), then the\\nacceptance probability has the simple form\\nα(x,y) = min\\n( f (y)\\nf (x), 1\\n)\\n, (3.7)\\nand the MCMC algorithm is called a random walk sampler . A typical example israndom walk\\nsampler when, for a given current state x, the proposal state Y is of the form Y = x + Z,\\nwhere Z is generated from some spherically symmetric distribution, such as N(0,I).\\nWe now give an example illustrating the second approach.\\nExample 3.8 (Random Walk Sampler) Consider the two-dimensional pdf\\nf (x1,x2) = c e−1\\n4\\n√\\nx2\\n1+x2\\n2\\n\\x12\\nsin\\n\\x12\\n2\\nq\\nx2\\n1 + x2\\n2\\n\\x13\\n+ 1\\n\\x13\\n, −2π< x1 <2π, −2π< x2 <2π, (3.8)\\nwhere c is an unknown normalization constant. The graph of this pdf (unnormalized) is\\ndepicted in the left panel of Figure 3.5.\\n-6 -4 -2 0 2 4 6\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\nFigure 3.5: Left panel: the two-dimensional target pdf. Right panel: points from the random\\nwalk sampler are approximately distributed according to the target pdf.\\nThe following Python program implements a random walk sampler to (approximately)\\ndraw N = 104 dependent samples from the pdf f . At each step, given a current state x,\\na proposal Y is drawn from the N(x,I) distribution. That is, Y = x + Z, with Z bivariate\\nstandard normal. We see in the right panel of Figure 3.5 that the sampler works correctly.\\nThe starting point for the Markov chain is chosen as (0 ,0). Note that the normalization\\nconstant c is never required to be specified in the program.\\nrwsamp.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom numpy import pi, exp, sqrt, sin\\nfrom numpy.random import rand, randn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 98, 'page_label': '81'}, page_content=\"Monte Carlo Methods 81\\nN = 10000\\na = lambda x: -2*pi < x\\nb = lambda x: x < 2*pi\\nf = lambda x1, x2: exp(-sqrt(x1**2+x2**2)/4)*(\\nsin(2*sqrt(x1**2+x2**2))+1)*a(x1)*b(x1)*a(x2)*b(x2)\\nxx = np.zeros((N,2))\\nx = np.zeros((1,2))\\nfor i in range (1,N):\\ny = x + randn(1,2)\\nalpha = np.amin((f(y[0][0],y[0][1])/f(x[0][0],x[0][1]),1))\\nr = rand() < alpha\\nx = r*y + (1-r)*x\\nxx[i,:] = x\\nplt.scatter(xx[:,0], xx[:,1], alpha =0.4,s =2)\\nplt.axis( 'equal ')\\nplt.show()\\n3.2.5.2 Gibbs Sampler\\nThe Gibbs sampler Gibbs sampler[48] uses a somewhat di fferent methodology from the Metropolis–\\nHastings algorithm and is particularly useful for generatingn-dimensional random vectors.\\nThe key idea of the Gibbs sampler is to update the components of the random vector\\none at a time, by sampling them from conditional pdfs. Thus, Gibbs sampling can be\\nadvantageous if it is easier to sample from the conditional distributions than from the joint\\ndistribution.\\nSpecifically, suppose that we wish to sample a random vector X = [X1,..., Xn]⊤ ac-\\ncording to a target pdf f (x). Let f (xi |x1,..., xi−1, xi+1,..., xn) represent the conditional\\npdf2 of the i-th component, Xi, given the other components x1,..., xi−1,xi+1,..., xn. The\\nGibbs sampling algorithm is as follows.\\nAlgorithm 3.2.8: Gibbs Sampler\\ninput: Initial point X0, sample size N, and target pdf f .\\noutput: X1,..., XN approximately distributed according to f .\\n1 for t = 0 to N −1 do\\n2 Draw Y1 from the conditional pdf f (y1 |Xt,2,..., Xt,n).\\n3 for i = 2 to n do\\n4 Draw Yi from the conditional pdf f (yi |Y1,..., Yi−1,Xt,i+1,..., Xt,n).\\n5 Xt+1 ←Y\\n6 return X1,..., XN\\nThere exist many variants of the Gibbs sampler, depending on the steps required to\\nupdate Xt to Xt+1 — called the cycle of the Gibbs algorithm. In the algorithm above, the cycle\\n2In this section we employ a Bayesian notation style, using the same letter f for different (conditional)\\ndensities.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 99, 'page_label': '82'}, page_content='82 Monte Carlo Sampling\\ncycle consists of Steps 2–5, in which the components are updated in a fixed order 1→2 →\\n···→ n. For this reason Algorithm 3.2.8 is also called the systematic Gibbs sampler.systematic\\nGibbs sampler In the random-order Gibbs sampler, the order in which the components are updated\\nrandom-order\\nGibbs sampler in each cycle is a random permutation of {1,..., n}(see Exercise 9). Other modifications\\n☞115 are to update the components in blocks (i.e., several at the same time), or to update only\\na random selection of components. The variant where in each cycle only a single random\\ncomponent is updated is called therandom Gibbs sampler. In the reversible Gibbs samplerrandom Gibbs\\nsampler\\nreversible\\nGibbs sampler\\na cycle consists of the coordinate-wise updating 1 →2 →···→ n −1 →n →n −1 →\\n···→ 2 →1. In all cases, except for the systematic Gibbs sampler, the resulting Markov\\nchain {Xt,t = 1,2,... }is reversible and hence its limiting distribution is precisely f (x).\\n☞452 Unfortunately, the systematic Gibbs Markov chain is not reversible and so the detailed\\nbalance equations are not satisfied. However, a similar result holds, due to Hammersley and\\nClifford, under the so-called positivity condition: if at a point x = (x1,..., xn) all marginal\\ndensities f (xi) >0,i = 1,..., n, then the joint density f (x) >0.\\nTheorem 3.2: Hammersley–Clifford Balance for the Gibbs Sampler\\nLet q1→n(y |x) denote the transition density of the systematic Gibbs sampler, and let\\nqn→1(x |y) be the transition density of the reverse move, in the order n →n −1 →\\n···→ 1. Then, if the positivity condition holds,\\nf (x) q1→n(y |x) = f (y) qn→1(x |y). (3.9)\\nProof: For the forward move we have:\\nq1→n(y |x) = f (y1 |x2,..., xn) f (y2 |y1,x3,..., xn) ··· f (yn |y1,..., yn−1),\\nand for the reverse move:\\nqn→1(x |y) = f (xn |y1,..., yn−1) f (xn−1 |y1,..., yn−2,xn) ··· f (x1 |x2,..., xn).\\nConsequently,\\nq1→n(y |x)\\nqn→1(x |y) =\\nnY\\ni=1\\nf (yi |y1,..., yi−1,xi+1,..., xn)\\nf (xi |y1,..., yi−1,xi+1,..., xn)\\n=\\nnY\\ni=1\\nf (y1,..., yi,xi+1,..., xn)\\nf (y1,..., yi−1,xi,..., xn)\\n= f (y) Qn−1\\ni=1 f (y1,..., yi,xi+1,..., xn)\\nf (x) Qn\\nj=2 f (y1,..., yj−1,xj,..., xn)\\n= f (y) Qn−1\\ni=1 f (y1,..., yi,xi+1,..., xn)\\nf (x) Qn−1\\nj=1 f (y1,..., yj,xj+1,..., xn)\\n= f (y)\\nf (x).\\nThe result follows by rearranging the last identity. The positivity condition ensures that we\\ndo not divide by 0 along the line. □\\nIntuitively, the long-run proportion of transitions x →y for the “forward move” chain\\nis equal to the long-run proportion of transitions y → x for the “reverse move” chain.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 100, 'page_label': '83'}, page_content='Monte Carlo Methods 83\\nTo verify that the Markov chain X0,X1,... for the systematic Gibbs sampler indeed has\\nlimiting pdf f (x), we need to check that the global balance equations (C.42) hold. By ☞ 452\\nintegrating (in the continuous case) both sides in (3.9) with respect tox, we see that indeed\\nZ\\nf (x) q1→n(y |x) dx = f (y).\\nExample 3.9 (Gibbs Sampler for the Bayesian Normal Model) Gibbs samplers are\\noften applied in Bayesian statistics, to sample from the posterior pdf. Consider for instance\\nthe Bayesian normal model ☞ 51\\nf (µ,σ2) = 1/σ2\\n(x |µ,σ2) ∼N(µ1,σ2I).\\nHere the prior for ( µ,σ2) is improper. improper priorThat is, it is not a pdf in itself, but by obstinately\\napplying Bayes’ formula it does yield a proper posterior pdf. In some sense this prior\\nconveys the least amount of information about µand σ2. Following the same procedure as\\nin Example 2.8, we find the posterior pdf:\\nf (µ,σ2 |x) ∝\\n\\x10\\nσ2\\x11−n/2−1\\nexp\\n(\\n−1\\n2\\nP\\ni(xi −µ)2\\nσ2\\n)\\n. (3.10)\\nNote that µand σ2 here are the “variables” andx is a fixed data vector. To simulate samples\\nµand σ2 from (3.10) using the Gibbs sampler, we need the distributions of both (µ|σ2,x)\\nand (σ2 |µ,x). To find f (µ|σ2,x), view the right-hand side of (3.10) as a function of µ\\nonly, regarding σ2 as a constant. This gives\\nf (µ|σ2,x) ∝exp\\n(\\n−nµ2 −2µP\\ni xi\\n2σ2\\n)\\n= exp\\n(\\n−µ2 −2µx\\n2(σ2/n)\\n)\\n∝exp\\n(\\n−1\\n2\\n(µ−x)2\\nσ2/n\\n)\\n. (3.11)\\nThis shows that (µ|σ2,x) has a normal distribution with mean x and variance σ2/n.\\nSimilarly, to find f (σ2 |µ,x), view the right-hand side of (3.10) as a function of σ2,\\nregarding µas a constant. This gives\\nf (σ2 |µ,x) ∝(σ2)−n/2−1 exp\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3−1\\n2\\nnX\\ni=1\\n(xi −µ)2/σ2\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe, (3.12)\\nshowing that ( σ2 |µ,x) has an inverse-gamma distribution with parameters n/2 and ☞ 425Pn\\ni=1(xi −µ)2/2. The Gibbs sampler thus involves the repeated simulation of\\n(µ|σ2,x) ∼N\\n\\x10\\nx, σ2/n\\n\\x11\\nand ( σ2 |µ,x) ∼InvGamma\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edn/2,\\nnX\\ni=1\\n(xi −µ)2/2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nSimulating X ∼InvGamma(α,λ) is achieved by first generatingZ ∼Gamma(α,λ) and\\nthen returning X = 1/Z.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 101, 'page_label': '84'}, page_content=\"84 Monte Carlo Sampling\\nIn our parameterization of the Gamma(α,λ) distribution, λ is the rate parameter.\\nMany software packages instead use the scale parameter c = 1/λ. Be aware of this\\nwhen simulating Gamma random variables.\\nThe Python script below defines a small data set of size n = 10 (which was randomly\\nsimulated from a standard normal distribution), and implements the systematic Gibbs\\nsampler to simulate from the posterior distribution, using N = 105 samples.\\ngibbsamp.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nx = np.array([[-0.9472, 0.5401, -0.2166, 1.1890, 1.3170,\\n-0.4056, -0.4449, 1.3284, 0.8338, 0.6044]])\\nn=x.size\\nsample_mean = np.mean(x)\\nsample_var = np.var(x)\\nsig2 = np.var(x)\\nmu=sample_mean\\nN=10**5\\ngibbs_sample = np.array(np.zeros((N, 2)))\\nfor k in range (N):\\nmu=sample_mean + np.sqrt(sig2/n)*np.random.randn()\\nV=np. sum ((x-mu)**2)/2\\nsig2 = 1/np.random.gamma(n/2, 1/V)\\ngibbs_sample[k,:]= np.array([mu, sig2])\\nplt.scatter(gibbs_sample[:,0], gibbs_sample[:,1],alpha =0.1,s =1)\\nplt.plot(np.mean(x), np.var(x), 'wo')\\nplt.show()\\n-1 -0.5 0 0.5 1 1.5 2\\n7\\n0\\n0.5\\n1\\n1.5\\n^f(7 j x)\\nFigure 3.6: Left: approximate draws from the posterior pdf f (µ,σ2 |x) obtained via the\\nGibbs sampler. Right: estimate of the posterior pdf f (µ|x).\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 102, 'page_label': '85'}, page_content='Monte Carlo Methods 85\\nThe left panel of Figure 3.6 shows the ( µ,σ2) points generated by the Gibbs sampler.\\nAlso shown, via the white circle, is the point (x,s2), where x = 0.3798 is the sample mean\\nand s2 = 0.6810 the sample variance. This posterior point cloud visualizes the considerable\\nuncertainty in the estimates. By projecting the ( µ,σ2) points onto the µ-axis — that is,\\nby ignoring the σ2 values — one obtains (approximate) samples from the posterior pdf\\nof µ; that is, f (µ|x). The right panel of Figure 3.6 shows a kernel density estimate (see\\nSection 4.4) of this pdf. The corresponding 0 .025 and 0.975 sample quantiles were found ☞ 134\\nto be −0.2054 and 0.9662, respectively, giving the 95% credible interval (−0.2054,0.9662)\\nfor µ, which contains the true expectation 0. Similarly, an estimated 95% credible interval\\nfor σ2 is (0.3218,2.2485), which contains the true variance 1.\\n3.3 Monte Carlo Estimation\\nIn this section we describe how Monte Carlo simulation can be used to estimate complic-\\nated integrals, probabilities, and expectations. A number of variance reduction techniques\\nare introduced as well, including the recent cross-entropy method.\\n3.3.1 Crude Monte Carlo\\nThe most common setting for Monte Carlo estimation is the following: Suppose we wish to\\ncompute the expectation µ= EY of some (say continuous) random variable Y with pdf f ,\\nbut the integral EY =\\nR\\ny f(y) dy is difficult to evaluate. For example, if Y is a complicated\\nfunction of other random variables, it would be di fficult to obtain an exact expression for\\nf (y). The idea of crude Monte Carlo — sometimes abbreviated as CMC — is to approx- crude Monte\\nCarloimate µby simulating many independent copies Y1,..., YN of Y and then take their sample\\nmean Y as an estimator of µ. All that is needed is an algorithm to simulate such copies.\\nBy the Law of Large Numbers, Y converges to µas N →∞, provided the expectation ☞ 446\\nof Y exists. Moreover, by the Central Limit Theorem, Y approximately has a N(µ,σ2/N) ☞ 447\\ndistribution for large N, provided that the variance σ2 = VarY <∞. This enables the con-\\nstruction of an approximate (1 −α) confidence interval for µ: confidence\\ninterval \\nY −z1−α/2\\nS√\\nN\\n, Y + z1−α/2\\nS√\\nN\\n!\\n, (3.13)\\nwhere S is the sample standard deviation of the {Yi}and zγ denotes the γ-quantile of the\\nN(0,1) distribution; see also Section C.13. Instead of specifying the confidence interval, ☞ 457\\none often reports only the sample mean and the estimated standard error: S/\\n√\\nN, or the estimated\\nstandard errorestimated relative error: S/(Y\\n√\\nN). The basic estimation procedure for independent data\\nestimated\\nrelative erroris summarized in Algorithm 3.3.1 below.\\nIt is often the case that the output Y is a function of some underlying random vector or\\nstochastic process; that is, Y = H(X), where H is a real-valued function and X is a random\\nvector or process. The beauty of Monte Carlo for estimation is that (3.13) holds regardless\\nof the dimension of X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 103, 'page_label': '86'}, page_content=\"86 Monte Carlo Estimation\\nAlgorithm 3.3.1: Crude Monte Carlo for Independent Data\\ninput: Simulation algorithm for Y ∼f , sample size N, confidence level 1 −α.\\noutput: Point estimate and approximate (1 −α) confidence interval for µ= EY.\\n1 Simulate Y1,..., YN\\niid\\n∼f .\\n2 Y ←1\\nN\\nPN\\ni=1 Yi\\n3 S 2 ← 1\\nN−1\\nPN\\ni=1(Yi −Y)2\\n4 return Y and the interval (3.13).\\nExample 3.10 (Monte Carlo Integration) In Monte Carlo integration , simulation isMonte Carlo\\nintegration used to evaluate complicated integrals. Consider, for example, the integral\\nµ=\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\np\\n|x1 + x2 + x3|e−(x2\\n1+x2\\n2+x2\\n3)/2 dx1 dx2 dx3.\\nDefining Y = |X1 + X2 + X3|1/2(2π)3/2, with X1,X2,X3\\niid\\n∼N(0,1), we can write µ = EY.\\nUsing the following Python program, with a sample size of N = 106, we obtained an\\nestimate Y = 17.031 with an approximate 95% confidence interval (17.017,17.046).\\nmcint.py\\nimport numpy as np\\nfrom numpy import pi\\nc = (2*pi)**(3/2)\\nH = lambda x: c*np.sqrt(np. abs (np. sum (x,axis=1)))\\nN = 10**6\\nz = 1.96\\nx = np.random.randn(N,3)\\ny = H(x)\\nmY = np.mean(y)\\nsY = np.std(y)\\nRE = sY/mY/np.sqrt(N)\\nprint ('Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}) '.format (\\nmY, mY*(1-z*RE), mY*(1+z*RE)))\\nEstimate = 17.031, CI = (17.017,17.046)\\nExample 3.11 (Example 2.1 (cont.)) We return to the bias–variance tradeo ff in Ex-\\nample 2.1. Figure 2.7 gives estimates of the (squared-error) generalization risk (2.5) as☞26\\n☞29\\n☞23\\na function of the number of parameters in the model. But how accurate are these estim-\\nates? Because we know in this case the exact model for the data, we can use Monte Carlo\\nsimulation to estimate the generalization risk (for a fixed training set) and the expected\\ngeneralization risk (averaged over all training sets) precisely. All we need to do is repeat\\nthe data generation, fitting, and validation steps many times and then take averages of the\\nresults. The following Python code repeats 100 times:\\n1. Simulate the training set of size n = 100.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 104, 'page_label': '87'}, page_content='Monte Carlo Methods 87\\n2. Fit models up to size k = 8.\\n3. Estimate the test loss using a test set with the same sample size n = 100.\\nFigure 3.7 shows that there is some variation in the test losses, due to the randomness in\\nboth the training and test sets. To obtain an accurate estimate of the expected generalization\\nrisk (2.6), take the average of the test losses. We see that fork ⩽8 the estimate in Figure 2.7\\nis close to the true expected generalization risk.\\n1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\nNumber of parameters p\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200Test loss\\nFigure 3.7: Independent estimates of the test loss show some variability.\\nCMCtestloss.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom numpy.random import rand, randn\\nfrom numpy.linalg import solve\\ndef generate_data(beta, sig, n):\\nu = rand(n, 1)\\ny = (u ** np.arange(0, 4)) @ beta + sig * randn(n, 1)\\nreturn u, y\\nbeta = np.array([[10, -140, 400, -250]]).T\\nn = 100\\nsig = 5\\nbetahat = {}\\nplt.figure(figsize=[6,5])\\ntotMSE = np.zeros(8)\\nmax_p = 8\\np_range = np.arange(1, max_p + 1, 1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 105, 'page_label': '88'}, page_content=\"88 Monte Carlo Estimation\\nfor N in range (0,100):\\nu, y = generate_data(beta, sig, n) #training data\\nX = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX = np.hstack((X, u**(p-1)))\\nbetahat[p] = solve(X.T @ X, X.T @ y)\\nu_test, y_test = generate_data(beta, sig, n) #test data\\nMSE = []\\nX_test = np.ones((n, 1))\\nfor p in p_range:\\nif p > 1:\\nX_test = np.hstack((X_test, u_test**(p-1)))\\ny_hat = X_test @ betahat[p] # predictions\\nMSE.append(np. sum ((y_test - y_hat)**2/n))\\ntotMSE = totMSE + np.array(MSE)\\nplt.plot(p_range, MSE, 'C0',alpha=0.1)\\nplt.plot(p_range,totMSE/N, 'r-o')\\nplt.xticks(ticks=p_range)\\nplt.xlabel( 'Number of parameters $p$ ')\\nplt.ylabel( 'Test loss ')\\nplt.tight_layout()\\nplt.savefig( 'MSErepeatpy.pdf ',format ='pdf')\\nplt.show()\\n3.3.2 Bootstrap Method\\nThe bootstrap method [37] combines CMC estimation with the resampling procedure of\\nSection 3.2.4. The idea is as follows: Suppose we wish to estimate a number µvia some☞76\\nestimator Y = H(T), where T := {X1,..., Xn}is an iid sample from some unknown cdf\\nF. It is assumed that Y does not depend on the order of the {Xi}. To assess the quality (for\\nexample, accuracy) of the estimatorY, one could draw independent replicationsT1,..., TN\\nof Tand find sample estimates for quantities such as the variance VarY, the bias EY −µ,\\nand the mean squared error E(Y −µ)2. However, it may be too time-consuming or simply\\nnot feasible to obtain such replications. An alternative is to resample the original data.\\nTo reiterate, given an outcome τ = {x1,..., xn}of T, we simulate an iid sample T∗ :=\\n{X∗\\n1,..., X∗\\nn}from the empirical cdf Fn, via Algorithm 3.2.6 (hence the resampling size is☞76\\nN = n here).\\nThe rationale is that the empirical cdf Fn is close to the actual cdf F and gets closer as\\nn gets larger. Hence, any quantities depending onF, such as EFg(Y), where g is a function,\\ncan be approximated by EFn g(Y). The latter is usually still di fficult to evaluate, but it can\\nbe simply estimated via CMC as\\n1\\nK\\nKX\\ni=1\\ng(Y∗\\ni ),\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 106, 'page_label': '89'}, page_content='Monte Carlo Methods 89\\nwhere Y∗\\n1 ,..., Y∗\\nK are independent random variables, each distributed as Y∗ = H(T∗). This\\nseemingly self-referent procedure is called bootstrapping — alluding to Baron von Mün- bootstrapping\\nchausen, who pulled himself out of a swamp by his own bootstraps. As an example, the\\nbootstrap estimate of the expectation of Y is\\ncEY = Y\\n∗\\n= 1\\nK\\nKX\\ni=1\\nY∗\\ni ,\\nwhich is simply the sample mean of {Y∗\\ni }. Similarly, the bootstrap estimate for VarY is the\\nsample variance\\n[VarY = 1\\nK −1\\nKX\\ni=1\\n(Y∗\\ni −Y\\n∗\\n)2. (3.14)\\nBootstrap estimators for the bias and MSE are Y\\n∗\\n−Y and 1\\nK\\nPK\\ni=1(Y∗\\ni −Y)2, respectively.\\nNote that for these estimators the unknown quantityµis replaced with its original estimator\\nY. Confidence intervals can be constructed in the same fashion. We mention two variants:\\nthe normal method and the percentile method. In the normal method, a 1 −αconfidence normal method\\npercentile\\nmethod\\ninterval for µis given by\\n(Y ±z1−α/2S ∗),\\nwhere S ∗ is the bootstrap estimate of the standard deviation of Y; that is, the square root\\nof (3.14). In the percentile method, the upper and lower bounds of the 1 −α confidence\\ninterval for µare given by the 1 −α/2 and α/2 quantiles of Y, which in turn are estimated\\nvia the corresponding sample quantiles of the bootstrap sample {Y∗\\ni }.\\nThe following example illustrates the usefulness of the bootstrap method for ratio es-\\ntimation and also introduces the renewal reward processmodel for data.\\nExample 3.12 (Bootstrapping the Ratio Estimator) A common scenario in stochastic\\nsimulation is that the output of the simulation consists of independent pairs of data\\n(C1,R1),(C2,R2),... , where each C is interpreted as the length of a period of time — a so-\\ncalled cycle — and R is the reward obtained during that cycle. Such a collection of random\\nvariables {(Ci,Ri)}is called a renewal reward process renewal\\nreward process\\n. Typically, the rewardRi depends on\\nthe cycle length Ci. Let At be the average reward earned by time t; that is, At = PNt\\ni=1 Ri/t,\\nwhere Nt = max{n : C1 + ··· + Cn ⩽t}counts the number of complete cycles at time t. It\\ncan be shown, see Exercise 20, that if the expectations of the cycle length and reward are ☞ 119\\nfinite, then At converges to the constant ER/EC. This ratio can thus be interpreted as the\\nlong-run average reward long-run\\naverage reward\\n.\\nEstimation of the ratio ER/EC from data ( C1,R1),..., (Cn,Rn) is easy: take the ratio\\nestimator ratio estimator\\nA = R\\nC\\n.\\nHowever, this estimator A is not unbiased and it is not obvious how to derive confidence\\nintervals. Fortunately, the bootstrap method can come to the rescue: simply resample the\\npairs {(Ci,Ri)}, obtain ratio estimators A∗\\n1,..., A∗\\nK, and from these compute quantities of\\ninterest such as confidence intervals.\\nAs a concrete example, let us return to the Markov chain in Example 3.6. Recall that ☞ 75\\nthe chain starts at state 1 at time 0. After a certain amount of time T1, the process returns'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 107, 'page_label': '90'}, page_content='90 Monte Carlo Estimation\\nto state 1. The time steps 0 ,..., T1 −1 form a natural “cycle” for this process, as from\\ntime T1 onwards the process behaves probabilistically exactly the same as when it started,\\nindependently of X0,..., XT1−1. Thus, if we define T0 = 0, and let Ti be the i-th time that\\nthe chain returns to state 1, then we can break up the time interval into independent cycles\\nof lengths Ci = Ti −Ti−1, i = 1,2,... . Now suppose that during the i-th cycle a reward\\nRi =\\nTi−1X\\nt=Ti−1\\nϱt−Ti−1 r(Xt)\\nis received, where r(i) is some fixed reward for visiting state i ∈{1,2,3,4}and ϱ ∈(0,1)\\nis a discounting factor. Clearly,{(Ci,Ri)}is a renewal reward process. Figure 3.8 shows the\\noutcomes of 1000 pairs (C,R), using r(1) = 4,r(2) = 3,r(3) = 10,r(4) = 1, and ϱ= 0.9.\\n0 10 20 30 40 50 60 70\\nCycle length\\n0\\n10\\n20\\n30\\n40\\n50\\n60Reward\\nFigure 3.8: Each circle represents a (cycle length, reward) pair. The varying circle sizes\\nindicate the number of occurrences for a given pair. For example, (2,15.43) is the most\\nlikely pair here, occurring 186 out of a 1000 times. It corresponds to the cycle path 1 →\\n3 →2 →1.\\nThe long-run average reward is estimated as 2.50 for our data. But how accurate is this\\nestimate? Figure 3.9 shows a density plot of the bootstrapped ratio estimates, where we\\nindependently resampled the data pairs 1000 times.\\n2.2\\n 2.4\\n 2.6\\n 2.8\\nlong-run average reward\\n0\\n2\\n4density\\nFigure 3.9: Density plot of the bootstrapped ratio estimates for the Markov chain renewal\\nreward process.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 108, 'page_label': '91'}, page_content=\"Monte Carlo Methods 91\\nFigure 3.9 indicates that the true long-run average reward lies between 2.2 and 2.8\\nwith high confidence. More precisely, the 99% bootstrap confidence interval (percentile\\nmethod) is here (2.27, 2.77). The following Python script spells out the procedure.\\nratioest.py\\nimport numpy as np, matplotlib.pyplot as plt, seaborn as sns\\nfrom numba import jit\\nnp.random.seed(123)\\nn = 1000\\nP = np.array([[0, 0.2, 0.5, 0.3],\\n[0.5 ,0, 0.5, 0],\\n[0.3, 0.7, 0, 0],\\n[0.1, 0, 0, 0.9]])\\nr = np.array([4,3,10,1])\\nCorg = np.array(np.zeros((n,1)))\\nRorg = np.array(np.zeros((n,1)))\\nrho=0.9\\n@jit() #for speed -up; see Appendix\\ndef generate_cyclereward(n):\\nfor i in range (n):\\nt=1\\nxreg = 1 #regenerative state (out of 1,2,3,4)\\nreward = r[0]\\nx= np.amin(np.argwhere(np.cumsum(P[xreg-1,:]) > np.random.\\nrand())) + 1\\nwhile x != xreg:\\nt += 1\\nreward += rho**(t-1)*r[x-1]\\nx = np.amin(np.where(np.cumsum(P[x-1,:]) > np.random.rand\\n())) + 1\\nCorg[i] = t\\nRorg[i] = reward\\nreturn Corg, Rorg\\nCorg, Rorg = generate_cyclereward(n)\\nAorg = np.mean(Rorg)/np.mean(Corg)\\nK = 5000\\nA = np.array(np.zeros((K,1)))\\nC = np.array(np.zeros((n,1)))\\nR = np.array(np.zeros((n,1)))\\nfor i in range (K):\\nind = np.ceil(n*np.random.rand(1,n)).astype( int )[0]-1\\nC = Corg[ind]\\nR = Rorg[ind]\\nA[i] = np.mean(R)/np.mean(C)\\nplt.xlabel( 'long-run average reward ')\\nplt.ylabel( 'density ')\\nsns.kdeplot(A.flatten(),shade=True)\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 109, 'page_label': '92'}, page_content='92 Monte Carlo Estimation\\n3.3.3 Variance Reduction\\nThe estimation of performance measures in Monte Carlo simulation can be made more\\nefficient by utilizing known information about the simulation model. Variance reduction\\ntechniques include antithetic variables, control variables, importance sampling, conditional\\nMonte Carlo, and stratified sampling; see, for example, [71, Chapter 9]. We shall only deal\\nwith control variables and importance sampling here.\\nSuppose Y is the output of a simulation experiment. A random variable eY, obtained\\nfrom the same simulation run, is called a control variablecontrol\\nvariable\\nfor Y if Y and eY are correlated\\n(negatively or positively) and the expectation of eY is known. The use of control variables\\nfor variance reduction is based on the following theorem. We leave its proof to Exercise 21.\\n☞119\\nTheorem 3.3: Control Variable Estimation\\nLet Y1,..., YN be the output of N independent simulation runs and let eY1,..., eYN be\\nthe corresponding control variables, with EeYk = eµknown. Let ϱY,eY be the correlation\\ncoefficient between each Yk and eYk. For each α∈Rthe estimator\\nbµ(c) = 1\\nN\\nNX\\nk=1\\nh\\nYk −α\\n\\x10eYk −eµ\\n\\x11i\\n(3.15)\\nis an unbiased estimator for µ= EY. The minimal variance of bµ(c) is\\nVar bµ(c) = 1\\nN (1 −ϱ2\\nY,eY ) Var Y, (3.16)\\nwhich is obtained for α= ϱY,eY\\nq\\nVarY/VareY.\\nFrom (3.16) we see that, by using the optimal αin (3.15), the variance of the control\\nvariate estimator is a factor 1 −ϱ2\\nY,eY smaller than the variance of the crude Monte Carlo\\nestimator. Thus, if eY is highly correlated with Y, a significant variance reduction can be\\nachieved. The optimalαis usually unknown, but it can be easily estimated from the sample\\ncovariance matrix of {(Yk,eYk)}.☞456\\nIn the next example, we estimate the multiple integral in Example 3.10 using control\\nvariables.\\nExample 3.13 (Monte Carlo Integration (cont.))☞86 The random variable Y = |X1 + X2 +\\nX3|1/2(2π)3/2 is positively correlated with the random variable eY = X2\\n1 + X2\\n2 + X2\\n3 , for the\\nsame choice of X1,X2,X3\\niid\\n∼N(0,1). As EeY = Var(X1 + X2 + X3) = 3, we can use it as a\\ncontrol variable to estimate the expectation of Y. The following Python program is based\\non Theorem 3.3. It imports the crude Monte Carlo sampling code from Example 3.10.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 110, 'page_label': '93'}, page_content='Monte Carlo Methods 93\\nmcintCV.py\\nfrom mcint import *\\nYc = np. sum (x**2, axis=1) # control variable data\\nyc = 3 # true expectation of control variable\\nC = np.cov(y,Yc) # sample covariance matrix\\ncor = C[0][1]/np.sqrt(C[0][0]*C[1][1])\\nalpha = C[0][1]/C[1][1]\\nest = np.mean(y-alpha*(Yc-yc))\\nRECV = np.sqrt((1-cor**2)*C[0][0]/N)/est #relative error\\nprint (\\'Estimate = {:3.3f}, CI = ({:3.3f},{:3.3f}), Corr = {:3.3f} \\'.\\nformat (est, est*(1-z*RECV), est*(1+z*RECV),cor))\\nEstimate = 17.045, CI = (17.032,17.057), Corr = 0.480\\nA typical estimate of the correlation coefficient ϱY,eY is 0.48, which gives a reduction of\\nthe variance with a factor 1−0.482 ≈0.77 — a simulation speed-up of 23% compared with\\ncrude Monte Carlo. Although the gain is small in this case, due to the modest correlation\\nbetween Y and eY, little extra work was required to achieve this variance reduction.\\nOne of the most important variance reduction techniques is importance sampling importance\\nsampling\\n. This\\ntechnique is especially useful for the estimation of very small probabilities. The standard\\nsetting is the estimation of a quantity\\nµ= Ef H(X) =\\nZ\\nH(x) f (x) dx, (3.17)\\nwhere H is a real-valued function and f the probability density of a random vector X,\\ncalled the nominal pdf. The subscript f is added to the expectation operator to indicate that nominal pdf\\nit is taken with respect to the density f .\\nLet g be another probability density such that g(x) = 0 implies that H(x) f (x) = 0.\\nUsing the density g we can represent µas\\nµ=\\nZ\\nH(x) f (x)\\ng(x) g(x) dx = Eg\\n\"\\nH(X) f (X)\\ng(X)\\n#\\n. (3.18)\\nConsequently, if X1,..., XN ∼iid g, then\\nbµ= 1\\nN\\nNX\\nk=1\\nH(Xk) f (Xk)\\ng(Xk) (3.19)\\nis an unbiased estimator of µ. This estimator is called the importance sampling estimator importance\\nsampling\\nestimator\\nand g is called the importance sampling density. The ratio of densities, f (x)/g(x), is called\\nthe likelihood ratio. The importance sampling pseudo-code is given in Algorithm 3.3.2. likelihood ratio'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 111, 'page_label': '94'}, page_content=\"94 Monte Carlo Estimation\\nAlgorithm 3.3.2: Importance Sampling Estimation\\ninput: Function H, importance sampling density g such that g(x) = 0 for all x for\\nwhich H(x) f (x) = 0, sample size N, confidence level 1 −α.\\noutput: Point estimate and approximate (1 −α) confidence interval for\\nµ= EH(X), where X ∼f .\\n1 Simulate X1,..., XN\\niid\\n∼g and let Yi = H(Xi) f (Xi)/g(Xi), i = 1,..., N.\\n2 Estimate µvia bµ= Y and determine an approximate (1 −α) confidence interval as\\nI:=\\n \\nbµ−z1−α/2\\nS√\\nN\\n, bµ+ z1−α/2\\nS√\\nN\\n!\\n,\\nwhere zγ denotes the γ-quantile of the N(0,1) distribution and S is the sample\\nstandard deviation of Y1,..., YN.\\n3 return bµand the interval I.\\nExample 3.14 (Importance Sampling) Let us examine the workings of importance\\nsampling by estimating the area, µsay, under the graph of the function\\nM(x1,x2) = e−1\\n4\\n√\\nx2\\n1+x2\\n2\\n\\x12\\nsin\\n\\x12\\n2\\nq\\nx2\\n1 + x2\\n2\\n\\x13\\n+ 1\\n\\x13\\n, (x1,x2) ∈R2. (3.20)\\nWe saw a similar function in Example 3.8 (but note the di fferent domain). A natural ap-☞80\\nproach to estimate the area is to truncate the domain to the square [−b,b]2, for large enough\\nb, and to estimate the integral\\nµb =\\nZ b\\n−b\\nZ b\\n−b\\n(2b)2 M(x)|      {z      }\\nH(x)\\nf (x) dx = Ef H(X)\\nvia crude Monte Carlo, where f (x) = 1/(2b)2,x ∈[−b,b]2, is the pdf of the uniform distri-\\nbution on [−b,b]2. Here is the Python code which does just that.\\nimpsamp1.py\\nimport numpy as np\\nfrom numpy import exp, sqrt, sin, pi, log, cos\\nfrom numpy.random import rand\\nb = 1000\\nH = lambda x1, x2: (2*b)**2 * exp(-sqrt(x1**2+x2**2)/4)*(sin(2*sqrt(\\nx1**2+x2**2))+1)*(x1**2 + x2**2 < b**2)\\nf = 1/((2*b)**2)\\nN = 10**6\\nX1 = -b + 2*b*rand(N,1)\\nX2 = -b + 2*b*rand(N,1)\\nZ = H(X1,X2)\\nestCMC = np.mean(Z).item() # to obtain scalar\\nRECMC = np.std(Z)/estCMC/sqrt(N).item()\\nprint ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estCMC*(1-1.96*\\nRECMC), estCMC*(1+1.96*RECMC),RECMC))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 112, 'page_label': '95'}, page_content=\"Monte Carlo Methods 95\\nCI = (82.663,135.036), RE = 0.123\\nFor a truncation level of b = 1000 and a sample size of N = 106, a typical estimate is\\n108.8, with an estimated relative error of 0.123. We have two sources of error here. The\\nfirst is the error in approximatingµby µb. However, as the functionH decays exponentially\\nfast, b = 1000 is more than enough to ensure this error is negligible. The second type of\\nerror is the statistical error, due to the estimation process itself. This can be quantified by\\nthe estimated relative error, and can be reduced by increasing the sample size.\\nLet us now consider an importance sampling approach in which the importance\\nsampling pdf g is radially symmetric and decays exponentially in the radius, similar to the\\nfunction H. In particular, we simulate (X1,X2) in a way akin to Example 3.1, by first gen- ☞ 69\\nerating a radius R ∼Exp(λ) and an angle Θ ∼U(0,2π), and then returning X1 = R cos(Θ)\\nand X2 = R sin(Θ). By the Transformation Rule (Theorem C.4) we then have ☞ 433\\ng(x) = fR,Θ(r,θ)1\\nr = λe−λr 1\\n2π\\n1\\nr = λe−λ\\n√\\nx2\\n1+x2\\n2\\n2π\\nq\\nx2\\n1 + x2\\n2\\n, x ∈R2 \\\\{0}.\\nThe following code, which imports the one given above, implements the importance\\nsampling steps, using the parameter λ= 0.1.\\nimpsamp2.py\\nfrom impsamp1 import *\\nlam = 0.1;\\ng = lambda x1, x2: lam*exp(-sqrt(x1**2 + x2**2)*lam)/sqrt(x1**2 + x2\\n**2)/(2*pi);\\nU = rand(N,1); V = rand(N,1)\\nR = -log(U)/lam\\nX1 = R*cos(2*pi*V)\\nX2 = R*sin(2*pi*V)\\nZ = H(X1,X2)*f/g(X1,X2)\\nestIS = np.mean(Z).item() # obtain scalar\\nREIS = np.std(Z)/estIS/sqrt(N).item()\\nprint ('CI = ({:3.3f},{:3.3f}), RE = {: 3.3f} '.format (estIS*(1-1.96*\\nREIS), estIS*(1+1.96*REIS),REIS))\\nCI = (100.723,101.077), RE = 0.001\\nA typical estimate is 100.90 with an estimated relative error of 1 ·10−4, which gives\\na substantial variance reduction. In terms of approximate 95% confidence intervals, we\\nhave (82.7,135.0) in the CMC case versus (100.7,101.1) in the importance sampling case.\\nOf course, we could have reduced the truncation level b to improve the performance of\\nCMC, but then the approximation error might become more significant. For the importance\\nsampling case, the relative error is hardly affected by the threshold level, but does depend\\non the choice of λ. We chose λsuch that the decay rate is slower than the decay rate of the\\nfunction H, which is 0.25.\\nAs illustrated in the above example, a main difficulty in importance sampling is how to\\nchoose the importance sampling distribution. A poor choice of g may seriously affect the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 113, 'page_label': '96'}, page_content='96 Monte Carlo for Optimization\\naccuracy of both the estimate and the confidence interval. The theoretically optimal choice\\ng∗ for the importance sampling density minimizes the variance of bµ and is therefore the\\nsolution to the functional minimization program\\nmin\\ng\\nVarg\\n \\nH(X) f (X)\\ng(X)\\n!\\n. (3.21)\\nIt is not difficult to show, see also Exercise 22, that if either H(x) ⩾0 or H(x) ⩽0 for all☞119\\nx, then the optimal importance sampling pdfoptimal\\nimportance\\nsampling pdf\\nis\\ng∗(x) = H(x) f (x)\\nµ . (3.22)\\nNamely, in this caseVarg∗bµ= Varg∗(H(X) f (X)/g(X)) = Varg∗µ= 0, so that the estimatorbµ\\nis constant under g∗. An obvious difficulty is that the evaluation of the optimal importance\\nsampling density g∗is usually not possible, since g∗(x) in (3.22) depends on the unknown\\nquantity µ. Nevertheless, one can typically choose a good importance sampling density g\\n“close” to the minimum variance density g∗.\\nOne of the main considerations for choosing a good importance sampling pdf is that\\nthe estimator (3.19) should have finite variance. This is equivalent to the requirement\\nthat\\nEg\\n\"\\nH2(X) f 2(X)\\ng2(X)\\n#\\n= Ef\\n\"\\nH2(X) f (X)\\ng(X)\\n#\\n<∞. (3.23)\\nThis suggests that g should not have lighter tails than f and that, preferably, the\\nlikelihood ratio, f /g, should be bounded.\\n3.4 Monte Carlo for Optimization\\nIn this section we describe several Monte Carlo methods for optimization. Such random-\\nized algorithms can be useful for solving optimization problems with many local optima\\nand complicated constraints, possibly involving a mix of continuous and discrete variables.\\nRandomized algorithms are also used to solve noisy optimization problems, in which the\\nobjective function is unknown and has to be obtained via Monte Carlo simulation.\\n3.4.1 Simulated Annealing\\nSimulated annealing is a Monte Carlo technique for minimization that emulates the phys-Simulated\\nannealing ical state of atoms in a metal when the metal is heated up and then slowly cooled down.\\nWhen the cooling is performed very slowly, the atoms settle down to a minimum-energy\\nstate. Denoting the state as x and the energy of a state as S (x), the probability distribution\\nof the (random) states is described by the Boltzmann pdf\\nf (x) ∝e−S (x)\\nkT , x ∈X,\\nwhere k is Boltzmann’s constant and T is the temperature.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 114, 'page_label': '97'}, page_content='Monte Carlo Methods 97\\nGoing beyond the physical interpretation, suppose that S (x) is an arbitrary function to\\nbe minimized, with x taking values in some discrete or continuous set X. The Gibbs pdf Gibbs pdfcorresponding to S (x) is defined as\\nfT (x) = e−S (x)\\nT\\nzT\\n, x ∈X,\\nprovided that the normalization constant zT := P\\nx exp(−S (x)/T) is finite. Note that this\\nis simply the Boltzmann pdf with the Boltzmann constant k removed. As T →0, the pdf\\nbecomes more and more peaked around the set of global minimizers of S .\\nThe idea of simulated annealing is to create a sequence of pointsX1,X2,... that are ap-\\nproximately distributed according to pdfs fT1 (x), fT2 (x),... , where T1,T2,... is a sequence\\nof “temperatures” that decreases (is “cooled”) to 0 — known as the annealing schedule. If annealing\\nscheduleeach Xt were sampled exactly from fTt , then Xt would converge to a global minimum of\\nS (x) as Tt →0. However, in practice sampling isapproximate and convergence to a global\\nminimum is not assured. A generic simulated annealing algorithm is as follows.\\nAlgorithm 3.4.1: Simulated Annealing\\ninput: Annealing schedule T0,T1,..., , function S , initial value x0.\\noutput: Approximations to the global minimizer x∗and minimum value S (x∗).\\n1 Set X0 ←x0 and t ←1.\\n2 while not stopping do\\n3 Approximately simulate Xt from fTt (x).\\n4 t ←t + 1\\n5 return Xt,S (Xt)\\nA popular annealing schedule is geometric cooling, where Tt = βTt−1, t = 1,2,... , for geometric\\ncoolinga given initial temperature T0 and a cooling factor β ∈(0,1). Appropriate values for T0\\ncooling factorand βare problem-dependent and this has traditionally required tuning on the part of the\\nuser. A possible stopping criterion is to stop after a fixed number of iterations, or when the\\ntemperature is “small enough”.\\nApproximate sampling from a Gibbs distribution is most often carried out via Markov\\nchain Monte Carlo. For each iteration t, the Markov chain should theoretically run for a\\nlarge number of steps to accurately sample from the Gibbs pdf fTt . However, in practice,\\none often only runs a single step of the Markov chain, before updating the temperature, as\\nin Algorithm 3.4.2 below.\\nTo sample from a Gibbs distributionfT , this algorithm uses a random walk Metropolis–\\nHastings sampler. From (3.7), the acceptance probability of a proposal y is thus ☞ 80\\nα(x,y) = min\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\ne−1\\nT S (y)\\ne−1\\nT S (x)\\n, 1\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe = min\\nn\\ne−1\\nT (S (y)−S (x)), 1\\no\\n.\\nHence, if S (y) < S (x), then the proposal is aways accepted. Otherwise, the proposal is\\naccepted with probability exp(−1\\nT (S (y) −S (x))).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 115, 'page_label': '98'}, page_content='98 Monte Carlo for Optimization\\nAlgorithm 3.4.2: Simulated Annealing with a Random Walk Sampler\\ninput: Objective function S , starting state X0, initial temperature T0, number of\\niterations N, symmetric proposal density q(y |x), constant β.\\noutput: Approximate minimizer and minimum value of S .\\n1 for t = 0 to N −1 do\\n2 Simulate a new state Y from the symmetric proposal q(y |Xt).\\n3 if S (Y) <S (Xt) then\\n4 Xt+1 ←Y\\n5 else\\n6 Draw U ∼U(0,1).\\n7 if U ⩽e−(S (Y)−S (Xt))/Tt then\\n8 Xt+1 ←Y\\n9 else\\n10 Xt+1 ←Xt\\n11 Tt+1 ←βTt\\n12 return XN and S (XN)\\nExample 3.15 (Simulated Annealing for Minimization) Let us minimize the “wig-\\ngly” function depicted in the bottom panel of Figure 3.10 and given by:\\nS (x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n−e−x2/100 sin(13x −x4)5 sin(1 −3x2)2, if −2 ⩽x ⩽2,\\n∞, otherwise.\\n0\\n1\\n2\\n3\\n4\\n5f (x) = e!S(x)=T\\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2\\nx\\n-1\\n0\\n1\\nS(x)\\nT = 1\\nT = 0:4\\nT = 0:2\\nFigure 3.10: Lower panel: the “wiggly” function S (x). Upper panel: three (normalized)\\nGibbs pdfs for temperatures T = 1,0.4,0.2. As the temperature decreases, the Gibbs pdf\\nconverges to the pdf that has all its mass concentrated at the minimizer of S .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 116, 'page_label': '99'}, page_content=\"Monte Carlo Methods 99\\nThe function has many local minima and maxima, with a global minimum around 1.4.\\nThe figure also illustrates the relationship betweenS and the (unnormalized) Gibbs pdf fT .\\nThe following Python code implements a slight variant of Algorithm 3.4.2 where, in-\\nstead of stopping after a fixed number of iterations, the algorithm stops when the temper-\\nature is lower than some threshold (here 10−3).\\nInstead of stopping after a fixed number N of iterations or when the temperature\\nis low enough, it is useful to stop when consecutive function values are closer than\\nsome distance εto each other, or when the best found function value has not changed\\nover a fixed number d of iterations.\\nFor a “current” state x, the proposal state Y is here drawn from the N(x,0.52) distri-\\nbution. We use geometric cooling with decay parameter β= 0.999 and initial temperature\\nT0 = 1. We set the initial state to x0 = 0. Figure 3.11 depicts a realization of the sequence\\nof states xt for t = 0,1,... . After initially fluctuating wildly, the sequence settles down\\nto a value around 1.37, with S (1.37) = −0.92, corresponding to the global optimizer and\\nminimum, respectively.\\nsimann.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef wiggly(x):\\ny = -np.exp(x**2/100)*np.sin(13*x-x**4)**5*np.sin(1-3*x**2)**2\\nind = np.vstack((np.argwhere(x<-2),np.argwhere(x>2)))\\ny[ind]= float ('inf')\\nreturn y\\nS = wiggly\\nbeta = 0.999\\nsig = 0.5\\nT=1\\nx= np.array([0])\\nxx=[]\\nSx=S(x)\\nwhile T>10**(-3):\\nT=beta*T\\ny = x+sig*np.random.randn()\\nSy = S(y)\\nalpha = np.amin((np.exp(-(Sy-Sx)/T),1))\\nif np.random.uniform()<alpha:\\nx=y\\nSx=Sy\\nxx=np.hstack((xx,x))\\nprint ('minimizer = {:3.3f}, minimum ={:3.3f} '.format (x[0],Sx[0]))\\nplt.plot(xx)\\nplt.show()\\nminimizer = 1.365, minimum = -0.958\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 117, 'page_label': '100'}, page_content='100 Monte Carlo for Optimization\\n0\\n 1000\\n 2000\\n 3000\\n 4000\\n 5000\\n 6000\\n 7000\\nnumber of iterations\\n2.0\\n1.5\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nstate\\nFigure 3.11: Typical states generated by the simulated annealing algorithm.\\n3.4.2 Cross-Entropy Method\\nThe cross-entropy (CE) method [103] is a simple Monte Carlo algorithm that can be usedcross-entropy\\nfor both optimization and estimation.\\nThe basic idea of the CE method for minimizing a function S on a set Xis to define\\na parametric family of probability densities {f (·|v),v ∈V} on Xand to iteratively update\\nthe parameter v so that f (·|v) places more mass on states x that have smaller S values than\\non the previous iteration. In particular, the CE algorithm has two basic phases:\\n• Sampling: Samples X1,..., XN are drawn independently according to f (·|v). The\\nobjective function S is evaluated at these points.\\n• Updating: A new parameter v′is selected on the basis of those Xi for which S (Xi) ⩽\\nγfor some level γ. These {Xi}form the elite sample set, E.elite sample\\nAt each iteration the level parameter γ is chosen as the worst of the Nelite := ⌈ϱN⌉\\nbest performing samples, where ϱ∈(0,1) is the rarity parameterrarity\\nparameter\\n— typically, ϱ= 0.1 or\\nϱ= 0.01. The parameter v is updated as a smoothed averageαv′+(1−α)v, where α∈(0,1)\\nis the smoothing parametersmoothing\\nparameter\\nand\\nv′:= argmax\\nv∈V\\nX\\nX∈E\\nln f (X |v). (3.24)\\nThe updating rule (3.24) is the result of minimizing the Kullback–Leibler divergence\\nbetween the conditional density of X ∼f (x |v) given S (X) ⩽γ, and f (x; v); see [103].\\nNote that (3.24) yields the maximum likelihood estimator (MLE) of v based on the elite☞456\\nsamples. Hence, for many specific families of distributions, explicit solutions can be found.\\nAn important example is where X ∼N(µ,diag(σ2)); that is, X has independent Gaussian'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 118, 'page_label': '101'}, page_content='Monte Carlo Methods 101\\ncomponents. In this case, the mean vector µ and the vector of variances σ2 are simply\\nupdated via the sample mean and sample variance of the elite samples. This is known as\\nnormal updating. A generic CE procedure for minimization is given in Algorithm 3.4.3. normal\\nupdating\\nAlgorithm 3.4.3: Cross-Entropy Method for Minimization\\ninput: Function S,initial sampling parameter v0, sample size N, rarity parameter\\nϱ, smoothing parameter α.\\noutput: Approximate minimum of S and optimal sampling parameter v.\\n1 Initialize v0, set Nelite ←⌈ϱN⌉and t ←0.\\n2 while a stopping criterion is not met do\\n3 t ←t + 1\\n4 Simulate an iid sample X1,..., XN from the density f (·|vt−1).\\n5 Evaluate the performances S (X1),..., S (XN) and sort them from smallest to\\nlargest: S (1),..., S (N).\\n6 Let γt be the sample ϱ-quantile of the performances:\\nγt ←S (Nelite). (3.25)\\n7 Determine the set of elite samples Et = {Xi : S (Xi) ⩽γt}.\\n8 Let v′\\nt be the MLE of the elite samples:\\nv′\\nt ←argmax\\nv\\nX\\nX∈Et\\nln f (X |v). (3.26)\\n9 Update the sampling parameter as\\nvt ←αv′\\nt + (1 −α)vt−1. (3.27)\\n10 return γt, vt\\nThe CE algorithm produces a sequence of pairs ( γ1,v1),(γ2,v2),..., such that γt con-\\nverges (approximately) to the minimal function value, and f (·|vt) to a degenerate pdf that\\n(approximately) concentrates all its mass at a minimizer of S , as t →∞. A possible stop-\\nping condition is to stop when the sampling distribution f (·|vt) is su fficiently close to a\\ndegenerate distribution. For normal updating this means that the standard deviation is suf-\\nficiently small.\\nThe output of the CE algorithm could also include the overall best function value\\nand corresponding solution.\\nIn the following example, we minimize the same function as in Example 3.15, but ☞ 97\\ninstead use the CE algorithm.\\nExample 3.16 (Cross-Entropy Method for Minimization) In this case we take the\\nfamily of normal distributions{N(µ,σ2)}for the sampling step (Step 4 of Algorithm 3.4.3),\\nstarting with µ= 0 and σ= 3. The choice of the initial parameter is quite arbitrary, as long\\nas σis large enough to sample a wide range of points. We takeN = 100 samples at each it-\\neration, set ϱ= 0.1, and keep the Nelite = 10 = ⌈Nϱ⌉smallest ones as the elite samples. The\\nparameters µand σare then updated via the sample mean and sample standard deviation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 119, 'page_label': '102'}, page_content=\"102 Monte Carlo for Optimization\\nof the elite samples. In this case we do not use any smoothing ( α = 1). In the following\\nPython code the 100 ×2 matrix Sx stores the x-values in the first column and the func-\\ntion values in the second column. The rows of this matrix are sorted in ascending order\\naccording to the function values, giving the matrix sortSx. The first Nelite = 10 rows of\\nthis sorted matrix correspond to the elite samples and their function values. The updating\\nof µ and σ is done in Lines 14 and 15. Figure 3.12 shows how the pdfs of the N(µt,σ2\\nt )\\nsampling distributions degenerate to the point mass at the global minimizer 1.366.\\nCEmethod.py\\nfrom simann import wiggly\\nimport numpy as np\\nnp.set_printoptions(precision=3)\\nmu, sigma = 0, 3\\nN, Nel = 100, 10\\neps = 10**-5\\nS = wiggly\\nwhile sigma > eps:\\nX = np.random.randn(N,1)*sigma + np.array(np.ones((N,1)))*mu\\nSx = np.hstack((X, S(X)))\\nsortSx = Sx[Sx[:,1].argsort(),]\\nElite = sortSx[0:Nel,:-1]\\nmu = np.mean(Elite, axis=0)\\nsigma = np.std(Elite, axis=0)\\nprint ('S(mu)= {}, mu: {}, sigma: {}\\\\n '.format (S(mu), mu, sigma))\\nS(mu)= [0.071], mu: [0.414], sigma: [0.922]\\nS(mu)= [0.063], mu: [0.81], sigma: [0.831]\\nS(mu)= [-0.033], mu: [1.212], sigma: [0.69]\\nS(mu)= [-0.588], mu: [1.447], sigma: [0.117]\\nS(mu)= [-0.958], mu: [1.366], sigma: [0.007]\\nS(mu)= [-0.958], mu: [1.366], sigma: [0.]\\nS(mu)= [-0.958], mu: [1.366], sigma: [3.535e-05]\\nS(mu)= [-0.958], mu: [1.366], sigma: [2.023e-06]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 120, 'page_label': '103'}, page_content='Monte Carlo Methods 103\\n-2 -1 0 1 2 3 x\\n0\\n0.5\\n1\\n1.5\\n2\\nf (x; 7; <)\\n5 \\n4 \\n2 \\n 1 iteration 0\\n3 \\nFigure 3.12: The normal pdfs of the first six sampling distributions, truncated to the interval\\n[−2,3]. The initial sampling distribution is N(0,32).\\n3.4.3 Splitting for Optimization\\nMinimizing a function S (x), x ∈X is closely related to drawing a random sample from a\\nlevel set of the form {x ∈X : S (x) ⩽γ}. Suppose S has minimum value γ∗attained at x∗. level set\\nAs long as γ ⩾γ∗, this level set contains the minimizer. Moreover, if γis close to γ∗, the\\nvolume of this level set will be small. So, a randomly selected point from this set is expected\\nto be close to x∗. Thus, by gradually decreasing the level parameter γ, the level sets will\\ngradually shrink towards the set {x∗}. Indeed, the CE method was developed with exactly\\nthis connection in mind; see, e.g., [102]. Note that the CE method employs a parametric\\nsampling distribution to obtain samples from the level sets (the elite samples). In [34]\\na non-parametric sampling mechanism is introduced that uses an evolving collection of\\nparticles. The resulting optimization algorithm, calledsplitting for continuous optimization splitting for\\ncontinuous\\noptimization(SCO), provides a fast and accurate way to optimize complicated continuous functions. The\\ndetails of SCO are given in Algorithm 3.4.4.\\nAt iteration t = 0, the algorithm starts with a population of particles Y0 = {Y1,..., YN}\\nthat are uniformly generated on some bounded region B, which is large enough to contain\\na global minimizer. The function values of all particles in Y0 are sorted, and the best\\nNelite = ⌈Nϱ⌉form the elite particle set X1, exactly as in the CE method. Next, the elite\\nparticles are “split” into ⌊N/Nelite⌋children particles, adding one extra child to some of\\nthe elite particles to ensure that the total number of children is again N. The purpose of\\nLine 4 is to randomize which elite particles receive an extra child. Lines 8–15 describe\\nhow the children of the i-th elite particle are generated. First, in Line 9, we select one\\nof the other elite particles uniformly at random. The same line defines an n-dimensional\\nvector σi whose components are the absolute differences between the vectors X(i) and X(I),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 121, 'page_label': '104'}, page_content='104 Monte Carlo for Optimization\\nAlgorithm 3.4.4: Splitting for Continuous Optimization (SCO)\\ninput: Objective function S , sample size N, rarity parameter ϱ, scale factor w,\\nbounded region B⊂X that is known to contain a global minimizer, and\\nmaximum number of attempts MaxTry.\\noutput: Final iteration number t and sequence (Xbest,1,b1),..., (Xbest,t,bt) of best\\nsolutions and function values at each iteration.\\n1 Simulate Y0 = {Y1,..., YN}uniformly on B. Set t ←0 and Nelite ←⌈Nϱ⌉.\\n2 while stopping condition is not satisfied do\\n3 Determine the Nelite smallest values, S (1) ⩽··· ⩽S (Nelite), of {S (X),X ∈Yt},\\nand store the corresponding vectors, X(1),..., X(Nelite), in Xt+1. Set bt+1 ←S (1)\\nand Xbest,t+1 ←X(1).\\n4 Draw Bi ∼Bernoulli(1\\n2 ), i = 1,..., Nelite, with PNelite\\ni=1 Bi = N mod Nelite.\\n5 for i = 1 to Nelite do\\n6 Ri ←\\nj\\nN\\nNelite\\nk\\n+ Bi // random splitting factor\\n7 Y ←X(i); Y′←Y\\n8 for j = 1 to Ri do\\n9 Draw I ∈{1,..., Nelite}\\\\{i}uniformly and let σi ←w|X(i) −X(I)|.\\n10 Simulate a uniform permutation π= (π1,...,π n) of (1,..., n).\\n11 for k = 1 to n do\\n12 for Try = 1 to MaxTrydo\\n13 Y′(πk) ←Y(πk) + σi(πk)Z, Z ∼N(0,1)\\n14 if S (Y′) <S (Y) then Y ←Y′and break.\\n15 Add Y to Yt+1\\n16 t ←t + 1\\n17 return {(Xbest,k,bk),k = 1,..., t}\\nmultiplied by a constant w. That is,\\nσi = w |X(i) −X(I)|:= w\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n|X(i),1 −X(I),1|\\n|X(i),2 −X(I),2|\\n...\\n|X(i),n −X(I),n|\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nNext, a uniform random permutation π of (1,..., n) is simulated (see Exercise 9). Lines☞115\\n11–14 describe how, starting from a candidate child point Y, each coordinate of Y is re-\\nsampled, in the order determined by π, by adding a standard normal random variable to\\nthat component, multiplied by the corresponding component of σi (Line 13). If the result-\\ning Y′ has a function value that is less than that of Y, then the new candidate is accepted.\\nOtherwise, the same coordinate is tried again. If no improvement is found in MaxTry at-\\ntempts, the original component is retained. This process is performed for all elite samples,\\nto produce the first-generation population Y1. The procedure is then repeated for iterations\\nt = 1,2,... , until some stopping criterion is met, e.g., when the best found function value\\ndoes not change for a number of consecutive iterations, or when the total number of func-\\ntion evaluations exceeds some threshold. The best found function value and corresponding'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 122, 'page_label': '105'}, page_content='Monte Carlo Methods 105\\nargument (particle) are returned at the conclusion of the algorithm.\\nThe input variable MaxTry governs how much computational time is dedicated to up-\\ndating a component. In most cases we have encountered, the choices w = 0.5 and MaxTry\\n= 5 work well. Empirically, relatively high value for ϱwork well, such as ϱ = 0.4,0.8, or\\neven ϱ = 1. The latter case means that at each stage t all samples from Yt−1 carry over to\\nthe elite set Xt.\\nExample 3.17 (Test Problem 112) Hock and Schittkowski [58] provide a rich source\\nof test problems for multiextremal optimization. A challenging one is Problem 112, where\\nthe goal is to find x so as to minimize the function\\nS (x) =\\n10X\\nj=1\\nxj\\n \\ncj + ln xj\\nx1 + ··· + x10\\n!\\n,\\nsubject to the following set of constraints:\\nx1 + 2x2 + 2x3 + x6 + x10 −2 = 0,\\nx4 + 2x5 + x6 + x7 −1 = 0,\\nx3 + x7 + x8 + 2x9 + x10 −1 = 0,\\nxj ⩾ 0.000001, j = 1,..., 10,\\nwhere the constants {ci}are given in Table 3.1.\\nTable 3.1: Constants for Test Problem 112.\\nc1 = −6.089 c2 = −17.164 c3 = −34.054 c4 = −5.914 c5 = −24.721\\nc6 = −14.986 c7 = −24.100 c8 = −10.708 c9 = −26.662 c10 = −22.179\\nThe best known minimal value in [58] was −47.707579. In [89] a better solution was\\nfound, −47.760765, using a genetic algorithm. The corresponding solution vector was\\ncompletely di fferent from the one in [58]. A further improvement, −47.76109081, was\\nfound in [70], using the CE method, giving a similar solution vector to that in [89]:\\n0.04067247 0.14765159 0.78323637 0.00141368 0.48526222\\n0.00069291 0.02736897 0.01794290 0.03729653 0.09685870\\nTo obtain a solution with SCO, we first converted this 10-dimensional problem into a\\n7-dimensional one by defining the objective function\\nS 7(y) = S (x),\\nwhere x2 = y1,x3 = y2,x5 = y3,x6 = y4,x7 = y5,x9 = y6,x10 = y7, and\\nx1 = 2 −(2y1 + 2y2 + y4 + x7),\\nx4 = 1 −(2y3 + y4 + y5),\\nx8 = 1 −(y2 + y5 + 2y6 + y7),\\nsubject to x1,..., x10 ⩾0.000001, where the {xi}are taken as functions of the {yi}. We then\\nadopted a penalty approach (see Section B.4) by adding a penalty function to the original ☞ 415'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 123, 'page_label': '106'}, page_content='106 Monte Carlo for Optimization\\nobjective function:\\neS 7(y) = S (x) + 1000\\n10X\\ni=1\\nmax{−(xi −0.000001),0},\\nwhere, again, the {xi}are defined in terms of the {yi}as above.\\nOptimizing this last function with SCO, we found, in less time than the other al-\\ngorithms, a slightly smaller function value: −47.761090859365858, with solution vector\\n0.040668102417464 0.147730393049955 0.783153291185250 0.001414221643059\\n0.485246633088859 0.000693172682617 0.027399339496606 0.017947274343948\\n0.037314369272343 0.096871356429511\\nin line with the earlier solutions.\\n3.4.4 Noisy Optimization\\nIn noisy optimizationnoisy\\noptimization\\n, the objective function is unknown, but estimates of function val-\\nues are available, e.g., via simulation. For example, to find an optimal prediction function\\ng in supervised learning, the exact risk ℓ(g) = ELoss(Y,g(x)) is usually unknown and\\nonly estimates of the risk are available. Optimizing the risk is thus typically a noisy op-☞20\\ntimization problem. Noisy optimization features prominently in simulation studies where\\nthe behavior of some system (e.g., vehicles on a road network) is simulated under certain\\nparameters (e.g., the lengths of the tra ffic light intervals) and the aim is to choose those\\nparameters optimally (e.g., to maximize the traffic throughput). For each parameter setting\\nthe exact value for the objective function is unknown but estimates can be obtained via the\\nsimulation.\\nIn general, suppose the goal is to minimize a function S , where S is unknown, but\\nan estimate of S (x) can be obtained for any choice of x ∈X. Because the gradient ∇S is\\nunknown, one cannot directly apply classical optimization methods. Thestochastic approx-\\nimation method mimics the classical gradient descent method by replacing a deterministicstochastic\\napproximation gradient with an estimate c∇S (x).\\nA simple estimator for the i-th component of ∇S (x) (that is, ∂S (u)/∂xi), is the central\\ndifference estimatorcentral\\ndifference\\nestimator bS (x + ei δ/2) −bS (x −ei δ/2)\\nδ , (3.28)\\nwhere ei denotes the i-th unit vector, andbS (x+ei δ/2) and bS (x−ei δ/2) can be any estimators\\nof S (x + ei δ/2) and S (x −ei δ/2), respectively. The difference parameter δ >0 should be\\nsmall enough to reduce the bias of the estimator, but large enough to keep the variance of\\nthe estimator small.\\nTo reduce the variance in the estimator (3.28) it is important to have bS (x + ei δ/2)\\nand bS (x −ei δ/2) positively correlated. This can for example be achieved by using\\ncommon random numberscommon random\\nnumbers\\nin the simulation.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 124, 'page_label': '107'}, page_content='Monte Carlo Methods 107\\nIn direct analogy to gradient descent methods, the stochastic approximation method ☞ 412\\nproduces a sequence of iterates, starting with some x1 ∈X, via\\nxt+1 = xt −βt c∇S (xt), (3.29)\\nwhere β1,β2,... is a sequence of strictly positive step sizes. A generic stochastic approx-\\nimation algorithm for minimizing a function S is thus as follows.\\nAlgorithm 3.4.5: Stochastic Approximation\\ninput: A mechanism to estimate any gradient ∇S (x) and step sizes β1,β2,... .\\noutput: Approximate optimizer of S .\\n1 Initialize x1 ∈X. Set t ←1.\\n2 while a stopping criterion is not met do\\n3 Obtain an estimated gradient c∇S (xt) of S at xt.\\n4 Determine a step size βt.\\n5 Set xt+1 ←xt −βt c∇S (xt).\\n6 t ←t + 1\\n7 return xt\\nWhen c∇S (xt) is an unbiased estimator of ∇S (xt) in (3.29) the stochastic approxima-\\ntion Algorithm 3.4.5 is referred to as the Robbins–Monro algorithm. When finite di ffer- Robbins–Monroences are used to estimate c∇S (xt), as in (3.28), the resulting algorithm is known as the\\nKiefer–Wolfowitz algorithm. In Section 9.4.1 we will see how stochastic gradient descent Kiefer–\\nWolfowitzis employed in deep learning to minimize the training loss, based on a “minibatch” of\\ntraining data. ☞ 335\\nIt can be shown [72] that, under certain regularity conditions on S , the sequence\\nx1,x2,... converges to the true minimizer x∗ when the step sizes decrease slowly enough\\nto 0; in particular, when\\n∞X\\nt=1\\nβt = ∞ and\\n∞X\\nt=1\\nβ2\\nt <∞. (3.30)\\nIn practice, one rarely uses step sizes that satisfy (3.30), as the convergence of the\\nsequence will be too slow to be of practical use.\\nAn alternative approach to stochastic approximation is the stochastic counterpart stochastic\\ncounterpartmethod, also called sample average approximation. It can be applied in situations where\\nthe noisy objective function is of the form\\nS (x) = EeS (x,ξ), x ∈X, (3.31)\\nwhere ξis a random vector that can be simulated andeS (x,ξ) can be evaluated exactly. The\\nidea is to replace the optimization of (3.31) with that of the sample average\\nbS (x) = 1\\nN\\nNX\\ni=1\\neS (x,ξi), x ∈X, (3.32)\\nwhere ξ1,..., ξN are iid copies of ξ. Note that bS is a deterministic function of x and so can\\nbe optimized using any optimization algorithm. A solution to this sample average version\\nis taken to be an estimator of a solution x∗to the original problem (3.31).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 125, 'page_label': '108'}, page_content='108 Monte Carlo for Optimization\\nExample 3.18 (Determining Good Importance Sampling Parameters) The selection\\nof good importance sampling parameters can be viewed as a stochastic optimization prob-\\nlem. Consider, for instance, the importance sampling estimator in Example 3.14. Recall☞94\\nthat the nominal distribution is the uniform distribution on the square [−b,b]2, with pdf\\nfb(x) = 1\\n(2b)2 , x ∈[−b,b]2,\\nwhere b is large enough to ensure thatµb is close to µ; in that example, we choseb = 1000.\\nThe importance sampling pdf is\\ngλ(x) = fR,Θ(r,θ)1\\nr = λe−λr 1\\n2π\\n1\\nr = λe−λ\\n√\\nx2\\n1+x2\\n2\\n2π\\nq\\nx2\\n1 + x2\\n2\\n, x = (x1,x2) ∈R2 \\\\{0},\\nwhich depends on a free parameter λ. In the example we chose λ = 0.1. Is this the best\\nchoice? Maybe λ= 0.05 or 0.2 would have resulted in a more accurate estimate. The im-\\nportant thing to realize is that the “e ffectiveness” of λ can be measured in terms of the\\nvariance of the estimator bµin (3.19), which is given by☞93\\n1\\nN Vargλ\\n \\nH(X) f (X)\\ngλ(X)\\n!\\n= 1\\nN Egλ\\n\"\\nH2(X) f 2(X)\\ng2\\nλ(X)\\n#\\n−µ2\\nN = 1\\nN Ef\\n\"\\nH2(X) f (X)\\ngλ(X)\\n#\\n−µ2\\nN .\\nHence, the optimal parameter λ∗ minimizes the function S (λ) = Ef [H2(X) f (X)/gλ(X)],\\nwhich is unknown, but can be estimated from simulation. To solve this stochastic minim-\\nization problem, we first use stochastic approximation. Thus, at each step of the algorithm,\\nthe gradient of S (λ) is estimated from realizations of bS (λ) = H2(X) f (X)/gλ(X), where\\nX ∼ fb. As in the original problem (that is, the estimation of µ), the parameter b should\\nbe large enough to avoid any bias in the estimator of λ∗, but also small enough to en-\\nsure a small variance. The following Python code implements a particular instance of Al-\\ngorithm 3.4.5. For sampling from fb here, we used b = 100 instead of b = 1000, as this will\\nimprove the crude Monte Carlo estimation ofλ∗, without noticeably affecting the bias. The\\ngradient of S (λ) is estimated in Lines 11–17, using the central difference estimator (3.28).\\nNotice how for theS (λ−δ/2) and S (λ+δ/2) the same random vector X = [X1,X2]⊤is used.\\nThis significantly reduces the variance of the gradient estimator; see also Exercise 23. The☞119\\nstep size βt should be such that βt c∇S (xt) ≈λt. Given the large gradient here, we choose\\nβ0 = 10−7 and decrease it each step by a factor of 0 .99. Figure 3.13 shows how the se-\\nquence λ0,λ1,... decreases towards approximately 0 .125, which we take as an estimator\\nfor the optimal importance sampling parameter λ∗.\\nstochapprox.py\\nimport numpy as np\\nfrom numpy import pi\\nimport matplotlib.pyplot as plt\\nb=100 # choose b large enough , but not too large\\ndelta = 0.01\\nH = lambda x1, x2: (2*b)**2*np.exp(-np.sqrt(x1**2 + x2**2)/4)*(np.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 126, 'page_label': '109'}, page_content='Monte Carlo Methods 109\\nsin(2*np.sqrt(x1**2+x2**2)+1))*(x1**2+x2**2<b**2)\\nf = 1/(2*b)**2\\ng = lambda x1, x2, lam: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.\\nsqrt(x1**2+x2**2)/(2*pi)\\nbeta = 10**-7 #step size very small , as the gradient is large\\nlam=0.25\\nlams = np.array([lam])\\nN=10**4\\nfor i in range (200):\\nx1 = -b + 2*b*np.random.rand(N,1)\\nx2 = -b + 2*b*np.random.rand(N,1)\\nlamL = lam - delta/2\\nlamR = lam + delta/2\\nestL = np.mean(H(x1,x2)**2*f/g(x1, x2, lamL))\\nestR = np.mean(H(x1,x2)**2*f/g(x1, x2, lamR)) #use SAME x1,x2\\ngr = (estR-estL)/delta #gradient\\nlam = lam - gr*beta #gradient descend\\nlams = np.hstack((lams, lam))\\nbeta = beta*0.99\\nlamsize= range (0, (lams.size))\\nplt.plot(lamsize, lams)\\nplt.show()\\n0\\n 25\\n 50\\n 75\\n 100\\n 125\\n 150\\n 175\\n 200\\nsteps\\n0.12\\n0.14\\n0.16\\n0.18\\n0.20\\n0.22\\n0.24\\nFigure 3.13: The stochastic optimization algorithm produces a sequence λt,t = 0,1,2,...\\nthat tends to an approximate estimate of the optimal importance sampling parameter λ∗ ≈\\n0.125.\\nNext, we estimate λ∗using a stochastic counterpart approach. As the objective function\\nS (λ) is of the form (3.31) (with λtaking the role of x and X the role of ξ), we obtain the\\nsample average\\nbS (λ) = 1\\nN\\nNX\\ni=1\\nH2(Xi) f (Xi)\\ngλ(Xi), (3.33)\\nwhere X1,..., XN ∼iid fb. Once the X1,..., XN ∼iid fb have been simulated, bS (λ) is a de-\\nterministic function of λ, which can be optimized by any means. We take the most basic'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 127, 'page_label': '110'}, page_content=\"110 Monte Carlo for Optimization\\napproach and simply evaluate the function for λ = 0.01,0.02,..., 0.3 and select the min-\\nimizing λon this grid. The code is given below and Figure 3.14 shows bS (λ) as a function\\nof λ. The minimum value found was 0.60·104 for minimizerbλ∗= 0.12, which is in accord-\\nance with the value obtained via stochastic approximation. The sensitivity of this estimate\\ncan be assessed from the graph: for a wide range of values (say from 0.04 to 0.15) bS stays\\nrather flat. So any of these values could be used in an importance sampling procedure to\\nestimate µ. However, very small values (less than 0.02) and large values (greater than 0.25)\\nshould be avoided. Our original choice of λ= 0.1 was therefore justified and we could not\\nhave done much better.\\nstochcounterpart.py\\nfrom stochapprox import *\\nlams = np.linspace(0.01, 0.31, 1000)\\nres=[]\\nres = np.array(res)\\nfor i in range (lams.size):\\nlam = lams[i]\\nnp.random.seed(1)\\ng = lambda x1, x2: lam*np.exp(-np.sqrt(x1**2+x2**2)*lam)/np.sqrt\\n(x1**2+x2**2)/(2*pi)\\nX=-b+2*b*np.random.rand(N,1)\\nY=-b+2*b*np.random.rand(N,1)\\nZ=H(X,Y)**2*f/g(X,Y)\\nestCMC = np.mean(Z)\\nres = np.hstack((res, estCMC))\\nplt.plot(lams, res)\\nplt.xlabel(r '$\\\\lambda$ ')\\nplt.ylabel(r '$\\\\hat{S}(\\\\lambda)$ ')\\nplt.ticklabel_format(style= 'sci', axis= 'y', scilimits=(0,0))\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 128, 'page_label': '111'}, page_content='Monte Carlo Methods 111\\n0.00\\n 0.05\\n 0.10\\n 0.15\\n 0.20\\n 0.25\\n 0.30\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0S( )\\n1e4\\nFigure 3.14: The stochastic counterpart method replaces the unknown S (λ) (that is, the\\nscaled variance of the importance sampling estimator) with its sample average, bS (λ). The\\nminimum value of bS is attained around λ= 0.12.\\nA third method for stochastic optimization is the cross-entropy method. In particular,\\nAlgorithm 3.4.3 can easily be modified to minimize noisy functions S (x) = EeS (x,ξ), as ☞ 101\\ndefined in (3.31). The only change required in the algorithm is that every function value\\nS (x) be replaced by its estimate bS (x). Depending on the level of noise in the function, the\\nsample size N might have to be increased considerably.\\nExample 3.19 (Cross-Entropy Method for Noisy Optimization) To explore the use\\nof the CE method for noisy optimization, take the following noisy discrete optimization\\nproblem. Suppose there is a “black box” that contains an unknown binary sequence of n\\nbits. If one feeds the black box any input vector, it will first scramble the input by inde-\\npendently flipping the bits (changing 0 to 1 and 1 to 0) with a probabilityθand then return\\nthe number of bits that do not match the true (unknown) binary sequence. This is illustrated\\nin Figure 3.15 for n = 10.\\nFigure 3.15: A noisy optimization function as a black box. The input to the black box is a\\nbinary vector. Inside the black box the digits of the input vector are scrambled by flipping\\nbits with probability θ. The output is the number of bits of the scrambled vector that do not\\nmatch the true (unknown) binary vector.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 129, 'page_label': '112'}, page_content='112 Monte Carlo for Optimization\\nDenoting by S (x) the true number of matching digits for a binary input vector x, the\\nblack box thus returns a noisy estimate bS (x). The objective is to estimate the binary se-\\nquence inside the black box, by feeding it with many input vectors and observing their\\noutput. Or, to put it in a different way, to minimize S (x) using bS (x) as a proxy. Since there\\nare 2n possible input vectors, it is infeasible to try all possible vectors x even for moderate\\nn.\\nThe following Python program implements the noisy function bS (x) for n = 100. Each\\ninput bit is flipped with a rather high probability θ= 0.4, so that the output is a poor indic-\\nator of how many bits actually match the true vector. This true vector has 1s at positions\\n1,..., 50 and 0s at 51,..., 100.\\nSnoisy.py\\nimport numpy as np\\ndef Snoisy(X): #takes a matrix\\nn = X.shape[1]\\nN = X.shape[0]\\n# true binary vector\\nxorg = np.hstack((np.ones((1,n//2)), np.zeros((1,n//2))))\\ntheta = 0.4 # probability to flip the input\\n# storing the number of bits unequal to the true vector\\ns = np.zeros(N)\\nfor i in range (0,N):\\n# determine which bits to flip\\nflip = (np.random.uniform(size=(n)) < theta).astype( int )\\nind = flip>0\\nX[i][ind] = 1-X[i][ind]\\ns[i] = (X[i] != xorg). sum ()\\nreturn s\\nThe CE code below to optimize S (x) is quite similar to the continuous optimization\\ncode in Example 3.16. However, instead of sampling iid random variablesX1,..., XN from☞101\\na normal distribution, we now sample iid binary vectorsX1,..., XN from a Ber(p) distribu-\\ntion. More precisely, given a row vector of probabilitiesp = [p1,..., pn], we independently\\nsimulate the components X1,..., Xn of each binary vector X according to Xi ∼Ber(pi),\\ni = 1,..., n. After each iteration, the vector p is updated as the (vector) mean of the elite\\nsamples. The sample size isN = 1000 and the number of elite samples is 100. The compon-\\nents of the initial sampling vectorp are all equal to 1/2; that is, theX are initially uniformly\\nsampled from the set of all binary vectors of length n = 100. At each subsequent iteration\\nthe parameter vector is updated via the mean of the elite samples and evolves towards a\\ndegenerate vector p∗with only 1s and 0s. Sampling from such a Ber(p∗) distribution gives\\nan outcome x∗= p∗, which can be taken as an estimate for the minimizer of S ; that is, the\\ntrue binary vector hidden in the black box. The algorithm stops when p has degenerated\\nsufficiently.\\nFigure 3.16 shows the evolution of the vector of probabilities p. This figure may be\\nseen as the discrete analogue of Figure 3.12. We see that, despite the high noise, the CE\\nmethod is able to find the true state of the black box, and hence the minimum value of S .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 130, 'page_label': '113'}, page_content='Monte Carlo Methods 113\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0 10 20 30 40 50 60 70 80 90 100\\n0\\n0.5\\n1\\nFigure 3.16: Evolution of the vector of probabilities p = [p1,..., pn] towards the degener-\\nate solution.\\nCEnoisy.py\\nfrom Snoisy import Snoisy\\nimport numpy as np\\nn = 100\\nrho = 0.1\\nN = 1000; Nel = int (N*rho); eps = 0.01\\np = 0.5*np.ones(n)\\ni = 0\\npstart = p\\nps = np.zeros((1000,n))\\nps[0] = pstart\\npdist = np.zeros((1,1000))\\nwhile np.max (np.minimum(p,1-p)) > eps:\\ni += 1\\nX = (np.random.uniform(size=(N,n)) < p).astype( int )\\nX_tmp = np.array(X, copy=True)\\nSX = Snoisy(X_tmp)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 131, 'page_label': '114'}, page_content='114 Exercises\\nids = np.argsort(SX,axis=0)\\nElite = X[ids[0:Nel],:]\\np = np.mean(Elite,axis=0)\\nps[i] = p\\nprint (p)\\nFurther Reading\\nThe article [68] explores why the Monte Carlo method is so important in today’s quantitat-\\nive investigations. The Handbook of Monte Carlo Methods [71] provides a comprehensive\\noverview of Monte Carlo simulation that explores the latest topics, techniques, and real-\\nworld applications. Popular books on simulation and the Monte Carlo method include [42],\\n[75], and [104]. A classic reference on random variable generation is [32]. Easy introduc-\\ntions to stochastic simulation are given in [49], [98], and [100]. More advanced theory\\ncan be found in [5]. Markov chain Monte Carlo is detailed in [50] and [99]. The research\\nmonograph on the cross-entropy method is [103] and a tutorial is provided in [30]. A range\\nof optimization applications of the CE method is given in [16]. Theoretical results on ad-\\naptive tuning schemes for simulated annealing may be found, for example, in [111]. There\\nare several established ways for gradient estimation. These include the finite di fference\\nmethod, infinitesimal perturbation analysis, the score function method, and the method of\\nweak derivatives; see, for example, [51, Chapter 7].\\nExercises\\n1. We can modify the Box–Muller method in Example 3.1 to draw X and Y uniformly☞69\\non the unit disc, {(x,y) ∈R2 : x2 +y2 ⩽1}, in the following way: Independently draw\\na radius R and an angle Θ ∼U(0,2π), and return X = R cos(Θ),Y = R sin(Θ). The\\nquestion is how to draw R.\\n(a) Show that the cdf of R is given by FR(r) = r2 for 0 ⩽r ⩽1 (with FR(r) = 0 and\\nFR(r) = 1 for r <0 and r >1, respectively).\\n(b) Explain how to simulate R using the inverse-transform method.\\n(c) Simulate 100 independent draws of [ X,Y]⊤according to the method described\\nabove.\\n2. A simple acceptance–rejection method to simulate a vector X in the unit d-ball {x ∈\\nRd : ∥x∥⩽1}is to first generate X uniformly in the hyper cube [−1,1]d and then to\\naccept the point only if∥X∥⩽1. Determine an analytic expression for the probability\\nof acceptance as a function of d and plot this for d = 1,..., 50.\\n3. Let the random variable X have pdf\\nf (x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1\\n2 x , 0 ⩽x <1,\\n1\\n2 , 1 ⩽x ⩽5\\n2 .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 132, 'page_label': '115'}, page_content='Monte Carlo Methods 115\\nSimulate a random variable from f (x), using\\n(a) the inverse-transform method;\\n(b) the acceptance–rejection method, using the proposal density\\ng(x) = 8\\n25 x , 0 ⩽x ⩽5\\n2 .\\n4. Construct simulation algorithms for the following distributions:\\n(a) The Weib(α,λ) distribution, with cdf F(x) = 1 −e−(λx)α\\n,x ⩾0, where λ> 0 and\\nα> 0.\\n(b) The Pareto(α,λ) distribution, with pdf f (x) = αλ(1 + λx)−(α+1),x ⩾0, where\\nλ> 0 and α> 0.\\n5. We wish to sample from the pdf\\nf (x) = x e−x, x ⩾0,\\nusing acceptance–rejection with the proposal pdf g(x) = e−x/2/2, x ⩾0.\\n(a) Find the smallest C for which Cg(x) ⩾f (x) for all x.\\n(b) What is the e fficiency of this acceptance–rejection method?\\n6. Let [ X,Y]⊤ be uniformly distributed on the triangle with corners (0 ,0),(1,2), and\\n(−1,1). Give the distribution of [U,V]⊤defined by the linear transformation\\n\"U\\nV\\n#\\n=\\n\"1 2\\n3 4\\n#\" X\\nY\\n#\\n.\\n7. Explain how to generate a random variable from the extreme value distribution ,\\nwhich has cdf\\nF(x) = 1 −e−exp( x−µ\\nσ ) , −∞< x <∞, (σ> 0),\\nvia the inverse-transform method.\\n8. Write a program that generates and displays 100 random vectors that are uniformly\\ndistributed within the ellipse\\n5 x2 + 21 x y + 25 y2 = 9.\\n[Hint: Consider generating uniformly distributed samples within the circle of radius\\n3 and use the fact that linear transformations preserve uniformity to transform the\\ncircle to the given ellipse.]\\n9. Suppose that Xi ∼Exp(λi), independently, for alli = 1,..., n. Let Π = [Π1,..., Πn]⊤\\nbe the random permutation induced by the ordering XΠ1 < XΠ2 < ··· < XΠn , and\\ndefine Z1 := XΠ1 and Zj := XΠj −XΠj−1 for j = 2,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 133, 'page_label': '116'}, page_content='116 Exercises\\n(a) Determine an n ×n matrix A such that Z = AX and show that det(A) = 1.\\n(b) Denote the joint pdf of X and Π as\\nfX,Π(x,π) =\\nnY\\ni=1\\nλπi exp \\x00−λπi xπi\\n\\x01 ×1{xπ1 <··· < xπn }, x ⩾0, π∈Pn,\\nwhere Pn is the set of all n! permutations of {1,..., n}. Use the multivariate\\ntransformation formula (C.22) to show that☞432\\nfZ,Π(z,π) = exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−\\nnX\\ni=1\\nzi\\nX\\nk⩾i\\nλπk\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nnY\\ni=1\\nλi, z ⩾0, π∈Pn.\\nHence, conclude that the probability mass function of the random permutation\\nΠ is:\\nP[Π = π] =\\nnY\\ni=1\\nλπi\\nP\\nk⩾i λπk\\n, π∈Pn.\\n(c) Write pseudo-code to simulate a uniform random permutation Π ∈Pn; that is,\\nsuch that P[Π = π] = 1\\nn! , and explain how this uniform random permutation\\ncan be used to reshuffle a training set τn.\\n10. Consider the Markov chain with transition graph given in Figure 3.17, starting in\\nstate 1.\\nStart 0.5\\n1\\n0.8\\n0.9 0 .2\\n0.10.5\\n0.2\\n0.3\\n0.5\\n0.3\\n0.7\\n4\\n3\\n1\\n2 6\\n5\\nFigure 3.17: The transition graph for the Markov chain {Xt,t = 0,1,2,... }.\\n(a) Construct a computer program to simulate the Markov chain, and show a real-\\nization for N = 100 steps.\\n(b) Compute the limiting probabilities that the Markov chain is in state 1,2,. . . ,6,\\nby solving the global balance equations (C.42).☞452\\n(c) Verify that the exact limiting probabilities correspond to the average fraction\\nof times that the Markov process visits states 1,2,. . . ,6, for a large number of\\nsteps N.\\n11. As a generalization of Example C.9, consider a random walk on an arbitrary undir-☞453\\nected connected graph with a finite vertex set V. For any vertex v ∈V, let d(v) be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 134, 'page_label': '117'}, page_content='Monte Carlo Methods 117\\nthe number of neighbors ofv — called the degree of v. The random walk can jump to\\neach one of the neighbors with probability 1/d(v) and can be described by a Markov\\nchain. Show that, if the chain is aperiodic, the limiting probability that the chain is\\nin state v is equal to d(v)/P\\nv′∈Vd(v′).\\n12. Let U,V ∼iid U(0,1). The reason why in Example 3.7 the sample mean and sample ☞ 76\\nmedian behave very di fferently is that E[U/V] = ∞, while the median of U/V is\\nfinite. Show this, and compute the median. [Hint: start by determining the cdf of\\nZ = U/V by writing it as an expectation of an indicator function.]\\n13. Consider the problem of generating samples from Y ∼Gamma(2,10).\\n(a) Direct simulation: Let U1,U2 ∼iid U(0,1). Show that−ln(U1)/10−ln(U2)/10 ∼\\nGamma(2,10). [Hint: derive the distribution of −ln(U1)/10 and use Ex-\\nample C.1.] ☞ 427\\n(b) Simulation via MCMC: Implement an independence sampler to simulate from\\nthe Gamma(2,10) target pdf\\nf (x) = 100 x e−10x, x ⩾0,\\nusing proposal transition density q(y |x) = g(y), where g(y) is the pdf of an\\nExp(5) random variable. Generate N = 500 samples, and compare the true cdf\\nwith the empirical cdf of the data.\\n14. Let X = [X,Y]⊤be a random column vector with a bivariate normal distribution with\\nexpectation vector µ= [1,2]⊤and covariance matrix\\nΣ =\\n\"1 a\\na 4\\n#\\n.\\n(a) What are the conditional distributions of ( Y |X = x) and (X |Y = y)? [Hint: use\\nTheorem C.8.] ☞ 436\\n(b) Implement a Gibbs sampler to draw 103 samples from the bivariate distribution\\nN(µ,Σ) for a = 0, 1, and 1.75, and plot the resulting samples.\\n15. Here the objective is to sample from the 2-dimensional pdf\\nf (x,y) = c e−(xy+x+y), x ⩾0, y ⩾0,\\nfor some normalization constant c, using a Gibbs sampler. Let (X,Y) ∼f .\\n(a) Find the conditional pdf of X given Y = y, and the conditional pdf of Y given\\nX = x.\\n(b) Write working Python code that implements the Gibbs sampler and outputs\\n1000 points that are approximately distributed according to f .\\n(c) Describe how the normalization constant c could be estimated via Monte Carlo\\nsimulation, using random variables X1,..., XN,Y1,..., YN\\niid\\n∼Exp(1).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 135, 'page_label': '118'}, page_content='118 Exercises\\n16. We wish to estimate µ =\\nR 2\\n−2 e−x2/2 dx =\\nR\\nH(x) f (x) dx via Monte Carlo simulation\\nusing two di fferent approaches: (1) defining H(x) = 4 e−x2/2 and f the pdf of the\\nU[−2,2] distribution and (2) defining H(x) =\\n√\\n2π1{−2 ⩽x ⩽2}and f the pdf of\\nthe N(0,1) distribution.\\n(a) For both cases estimate µvia the estimator bµ\\nbµ= N−1\\nNX\\ni=1\\nH(Xi). (3.34)\\nUse a sample size of N = 1000.\\n(b) For both cases estimate the relative error κof bµusing N = 100.\\n(c) Give a 95% confidence interval for µfor both cases using N = 100.\\n(d) From part (b), assess how large N should be such that the relative width of the\\nconfidence interval is less than 0 .01, and carry out the simulation with this N.\\nCompare the result with the true value of µ.\\n17. Consider estimation of the tail probability µ= P[X ⩾γ] of some random variable X,\\nwhere γis large. The crude Monte Carlo estimator of µis\\nbµ= 1\\nN\\nNX\\ni=1\\nZi, (3.35)\\nwhere X1,..., XN are iid copies of X and Zi = 1{Xi ⩾γ}, i = 1,..., N.\\n(a) Show that bµis unbiased; that is, Ebµ= µ.\\n(b) Express the relative error of bµ, i.e.,\\nRE =\\np\\nVarbµ\\nEbµ ,\\nin terms of N and µ.\\n(c) Explain how to estimate the relative error of bµ from outcomes x1,..., xN of\\nX1,..., XN, and how to construct a 95% confidence interval for µ.\\n(d) An unbiased estimator Z of µis said to be logarithmically efficient if\\nlim\\nγ→∞\\nln EZ2\\nln µ2 = 1. (3.36)\\nShow that the CMC estimator (3.35) withN = 1 is not logarithmically efficient.\\n18. One of the test cases in [70] involves the minimization of the Hougen function. Im-\\nplement a cross-entropy and a simulated annealing algorithm to carry out this optim-\\nization task.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 136, 'page_label': '119'}, page_content='Monte Carlo Methods 119\\n19. In the binary knapsack problem, the goal is to solve the optimization problem:\\nmax\\nx∈{0,1}n\\np⊤x,\\nsubject to the constraints\\nAx ⩽c,\\nwhere p and w are n ×1 vectors of non-negative numbers, A = (ai j) is an m ×n\\nmatrix, and c is an m ×1 vector. The interpretation is that xj = 1 or 0 depending\\non whether item j with value pj is packed into the knapsack or not , j = 1,..., n;\\nThe variable ai j represents the i-th attribute (e.g., volume, weight) of the j-th item.\\nAssociated with each attribute is a maximal capacity, e.g.,c1 could be the maximum\\nvolume of the knapsack, c2 the maximum weight, etc.\\nWrite a CE program to solve the Sento1.dat knapsack problem at http://peop\\nle.brunel.ac.uk/~mastjjb/jeb/orlib/files/mknap2.txt, as described in\\n[16].\\n20. Let ( C1,R1),(C2,R2),... be a renewal reward process, with ER1 <∞ and\\nEC1 <∞. Let At = PNt\\ni=1 Ri/t be the average reward at time t = 1,2,... , where\\nNt = max{n : Tn ⩽t}and we have defined Tn = Pn\\ni=1 Ci as the time of the n-th re-\\nnewal.\\n(a) Show that Tn/n\\na.s.\\n−→EC1 as n →∞.\\n(b) Show that Nt\\na.s.\\n−→∞as t →∞.\\n(c) Show that Nt/t\\na.s.\\n−→1/EC1 as t →∞. [Hint: Use the fact that TNt ⩽t ⩽TNt+1 for\\nall t = 1,2,... .]\\n(d) Show that\\nAt\\na.s.\\n−→ER1\\nEC1\\nas t →∞.\\n21. Prove Theorem 3.3. ☞ 92\\n22. Prove that if H(x) ⩾0 the importance sampling pdf g∗ in (3.22) gives the zero- ☞ 96\\nvariance importance sampling estimator bµ= µ.\\n23. Let X and Y be random variables (not necessarily independent) and suppose we wish\\nto estimate the expected difference µ= E[X −Y] = EX −EY.\\n(a) Show that if X and Y are positively correlated, the variance of X −Y is smaller\\nthan if X and Y are independent.\\n(b) Suppose now that X and Y have cdfs F and G, respectively, and are\\nsimulated via the inverse-transform method: X = F−1(U), Y = G−1(V), with\\nU,V ∼U(0,1), not necessarily independent. Intuitively, one might expect that\\nif U and V are positively correlated, the variance ofX−Y would be smaller than\\nif U and V are independent. Show that this is not always the case by providing\\na counter-example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 137, 'page_label': '120'}, page_content='120 Exercises\\n(c) Continuing (b), assume now that F and G are continuous. Show that the vari-\\nance of X −Y by taking common random numbers U = V is no larger than\\nwhen U and V are independent. [Hint: Use the following lemma of Hoe ffding\\n[41]: If (X,Y) have joint cdf H with marginal cdfs of X and Y being F and G,\\nrespectively, then\\nCov(X,Y) =\\nZ ∞\\n−∞\\nZ ∞\\n−∞\\n(H(x,y) −F(x) G(y)) dx dy,\\nprovided Cov(X,Y) exists.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 138, 'page_label': '121'}, page_content='CHAPTER 4\\nUNSUPERVISED LEARNING\\nWhen there is no distinction between response and explanatory variables, unsu-\\npervised methods are required to learn the structure of the data. In this chapter we\\nlook at various unsupervised learning techniques, such as density estimation, cluster-\\ning, and principal component analysis. Important tools in unsupervised learning in-\\nclude the cross-entropy training loss, mixture models, the Expectation–Maximization\\nalgorithm, and the Singular Value Decomposition.\\n4.1 Introduction\\nIn contrast to supervised learning, where an “output” (response) variable y is explained by\\nan “input” (explanatory) vector x, in unsupervised learning there is no response variable\\nand the overall goal is to extract useful information and patterns from the data, e.g., in\\nthe form τ= {x1,..., xn}or as a matrix X⊤= [x1,..., xn]. In essence, the objective of\\nunsupervised learning is to learn about the underlying probability distribution of the data.\\nWe start in Section 4.2 by setting up a framework for unsupervised learning that is\\nsimilar to the framework used for supervised learning in Section 2.3. That is, we formulate ☞ 23\\nunsupervised learning in terms of risk and loss minimization; but now involving the cross-\\nentropy risk, rather than the squared-error risk. In a natural way this leads to fundamental\\nlearning concepts such as likelihood, Fisher information, and the Akaike information cri-\\nterion. Section 4.3 introduces the Expectation–Maximization (EM) algorithm as a useful\\nmethod for maximizing likelihood functions when their solution cannot be found easily in\\nclosed form.\\nIf the data forms an iid sample from some unknown distribution, the “empirical dis-\\ntribution” of the data provides valuable information about the unknown distribution. In\\nSection 4.4 we formalize the concept of the empirical distribution (a generalization of the\\nempirical cdf) and explain how we can produce an estimate of the underlying probability ☞ 11\\ndensity function of the data using kernel density estimators.\\nMost unsupervised learning techniques focus on identifying certain traits of the under-\\nlying distribution, such as its local maximizers. A related idea is to partition the data into\\nclusters of points that are in some sense “similar” to each other. In Section 4.5 we formu-\\nlate the clustering problem in terms of a mixture model. In particular, the data are assumed ☞ 135\\n121'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 139, 'page_label': '122'}, page_content='122 Risk and Loss in Unsupervised Learning\\nto come from a mixture of (usually Gaussian) distributions, and the objective is to recover\\nthe parameters of the mixture distributions from the data. The principal tool for parameter\\nestimation in mixture models is the EM algorithm.\\nSection 4.6 discusses a more heuristic approach to clustering, where the data are\\ngrouped according to certain “cluster centers”, whose positions are found by solving an\\noptimization problem. Section 4.7 describes how clusters can be constructed in a hierarch-\\nical manner.\\nFinally, in Section 4.8 we discuss the unsupervised learning technique called Principal\\nComponent Analysis (PCA), which is an important tool for reducing the dimensionality of\\nthe data.\\nWe will revisit various unsupervised learning techniques in subsequent chapters onsu-\\npervised learning. For example, cross-entropy training loss minimization will be important\\nin logistic regression (Section 5.7) and classification (Chapter 7), and PCA can be used☞204\\n☞251 for variable selection and dimensionality reduction, to make models easier to train and\\nincrease their predictive power; see e.g., Sections 6.8 and 7.4.\\n4.2 Risk and Loss in Unsupervised Learning\\nIn unsupervised learning, the training data T:= {X1,..., Xn}only consists of (what are\\nusually assumed to be) independent copies of a feature vector X; there is no response\\ndata. Suppose our objective is to learn the unknown pdf f of X based on an outcome\\nτ= {x1,..., xn}of the training data T. Conveniently, we can follow the same line of reas-\\noning as for supervised learning, discussed in Sections 2.3–2.5. Table 4.1 gives a summary☞23\\nof definitions for the case of unsupervised learning. Compare this with Table 2.1 for the\\nsupervised case.☞25\\nSimilar to supervised learning, we wish to find a functiong, which is now a probability\\ndensity (continuous or discrete), that best approximates the pdf f in terms of minimizing a\\nrisk\\nℓ(g) := ELoss( f (X),g(X)), (4.1)\\nwhere Loss is a loss function. In (2.27), we already encountered the Kullback–Leibler risk\\nℓ(g) := Eln f (X)\\ng(X) = Eln f (X) −Eln g(X). (4.2)\\nIf Gis a class of functions that contains f , then minimizing the Kullback–Leibler risk over\\nGwill yield the (correct) minimizer f . Of course, the problem is that minimization of (4.2)\\ndepends on f , which is generally not known. However, since the term Eln f (X) does not\\ndepend on g, it plays no role in the minimization of the Kullback–Leibler risk. By removing\\nthis term, we obtain the cross-entropy riskcross-entropy\\nrisk\\n(for discrete X replace the integral with a sum):\\nℓ(g) := −Eln g(X) = −\\nZ\\nf (x) ln g(x) dx. (4.3)\\nThus, minimizing the cross-entropy risk (4.3) over all g ∈G, again gives the minimizer\\nf , provided that f ∈G. Unfortunately, solving (4.3) is also infeasible in general, as it still'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 140, 'page_label': '123'}, page_content='Unsupervised Learning 123\\nTable 4.1: Summary of definitions for unsupervised learning.\\nx Fixed feature vector.\\nX Random feature vector.\\nf (x) Pdf of X evaluated at the point x.\\nτor τn Fixed training data {xi,i = 1,..., n}.\\nTor Tn Random training data {Xi,i = 1,..., n}.\\ng Approximation of the pdf f .\\nLoss( f (x),g(x)) Loss incurred when approximating f (x) with g(x).\\nℓ(g) Risk for approximation function g; that is, ELoss( f (X),g(X)).\\ngG Optimal approximation function in function class G; that is,\\nargming∈Gℓ(g).\\nℓτ(g) Training loss for approximation function (guess) g; that is,\\nthe sample average estimate of ℓ(g) based on a fixed training\\nsample τ.\\nℓT(g) The same as ℓτ(g), but now for a random training sample T.\\ngG\\nτ or gτ The learner: argming∈Gℓτ(g). That is, the optimal approxima-\\ntion function based on a fixed training set τand function class\\nG. We suppress the superscript Gif the function class is impli-\\ncit.\\ngG\\nT or gT The learner for a random training set T.\\ndepends on f . Instead, we seek to minimize the cross-entropy training loss cross-entropy\\ntraining loss\\n:\\nℓτ(g) := 1\\nn\\nnX\\ni=1\\nLoss( f (xi),g(xi)) = −1\\nn\\nnX\\ni=1\\nln g(xi) (4.4)\\nover the class of functionsG, where τ= {x1,..., xn}is an iid sample from f . This optimiz-\\nation is doable without knowing f and is equivalent to solving the maximization problem\\nmax\\ng∈G\\nnX\\ni=1\\nln g(xi). (4.5)\\nA key step in setting up the learning procedure is to select a suitable function class Gover\\nwhich to optimize. The standard approach is to parameterize g with a parameter θand let\\nGbe the class of functions{g(·|θ),θ∈Θ}for some p-dimensional parameter set Θ. For the\\nremainder of Section 4.2, we will be using this function class, as well as the cross-entropy\\nrisk.\\nThe function θ 7→g(x |θ) is called the likelihood function likelihood\\nfunction\\n. It gives the likelihood of\\nthe observed feature vector x under g(·|θ), as a function of the parameter θ. The natural\\nlogarithm of the likelihood function is called the log-likelihood function and its gradient\\nwith respect to θis called the score function score function, denoted S(x |θ); that is,\\nS(x |θ) := ∂ln g(x |θ)\\n∂θ =\\n∂g(x |θ)\\n∂θ\\ng(x |θ). (4.6)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 141, 'page_label': '124'}, page_content='124 Risk and Loss in Unsupervised Learning\\nThe random score S(X |θ), with X ∼g(·|θ), is of particular interest. In many cases, its\\nexpectation is equal to the zero vector; namely,\\nEθS(X |θ) =\\nZ ∂g(x |θ)\\n∂θ\\ng(x |θ) g(x |θ) dx\\n=\\nZ ∂g(x |θ)\\n∂θ dx =\\n∂\\nR\\ng(x |θ) dx\\n∂θ = ∂1\\n∂θ = 0,\\n(4.7)\\nprovided that the interchange of differentiation and integration is justified. This is true for\\na large number of distributions, including the normal, exponential, and binomial distri-\\nbutions. Notable exceptions are distributions whose support depends on the distributional\\nparameter; for example the U(0,θ) distribution.\\nIt is important to see whether expectations are taken with respect to X ∼g(·|θ) or\\nX ∼f . We use the expectation symbols Eθ and Eto distinguish the two cases.\\nFrom now on we simply assume that the interchange of di fferentiation and integration\\nis permitted; see, e.g., [76] for su fficient conditions. The covariance matrix of the random\\nscore S(X |θ) is called the Fisher information matrixFisher\\ninformation\\nmatrix\\n, which we denote by F or F(θ) to\\nshow its dependence on θ. Since the expected score is 0, we have\\nF(θ) = Eθ[S(X |θ) S(X |θ)⊤]. (4.8)\\nA related matrix is the expected Hessian matrix of −ln g(X |θ):☞398\\nH(θ) := E\\n\"\\n−∂S(X |θ)\\n∂θ\\n#\\n= −E\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2 ln g(X |θ)\\n∂2θ1\\n∂2 ln g(X |θ)\\n∂θ1∂θ2\\n··· ∂2 ln g(X |θ)\\n∂θ1∂θp\\n∂2 ln g(X |θ)\\n∂θ2∂θ1\\n∂2 ln g(X |θ)\\n∂2θ2\\n··· ∂2 ln g(X |θ)\\n∂θ2∂θp\\n... ... ... ...\\n∂2 ln g(X |θ)\\n∂θp∂θ1\\n∂2 ln g(X |θ)\\n∂θp∂θ2\\n··· ∂2 ln g(X |θ)\\n∂2θp\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (4.9)\\nNote that the expectation here is with respect to X ∼f . It turns out that if f = g(·|θ), the\\ntwo matrices are the same; that is,\\nF(θ) = H(θ), (4.10)\\nprovided that we may swap the order of differentiation and integration (expectation). This\\nresult is called the information matrix equalityinformation\\nmatrix equality\\n. We leave the proof as Exercise 1.\\nThe matrices F(θ) and H(θ) play important roles in approximating the cross-entropy\\nrisk for large n. To set the scene, let gG = g(·|θ∗) be the minimizer of the cross-entropy\\nrisk\\nr(θ) := −Eln g(X |θ).\\nWe assume thatr, as a function ofθ, is well-behaved; in particular, that in the neighborhood\\nof θ∗it is strictly convex and twice continuously differentiable (this holds true, for example,\\nif g is a Gaussian density). It follows that θ∗is a root of ES(X |θ), because\\n0 = ∂r(θ∗)\\n∂θ = −∂Eln g(X |θ∗)\\n∂θ = −E∂ln g(X |θ∗)\\n∂θ = −ES(X |θ∗),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 142, 'page_label': '125'}, page_content='Unsupervised Learning 125\\nagain provided that the order of di fferentiation and integration (expectation) can be\\nswapped. In the same way, H(θ) is then the Hessian matrix of r. Let g(·|bθn) be the minim-\\nizer of the training loss\\nrTn (θ) := −1\\nn\\nnX\\ni=1\\nln g(Xi |θ),\\nwhere Tn = {X1,..., Xn}is a random training set. Let r∗ be the smallest possible cross-\\nentropy risk, taken over all functions; clearly, r∗ = −Eln f (X), where X ∼ f . Similar to\\nthe supervised learning case, we can decompose the generalization risk,ℓ(g(·|bθn)) = r(bθn),\\ninto\\nr(bθn) = r∗+ r(θ∗) −r∗\\n|     {z     }\\napprox. error\\n+ r(bθn) −r(θ∗)|         {z         }\\nstatistical error\\n= r(θ∗) −Eln g(X |θ∗)\\ng(X |bθn)\\n.\\nThe following theorem specifies the asymptotic behavior of the components of the gener-\\nalization risk. In the proof we assume that bθn\\nP\\n−→θ∗as n →∞. ☞ 439\\nTheorem 4.1: Approximating the Cross-Entropy Risk\\nIt holds asymptotically (n →∞) that\\nEr(bθn) −r(θ∗) ≃tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n/(2n), (4.11)\\nwhere\\nr(θ∗) ≃ErTn (bθn) + tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n/(2n). (4.12)\\nProof: A Taylor expansion of r(bθn) around θ∗gives the statistical error ☞ 400\\nr(bθn) −r(θ∗) = (bθn −θ∗)⊤∂r(θ∗)\\n∂θ| {z }\\n= 0\\n+1\\n2(bθn −θ∗)⊤H(θn)(bθn −θ∗), (4.13)\\nwhere θn lies on the line segment betweenθ∗andbθn. For largen we may replace H(θn) with\\nH(θ∗) as, by assumption, bθn converges to θ∗. The matrix H(θ∗) is positive definite because\\nr(θ) is strictly convex atθ∗by assumption, and therefore invertible. It is important to realize\\nthat bθn is in fact an M-estimator of θ∗. In particular, in the notation of Theorem C.19, we ☞ 449\\nhave ψ= S, A = H(θ∗), and B = F(θ∗). Consequently, by that same theorem,\\n√n (bθn −θ∗)\\nd\\n−→N\\n\\x10\\n0,H−1(θ∗) F(θ∗) H−⊤(θ∗)\\n\\x11\\n. (4.14)\\nCombining (4.13) with (4.14), it follows from Theorem C.2 that asymptotically the ☞ 430\\nexpected estimation error is given by (4.11).\\nNext, we consider a Taylor expansion of rTn (θ∗) around bθn:\\nrTn (θ∗) = rTn (bθn) + (θ∗−bθn)⊤∂rTn (bθn)\\n∂θ|   {z   }\\n= 0\\n+1\\n2(θ∗−bθn)⊤HTn (θn)(θ∗−bθn), (4.15)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 143, 'page_label': '126'}, page_content='126 Risk and Loss in Unsupervised Learning\\nwhere HTn (θn) := −1\\nn\\nPn\\ni=1\\n∂S(Xi |θn)\\n∂θ is the Hessian of rTn (θ) at some θn between bθn and θ∗.\\nTaking expectations on both sides of (4.15), we obtain\\nr(θ∗) = ErTn (bθn) + 1\\n2E(θ∗−bθn)⊤HTn (θn)(θ∗−bθn).\\nReplacing HTn (θn) with H(θ∗) for large n and using (4.14), we have\\nn E(θ∗−bθn)⊤HTn (θn)(θ∗−bθn) −→tr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n, n →∞.\\nTherefore, asymptotically as n →∞, we have (4.12). □\\nTheorem 4.1 has a number of interesting consequences:\\n1. Similar to Section 2.5.1, the training loss ℓTn (gTn ) = rTn (bθn) tends to underestimate the☞35\\nrisk ℓ(gG) = r(θ∗), because the training set Tn is used to both train g ∈G (that is, estimate\\nθ∗) and to estimate the risk. The relation (4.12) tells us that on average the training loss\\nunderestimates the true risk by tr(F(θ∗) H−1(θ∗))/(2n).\\n2. Adding equations (4.11) and (4.12), yields the following asymptotic approximation to\\nthe expected generalization risk:\\nEr(bθn) ≃ErTn (bθn) + 1\\nntr\\n\\x10\\nF(θ∗) H−1(θ∗)\\n\\x11\\n(4.16)\\nThe first term on the right-hand side of (4.16) can be estimated (without bias) via the\\ntraining loss rTn (bθn). As for the second term, we have already mentioned that when the\\ntrue model f ∈G, then F(θ∗) = H(θ∗). Therefore, when Gis deemed to be a su fficiently\\nrich class of models parameterized by a p-dimensional vector θ, we may approximate the\\nsecond term as tr(F(θ∗)H−1(θ∗))/n ≈tr(Ip)/n = p/n. This suggests the following heuristic\\napproximation to the (expected) generalization risk:\\nEr(bθn) ≈rTn (bθn) + p\\nn . (4.17)\\n3. Multiplying both sides of (4.16) by 2n and substituting tr\\n\\x10\\nF(θ∗)H−1(θ∗)\\n\\x11\\n≈p, we obtain\\nthe approximation:\\n2n r(bθn) ≈−2\\nnX\\ni=1\\nln g(Xi |bθn) + 2p. (4.18)\\nThe right-hand side of (4.18) is called the Akaike information criterionAkaike\\ninformation\\ncriterion\\n(AIC). Just like\\n(4.17), the AIC approximation can be used to compare the difference in generalization risk\\nof two or more learners. We prefer the learner with the smallest (estimated) generalization\\nrisk.\\nSuppose that, for a training set T, the training loss rT(θ) has a unique minimum point\\nbθwhich lies in the interior of Θ. If rT(θ) is a differentiable function with respect to θ, then\\nwe can find the optimal parameter bθby solving\\n∂rT(θ)\\n∂θ = 1\\nn\\nnX\\ni=1\\nS(Xi |θ)\\n|           {z           }\\nST(θ)\\n= 0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 144, 'page_label': '127'}, page_content='Unsupervised Learning 127\\nIn other words, the maximum likelihood estimatebθfor θis obtained by solving the root of\\nthe average score function, that is, by solving\\nST(θ) = 0. (4.19)\\nIt is often not possible to find bθ in an explicit form. In that case one needs to solve the\\nequation (4.19) numerically. There exist many standard techniques for root-finding, e.g.,\\nvia Newton’s method (see Section B.3.1), whereby, starting from an initial guess θ0, sub- Newton’s\\nmethod\\n☞ 409\\nsequent iterates are obtained via the iterative scheme\\nθt+1 = θt + H−1\\nT (θt) ST(θt),\\nwhere\\nHT(θ) := −∂ST(θ)\\n∂θ = 1\\nn\\nnX\\ni=1\\n−∂S(Xi |θ)\\n∂θ\\nis the average Hessian matrix of {−ln g(Xi |θ)}n\\ni=1. Under f = g(·|θ), the expectation of\\nHT(θ) is equal to the information matrix F(θ), which does not depend on the data. This\\nsuggests an alternative iterative scheme, called Fisher’s scoring method Fisher’s\\nscoring method\\n:\\nθt+1 = θt + F−1(θt) ST(θt), (4.20)\\nwhich is not only easier to implement (if the information matrix can be readily evaluated),\\nbut also is more numerically stable.\\nExample 4.1 (Maximum Likelihood for the Gamma Distribution) We wish to ap-\\nproximate the density of the Gamma(α∗,λ∗) distribution for some true but unknown para-\\nmeters α∗ and λ∗, on the basis of a training set τ = {x1,..., xn}of iid samples from this\\ndistribution. Choosing our approximating function g(·|α,λ) in the same class of gamma\\ndensities,\\ng(x |α,λ) = λαxα−1e−λx\\nΓ(α) , x ⩾0, (4.21)\\nwith α> 0 and λ> 0, we seek to solve (4.19). Taking the logarithm in (4.21), the log-\\nlikelihood function is given by\\nl(x |α,λ) := αln λ−ln Γ(α) + (α−1) ln x −λx.\\nIt follows that\\nS(α,λ) =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂\\n∂αl(x |α,λ)\\n∂\\n∂λl(x |α,λ)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\"ln λ−ψ(α) + ln x\\nα\\nλ −x\\n#\\n,\\nwhere ψis the derivative of lnΓ: the so-called digamma function digamma\\nfunction\\n. Hence,\\nH(α,λ) = −E\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2\\n∂α2 l(X |α,λ) ∂2\\n∂α∂λl(X |α,λ)\\n∂2\\n∂α∂λl(X |α,λ) ∂2\\n∂λ2 l(X |α,λ)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = −E\\n\"−ψ′(α) 1\\nλ1\\nλ −α\\nλ2\\n#\\n=\\n\"ψ′(α) −1\\nλ\\n−1\\nλ\\nα\\nλ2\\n#\\n.\\nFisher’s scoring method (4.20) can now be used to solve (4.19), with\\nSτ(α,λ) =\\n\"ln λ−ψ(α) + n−1 Pn\\ni=1 ln xi\\nα\\nλ −n−1 Pn\\ni=1 xi\\n#\\nand F(α,λ) = H(α,λ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 145, 'page_label': '128'}, page_content='128 Expectation–Maximization (EM) Algorithm\\n4.3 Expectation–Maximization (EM) Algorithm\\nThe Expectation–Maximization algorithm (EM) is a general algorithm for maximization of\\ncomplicated (log-)likelihood functions, through the introduction of auxiliary variables.\\nTo simplify the notation in this section, we use a Bayesian notation system, where\\nthe same symbol is used for different (conditional) probability densities.\\nAs in the previous section, given independent observations τ= {x1,..., xn}from some\\nunknown pdf f , the objective is to find the best approximation to f in a function class\\nG= {g(·|θ),θ∈Θ}by solving the maximum likelihood problem:\\nθ∗= argmax\\nθ∈Θ\\ng(τ|θ), (4.22)\\nwhere g(τ|θ) : = g(x1 |θ) ··· g(xn |θ). The key element of the EM algorithm is the aug-\\nmentation of the data τwith a suitable vector of latent variableslatent\\nvariables\\n, z, such that\\ng(τ|θ) =\\nZ\\ng(τ,z |θ) dz.\\nThe function θ7→g(τ,z |θ) is usually referred to as the complete-data likelihoodcomplete-data\\nlikelihood\\nfunction.\\nThe choice of the latent variables is guided by the desire to make the maximization of\\ng(τ,z |θ) much easier than that of g(τ|θ).\\nSuppose p denotes an arbitrary density of the latent variables z. Then, we can write:\\nln g(τ|θ) =\\nZ\\np(z) lng(τ|θ) dz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)/p(z)\\ng(z |τ,θ)/p(z)\\n!\\ndz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz −\\nZ\\np(z) ln\\n g(z |τ,θ)\\np(z)\\n!\\ndz\\n=\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz + D(p,g(·|τ,θ)), (4.23)\\nwhere D(p,g(·|τ,θ)) is the Kullback–Leibler divergence from the density p to g(·|τ,θ).☞42\\nSince D⩾0, it follows that\\nln g(τ|θ) ⩾\\nZ\\np(z) ln\\n g(τ,z |θ)\\np(z)\\n!\\ndz =: L(p,θ)\\nfor all θand any density p of the latent variables. In other words, L(p,θ) is a lower bound\\non the log-likelihood that involves the complete-data likelihood. The EM algorithm then\\naims to increase this lower bound as much as possible by starting with an initial guess θ(0)\\nand then, for t = 1,2,... , solving the following two steps:\\n1. p(t) = argmaxp L(p,θ(t−1)),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 146, 'page_label': '129'}, page_content='Unsupervised Learning 129\\n2. θ(t) = argmaxθ∈Θ L(p(t),θ).\\nThe first optimization problem can be solved explicitly. Namely, by (4.23), we have\\nthat\\np(t) = argmin\\np\\nD(p,g(·|τ,θ(t−1))) = g(·|τ,θ(t−1)).\\nThat is, the optimal density is the conditional density of the latent variables given the data\\nτand the parameter θ(t−1). The second optimization problem can be simplified by writing\\nL(p(t),θ) = Q(t)(θ) −Ep(t) ln p(t)(Z), where\\nQ(t)(θ) := Ep(t) ln g(τ,Z |θ)\\nis the expected complete-data log-likelihood under Z ∼p(t). Consequently, the maximiza-\\ntion of L(p(t),θ) with respect to θis equivalent to finding\\nθ(t) = argmax\\nθ∈Θ\\nQ(t)(θ).\\nThis leads to the following generic EM algorithm.\\nAlgorithm 4.3.1: Generic EM Algorithm\\ninput: Data τ, initial guess θ(0).\\noutput: Approximation of the maximum likelihood estimate.\\n1 t ←1\\n2 while a stopping criterion is not met do\\n3 Expectation Step: Find p(t)(z) := g(z |τ,θ(t−1)) and compute the expectation\\nQ(t)(θ) := Ep(t) ln g(τ,Z |θ). (4.24)\\n4 Maximization Step: Let θ(t) ←argmaxθ∈Θ Q(t)(θ).\\n5 t ←t + 1\\n6 return θ(t)\\nA possible stopping criterion is to stop when\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\nln g(τ|θ(t)) −ln g(τ|θ(t−1))\\nln g(τ|θ(t))\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c ⩽ε\\nfor some small tolerance ε> 0.\\nRemark 4.1 (Properties of the EM Algorithm) The identity (4.23) can be used to\\nshow that the likelihood g(τ|θ(t)) does not decrease with every iteration of the algorithm.\\nThis property is one of the strengths of the algorithm. For example, it can be used to debug\\ncomputer implementations of the EM algorithm: if the likelihood is observed to decrease\\nat any iteration, then one has detected a bug in the program.\\nThe convergence of the sequence {θ(t)}to a global maximum (if it exists) is highly\\ndependent on the initial valueθ(0) and, in many cases, an appropriate choice ofθ(0) may not\\nbe clear. Typically, practitioners run the algorithm from di fferent random starting points\\nover Θ, to ascertain empirically that a suitable optimum is achieved.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 147, 'page_label': '130'}, page_content='130 Expectation–Maximization (EM) Algorithm\\nExample 4.2 (Censored Data) Suppose the lifetime (in years) of a certain type of\\nmachine is modeled via a N(µ,σ2) distribution. To estimate µ and σ2, the lifetimes of\\nn (independent) machines are recorded up to c years. Denote these censored lifetimes\\nby x1,..., xn. The {xi}are thus realizations of iid random variables {Xi}, distributed as\\nmin{Y,c}, where Y ∼N(µ,σ2).\\nBy the law of total probability (see (C.9)), the marginal pdf of each X can be written☞428\\nas:\\ng(x |µ,σ2) = Φ((c −µ)/σ)|          {z          }\\nP[Y<c]\\nφσ2 (x −µ)\\nΦ((c −µ)/σ)1{x <c}+ Φ((c −µ)/σ)|          {z          }\\nP[Y⩾c]\\n1{x = c},\\nwhere φσ2 (·) is the pdf of the N(0,σ2) distribution, Φ is the cdf of the standard normal\\ndistribution, and Φ := 1 −Φ. It follows that the likelihood of the data τ= {x1,..., xn}as a\\nfunction of the parameter θ:= [µ,σ2]⊤is:\\ng(τ|θ) =\\nY\\ni:xi<c\\nexp\\n\\x10\\n−(xi−µ)2\\n2σ2\\n\\x11\\n√\\n2πσ2\\n×\\nY\\ni:xi=c\\nΦ((c −µ)/σ).\\nLet nc be the total number of xi such that xi = c. Using nc latent variables z = [z1,..., znc ]⊤,\\nwe can write the joint pdf:\\ng(τ,z |θ) = 1\\n(2πσ2)n/2 exp\\n \\n−\\nP\\ni:xi<c(xi −µ)2\\n2σ2 −\\nPnc\\ni=1(zi −µ)2\\n2σ2\\n!\\n1\\n\\x1a\\nmin\\ni\\nzi ⩾c\\n\\x1b\\n,\\nso that\\nR\\ng(τ,z |θ) dz = g(τ|θ). We can thus apply the EM algorithm to maximize the like-\\nlihood, as follows.\\nFor the E(xpectation)-step, we have for a fixed θ:\\ng(z |τ,θ) =\\nncY\\ni=1\\ng(zi |τ,θ),\\nwhere g(z |τ,θ) = 1{z ⩾c}φσ2 (z −µ)/Φ((c −µ)/σ) is simply the pdf of the N(µ,σ2)\\ndistribution, truncated to [c,∞).\\nFor the M(aximization)-step, we compute the expectation of the complete log-\\nlikelihood with respect to a fixed g(z |τ,θ) and use the fact that Z1,..., Znc are iid:\\nEln g(τ,Z |θ) = −\\nP\\ni:xi<c(xi −µ)2\\n2σ2 −ncE(Z −µ)2\\n2σ2 −n\\n2 ln σ2 −n\\n2 ln(2π),\\nwhere Z has a N(µ,σ2) distribution, truncated to [ c,∞). To maximize the last expression\\nwith respect to µwe set the derivative with respect to µto zero, and obtain:\\nµ= ncEZ + P\\ni:xi<c xi\\nn .\\nSimilarly, setting the derivative with respect toσ2 to zero gives:\\nσ2 = ncE(Z −µ)2 + P\\ni:xi<c(xi −µ)2\\nn .\\nIn summary, the EM iterates for t = 1,2,... are as follows.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 148, 'page_label': '131'}, page_content='Unsupervised Learning 131\\nE-step. Given the current estimate θt := [µt,σ2\\nt ]⊤, compute the expectations νt := EZ and\\nζ2\\nt := E(Z −µt)2, where Z ∼N(µt,σ2\\nt ), conditional on Z ⩾c; that is,\\nνt := µt + σ2\\nt\\nφσ2\\nt\\n(c −µt)\\nΦ((c −µt)/σt)\\nζ2\\nt := σ2\\nt\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 + (c −µt)\\nφσ2\\nt\\n(c −µt)\\nΦ((c −µt)/σt)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nM-step. Update the estimate to θt+1 := [µt+1,σ2\\nt+1]⊤via the formulas:\\nµt+1 = ncνt + P\\ni:xi<c xi\\nn\\nσ2\\nt+1 = ncζ2\\nt + P\\ni:xi<c(xi −µt+1)2\\nn .\\n4.4 Empirical Distribution and Density Estimation\\nIn Section 1.5.2.3 we saw how the empirical cdf bFn, obtained from an iid training set ☞ 11\\nτ= {x1,..., xn}from an unknown distribution on R, gives an estimate of the unknown cdf\\nF of this sampling distribution. The function bFn is a genuine cdf, as it is right-continuous,\\nincreasing, and lies between 0 and 1. The corresponding discrete probability distribution\\nis called the empirical distribution empirical\\ndistribution\\nof the data. A random variable X distributed according\\nto this empirical distribution takes the values x1,..., xn with equal probability 1 /n. The\\nconcept of empirical distribution naturally generalizes to higher dimensions: a random\\nvector X that is distributed according to the empirical distribution ofx1,..., xn has discrete\\npdf P[X = xi] = 1/n,i = 1,..., n. Sampling from such a distribution — in other words\\nresampling the original data — was discussed in Section 3.2.4. The preeminent usage of ☞ 76\\nsuch sampling is the bootstrap method, discussed in Section 3.3.2. ☞ 88\\nIn a way, the empirical distribution is the natural answer to the unsupervised learning\\nquestion: what is the underlying probability distribution of the data? However, the empir-\\nical distribution is, by definition, a discrete distribution, whereas the true sampling distri-\\nbution might be continuous. For continuous data it makes sense to also consider estimation\\nof the pdf of the data. A common approach is to estimate the density via a kernel density\\nestimate (KDE), the most prevalent learner to carry this out is given next.\\nDefinition 4.1: Gaussian KDE\\nLet x1,..., xn ∈Rd be the outcomes of an iid sample from a continuous pdf f . A\\nGaussian kernel density estimate Gaussian\\nkernel density\\nestimate\\nof f is a mixture of normal pdfs, of the form\\ngτn (x |σ) = 1\\nn\\nnX\\ni=1\\n1\\n(2π)d/2σd e−∥x−xi∥2\\n2σ2 , x ∈Rd, (4.25)\\nwhere σ> 0 is called the bandwidth.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 149, 'page_label': '132'}, page_content='132 Empirical Distribution and Density Estimation\\nWe see that gτn in (4.25) is the average of a collection of n normal pdfs, where each\\nnormal distribution is centered at the data pointxi and has covariance matrixσ2Id. A major\\nquestion is how to choose the bandwidth σso as to best approximate the unknown pdf f .\\nChoosing very small σ will result in a “spiky” estimate, whereas a large σ will produce\\nan over-smoothed estimate that may not identify important peaks that are present in the\\nunknown pdf. Figure 4.1 illustrates this phenomenon. In this case the data are comprised\\nof 20 points uniformly drawn from the unit square. The true pdf is thus 1 on [0 ,1]2 and 0\\nelsewhere.\\nFigure 4.1: Two two-dimensional Gaussian KDEs, withσ= 0.01 (left) andσ= 0.1 (right).\\nLet us write the Gaussian KDE in (4.25) as\\ngτn (x |σ) = 1\\nn\\nnX\\ni=1\\n1\\nσd ϕ\\n\\x12x −xi\\nσ\\n\\x13\\n, (4.26)\\nwhere\\nϕ(z) = 1\\n(2π)d/2 e−∥z∥2\\n2 , z ∈Rd (4.27)\\nis the pdf of the d-dimensional standard normal distribution. By choosing a different prob-\\nability density ϕin (4.26), satisfying ϕ(x) = ϕ(−x) for all x, we can obtain a wide variety\\nof kernel density estimates. A simple pdf ϕis, for example, the uniform pdf on [−1,1]d:\\nϕ(z) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n2−d, if z ∈[−1,1]d,\\n0, otherwise.\\nFigure 4.2 shows the graph of the corresponding KDE, using the same data as in Figure 4.1\\nand with bandwidth σ = 0.1. We observe qualitatively similar behavior for the Gaussian\\nand uniform KDEs. As a rule, the choice of the functionϕis less important than the choice\\nof the bandwidth in determining the quality of the estimate.\\nThe important issue of bandwidth selection has been extensively studied for one-\\ndimensional data. To explain the ideas, we use our usual setup and let τ = {x1,..., xn}\\nbe the observed (one-dimensional) data from the unknown pdf f . First, we define the loss\\nfunction as\\nLoss( f (x),g(x)) = ( f (x) −g(x))2\\nf (x) . (4.28)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 150, 'page_label': '133'}, page_content='Unsupervised Learning 133\\nFigure 4.2: A two-dimensional uniform KDE, with bandwidth σ= 0.1.\\nThe risk to minimize is thus ℓ(g) := Ef Loss( f (X),g(X)) =\\nR\\n( f (x) −g(x))2 dx.We bypass\\nthe selection of a class of approximation functions by choosing the learner to be specified\\nby (4.25) for a fixed σ. The objective is now to find a σthat minimizes the generalization\\nrisk ℓ(gτ(·|σ)) or the expected generalization risk Eℓ(gT(·|σ)). The generalization risk is\\nin this caseZ\\n( f (x) −gτ(x |σ))2 dx =\\nZ\\nf 2(x) dx −2\\nZ\\nf (x)gτ(x |σ) dx +\\nZ\\ng2\\nτ(x |σ) dx.\\nMinimizing this expression with respect toσis equivalent to minimizing the last two terms,\\nwhich can be written as\\n−2 Ef gτ(X |σ) +\\nZ \\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n1\\nn\\nnX\\ni=1\\n1\\nσϕ\\n\\x12x −xi\\nσ\\n\\x13\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n2\\ndx.\\nThis expression in turn can be estimated by using a test sample{x′\\n1 ..., x′\\nn′}from f , yielding\\nthe following minimization problem:\\nmin\\nσ\\n−2\\nn′\\nn′\\nX\\ni=1\\ngτ(x′\\ni |σ) + 1\\nn2\\nnX\\ni=1\\nnX\\nj=1\\nZ 1\\nσ2 ϕ\\n\\x12x −xi\\nσ\\n\\x13\\nϕ\\n\\x12x −xj\\nσ\\n\\x13\\ndx,\\nwhere\\nR 1\\nσ2 ϕ\\n\\x10x−xi\\nσ\\n\\x11\\nϕ\\n\\x10x−xj\\nσ\\n\\x11\\ndx = 1√\\n2σϕ\\n\\x10xi−xj√\\n2σ\\n\\x11\\nin the case of the Gaussian kernel (4.27) with\\nd = 1. To estimate σin this way clearly requires a test sample, or at least an application of ☞ 38\\ncross-validation. Another approach is to minimize the expected generalization risk, (that\\nis, averaged over all training sets):\\nE\\nZ\\n( f (x) −gT(x |σ))2 dx.\\nThis is called the mean integrated squared error mean integrated\\nsquared error\\n(MISE). It can be decomposed into an\\nintegrated squared bias and integrated variance component:\\nZ\\n( f (x) −EgT(x |σ))2 dx +\\nZ\\nVar(gT(x |σ)) dx.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 151, 'page_label': '134'}, page_content='134 Empirical Distribution and Density Estimation\\nA typical analysis now proceeds by investigating how the MISE behaves for largen, under\\nvarious assumptions on f . For example, it is shown in [114] that, for σ→0 and nσ→∞,\\nthe asymptotic approximation to the MISE of the Gaussian kernel density estimator (4.25)\\n(for d = 1) is given by\\n1\\n4 σ4 ∥f ′′∥2 + 1\\n2n\\n√\\nπσ2\\n, (4.29)\\nwhere ∥f ′′∥2 :=\\nR\\n( f ′′(x))2 dx. The asymptotically optimal value of σis the minimizer\\nσ∗:=\\n 1\\n2n √π∥f ′′∥2\\n!1/5\\n. (4.30)\\nTo compute the optimal σ∗ in (4.30), one needs to estimate the functional ∥f ′′∥2. The\\nGaussian rule of thumbGaussian rule\\nof thumb\\nis to assume that f is the density of theN(x,s2) distribution, where\\nx and s2 are the sample mean and variance of the data, respectively [113]. In this case\\n∥f ′′∥2 = s−5π−1/23/8 and the Gaussian rule of thumb becomes:\\nσrot =\\n 4 s5\\n3 n\\n!1/5\\n≈1.06 s n−1/5.\\nWe recommend, however, the fast and reliable theta KDEtheta KDE of [14], which chooses the\\nbandwidth in an optimal way via a fixed-point procedure. Figures 4.1 and 4.2 illustrate a\\ncommon problem with traditional KDEs: for distributions on a bounded domain, such as\\nthe uniform distribution on [0,1]2, the KDE assigns positive probability mass outside this\\ndomain. An additional advantage of the theta KDE is that it largely avoids this boundary\\neffect. We illustrate the theta KDE with the following example.\\nExample 4.3 (Comparison of Gaussian and theta KDEs) The following Python pro-\\ngram draws an iid sample from the Exp(1) distribution and constructs a Gaussian kernel\\ndensity estimate. We see in Figure 4.3 that with an appropriate choice of the bandwidth\\na good fit to the true pdf can be achieved, except at the boundary x = 0. The theta KDE\\ndoes not exhibit this boundary e ffect. Moreover, it chooses the bandwidth automatically,\\nto achieve a superior fit. The theta KDE source code is available as kde.py on the book’s\\nGitHub site.\\n0\\n 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n Gaussian KDE\\nTheta KDE\\nTrue density\\nFigure 4.3: Kernel density estimates for Exp(1)-distributed data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 152, 'page_label': '135'}, page_content='Unsupervised Learning 135\\ngausthetakde.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom kde import *\\nsig = 0.1; sig2 = sig**2; c = 1/np.sqrt(2*np.pi)/sig #Constants\\nphi = lambda x,x0: np.exp(-(x-x0)**2/(2*sig2)) #Unscaled Kernel\\nf = lambda x: np.exp(-x)*(x >= 0) # True PDF\\nn = 10**4 # Sample Size\\nx = -np.log(np.random.uniform(size=n)) # Generate Data via IT method\\nxx = np.arange(-0.5,6,0.01, dtype = \"d\") # Plot Range\\nphis = np.zeros( len (xx))\\nfor i in range (0,n):\\nphis = phis + phi(xx,x[i])\\nphis = c*phis/n\\nplt.plot(xx,phis, \\'r\\')# Plot Gaussian KDE\\n[bandwidth,density,xmesh,cdf] = kde(x,2**12,0, max (x))\\nidx = (xmesh <= 6)\\nplt.plot(xmesh[idx],density[idx]) # Plot Theta KDE\\nplt.plot(xx,f(xx)) # Plot True PDF\\n4.5 Clustering via Mixture Models\\nClustering is concerned with the grouping of unlabeled feature vectors into clusters, such\\nthat samples within a cluster are more similar to each other than samples belonging to\\ndifferent clusters. Usually, it is assumed that the number of clusters is known in advance,\\nbut otherwise no prior information is given about the data. Applications of clustering can\\nbe found in the areas of communication, data compression and storage, database searching,\\npattern matching, and object recognition.\\nA common approach to clustering analysis is to assume that the data comes from a mix-\\nture of (usually Gaussian) distributions, and thus the objective is to estimate the parameters\\nof the mixture model by maximizing the likelihood function for the data. Direct optimiza-\\ntion of the likelihood function in this case is not a simple task, due to necessary constraints\\non the parameters (more about this later) and the complicated nature of the likelihood func-\\ntion, which in general has a great number of local maxima and saddle-points. A popular\\nmethod to estimate the parameters of the mixture model is the EM algorithm, which was\\ndiscussed in a more general setting in Section 4.3. In this section we explain the basics of ☞ 128\\nmixture modeling and explain the workings of the EM method in this context. In addition,\\nwe show how direct optimization methods can be used to maximize the likelihood.\\n4.5.1 Mixture Models\\nLet T := {X1,..., Xn}be iid random vectors taking values in some set X⊆ Rd, each Xi\\nbeing distributed according to the mixture density mixture density\\ng(x |θ) = w1ϕ1(x) + ··· + wKϕK(x), x ∈X, (4.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 153, 'page_label': '136'}, page_content='136 Clustering via Mixture Models\\nwhere ϕ1,...,ϕ K are probability densities (discrete or continuous) on X, and the positive\\nweights w1,..., wK sum up to 1. This mixture pdf can be interpreted in the following way.weights\\nLet Z be a discrete random variable taking values 1,2,..., K with probabilities w1,..., wK,\\nand let X be a random vector whose conditional pdf, givenZ = z, is ϕz. By the product rule\\n(C.17), the joint pdf of Z and X is given by☞431\\nϕZ,X(z,x) = ϕZ(z) ϕX |Z(x |z) = wz ϕz(x)\\nand the marginal pdf of X is found by summing the joint pdf over the values of z, which\\ngives (4.31). A random vector X ∼g can thus be simulated in two steps:\\n1. First, draw Z according to the probabilities P[Z = z] = wz, z = 1,..., K.\\n2. Then draw X according to the pdf ϕZ.\\nAs Tonly contain the {Xi}variables, the {Zi}are viewed as latent variables. We can inter-\\npret Zi as the hidden label of the cluster to which Xi belongs.\\nTypically, eachϕk in (4.31) is assumed to be known up to some parameter vector ηk. It\\nis customary1 in clustering analysis to work with Gaussian mixtures; that is, each density\\nϕk is Gaussian with some unknown expectation vector µk and covariance matrix Σk. We\\ngather all unknown parameters, including the weights {wk}, into a parameter vector θ. As\\nusual, τ = {x1,..., xn}denotes the outcome of T. As the components of Tare iid, their\\n(joint) pdf is given by\\ng(τ|θ) :=\\nnY\\ni=1\\ng(xi |θ) =\\nnY\\ni=1\\nKX\\nk=1\\nwk ϕk(xi |µk,Σk). (4.32)\\nFollowing the same reasoning as for (4.5), we can estimate θfrom an outcome τby max-\\nimizing the log-likelihood function\\nl(θ|τ) :=\\nnX\\ni=1\\nln g(xi |θ) =\\nnX\\ni=1\\nln\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nKX\\nk=1\\nwk ϕk(xi |µk,Σk)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (4.33)\\nHowever, finding the maximizer of l(θ|τ) is not easy in general, since the function is typ-\\nically multiextremal.\\nExample 4.4 (Clustering via Mixture Models) The data depicted in Figure 4.4 con-\\nsists of 300 data points that were independently generated from three bivariate normal\\ndistributions, whose parameters are given in that same figure. For each of these three dis-\\ntributions, exactly 100 points were generated. Ideally, we would like to cluster the data into\\nthree clusters that correspond to the three cases.\\nTo cluster the data into three groups, a possible model for the data is to assume that\\nthe points are iid draws from an (unknown) mixture of three 2-dimensional Gaussian dis-\\ntributions. This is a sensible approach, although in reality the data were not simulated\\nin this way. It is instructive to understand the di fference between the two models. In the\\nmixture model, each cluster label Z takes the value {1,2,3}with equal probability, and\\nhence, drawing the labels independently, the total number of points in each cluster would\\n1Other common mixture distributions include Student t and Beta distributions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 154, 'page_label': '137'}, page_content='Unsupervised Learning 137\\n-8 -6 -4 -2 0 2 4\\n-6\\n-4\\n-2\\n0\\n2\\n4\\ncluster mean vector covariance matrix\\n1\\n\"−4\\n0\\n# \" 2 1 .4\\n1.4 1 .5\\n#\\n2\\n\"0.5\\n−1\\n# \" 2 −0.95\\n−0.95 1\\n#\\n3\\n\"−1.5\\n−3\\n# \" 2 0 .1\\n0.1 0 .1\\n#\\nFigure 4.4: Cluster the 300 data points (left) into three clusters, without making any as-\\nsumptions about the probability distribution of the data. In fact, the data were generated\\nfrom three bivariate normal distributions, whose parameters are listed on the right.\\nbe Bin(300,1/3) distributed. However, in the actual simulation, the number of points in\\neach cluster is exactly 100. Nevertheless, the mixture model would be an accurate (al-\\nthough not exact) model for these data. Figure 4.5 displays the “target” Gaussian mixture\\ndensity for the data in Figure 4.4; that is, the mixture with equal weights and with the exact\\nparameters as specified in Figure 4.4.\\nFigure 4.5: The target mixture density for the data in Figure 4.4.\\nIn the next section we will carry out the clustering by using the EM algorithm.\\n4.5.2 EM Algorithm for Mixture Models\\nAs we saw in Section 4.3, instead of maximizing the log-likelihood function (4.33) directly\\nfrom the data τ= {x1,..., xn}, the EM algorithm first augments the data data\\naugmentation\\nwith the vector of\\nlatent variables — in this case the hidden cluster labelsz = {z1,..., zn}. The idea is that τis'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 155, 'page_label': '138'}, page_content='138 Clustering via Mixture Models\\nonly the observed part of the complete random data ( T,Z), which were generated via the\\ntwo-step procedure described above. That is, for each data point X, first draw the cluster\\nlabel Z ∈{1,..., K}according to probabilities {w1,..., wK}and then, given Z = z, draw X\\nfrom ϕz. The joint pdf of Tand Z is\\ng(τ,z |θ) =\\nnY\\ni=1\\nwzi ϕzi (xi),\\nwhich is of a much simpler form than (4.32). It follows that the complete-data log-\\nlikelihoodcomplete-data\\nlog-likelihood\\nfunction\\nel(θ|τ,z) =\\nnX\\ni=1\\nln[wzi ϕzi (xi)] (4.34)\\nis often easier to maximize than the original log-likelihood (4.33), for any given (τ,z). But,\\nof course the latent variables z are not observed and soel(θ|τ,z) cannot be evaluated. In the\\nE-step of the EM algorithm, the complete-data log-likelihood is replaced with the expect-\\nation Ep el(θ|τ,Z), where the subscript p in the expectation indicates that Z is distributed\\naccording to the conditional pdf of Z given T= τ; that is, with pdf\\np(z) = g(z |τ,θ) ∝g(τ,z |θ). (4.35)\\nNote that p(z) is of the form p1(z1) ··· pn(zn) so that, givenT= τ, the components of Z are\\nindependent of each other. The EM algorithm for mixture models can now be formulated\\nas follows.\\nAlgorithm 4.5.1: EM Algorithm for Mixture Models\\ninput: Data τ, initial guess θ(0).\\noutput: Approximation of the maximum likelihood estimate.\\n1 t ←1\\n2 while a stopping criterion is not met do\\n3 Expectation Step: Find p(t)(z) := g(z |τ,θ(t−1)) and Q(t)(θ) := Ep(t)el(θ|τ,Z).\\n4 Maximization Step: Let θ(t) ←argmaxθ Q(t)(θ).\\n5 t ←t + 1\\n6 return θ(t)\\nA possible termination condition is to stop when\\n\\x0c\\x0c\\x0cl(θ(t) |τ) −l(θ(t−1) |τ)\\n\\x0c\\x0c\\x0c/\\n\\x0c\\x0c\\x0cl(θ(t) |τ)\\n\\x0c\\x0c\\x0c < ε\\nfor some small tolerance ε >0. As was mentioned in Section 4.3, the sequence of log-\\nlikelihood values does not decrease with each iteration. Under certain continuity con-\\nditions, the sequence {θ(t)}is guaranteed to converge to a local maximizer of the log-\\nlikelihood l. Convergence to a global maximizer (if it exists) depends on the appropriate\\nchoice for the starting value. Typically, the algorithm is run from different random starting\\npoints.\\nFor the case of Gaussian mixtures, each ϕk = ϕ(·|µk,Σk),k = 1,..., K is the density\\nof a d-dimensional Gaussian distribution. Let θ(t−1) be the current guess for the optimal\\nparameter vector, consisting of the weights {w(t−1)\\nk }, mean vectors {µ(t−1)\\nk }, and covariance\\nmatrices {Σ(t−1)\\nk }. We first determine p(t) — the pdf of Z conditional on T = τ— for the\\ngiven guessθ(t−1). As mentioned before, the components ofZ given T= τare independent,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 156, 'page_label': '139'}, page_content='Unsupervised Learning 139\\nso it suffices to specify the discrete pdf,p(t)\\ni say, of eachZi given the observed pointXi = xi.\\nThe latter can be found from Bayes’ formula:\\np(t)\\ni (k) ∝w(t−1)\\nk ϕk(xi |µ(t−1)\\nk ,Σ(t−1)\\nk ), k = 1,..., K. (4.36)\\nNext, in view of (4.34), the function Q(t)(θ) can be written as\\nQ(t)(θ) = Ep(t)\\nnX\\ni=1\\n\\x10\\nln wZi + ln ϕZi (xi |µZi ,ΣZi )\\n\\x11\\n=\\nnX\\ni=1\\nEp(t)\\ni\\nh\\nln wZi + ln ϕZi (xi |µZi ,ΣZi )\\ni\\n,\\nwhere the {Zi}are independent and Zi is distributed according to p(t)\\ni in (4.36). This com-\\npletes the E-step. In the M-step we maximize Q(t) with respect to the parameter θ; that is,\\nwith respect to the {wk}, {µk}, and {Σk}. In particular, we maximize\\nnX\\ni=1\\nKX\\nk=1\\np(t)\\ni (k) \\x02ln wk + ln ϕk(xi |µk,Σk)\\x03,\\nunder the conditionP\\nk wk = 1. Using Lagrange multipliers and the fact thatPK\\nk=1 p(t)\\ni (k) = 1\\ngives the solution for the {wk}:\\nwk = 1\\nn\\nnX\\ni=1\\np(t)\\ni (k), k = 1,..., K. (4.37)\\nThe solutions forµk and Σk now follow from maximizingPn\\ni=1 p(t)\\ni (k) ln ϕk(xi |µk,Σk), lead-\\ning to\\nµk =\\nPn\\ni=1 p(t)\\ni (k) xi\\nPn\\ni=1 p(t)\\ni (k)\\n, k = 1,..., K (4.38)\\nand\\nΣk =\\nPn\\ni=1 p(t)\\ni (k) (xi −µk)(xi −µk)⊤\\nPn\\ni=1 p(t)\\ni (k)\\n, k = 1,..., K, (4.39)\\nwhich are very similar to the well-known formulas for the MLEs of the parameters of a\\nGaussian distribution. After assigning the solution parameters to θ(t) and increasing the\\niteration counter t by 1, the steps (4.36), (4.37), (4.38), and (4.39) are repeated until con-\\nvergence is reached. Convergence of the EM algorithm is very sensitive to the choice of\\ninitial parameters. It is therefore recommended to try various different starting conditions.\\nFor a further discussion of the theoretical and practical aspects of the EM algorithm we\\nrefer to [85].\\nExample 4.5 (Clustering via EM) We return to the data in Example 4.4, depicted in\\nFigure 4.4, and adopt the model that the data is coming from a mixture of three bivariate\\nGaussian distributions.\\nThe Python code below implements the EM procedure described in Algorithm 4.5.1.\\nThe initial mean vectors{µk}of the bivariate Gaussian distributions are chosen (from visual\\ninspection) to lie roughly in the middle of each cluster, in this case [−2,−3]⊤,[−4,1]⊤, and\\n[0,−1]⊤. The corresponding covariance matrices are initially chosen as identity matrices,\\nwhich is appropriate given the observed spread of the data in Figure 4.4. Finally, the initial\\nweights are 1/3,1/3,1/3. For simplicity, the algorithm stops after 100 iterations, which in\\nthis case is more than enough to guarantee convergence. The code and data are available\\nfrom the book’s website in the GitHub folder Chapter4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 157, 'page_label': '140'}, page_content=\"140 Clustering via Mixture Models\\nEMclust.py\\nimport numpy as np\\nfrom scipy.stats import multivariate_normal\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\nW = np.array([[1/3,1/3,1/3]])\\nM = np.array([[-2.0,-4,0],[-3,1,-1]], dtype=np.float32)\\n# Note that if above *all* entries were written as integers , M would\\n# be defined to be of integer type , which will give the wrong answer\\nC = np.zeros((3,2,2))\\nC[:,0,0] = 1\\nC[:,1,1] = 1\\np = np.zeros((3,300))\\nfor i in range (0,100):\\n#E-step\\nfor k in range (0,K):\\nmvn = multivariate_normal( M[:,k].T, C[k,:,:] )\\np[k,:] = W[0,k]*mvn.pdf(Xmat)\\n# M-Step\\np = (p/ sum (p,0)) #normalize\\nW = np.mean(p,1).reshape(1,3)\\nfor k in range (0,K):\\nM[:,k] = (Xmat.T @ p[k,:].T)/ sum (p[k,:])\\nxm = Xmat.T - M[:,k].reshape(2,1)\\nC[k,:,:] = xm @ (xm*p[k,:]).T/ sum (p[k,:])\\nThe estimated parameters of the mixture distribution are given on the right-hand side\\nof Figure 4.6. After relabeling of the clusters, we can observe a close match with the\\nparameters in Figure 4.4.\\nThe ellipses on the left-hand side of Figure 4.6 show a close match between the 95%\\nprobability ellipses2 of the original Gaussian distributions (in gray) and the estimated ones.\\nA natural way to cluster each pointxi is to assign it to the clusterk for which the conditional\\nprobability pi(k) is maximal (with ties resolved arbitrarily). This gives the clustering of the\\npoints into red, green, and blue clusters in the figure.\\n2For each mixture component, the contour of the corresponding bivariate normal pdf is shown that en-\\ncloses 95% of the probability mass.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 158, 'page_label': '141'}, page_content='Unsupervised Learning 141\\n-6 -4 -2 0 2 4\\n-4\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nweight mean vector covariance matrix\\n0.33\\n\"−1.51\\n−3.01\\n# \" 1.75 0 .03\\n0.03 0 .095\\n#\\n0.32\\n\" −4.08\\n−0.033\\n# \" 1.37 0 .92\\n0.92 1 .03\\n#\\n0.35\\n\" 0.36\\n−0.88\\n# \" 1.93 −1.20\\n−1.20 1 .44\\n#\\nFigure 4.6: The results of the EM clustering algorithm applied to the data depicted in\\nFigure 4.4.\\nAs an alternative to the EM algorithm, one can of course use continuous multiextremal\\noptimization algorithms to directly optimize the log-likelihood functionl(θ|τ) = ln g(τ|θ)\\nin (4.33) over the set Θ of all possible θ. This is done for example in [15], demonstrating\\nsuperior results to EM when there are few data points. Closer investigation of the likelihood\\nfunction reveals that there is a hidden problem with any maximum likelihood approach for\\nclustering if Θ is chosen as large as possible — i.e., any mixture distribution is possible. To\\ndemonstrate this problem, consider Figure 4.7, depicting the probability density function,\\ng(·|θ) of a mixture of two Gaussian distributions, where θ = [w,µ1,σ2\\n1,µ2,σ2\\n2]⊤ is the\\nvector of parameters for the mixture distribution. The log-likelihood function is given by\\nl(θ|τ) = P4\\ni=1 ln g(xi |θ), where x1,..., x4 are the data (indicated by dots in the figure).\\n-4 -2 0 2 4 6 8\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nFigure 4.7: Mixture of two Gaussian distributions.\\nIt is clear that by fixing the mixing constant w at 0.25 (say) and centering the first\\ncluster at x1, one can obtain an arbitrarily large likelihood value by taking the variance of\\nthe first cluster to be arbitrarily small. Similarly, for higher dimensional data, by choosing\\n“point” or “line” clusters, or in general “degenerate” clusters, one can make the value of\\nthe likelihood infinite. This is a manifestation of the familiar overfitting problem for the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 159, 'page_label': '142'}, page_content='142 Clustering via Vector Quantization\\ntraining loss that we already encountered in Chapter 2. Thus, the unconstrained maximiza-\\ntion of the log-likelihood function is an ill-posed problem, irrespective of the choice of the\\noptimization algorithm!\\nTwo possible solutions to this “overfitting” problem are:\\n1. Restrict the parameter set Θ in such a way that degenerate clusters (sometimes called\\nspurious clusters) are not allowed.\\n2. Run the given algorithm and if the solution is degenerate, discard it and run the\\nalgorithm afresh. Keep restarting the algorithm until a non-degenerate solution is\\nobtained.\\nThe first approach is usually applied to multiextremal optimization algorithms and the\\nsecond is used for the EM algorithm.\\n4.6 Clustering via Vector Quantization\\nIn the previous section we introduced clustering via mixture models, as a form of paramet-\\nric density estimation (as opposed to the nonparametric density estimation in Section 4.4).\\nThe clusters were modeled in a natural way via the latent variables and the EM algorithm\\nprovided a convenient way to assign the cluster members. In this section we consider a\\nmore heuristic approach to clustering by ignoring the distributional properties of the data.\\nThe resulting algorithms tend to scale better with the number of samples n and the dimen-\\nsionality d.\\nIn mathematical terms, we consider the following clustering (also called data segment-\\nation) problem. Given a collection τ = {x1,..., xn}of data points in some d-dimensional\\nspace X, divide this data set into K clusters (groups) such that some loss function is min-\\nimized. A convenient way to determine these clusters is to first divide up the entire space\\nX, using some distance function dist(·,·) on this space. A standard choice is the Euclidean\\n(or L2) distance:\\ndist(x,x′) = ∥x −x′∥=\\nvt dX\\ni=1\\n(xi −x′\\ni )2.\\nOther commonly used distance measures on Rd include the Manhattan distanceManhattan\\ndistance\\n:\\ndX\\ni=1\\n|xi −x′\\ni |\\nand the maximum distancemaximum\\ndistance\\n:\\nmax\\ni=1,...,d\\n|xi −x′\\ni |.\\nOn the set of strings of length d, an often-used distance measure is the Hamming distanceHamming\\ndistance\\n:\\ndX\\ni=1\\n1{xi , x′\\ni },\\nthat is, the number of mismatched characters. For example, the Hamming distance between\\n010101 and 011010 is 4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 160, 'page_label': '143'}, page_content='Unsupervised Learning 143\\nWe can partition the space Xinto regions as follows: First, we choose K points\\nc1,..., cK called cluster centers or source vectors source vectors. For each k = 1,..., K, let\\nRk = {x ∈X : dist(x,ck) ⩽dist(x,ci) for all i , k}\\nbe the set of points in Xthat lie closer to ck than any other center. The regions or cells\\n{Rk}divide the space Xinto what is called a Voronoi diagram or a Voronoi tessellation Voronoi\\ntessellation\\n.\\nFigure 4.8 shows a V oronoi tessellation of the plane into ten regions, using the Euclidean\\ndistance. Note that here the boundaries between the V oronoi cells are straight line seg-\\nments. In particular, if cellRi and Rj share a border, then a point on this border must satisfy\\n∥x −ci∥= ∥x −cj∥; that is, it must lie on the line that passes through the point ( cj + ci)/2\\n(that is, the midway point of the line segment between ci and cj) and be perpendicular to\\ncj −ci.\\n-2 0 2 4\\n-2\\n0\\n2\\nFigure 4.8: A V oronoi tessellation of the plane into ten cells, determined by the (red) cen-\\nters.\\nOnce the centers (and thus the cells {Rk}) are chosen, the points in τcan be clustered\\naccording to their nearest center. Points on the boundary have to be treated separately. This\\nis a moot point for continuous data, as generally no data points will lie exactly on the\\nboundary.\\nThe main remaining issue is how to choose the centers so as to cluster the data in some\\noptimal way. In terms of our (unsupervised) learning framework, we wish to approximate\\na vector x via one of c1,..., cK, using a piecewise constant vector-valued function\\ng(x |C) :=\\nKX\\nk=1\\nck 1{x ∈Rk},\\nwhere C is the d ×K matrix [c1,..., cK]. Thus, g(x |C) = ck when x falls in region Rk (we\\nignore ties). Within this class Gof functions, parameterized by C, our aim is to minimize\\nthe training loss. In particular, for the squared-error loss, Loss(x,x′) = ∥x−x′∥2, the training\\nloss is\\nℓτn (g(·|C)) = 1\\nn\\nnX\\ni=1\\n∥xi −g(xi |C)∥2 = 1\\nn\\nKX\\nk=1\\nX\\nx∈Rk∩τn\\n||x −ck||2. (4.40)\\nThus, the training loss minimizes the average squared distance between the centers. This\\nframework also combines both the encoding and decoding steps in vector quantization vector\\nquantization'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 161, 'page_label': '144'}, page_content='144 Clustering via Vector Quantization\\n[125]. Namely, we wish to “quantize” or “encode” the vectors in τin such a way that each\\nvector is represented by one of K source vectors c1,..., cK, such that the loss (4.40) of this\\nrepresentation is minimized.\\nMost well-known clustering and vector quantization methods update the vector of cen-\\nters, starting from some initial choice and using iterative (typically gradient-based) proced-\\nures. It is important to realize that in this case (4.40) is seen as a function of the centers,\\nwhere each point x is assigned to the nearest center, thus determining the clusters. It is well\\nknown that this type of problem — optimization with respect to the centers — is highly\\nmultiextremal and, depending on the initial clusters, gradient-based procedures tend to\\nconverge to a local minimum rather than a global minimum.\\n4.6.1 K-Means\\nOne of the simplest methods for clustering is theK-means method. It is an iterative method\\nwhere, starting from an initial guess for the centers, new centers are formed by taking\\nsample means of the current points in each cluster. The new centers are thus the centroidscentroids\\nof the points in each cell. Although there exist many di fferent varieties of the K-means\\nalgorithm, they are all essentially of the following form:\\nAlgorithm 4.6.1: K-Means\\ninput: Collection of points τ= {x1,..., xn}, number of clusters K, initial centers\\nc1,..., cK.\\noutput: Cluster centers and cells (regions).\\n1 while a stopping criterion is not met do\\n2 R1,..., RK ←∅ (empty sets).\\n3 for i = 1 to n do\\n4 d ←[dist(xi,c1),..., dist(xi,cK)] // distances to centers\\n5 k ←argminj dj\\n6 Rk ←Rk ∪{xi} // assign xi to cluster k\\n7 for k = 1 to K do\\n8 ck ←\\nP\\nx∈Rk x\\n|Rk| // compute the new center as a centroid of points\\n9 return {ck}, {Rk}\\nThus, at each iteration, for a given choice of centers, each point in τ is assigned to\\nits nearest center. After all points have been assigned, the centers are recomputed as the\\ncentroids of all the points in the current cluster (Line 8). A typical stopping criterion is to\\nstop when the centers no longer change very much. As the algorithm is quite sensitive to\\nthe choice of the initial centers, it is prudent to try multiple starting values, e.g., chosen\\nrandomly from the bounding box of the data points.\\nWe can see the K-means method as a deterministic (or “hard”) version of the probab-\\nilistic (or “soft”) EM algorithm as follows. Suppose in the EM algorithm we have Gaus-\\nsian mixtures with a fixed covariance matrix Σk = σ2Id, k = 1,..., K, where σ2 should be\\nthought of as being infinitesimally small. Consider iterationt of the EM algorithm. Having\\nobtained the expectation vectors µ(t−1)\\nk and weights w(t−1)\\nk ,k = 1,..., K, each point xi is as-\\nsigned a cluster label Zi according to the probabilities p(t)\\ni (k),k = 1,..., K given in (4.36).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 162, 'page_label': '145'}, page_content=\"Unsupervised Learning 145\\nBut for σ2 →0 the probability distribution {p(t)\\ni (k)}becomes degenerate, putting all its\\nprobability mass on argmink ∥xi −µk∥2. This corresponds to the K-means rule of assigning\\nxi to its nearest cluster center. Moreover, in the M-step (4.38) each cluster centerµ(t)\\nk is now\\nupdated according to the average of the {xi}that have been assigned to cluster k. We thus\\nobtain the same deterministic updating rule as in K-means.\\nExample 4.6 (K-means Clustering) We cluster the data from Figure 4.4 viaK-means,\\nusing the Python implementation below. Note that the data points are stored as a 300 ×2\\nmatrix Xmat. We take the same starting centers as in the EM example:c1 = [−2,−3]⊤,c2 =\\n[−4,1]⊤, and c3 = [0,−1]⊤. Note also that squared Euclidean distances are used in the\\ncomputations, as these are slightly faster to compute than Euclidean distances (as no square\\nroot computations are required) while yielding exactly the same cluster center evaluations.\\nKmeans.py\\nimport numpy as np\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\nc = np.array([[-2.0,-4,0],[-3,1,-1]]) #initialize centers\\ncold = np.zeros(c.shape)\\ndist2 = np.zeros((K,n))\\nwhile np.abs (c - cold). sum () > 0.001:\\ncold = c.copy()\\nfor i in range (0,K): #compute the squared distances\\ndist2[i,:] = np. sum ((Xmat - c[:,i].T)**2, 1)\\nlabel = np.argmin(dist2,0) #assign the points to nearest centroid\\nminvals = np.amin(dist2,0)\\nfor i in range (0,K): # recompute the centroids\\nc[:,i] = np.mean(Xmat[np.where(label == i),:],1).reshape(1,2)\\nprint ('Loss = {:3.3f} '.format (minvals.mean()))\\nLoss = 2.288\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 163, 'page_label': '146'}, page_content=\"146 Clustering via Vector Quantization\\n-6 -4 -2 0 2 4\\n-5\\n-4\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nFigure 4.9: Results of the K-means algorithm applied to the data in Figure 4.4. The thick\\nblack circles are the centroids and the dotted lines define the cell boundaries.\\nWe found the cluster centers c1 = [−1.9286,−3.0416]⊤,c2 = [−3.9237,0.0131]⊤, and\\nc3 = [0.5611,−1.2980]⊤, giving the clustering depicted in Figure 4.9. The corresponding\\nloss (4.40) was found to be 2.288.\\n4.6.2 Clustering via Continuous Multiextremal Optimization\\nAs already mentioned, the exact minimization of the loss function (4.40) is di fficult to\\naccomplish via standard local search methods such as gradient descent, as the function\\nis highly multimodal. However, nothing is preventing us from using global optimization\\nmethods such as the CE or SCO methods discussed in Sections 3.4.2 and 3.4.3.☞100\\nExample 4.7 (Clustering via CE) We take the same data set as in Example 4.6 and\\ncluster the points via minimization of the loss (4.40) using the CE method. The Python\\ncode below is very similar to the code in Example 3.16, except that now we are dealing☞101\\nwith a six-dimensional optimization problem. The loss function is implemented in the func-\\ntion Scluster, which essentially reuses the squared distance computation of theK-means\\ncode in Example 4.6. The CE program typically converges to a loss of 2.287, correspond-\\ning to the (global) minimizers c1 = [−1.9286,−3.0416]⊤,c2 = [−3.8681,0.0456]⊤, and\\nc3 = [0.5880,−1.3526]⊤, which slightly differs from the local minimizers for the K-means\\nalgorithm.\\nclustCE.py\\nimport numpy as np\\nnp.set_printoptions(precision=4)\\nXmat = np.genfromtxt( 'clusterdata.csv ', delimiter= ',')\\nK = 3\\nn, D = Xmat.shape\\ndef Scluster(c):\\nn, D = Xmat.shape\\ndist2 = np.zeros((K,n))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 164, 'page_label': '147'}, page_content='Unsupervised Learning 147\\ncc = c.reshape(D,K)\\nfor i in range (0,K):\\ndist2[i,:] = np. sum ((Xmat - cc[:,i].T)**2, 1)\\nminvals = np.amin(dist2,0)\\nreturn minvals.mean()\\nnumvar = K*D\\nmu = np.zeros(numvar) #initialize centers\\nsigma = np.ones(numvar)*2\\nrho = 0.1\\nN = 500; Nel = int (N*rho); eps = 0.001\\nfunc = Scluster\\nbest_trj = np.array(numvar)\\nbest_perf = np.Inf\\ntrj = np.zeros(shape=(N,numvar))\\nwhile (np. max (sigma)>eps):\\nfor i in range (0,numvar):\\ntrj[:,i] = (np.random.randn(N,1)*sigma[i]+ mu[i]).reshape(N,)\\nS = np.zeros(N)\\nfor i in range (0,N):\\nS[i] = func(trj[i])\\nsortedids = np.argsort(S) # from smallest to largest\\nS_sorted = S[sortedids]\\nbest_trj = np.array(n)\\nbest_perf = np.Inf\\neliteids = sortedids[ range (0,Nel)]\\neliteTrj = trj[eliteids,:]\\nmu = np.mean(eliteTrj,axis=0)\\nsigma = np.std(eliteTrj,axis=0)\\nif (best_perf>S_sorted[0]):\\nbest_perf = S_sorted[0]\\nbest_trj = trj[sortedids[0]]\\nprint (best_perf)\\nprint (best_trj.reshape(2,3))\\n2.2874901831572947\\n[[-3.9238 -1.8477 0.5895]\\n[ 0.0134 -3.0292 -1.2442]]\\n4.7 Hierarchical Clustering\\nIt is sometimes useful to determine data clusters in a hierarchical manner; an example\\nis the construction of evolutionary relationships between animal species. Establishing a\\nhierarchy of clusters can be done in a bottom-up or a top-down manner. In the bottom-up\\napproach, also called agglomerative clustering agglomerative\\nclustering\\n, the data points are merged in larger and\\nlarger clusters until all the points have been merged into a single cluster. In the top-down'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 165, 'page_label': '148'}, page_content='148 Hierarchical Clustering\\nor divisive clusteringdivisive\\nclustering\\napproach, the data set is divided up into smaller and smaller clusters.\\nThe left panel of Figure 4.10 depicts a hierarchy of clusters.\\n7 8 6 1 2 3 4 5\\nLabels\\n10\\n20\\n30\\n40Distance\\nFigure 4.10: Left: a cluster hierarchy of 15 clusters. Right: the corresponding dendrogram.\\nIn Figure 4.10, each cluster is given a cluster identifier. At the lowest level are clusters\\ncomprised of the original data points (identifiers 1 ,..., 8). The union of clusters 1 and 2\\nform a cluster with identifier 9, and the union of 3 and 4 form a cluster with identifier 10.\\nIn turn the union of clusters 9 and 10 constitutes cluster 12, and so on.\\nThe right panel of Figure 4.10 shows a convenient way to visualize cluster hierarchies\\nusing a dendrogramdendrogram (from the Greek dendro for tree). A dendrogram not only summarizes\\nhow clusters are merged or split, but also shows the distance between clusters, here on the\\nvertical axis. The horizontal axis shows which cluster each data point (label) belongs to.\\nMany different types of hierarchical clustering can be performed, depending on how\\nthe distance is defined between two data points and between two clusters. Denote the data\\nset by X= {xi,i = 1,..., n}. As in Section 4.6, let dist(xi,xj) be the distance between data\\npoints xi and xj. The default choice is the Euclidean distance dist(xi,xj) = ∥xi −xj∥.\\nLet Iand Jbe two disjoint subsets of {1,..., n}. These sets correspond to two disjoint\\nsubsets (that is, clusters) of X: {xi,i = I}and {xj, j = J}. We denote the distance between\\nthese two clusters by d(I,J). By specifying the function d, we indicate how the clusters\\nare linked. For this reason it is also referred to as the linkagelinkage criterion. We give a number\\nof examples:\\n• Single linkage. The closest distance between the clusters.\\ndmin(I,J) := min\\ni∈I,j∈J\\ndist(xi,xj).\\n• Complete linkage. The furthest distance between the clusters.\\ndmax(I,J) := max\\ni∈I,j∈J\\ndist(xi,xj).\\n• Group average. The mean distance between the clusters. Note that this depends on\\nthe cluster sizes.\\ndavg(I,J) := 1\\n|I||J|\\nX\\ni∈I\\nX\\nj∈J\\ndist(xi,xj).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 166, 'page_label': '149'}, page_content='Unsupervised Learning 149\\nFor these linkage criteria, Xis usually assumed to be Rd with the Euclidean distance.\\nAnother notable measure for the distance between clusters is Ward’s minimum vari-\\nance linkage criterion. Ward’s linkageHere, the distance between clusters is expressed as the additional\\namount of “variance” (expressed in terms of the sum of squares) that would be intro-\\nduced if the two clusters were merged. More precisely, for any setKof indices (labels) let\\nxK = P\\nk∈K xk/|K|denote its corresponding cluster mean. Then\\ndWard(I,J) :=\\nX\\nk∈I∪J\\n∥xk −xI∪J∥2 −\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\ni∈I\\n∥xi −xI∥2 +\\nX\\nj∈J\\n∥xj −xJ∥2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (4.41)\\nIt can be shown (see Exercise 8) that the Ward linkage depends only on the cluster means\\nand the cluster sizes for Iand J:\\ndWard(I,J) = |I||J|\\n|I|+ |J|∥xI−xJ∥2.\\nIn software implementations, the Ward linkage function is often rescaled by mul-\\ntiplying it by a factor of 2. In this way, the distance between one-point clusters {xi}\\nand {xj}is the squared Euclidean distance ∥xi −xj∥2.\\nHaving chosen a distance on Xand a linkage criterion, a general agglomerative clus-\\ntering algorithm proceeds in the following “greedy” manner.\\nAlgorithm 4.7.1: Greedy Agglomerative Clustering\\ninput: Distance function dist, linkage function d, number of clusters K.\\noutput: The label sets for the tree.\\n1 Initialize the set of cluster identifiers: I= {1,..., n}.\\n2 Initialize the corresponding label sets: Li = {i}, i ∈I.\\n3 Initialize a distance matrix D = [di j] with di j = d({i},{j}).\\n4 for k = n + 1 to 2n −K do\\n5 Find i and j >i in Isuch that di j is minimal.\\n6 Create a new label set Lk := Li ∪Lj.\\n7 Add the new identifier k to Iand remove the old identifiers i and j from I.\\n8 Update the distance matrix D with respect to the identifiers i, j, and k.\\n9 return Li,i = 1,..., 2n −K\\nInitially, the distance matrix D contains the (linkage) distances between the one-point\\nclusters containing one of the data points x1,..., xn, and hence with identifiers 1 ,..., n.\\nFinding the shortest distance amounts to a table lookup in D. When the closest clusters\\nare found, they are merged into a new cluster, and a new identifier k (the smallest positive\\ninteger that has not yet been used as an identifier) is assigned to this cluster. The old iden-\\ntifiers i and j are removed from the cluster identifier set I. The matrix D is then updated\\nby adding a k-th column and row that contain the distances betweenk and any m ∈I. This\\nupdating step could be computationally quite costly if the cluster sizes are large and the\\nlinkage distance between the clusters depends on all points within the clusters. Fortunately,\\nfor many linkage functions, the matrix D can be updated in an efficient manner.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 167, 'page_label': '150'}, page_content='150 Hierarchical Clustering\\nSuppose that at some stage in the algorithm, clusters Iand J, with identifiers i and j,\\nrespectively, are merged into a cluster K= I∪J with identifier k. Let M, with identifier\\nm, be a previously assigned cluster. An update rule of the linkage distance dkm between K\\nand Mis called a Lance–WilliamsLance–\\nWilliams\\nupdate if it can be written in the form\\ndkm = αdim + βdjm + γdi j + δ|dim −djm|,\\nwhere α,...,δ depend only on simple characteristics of the clusters involved, such as the\\nnumber of elements within the clusters. Table 4.2 shows the update constants for a number\\nof common linkage functions. For example, for single linkage, dim is the minimal distance\\nbetween Iand M, and djm is the minimal distance between Jand M. The smallest of\\nthese is the minimal distance between Kand M. That is, dkm = min{dim,djm}= dim/2 +\\ndjm/2 −|dim −djm|/2.\\nTable 4.2: Constants for the Lance–Williams update rule for various linkage functions,\\nwith ni,nj,nm denoting the number of elements in the corresponding clusters.\\nLinkage α β γ δ\\nSingle 1 /2 1 /2 0 −1/2\\nComplete 1 /2 1 /2 0 1 /2\\nGroup avg. ni\\nni + nj\\nnj\\nni + nj\\n0 0\\nWard ni + nm\\nni + nj + nm\\nnj + nm\\nni + nj + nm\\n−nm\\nni + nj + nm\\n0\\nIn practice, Algorithm 4.7.1 is run until a single cluster is obtained. Instead of returning\\nthe label sets of all 2 n −1 clusters, a linkage matrixlinkage matrix is returned that contains the same\\ninformation. At the end of each iteration (Line 8) the linkage matrix stores the merged\\nlabels i and j, as well as the (minimal) distance di j. Other information such as the number\\nof elements in the merged cluster can also be stored. Dendrograms and cluster labels can be\\ndirectly constructed from the linkage matrix. In the following example, the linkage matrix\\nis returned by the method agg_cluster.\\nExample 4.8 (Agglomerative Hierarchical Clustering) The Python code below gives\\na basic implementation of Algorithm 4.7.1 using the Ward linkage function. The methods\\nfcluster and dendrogram from the scipy module can be used to identify the labels in\\na cluster and to draw the corresponding dendrogram.\\nAggCluster.py\\nimport numpy as np\\nfrom scipy.spatial.distance import cdist\\ndef update_distances(D,i,j, sizes): # distances for merged cluster\\nn = D.shape[0]\\nd = np.inf * np.ones(n+1)\\nfor k in range (n): # Update distances\\nd[k] = ((sizes[i]+sizes[k])*D[i,k] +\\n(sizes[j]+sizes[k])*D[j,k] -'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 168, 'page_label': '151'}, page_content=\"Unsupervised Learning 151\\nsizes[k]*D[i,j])/(sizes[i] + sizes[j] + sizes[k])\\ninfs = np.inf * np.ones(n) # array of infinity\\nD[i,:],D[:,i],D[j,:],D[:,j] = infs,infs,infs,infs # deactivate\\nnew_D = np.inf * np.ones((n+1,n+1))\\nnew_D[0:n,0:n] = D # copy old matrix into new_D\\nnew_D[-1,:], new_D[:,-1] = d,d # add new row and column\\nreturn new_D\\ndef agg_cluster(X):\\nn = X.shape[0]\\nsizes = np.ones(n)\\nD = cdist(X, X,metric = 'sqeuclidean ') # initialize dist. matrix\\n.\\nnp.fill_diagonal(D, np.inf * np.ones(D.shape[0]))\\nZ = np.zeros((n-1,4)) #linkage matrix encodes hierarchy tree\\nfor t in range (n-1):\\ni,j = np.unravel_index(D.argmin(), D.shape) # minimizer pair\\nsizes = np.append(sizes, sizes[i] + sizes[j])\\nZ[t,:]=np.array([i, j, np.sqrt(D[i,j]), sizes[-1]])\\nD = update_distances(D, i,j, sizes) # update distance matr.\\nreturn Z\\nimport scipy.cluster.hierarchy as h\\nX = np.genfromtxt( 'clusterdata.csv ',delimiter= ',') # read the data\\nZ = agg_cluster(X) # form the linkage matrix\\nh.dendrogram(Z) # SciPy can produce a dendrogram from Z\\n# fcluster function assigns cluster ids to all points based on Z\\ncl = h.fcluster(Z, criterion = 'maxclust ', t=3)\\nimport matplotlib.pyplot as plt\\nplt.figure(2), plt.clf()\\ncols = [ 'red','green ','blue ']\\ncolors = [cols[i-1] for i in cl]\\nplt.scatter(X[:,0], X[:,1],c=colors)\\nplt.show()\\nNote that the distance matrix is initialized with the squared Euclidean distance, so that\\nthe Ward linkage is rescaled by a factor of 2. Also, note that the linkage matrix stores\\nthe square root of the minimal cluster distances rather than the distances themselves. We\\nleave it as an exercise to check that by using these modifications the results agree with the\\nlinkage method from scipy; see Exercise 9.\\nIn contrast to the bottom-up (agglomerative) approach to hierarchical clustering, the\\ndivisive approach starts with one cluster, which is divided into two clusters that are as\\n“dissimilar” as possible, which can then be further divided, and so on. We can use the same\\nlinkage criteria as for agglomerative clustering to divide a parent cluster into two child\\nclusters by maximizing the distance between the child clusters. Although it is a natural to try\\nto group together data by separating dissimilar ones as far as possible, the implementation\\nof this idea tends to scale poorly with n. The problem is related to the well-knownmax-cut\\nproblem max-cut\\nproblem\\n: given an n ×n matrix of positive costs ci j,i, j ∈{1,..., n}, partition the index set\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 169, 'page_label': '152'}, page_content='152 Hierarchical Clustering\\nI= {1,..., n}into two subsets Jand Ksuch that the total cost across the sets, that is,\\nX\\nj∈J\\nX\\nk∈K\\ndjk,\\nis maximal. If instead we maximize according to theaverage distance, we obtain the group\\naverage linkage criterion.\\nExample 4.9 (Divisive Clustering via CE) The following Python code is used to di-\\nvide a small data set (of size 300) into two parts according to maximal group average link-\\nage. It uses a short cross-entropy algorithm similar to the one presented in Example 3.19.\\nGiven a vector of probabilities {pi,i = 1,..., n}, the algorithm generates an n ×n matrix☞111\\nof Bernoulli random variables with success probability pi for column i. For each row, the\\n0s and 1s divide the index set into two clusters, and the corresponding average linkage\\ndistance is computed. The matrix is then sorted row-wise according to these distances. Fi-\\nnally, the probabilities{pi}are updated according to the mean values of the best 10% rows.\\nThe process is repeated until the {pi}degenerate to a binary vector. This then presents the\\n(approximate) solution.\\nclustCE2.py\\nimport numpy as np\\nfrom numpy import genfromtxt\\nfrom scipy.spatial.distance import squareform\\nfrom scipy.spatial.distance import pdist\\nimport matplotlib.pyplot as plt\\ndef S(x,D):\\nV1 = np.where(x==0)[0] # {V1,V2} is the partition\\nV2 = np.where(x==1)[0]\\ntmp = D[V1]\\ntmp = tmp[:,V2]\\nreturn np.mean(tmp) # the size of the cut\\ndef maxcut(D,N,eps,rho,alpha):\\nn = D.shape[1]\\nNe = int (rho*N)\\np = 1/2*np.ones(n)\\np[0] = 1.0\\nwhile (np. max (np.minimum(p,np.subtract(1,p))) > eps):\\nx = np.array(np.random.uniform(0,1,(N,n))<=p, dtype=np.int64)\\nsx = np.zeros(N)\\nfor i in range (N):\\nsx[i] = S(x[i],D)\\nsortSX = np.flip(np.argsort(sx))\\n#print(\"gamma = \",sx[sortSX[Ne -1]], \" best=\",sx[sortSX [0]])\\nelIds = sortSX[0:Ne]\\nelites = x[elIds]\\npnew = np.mean(elites, axis=0)\\np = alpha*pnew + (1.0-alpha)*p'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 170, 'page_label': '153'}, page_content='Unsupervised Learning 153\\nreturn np.round (p)\\nXmat = genfromtxt( \\'clusterdata.csv \\', delimiter= \\',\\')\\nn = Xmat.shape[0]\\nD = squareform(pdist(Xmat))\\nN = 1000\\neps = 10**-2\\nrho = 0.1\\nalpha = 0.9\\n# CE\\npout = maxcut(D,N,eps,rho, alpha);\\ncutval = S(pout,D)\\nprint (\"cutvalue \",cutval)\\n#plot\\nV1 = np.where(pout==0)[0]\\nxblue = Xmat[V1]\\nV2 = np.where(pout==1)[0]\\nxred = Xmat[V2]\\nplt.scatter(xblue[:,0],xblue[:,1], c=\"blue\")\\nplt.scatter(xred[:,0],xred[:,1], c=\"red\")\\ncutvalue 4.625207676517948\\n6\\n 4\\n 2\\n 0\\n 2\\n 4\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nFigure 4.11: Division of the data in Figure 4.4 into two clusters, via the cross-entropy\\nmethod.\\n4.8 Principal Component Analysis (PCA)\\nThe main idea of principal component analysis principal\\ncomponent\\nanalysis\\n(PCA) is to reduce the dimensionality of\\na data set consisting of many variables. PCA is a feature reduction (or feature extraction)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 171, 'page_label': '154'}, page_content='154 Principal Component Analysis (PCA)\\nmechanism, that helps us to handle high-dimensional data with more features than is con-\\nvenient to interpret.\\n4.8.1 Motivation: Principal Axes of an Ellipsoid\\nConsider a d-dimensional normal distribution with mean vector 0 and covariance matrix\\nΣ. The corresponding pdf (see (2.33)) is☞46\\nf (x) = 1√(2π)n |Σ|\\ne−1\\n2 x⊤Σ−1 x, x ∈Rd.\\nIf we were to draw many iid samples from this pdf, the points would roughly have an\\nellipsoid pattern, as illustrated in Figure 3.1, and correspond to the contours of f : sets of☞71\\npoints x such that x⊤Σ−1 x = c, for some c ⩾0. In particular, consider the ellipsoid\\nx⊤Σ−1 x = 1, x ∈Rd. (4.42)\\nLet Σ = BB⊤, where B is for example the (lower) Cholesky matrix. Then, as explained☞373\\nin Example A.5, the ellipsoid (4.42) can also be viewed as the linear transformation of☞366\\nd-dimensional unit sphere via matrix B. Moreover, the principal axes of the ellipsoid canprincipal axes\\nbe found via a singular value decomposition (SVD) of B (or Σ); see Section A.6.5 andsingular value\\ndecomposition Example A.8. In particular, suppose that an SVD of B is\\n☞378 B = UDV⊤ (note that an SVD of Σ is then UD2U⊤).\\nThe columns of the matrix UD correspond to the principal axes of the ellipsoid, and the\\nrelative magnitudes of the axes are given by the elements of the diagonal matrixD. If some\\nof these magnitudes are small compared to the others, a reduction in the dimension of the\\nspace may be achieved by projecting each point x ∈Rd onto the subspace spanned by the\\nmain (say k ≪d) columns of U — the so-called principal componentsprincipal\\ncomponents\\n. Suppose without\\nloss of generality that the first k principal components are given by the first k columns of\\nU, and let Uk be the corresponding d ×k matrix.\\nWith respect to the standard basis{ei}, the vector x = x1e1 +···+ xded is represented by\\nthe d-dimensional vector [x1,..., xd]⊤. With respect to the orthonormal basis {ui}formed\\nby the columns of matrix U, the representation of x is U⊤x. Similarly, the projection of\\nany point x onto the subspace spanned by the first k principal vectors is represented by the\\nk-dimensional vector U⊤\\nk x, with respect to the orthonormal basis formed by the columns of\\nUk. So, the idea is that if a pointx lies close to its projectionUkU⊤\\nk x, we may represent it via\\nk numbers instead of d, using the combined features given by the k principal components.\\nSee Section A.4 for a review of projections and orthonormal bases.☞362\\nExample 4.10 (Principal Components) Consider the matrix\\nΣ =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n14 8 3\\n8 5 2\\n3 2 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nwhich can be written as Σ = BB⊤, with\\nB =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 2 3\\n0 1 2\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 172, 'page_label': '155'}, page_content='Unsupervised Learning 155\\nFigure 4.12 depicts the ellipsoid x⊤Σ−1 x = 1, which can be obtained by linearly transform-\\ning the points on the unit sphere by means of the matrix B. The principal axes and sizes of\\nthe ellipsoid are found through a singular value decomposition B = UDV⊤, where U and\\nD are\\nU =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0.8460 0 .4828 0 .2261\\n0.4973 −0.5618 −0.6611\\n0.1922 −0.6718 0 .7154\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb and D =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n4.4027 0 0\\n0 0 .7187 0\\n0 0 0 .3160\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nThe columns of U show the directions of the principal axes of the ellipsoid, and the di-\\nagonal elements of D indicate the relative magnitudes of the principal axes. We see that\\nthe first principal component is given by the first column of U, and the second principal\\ncomponent by the second column of U.\\nThe projection of the point x = [1.052,0.6648,0.2271]⊤onto the 1-dimensional space\\nspanned by the first principal component u1 = [0.8460,0.4972,0.1922]⊤ is z = u1u⊤\\n1 x =\\n[1.0696,0.6287,0.2429]⊤. With respect to the basis vectoru1, z is represented by the num-\\nber u⊤\\n1 z = 1.2643. That is, z = 1.2643u1.\\nFigure 4.12: A “surfboard” ellipsoid where one principal axis is significantly larger than\\nthe other two.\\n4.8.2 PCA and Singular Value Decomposition (SVD)\\nIn the setting above, we did not consider any data set drawn from a multivariate pdff . The\\nwhole analysis rested on linear algebra. In principal component analysis principal\\ncomponent\\nanalysis\\n(PCA) we start\\nwith data x1,..., xn, where each x is d-dimensional. PCA does not require assumptions\\nhow the data were obtained, but to make the link with the previous section, we can think\\nof the data as iid draws from a multivariate normal pdf.\\nLet us collect the data in a matrix X in the usual way; that is, ☞ 44\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11 x12 ... x1d\\nx21 x22 ... x2d\\n... ... ... ...\\nxn1 xn2 ... xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx⊤\\n1\\nx⊤\\n2\\n...\\nx⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 173, 'page_label': '156'}, page_content='156 Principal Component Analysis (PCA)\\nThe matrix X will be the PCA’s input. Under this setting, the data consists of points in d-\\ndimensional space, and our goal is to present the data using n feature vectors of dimension\\nk <d.\\nIn accordance with the previous section, we assume that underlying distribution of the\\ndata has expectation vector 0. In practice, this means that before PCA is applied, the data\\nneeds to be centered by subtracting the column mean in every column:\\nx′\\ni j = xi j −xj,\\nwhere xj = 1\\nn\\nPn\\ni=1 xi j.\\nWe assume from now on that the data comes from a generald-dimensional distribution\\nwith mean vector0 and some covariance matrixΣ. The covariance matrixΣ is by definition\\nequal to the expectation of the random matrix XX⊤, and can be estimated from the data\\nx1,..., xn via the sample average\\nbΣ = 1\\nn\\nnX\\ni=1\\nxi x⊤\\ni = 1\\nnX⊤X.\\nAs bΣ is a covariance matrix, we may conduct the same analysis forbΣ as we did for Σ in the\\nprevious section. Specifically, supposebΣ = UD2U⊤is an SVD ofbΣ and let Uk be the matrix\\nwhose columns are thek principal components; that is, thek columns of U corresponding to\\nthe largest diagonal elements in D2. Note that we have used D2 instead of D to be compat-\\nible with the previous section. The transformation zi = UkU⊤\\nk xi maps each vector xi ∈Rd\\n(thus, with d features) to a vector zi ∈Rd lying in the subspace spanned by the columns of\\nUk. With respect to this basis, the pointzi has representation zi = U⊤\\nk (UkU⊤\\nk xi) = U⊤\\nk xi ∈Rk\\n(thus with k features). The corresponding covariance matrix of the zi,i = 1,..., n is diag-\\nonal. The diagonal elements {dℓℓ}of D can be interpreted as standard deviations of the data\\nin the directions of the principal components. The quantityv = P\\nℓ=1 d2\\nℓℓ (that is, the trace of\\nD2) is thus a measure for the amount of variance in the data. The proportiond2\\nℓℓ/v indicates\\nhow much of the variance in the data is explained by the ℓ-th principal component.\\nAnother way to look at PCA is by considering the question: How can we best project the\\ndata onto a k-dimensional subspace in such a way that the total squared distance between\\nthe projected points and the original points is minimal? From Section A.4, we know that☞362\\nany orthogonal projection to a k-dimensional subspace Vk can be represented by a matrix\\nUkU⊤\\nk , where Uk = [u1,..., uk] and the {uℓ,ℓ = 1,..., k}are orthogonal vectors of length 1\\nthat span Vk. The above question can thus be formulated as the minimization program:\\nmin\\nu1,...,uk\\nnX\\ni=1\\n∥xi −UkU⊤\\nk xi∥2. (4.43)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 174, 'page_label': '157'}, page_content='Unsupervised Learning 157\\nNow observe that\\n1\\nn\\nnX\\ni=1\\n∥xi −UkU⊤\\nk xi∥2 = 1\\nn\\nnX\\ni=1\\n(x⊤\\ni −x⊤\\ni UkU⊤\\nk )(xi −UkU⊤\\nk xi)\\n= 1\\nn\\nnX\\ni=1\\n∥xi∥2\\n|       {z       }\\nc\\n−1\\nn\\nnX\\ni=1\\nx⊤\\ni UkU⊤\\nk xi = c −1\\nn\\nnX\\ni=1\\nkX\\nℓ=1\\ntr(x⊤\\ni uℓu⊤\\nℓ xi)\\n= c −1\\nn\\nkX\\nℓ=1\\nnX\\ni=1\\nu⊤\\nℓ xi x⊤\\ni uℓ = c −\\nkX\\nℓ=1\\nu⊤\\nℓ bΣ uℓ,\\nwhere we have used the cyclic property of a trace (Theorem A.1) and the fact that UkU⊤\\nk ☞ 357\\ncan be written as Pk\\nℓ=1 uℓu⊤\\nℓ . It follows that the minimization problem(4.43) is equivalent\\nto the maximization problem\\nmax\\nu1,...,uk\\nkX\\nℓ=1\\nu⊤\\nℓ bΣ uℓ. (4.44)\\nThis maximum can be at most Pk\\nℓ=1 d2\\nℓℓ and is attained precisely when u1,..., uk are the\\nfirst k principal components of bΣ.\\nExample 4.11 (Singular Value Decomposition) The following data set consists of in-\\ndependent samples from the three-dimensional Gaussian distribution with mean vector 0\\nand covariance matrix Σ given in Example 4.10:\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3.1209 1 .7438 0 .5479\\n−2.6628 −1.5310 −0.2763\\n3.7284 3 .0648 1 .8451\\n0.4203 0 .3553 0 .4268\\n−0.7155 −0.6871 −0.1414\\n5.8728 4 .0180 1 .4541\\n4.8163 2 .4799 0 .5637\\n2.6948 1 .2384 0 .1533\\n−1.1376 −0.4677 −0.2219\\n−1.2452 −0.9942 −0.4449\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nAfter replacing X with its centered version, an SVD UD2U⊤ of bΣ = X⊤X/n yields the\\nprincipal component matrix U and diagonal matrix D:\\nU =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−0.8277 0 .4613 0 .3195\\n−0.5300 −0.4556 −0.7152\\n−0.1843 −0.7613 0 .6216\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb and D =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3.3424 0 0\\n0 0 .4778 0\\n0 0 0 .1038\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nWe also observe that, apart from the sign of the first column, the principal component\\nmatrix U is similar to that in Example 4.10. Likewise for the matrixD. We see that 97.90%\\nof the total variance is explained by the first principal component. Figure 4.13 shows the\\nprojection of the centered data onto the subspace spanned by this principal component.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 175, 'page_label': '158'}, page_content=\"158 Principal Component Analysis (PCA)\\nx\\n4 2 0 2 4\\ny\\n2\\n1\\n0\\n1\\n2\\n3\\nz\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nFigure 4.13: Data from the “surfboard” pdf is projected onto the subspace spanned by the\\nlargest principal component.\\nThe following Python code was used.\\nPCAdat.py\\nimport numpy as np\\nX = np.genfromtxt( 'pcadat.csv ', delimiter= ',')\\nn = X.shape[0]\\nX = X - X.mean(axis=0)\\nG = X.T @ X\\nU, _ , _ = np.linalg.svd(G/n)\\n# projected points\\nY = X @ np.outer(U[:,0],U[:,0])\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfig = plt.figure()\\nax = fig.add_subplot(111, projection= '3d')\\nax.w_xaxis.set_pane_color((0, 0, 0, 0))\\nax.plot(Y[:,0], Y[:,1], Y[:,2], c= 'k', linewidth=1)\\nax.scatter(X[:,0], X[:,1], X[:,2], c= 'b')\\nax.scatter(Y[:,0], Y[:,1], Y[:,2], c= 'r')\\nfor i in range (n):\\nax.plot([X[i,0], Y[i,0]], [X[i,1],Y[i,1]], [X[i,2],Y[i,2]], 'b')\\nax.set_xlabel( 'x')\\nax.set_ylabel( 'y')\\nax.set_zlabel( 'z')\\nplt.show()\\nNext is an application of PCA to Fisher’s famous iris data set, already mentioned in\\nSection 1.1, and Exercise 1.5.☞2\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 176, 'page_label': '159'}, page_content=\"Unsupervised Learning 159\\nExample 4.12 (PCA for the Iris Data Set) The iris data set contains measurements\\non four features of the iris plant: sepal length and width, and petal length and width, for a\\ntotal of 150 specimens. The full data set also contains the species name, but for the purpose\\nof this example we ignore it.\\nFigure 1.9 shows that there is a significant correlation between the di fferent features. ☞ 17\\nCan we perhaps describe the data using fewer features by taking certain linear combin-\\nations of the original features? To investigate this, let us perform a PCA, first centering\\nthe data. The following Python code implements the PCA. It is assumed that a CSV file\\nirisX.csvhas been made that contains the iris data set (without the species information).\\nPCAiris.py\\nimport seaborn as sns, numpy as np\\nnp.set_printoptions(precision=4)\\nX = np.genfromtxt( 'IrisX.csv ',delimiter= ',')\\nn = X.shape[0]\\nX = X - np.mean(X, axis=0)\\n[U,D2,UT]= np.linalg.svd((X.T @ X)/n)\\nprint ('U = \\\\n ', U); print ('\\\\n diag(D^2) = ', D2)\\nz = U[:,0].T @ X.T\\nsns.kdeplot(z, bw=0.15)\\nU =\\n[[-0.3614 -0.6566 0.582 0.3155]\\n[ 0.0845 -0.7302 -0.5979 -0.3197]\\n[-0.8567 0.1734 -0.0762 -0.4798]\\n[-0.3583 0.0755 -0.5458 0.7537]]\\ndiag(D^2) = [4.2001 0.2411 0.0777 0.0237]\\nThe output above shows the principal component matrix (which we calledU) as well as\\nthe diagonal of matrix D2. We see that a large proportion of the variance, 4.2001/(4.2001+\\n0.2411+0.0777+0.0237) = 92.46%, is explained by the first principal component. Thus, it\\nmakes sense to transform each data point x ∈R4 to u⊤\\n1 x ∈R. Figure 4.14 shows the kernel\\ndensity estimate of the transformed data. Interestingly, we see two modes, indicating at\\nleast two clusters in the data.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 177, 'page_label': '160'}, page_content='160 Exercises\\n-4 -3 -2 -1 0 1 2 3 4\\nPCA-combined data\\n0\\n0.2\\n0.4\\n0.6kernel density estimate\\nFigure 4.14: Kernel density estimate of the PCA-combined iris data.\\nFurther Reading\\nVarious information-theoretic measures to quantify uncertainty, including the Shannon en-\\ntropy and Kullback–Leibler divergence, may be found in [28]. The Fisher information, the\\nprominent information measure in statistics, is discussed in detail in [78]. Akaike’s inform-\\nation criterion appeared in [2]. The EM algorithm was introduced in [31] and [85] gives an\\nin-depth treatment. Convergence proofs for the EM algorithm may be found in [19, 128].\\nA classical reference on kernel density estimation is [113], and [14] is the main reference\\nfor the theta kernel density estimator. Theory and applications on finite mixture models\\nmay be found in [86]. For more details on clustering applications and algorithms as well\\nas references on data compression, vector quantization, and pattern recognition, we refer\\nto [1, 35, 107, 125]. A useful modification of the K-means algorithm is the fuzzy K-means\\nalgorithm; see, e.g., [9]. A popular way to choose the starting positions inK-means is given\\nby the K-means++ heuristic, introduced in [4].\\nExercises\\n1. This exercise is to show that the Fisher information matrix F(θ) in (4.8) is equal to the\\nmatrix H(θ) in (4.9), in the special case where f = g(·|θ), and under the assumption that\\nintegration and differentiation orders can be interchanged.\\n(a) Let h be a vector-valued function and k a real-valued function. Prove the following\\nquotient rule for differentiationquotient rule\\nfor\\ndifferentiation\\n:\\n∂[h(θ)/k(θ)]\\n∂θ = 1\\nk(θ)\\n∂h(θ)\\n∂θ − 1\\nk2(θ)\\n∂k(θ)\\n∂θ h(θ)⊤. (4.45)\\n(b) Now take h(θ) = ∂g(X |θ)\\n∂θ and k(θ) = g(X |θ) in (4.45) and take expectations with\\nrespect to Eθ on both sides to show that\\n−H(θ) = Eθ\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\ng(X |θ)\\n∂∂g(X |θ)\\n∂θ\\n∂θ\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|                    {z                    }\\nA\\n−F(θ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 178, 'page_label': '161'}, page_content='Unsupervised Learning 161\\n(c) Finally show that A is the zero matrix.\\n2. Plot the mixture of N(0,1), U(0,1), and Exp(1) distributions, with weights w1 = w2 =\\nw3 = 1/3.\\n3. Denote the pdfs in Exercise 2 by f1, f2, f3, respectively. Suppose that X is simulated via\\nthe two-step procedure: First, draw Z from {1,2,3}, then draw X from fZ. How likely is it\\nthat the outcome x = 0.5 of X has come from the uniform pdf f2?\\n4. Simulate an iid training set of size 100 from the Gamma(2.3,0.5) distribution, and\\nimplement the Fisher scoring method in Example 4.1 to find the maximum likelihood es-\\ntimate. Plot the true and approximate pdfs.\\n5. Let T = {X1,..., Xn}be iid data from a pdf g(x |θ) with Fisher matrix F(θ). Explain\\nwhy, under the conditions where (4.7) holds,\\nST(θ) := 1\\nn\\nnX\\ni=1\\nS(Xi |θ)\\nfor large n has approximately a multivariate normal distribution with expectation vector 0\\nand covariance matrix F(θ)/n.\\n6. Figure 4.15 shows a Gaussian KDE with bandwidth σ= 0.2 on the points −0.5,0,\\n0.2,0.9, and 1 .5. Reproduce the plot in Python. Using the same bandwidth, plot also the\\nKDE for the same data, but now with ϕ(z) = 1/2,z ∈[−1,1].\\n-1 0 1 2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 4.15: The Gaussian KDE (solid line) is the equally weighted mixture of normal pdfs\\ncentered around the data and with standard deviation σ= 0.2 (dashed).\\n7. For fixed x′, the Gaussian kernel function\\nf (x |t) := 1√\\n2πt\\ne−1\\n2\\n(x−x′)2\\nt\\nis the solution to Fourier’s heat equation\\n∂\\n∂t f (x |t) = 1\\n2\\n∂2\\n∂x2 f (x |t), x ∈R, t >0,\\nwith initial condition f (x |0) = δ(x −x′) (the Dirac function at x′). Show this. As a con-\\nsequence, the Gaussian KDE is the solution to the same heat equation, but now with initial\\ncondition f (x |0) = n−1 Pn\\ni=1 δ(x −xi). This was the motivation for the theta KDE [14],\\nwhich is a solution to the same heat equation but now on a bounded interval.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 179, 'page_label': '162'}, page_content='162 Exercises\\n8. Show that the Ward linkage given in (4.41) is equal to\\ndWard(I,J) = |I||J|\\n|I|+ |J|∥xI−xJ∥2.\\n9. Carry out the agglomerative hierarchical clustering of Example 4.8 via the linkage\\nmethod from scipy.cluster.hierarchy. Show that the linkage matrices are the same.\\nGive a scatterplot of the data, color coded into K = 3 clusters.\\n10. Suppose that we have the data τn = {x1,..., xn}in R and decide to train the two-\\ncomponent Gaussian mixture model\\ng(x |θ) = w1\\n1q\\n2πσ2\\n1\\nexp\\n \\n−(x −µ1)2\\n2σ2\\n1\\n!\\n+ w2\\n1q\\n2πσ2\\n2\\nexp\\n \\n−(x −µ2)2\\n2σ2\\n2\\n!\\n,\\nwhere the parameter vector θ= [µ1,µ2,σ1,σ2,w1,w2]⊤belongs to the set\\nΘ = {θ: w1 + w2 = 1,w1 ∈[0,1],µi ∈R,σi >0, ∀i}.\\nSuppose that the training is via the maximum likelihood in (2.28). Show that\\nsup\\nθ∈Θ\\n1\\nn\\nnX\\ni=1\\nln g(xi |θ) = ∞.\\nIn other words, find a sequence of values for θ∈Θ such that the likelihood grows without\\nbound. How can we restrict the set Θ to ensure that the likelihood remains bounded?\\n11. A d-dimensional normal random vector X ∼N(µ,Σ) can be defined via an a ffine\\ntransformation, X = µ+ Σ1/2 Z, of a standard normal random vector Z ∼N(0,Id), where\\nΣ1/2(Σ1/2)⊤= Σ. In a similar way, we can define a d-dimensional Student random vector\\nX ∼tα(µ,Σ) via a transformation\\nX = µ+ 1√\\nS\\nΣ1/2 Z, (4.46)\\nwhere, Z ∼N(0,Id) and S ∼Gamma(α\\n2 ,α\\n2 ) are independent, α >0, and Σ1/2(Σ1/2)⊤ = Σ.\\nNote that we obtain the multivariate normal distribution as a limiting case for α→∞.\\n(a) Show that the density of the tα(0,Id) distribution is given by\\ntα(x) := Γ((α+ d)/2)\\n(πα)d/2Γ(α/2)\\n \\n1 + 1\\nα∥x∥2\\n!−α+d\\n2\\n.\\nBy the transformation rule (C.23), it follows that the density of X ∼tα(µ,Σ) is given☞433\\nby tα,Σ(x −µ), where\\ntα,Σ(x) := 1\\n|Σ1/2|tα(Σ−1/2 x).\\n[Hint: conditional on S = s, X has a N(0,Id/s) distribution.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 180, 'page_label': '163'}, page_content='Unsupervised Learning 163\\n(b) We wish to fit a tν(µ,Σ) distribution to given data τ= {x1,..., xn}in Rd via the EM\\nmethod. We use the representation (4.46) and augment the data with the vector S =\\n[S 1,..., S n]⊤of hidden variables. Show that the complete-data likelihood is given by\\ng(τ,s |θ) =\\nY\\ni\\n(α/2)α/2 s(α+d)/2−1\\ni exp(−si\\n2 α−si\\n2 ∥Σ−1/2(xi −µ)∥2)\\nΓ(α/2)(2π)d/2|Σ1/2| . (4.47)\\n(c) Show that, as a consequence, conditional on the data τand parameter θ, the hidden\\ndata are mutually independent, and\\n(S i |τ,θ) ∼Gamma\\n α+ d\\n2 ,α+ ∥Σ−1/2(xi −µ)∥2\\n2\\n!\\n, i = 1,..., n.\\n(d) At iteration t of the EM algorithm, let g(t)(s) = g(s |τ,θ(t−1)) be the density of the\\nmissing data, given the observed data τand the current parameter guess θ(t−1). Verify\\nthat the expected complete-data log-likelihood is given by:\\nEg(t) ln g(τ,S |θ) = nα\\n2 ln α\\n2 −nd\\n2 ln(2π) −n ln Γ\\n\\x12α\\n2\\n\\x13\\n−n\\n2 ln |Σ|\\n+ α+ d −2\\n2\\nnX\\ni=1\\nEg(t) ln S i −\\nnX\\ni=1\\nα+ ∥Σ−1/2(xi −µ)∥2\\n2 Eg(t) S i.\\nShow that\\nEg(t) S i = α(t−1) + d\\nα(t−1) + ∥(Σ(t−1))−1/2(xi −µ(t−1))∥2 =: w(t−1)\\ni\\nEg(t) ln S i = ψ\\n α(t−1) + d\\n2\\n!\\n−ln\\n α(t−1) + d\\n2\\n!\\n+ ln w(t−1)\\ni ,\\nwhere ψ:= (ln Γ)′is digamma function.\\n(e) Finally, show that in the M-step of the EM algorithm θ(t) is updated from θ(t−1) as\\nfollows:\\nµ(t) =\\nPn\\ni=1 w(t−1)\\ni xi\\nPn\\ni=1 w(t−1)\\ni\\nΣ(t) = 1\\nn\\nnX\\ni=1\\nw(t−1)\\ni (xi −µ(t))(xi −µ(t))⊤,\\nand α(t) is defined implicitly through the solution of the nonlinear equation:\\nln\\n\\x12α\\n2\\n\\x13\\n−ψ\\n\\x12α\\n2\\n\\x13\\n+ ψ\\n α(t) + d\\n2\\n!\\n−ln\\n α(t) + d\\n2\\n!\\n+ 1 +\\nPn\\ni=1\\n\\x10\\nln(w(t−1)\\ni ) −w(t−1)\\ni\\n\\x11\\nn = 0.\\n12. A generalization of both the gamma and inverse-gamma distribution is thegeneralized\\ninverse-gamma distribution generalized\\ninverse-gamma\\ndistribution\\n, which has density\\nf (s) = (a/b)p/2\\n2Kp(\\n√\\nab)\\nsp−1e−1\\n2 (as+b/s), a,b,s >0, p ∈R, (4.48)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 181, 'page_label': '164'}, page_content='164 Exercises\\nwhere Kp is the modified Bessel function of the second kindmodified Bessel\\nfunction of the\\nsecond kind\\n, which can be defined as the\\nintegral\\nKp(x) =\\nZ ∞\\n0\\ne−x cosh(t) cosh(pt) dt, x >0, p ∈R. (4.49)\\nWe write S ∼GIG(a,b,p) to denote that S has a pdf of the form (4.48). The function Kp\\nhas many interesting properties. Special cases include\\nK1/2(x) =\\nr\\nx π\\n2 e−x 1\\nx\\nK3/2(x) =\\nr\\nx π\\n2 e−x\\n 1\\nx + 1\\nx2\\n!\\nK5/2(x) =\\nr\\nx π\\n2 e−x\\n 1\\nx + 3\\nx2 + 3\\nx3\\n!\\n.\\nMore generally, Kp satisfies the recursion\\nKp+1(x) = Kp−1(x) + 2p\\nx Kp(x). (4.50)\\n(a) Using the change of variables e z = s √a/b, show that\\nZ ∞\\n0\\nsp−1e−1\\n2 (as+b/s) ds = 2Kp(\\n√\\nab)(b/a)p/2.\\n(b) Let S ∼GIG(a,b,p). Show that\\nES =\\n√\\nb Kp+1(\\n√\\nab)\\n√a Kp(\\n√\\nab)\\n(4.51)\\nand\\nES −1 =\\n√a Kp+1(\\n√\\nab)√\\nb Kp(\\n√\\nab)\\n−2p\\nb . (4.52)\\n13. In Exercise 11 we viewed the multivariate Student tα distribution as a scale-mixturescale-mixture\\nof the N(0,Id) distribution. In this exercise, we consider a similar transformation, but now\\nΣ1/2 Z ∼N(0,Σ) is not divided but is multiplied by\\n√\\nS , with S ∼Gamma(α/2,α/2):\\nX = µ+\\n√\\nS Σ1/2 Z, (4.53)\\nwhere S and Z are independent and α> 0.\\n(a) Show, using Exercise 12, that for Σ1/2 = Id and µ= 0, the random vector X has a\\nd-dimensional Bessel distributionBessel\\ndistribution\\n, with density:\\nκα(x) := 21−(α+d)/2α(α+d)/4 ∥x∥(α−d)/2\\nπd/2Γ(α/2) K(α−d)/2\\n\\x10\\n∥x∥√α\\n\\x11\\n, x ∈Rd,\\nwhere Kp is the modified Bessel function of the second kind given in (4.49). We write\\nX ∼Besselα(0,Id). A random vector X is said to have a Besselα(µ,Σ) distribution if'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 182, 'page_label': '165'}, page_content='Unsupervised Learning 165\\nit can be written in the form (4.53). By the transformation rule (C.23), its density is\\ngiven by 1√|Σ|κα(Σ−1/2(x −µ)). Special instances of the Bessel pdf include:\\nκ2(x) = exp(−\\n√\\n2 |x|)√\\n2\\nκ4(x) = 1 + 2 |x|\\n2 exp(−2 |x|)\\nκ4(x1,x2,x3) = 1\\nπexp\\n\\x12\\n−2\\nq\\nx2\\n1 + x2\\n2 + x2\\n3\\n\\x13\\nκd+1(x) = ((d + 1)/2)d/2 √π\\n(2π)d/2Γ((d + 1)/2) exp\\n\\x10\\n−\\n√\\nd + 1 ∥x∥\\n\\x11\\n, x ∈Rd.\\nNote that k2 is the (scaled) pdf of the double-exponential or Laplace distribution.\\n(b) Given the data τ = {x1,..., xn}in Rd, we wish to fit a Bessel pdf to the data by\\nemploying the EM algorithm, augmenting the data with the vector S = [S 1,..., S n]⊤\\nof missing data. We assume that α is known and α >d. Show that conditional on\\nτ (and given θ), the missing data vector S has independent components, with S i ∼\\nGIG(α,bi,(α−d)/2), with bi := ∥Σ−1/2(xi −µ)∥2, i = 1,..., n.\\n(c) At iteration t of the EM algorithm, let g(t)(s) = g(s |τ,θ(t−1)) be the density of the\\nmissing data, given the observed data τand the current parameter guess θ(t−1). Show\\nthat the expected complete-data log-likelihood is given by:\\nQ(t)(θ) := Eg(t) ln g(τ,S |θ) = −1\\n2\\nnX\\ni=1\\nbi(θ) w(t−1)\\ni + constant, (4.54)\\nwhere bi(θ) = ∥Σ−1/2(xi −µ)∥2 and\\nw(t−1)\\ni :=\\n√αK(α−d+2)/2\\n\\x10p\\nαbi(θ(t−1))\\n\\x11\\np\\nbi(θ(t−1)) K(α−d)/2\\n\\x10p\\nαbi(θ(t−1))\\n\\x11 − α−d\\nbi(θ(t−1)), i = 1,..., n.\\n(d) From (4.54) derive the M-step of the EM algorithm. That is, show how θ(t) is updated\\nfrom θ(t−1).\\n14. Consider the ellipsoid E = {x ∈Rd : xΣ−1 x = 1}in (4.42). Let UD2U⊤be an SVD of\\nΣ. Show that the linear transformation x 7→U⊤D−1 x maps the points on E onto the unit\\nsphere {z ∈Rd : ∥z∥= 1}.\\n15. Figure 4.13 shows how the centered “surfboard” data are projected onto the first\\ncolumn of the principal component matrix U. Suppose we project the data instead onto\\nthe plane spanned by the first two columns of U. What are a and b in the representation\\nax1 + bx2 = x3 of this plane?\\n16. Figure 4.14 suggests that we can assign each feature vector x in the iris data set to\\none of two clusters, based on the value of u⊤\\n1 x, where u1 is the first principal component.\\nPlot the sepal lengths against petal lengths and color the points for whichu⊤\\n1 x <1.5 differ-\\nently to points for which u⊤\\n1 x ⩾1.5. To which species of iris do these clusters correspond?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 183, 'page_label': '166'}, page_content='166'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 184, 'page_label': '167'}, page_content='CHAPTER 5\\nREGRESSION\\nMany supervised learning techniques can be gathered under the name “regression”.\\nThe purpose of this chapter is to explain the mathematical ideas behind regression\\nmodels and their practical aspects. We analyze the fundamental linear model in detail,\\nand also discuss nonlinear and generalized linear models.\\n5.1 Introduction\\nFrancis Galton observed in an article in 1889 that the heights of adult offspring are, on the\\nwhole, more “average” than the heights of their parents. Galton interpreted this as a degen-\\nerative phenomenon, using the term “regression” to indicate this “return to mediocrity”.\\nNowadays, regression regressionrefers to a broad class of supervised learning techniques where the\\naim is to predict a quantitative response (output) variable y via a function g(x) of an ex-\\nplanatory (input) vector x = [x1,..., xp]⊤, consisting of p features, each of which can be\\ncontinuous or discrete. For instance, regression could be used to predict the birth weight of\\na baby (the response variable) from the weight of the mother, her socio-economic status,\\nand her smoking habits (the explanatory variables).\\nLet us recapitulate the framework of supervised learning established in Chapter 2. The ☞ 19\\naim is to find a prediction function g that best guesses1 what the random output Y will be\\nfor a random input vector X. The joint pdf f (x,y) of X and Y is unknown, but a training\\nset τ= {(x1,y1),..., (xn,yn)}is available, which is thought of as the outcome of a random\\ntraining set T= {(X1,Y1),..., (Xn,Yn)}of iid copies of ( X,Y). Once we have selected a\\nloss function Loss(y,by), such as the squared-error loss squared-error\\nloss\\nLoss(y,by) = (y −by)2, (5.1)\\nthen the “best” prediction function g is defined as the one that minimizes the risk riskℓ(g) =\\nELoss(Y,g(X)). We saw in Section 2.2 that for the squared-error loss this optimal predic-\\ntion function is the conditional expectation\\ng∗(x) = E[Y |X = x].\\n1Recall the mnemonic use of “g” for “guess”\\n167'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 185, 'page_label': '168'}, page_content='168 Introduction\\nAs the squared-error loss is the most widely-used loss function for regression, we will\\nadopt this loss function in most of this chapter.\\nThe optimal prediction function g∗has to be learned from the training set τby minim-\\nizing the training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 (5.2)\\nover a suitable class of functions G. Note that in the above definition, the training set τis\\nassumed to be fixed. For a random training set T, we will write the training loss as ℓT(g).\\nThe function gG\\nτ that minimizes the training loss is the function we use for prediction —\\nthe so-called learnerlearner . When the function class Gis clear from the context, we drop the\\nsuperscript in the notation.\\nAs we already saw in (2.2), conditional on X = x, the response Y can be written as☞21\\nY = g∗(x) + ε(x),\\nwhere Eε(x) = 0. This motivates a standard modeling assumption in supervised learn-\\ning, in which the responses Y1,..., Yn, conditional on the explanatory variables X1 =\\nx1,..., Xn = xn, are assumed to be of the form\\nYi = g(xi) + εi, i = 1,..., n,\\nwhere the {εi}are independent with Eεi = 0 and Var εi = σ2 for some function g ∈G and\\nvariance σ2. The above model is usually further specified by assuming thatg is completely\\nknown up to an unknown parameter vector; that is,\\nYi = g(xi |β) + εi, i = 1,..., n. (5.3)\\nWhile the model (5.3) is described conditional on the explanatory variables, it will be\\nconvenient to make one further model simplification, and view (5.3) as if the {xi}were\\nfixed, while the {Yi}are random.\\nFor the remainder of this chapter, we assume that the training feature vectors{xi}are\\nfixed and only the responses are random; that is, T= {(x1,Y1),..., (xn,Yn)}.\\nThe advantage of the model (5.3) is that the problem of estimating the function g from\\nthe training data is reduced to the (much simpler) problem of estimating the parameter\\nvector β. An obvious disadvantage is that functions of the form g(·|β) may not accurately\\napproximate the true unknown g∗. The remainder of this chapter deals with the analysis\\nof models of the form (5.3). In the important case where the function g(·|β) is linear, the\\nanalysis proceeds through the class of linear models. If, in addition, the error terms{εi}are\\nassumed to be Gaussian, this analysis can be carried out using the rich theory of normal\\nlinear models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 186, 'page_label': '169'}, page_content='Regression 169\\n5.2 Linear Regression\\nThe most basic regression model involves a linear relationship between the response and a\\nsingle explanatory variable. In particular, we have measurements ( x1,y1),..., (xn,yn) that\\nlie approximately on a straight line, as in Figure 5.1.\\n-3 -2 -1 0 1 2 3\\n-5\\n0\\n5\\n10\\n15\\nFigure 5.1: Data from a simple linear regression model.\\nFollowing the general scheme captured in (5.3), a simple model for these data is that\\nthe {xi}are fixed and variables {Yi}are random such that\\nYi = β0 + β1 xi + εi, i = 1,..., n, (5.4)\\nfor certain unknown parameters β0 and β1. The {εi}are assumed to be independent with\\nexpectation 0 and unknown variance σ2. The unknown line\\ny = β0 + β1 x|    {z    }\\ng(x |β)\\n(5.5)\\nis called the regression line regression line. Thus, we view the responses as random variables that would\\nlie exactly on the regression line, were it not for some “disturbance” or “error” term repres-\\nented by the {εi}. The extent of the disturbance is modeled by the parameterσ2. The model\\nin (5.4) is calledsimple linear regression simple linear\\nregression\\nmodel\\n. This model can easily be extended to incorporate\\nmore than one explanatory variable, as follows.\\nDefinition 5.1: Multiple Linear Regression Model\\nIn a multiple linear regression model multiple linear\\nregression\\nmodel\\nthe response Y depends on a d-dimensional\\nexplanatory vector x = [x1,..., xd]⊤, via the linear relationship\\nY = β0 + β1 x1 + ··· + βd xd + ε, (5.6)\\nwhere Eε= 0 and Var ε= σ2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 187, 'page_label': '170'}, page_content='170 Linear Regression\\nThus, the data lie approximately on a d-dimensional affine hyperplane\\ny = β0 + β1 x1 + ··· + βd xd|                      {z                      }\\ng(x |β)\\n,\\nwhere we define β= [β0,β1,...,β d]⊤. The function g(x |β) is linear in β, but not linear in\\nthe feature vector x, due to the constant β0. However, augmenting the feature space with\\nthe constant 1, the mapping [1 ,x⊤]⊤ 7→g(x |β) := [1,x⊤] βbecomes linear in the feature\\nspace and so (5.6) becomes a linear model (see Section 2.1). Most software packages for☞44\\nregression include 1 as a feature by default.\\nNote that in (5.6) we only specified the model for a single pair (x,Y). The model for the\\ntraining set T = {(x1,Y1),..., (xn,Yn)}is simply that each Yi satisfies (5.6) (with x = xi)\\nand that the {Yi}are independent. Setting Y = [Y1,..., Yn]⊤, we can write the multiple\\nlinear regression model for the training data compactly as\\nY = Xβ+ ε, (5.7)\\nwhere ε= [ε1,...,ε n]⊤is a vector of iid copies of εand X is the model matrixmodel matrix given by\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x11 x12 ··· x1d\\n1 x21 x22 ··· x2d\\n... ... ... ... ...\\n1 xn1 xn2 ··· xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x⊤\\n1\\n1 x⊤\\n2\\n... ...\\n1 x⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nExample 5.1 (Multiple Linear Regression Model) Figure 5.2 depicts a realization of\\nthe multiple linear regression model\\nYi = xi1 + xi2 + εi, i = 1,..., 100,\\nwhere ε1,...,ε 100 ∼iid N(0,1/16). The fixed feature vectors (vectors of explanatory vari-\\nables) xi = [xi1,xi2]⊤,i = 1,..., 100 lie in the unit square.\\n1\\n0\\n0\\n1\\n2\\n1 0\\nFigure 5.2: Data from a multiple linear regression model.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 188, 'page_label': '171'}, page_content='Regression 171\\n5.3 Analysis via Linear Models\\nAnalysis of data from a linear regression model is greatly simplified through the linear\\nmodel representation (5.7). In this section we present the main ideas for parameter estima-\\ntion and model selection for a general linear model of the form\\nY = Xβ+ ε, (5.8)\\nwhere X is an n×p matrix, β= [β1,...,β p]⊤a vector of p parameters, and ε= [ε1,...,ε n]⊤\\nan n-dimensional vector of independent error terms, with Eεi = 0 and Var εi = σ2, i =\\n1,..., n. Note that the model matrix X is assumed to be fixed, and Y and εare random. A\\nspecific outcome of Y is denoted by y (in accordance with the notation in Section 2.8). ☞ 47\\nNote that the multiple linear regression model in (5.7) was defined using a different\\nparameterization; in particular, there we used β= [β0,β1,...,β d]⊤. So, when apply-\\ning the results in the present section to such models, be aware that p = d + 1. Also,\\nin this section a feature vector x includes the constant 1, so that X⊤= [x1,..., xn].\\n5.3.1 Parameter Estimation\\nThe linear model Y = Xβ+ εcontains two unknown parameters, βand σ2, which have\\nto be estimated from the training data τ. To estimate β, we can repeat exactly the same\\nreasoning used in our recurring polynomial regression Example 2.1 as follows. For a linear ☞ 26\\nprediction function g(x) = x⊤β, the (squared-error) training loss can be written as\\nℓτ(g) = 1\\nn ∥y −Xβ∥2,\\nand the optimal learner gτ minimizes this quantity, leading to the least-squares estimate bβ,\\nwhich satisfies the normal equations\\nX⊤X β= X⊤y. (5.9)\\nThe corresponding training loss can be taken as an estimate of σ2; that is,\\ncσ2 = 1\\nn ∥y −Xbβ∥2. (5.10)\\nTo justify the latter, note that σ2 is the second moment of the model errors εi,i = 1,..., n,\\nin (5.8) and could be estimated via the method of moments (see Section C.12.1) using the ☞ 455\\nsample average n−1 P\\ni ε2\\ni = ∥ε∥2/n = ∥Y −Xβ∥2/n, if βwere known. By replacing βwith\\nits estimator, we arrive at (5.10). Note that no distributional properties of the{εi}were used\\nother than Eεi = 0 and Var εi = σ2,i = 1,..., n. The vector e := y −Xbβ is called the\\nvector of residuals residualsand approximates the (unknown) vector of model errorsε. The quantity\\n∥e∥2 = Pn\\ni=1 e2\\ni is called the residual sum of squares(RSS). Dividing the RSS byn −p gives residual sum of\\nsquaresan unbiased estimate of σ2, which we call the estimated residual squared error(RSE); see\\nresidual\\nsquared errorExercise 12.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 189, 'page_label': '172'}, page_content='172 Analysis via Linear Models\\nIn terms of the notation given in the summary Table 2.1 for supervised learning, we\\nthus have:☞25\\n1. The (observed) training data is τ= {X,y}.\\n2. The function class Gis the class of linear functions of x; that is G= {g(·|β) : x 7→\\nx⊤β, β∈Rp}.\\n3. The (squared-error) training loss is ℓτ(g(·|β)) = ∥y −Xβ∥2/n.\\n4. The learner gτ is given by gτ(x) = x⊤bβ, where bβ= argminβ∈Rp ∥y −Xβ∥2.\\n5. The minimal training loss is ℓτ(gτ) = ∥y −Xbβ∥2/n = cσ2.\\n5.3.2 Model Selection and Prediction\\nEven if we restrict the learner to be a linear function, there is still the issue of which explan-\\natory variables (features) to include. While including too few features may result in large\\napproximation error (underfitting), including too many may result in largestatistical error\\n(overfitting). As discussed in Section 2.4, we need to select the features which provide the☞31\\nbest tradeoff between the approximation and statistical errors, so that the (expected) gener-\\nalization risk of the learner is minimized. Depending on how the (expected) generalization\\nrisk is estimated, there are a number of strategies for feature selection:\\n1. Use test data τ′= (X′,y′) that are obtained independently from the training data τ,\\nto estimate the generalization risk E∥Y −gτ(X)∥2 via the test loss (2.7). Then choose☞24\\nthe collection of features that minimizes the test loss. When there is an abundance of\\ndata, part of the data can be reserved as test data, while the remaining data is used as\\ntraining data.\\n2. When there is a limited amount of data, we can use cross-validation to estimate the\\nexpected generalization risk E∥Y −gT(X)∥2 (where Tis a random training set), as\\nexplained in Section 2.5.2. This is then minimized over the set of possible choices☞38\\nfor the explanatory variables.\\n3. When one has to choose between many potential explanatory variables, techniques\\nsuch as regularized least-squares and lasso regression become important. Such\\nmethods offer another approach to model selection, via the regularization (or ho-\\nmotopy) paths. This will be the topic of Section 6.2 in the next chapter.☞216\\n4. Rather than using computer-intensive techniques, such as the ones above, one can\\nuse theoretical estimates of the expected generalization risk, such as the in-sample\\nrisk, AIC, and BIC, as in Section 2.5, and minimize this to determine a good set of☞35\\nexplanatory variables.\\n5. All of the above approaches do not assume any distributional properties of the error\\nterms {εi}in the linear model, other than that they are independent with expectation\\n0 and variance σ2. If, however, they are assumed to have anormal (Gaussian) distri-\\nbution, (that is, {εi}∼iid N(0,σ2)), then the inclusion and exclusion of variables can'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 190, 'page_label': '173'}, page_content='Regression 173\\nbe decided by means of hypotheses tests. This is the classical approach to model\\nselection, and will be discussed in Section 5.4. As a consequence of the central limit\\ntheorem, one can use the same approach when the error terms are not necessarily\\nnormal, provided their variance is finite and the sample size n is large.\\n6. Finally, when using a Bayesian approach, comparison of two models can be achieved\\nby computing their so-called Bayes factor (see Section 2.9).\\nAll of the above strategies can be thought of as specifications of a simple rule formu-\\nlated by William of Occam, which can be interpreted as:\\nWhen presented with competing models, choose the simplest one that explains\\nthe data.\\nThis age-old principle, known asOccam’s razor Occam’s razor, is mirrored in a famous quote of Einstein:\\nEverything should be made as simple as possible, but not simpler.\\nIn linear regression, the number of parameters or predictors is usually a reasonable measure\\nof the simplicity of the model.\\n5.3.3 Cross-Validation and Predictive Residual Sum of Squares\\nWe start by considering the n-fold cross-validation, also called leave-one-out cross-\\nvalidation leave-one-out\\ncross-validation\\n, for the linear model (5.8). We partition the data into n data sets, leaving out\\nprecisely one observation per data set, which we then predict based on then −1 remaining\\nobservations; see Section 2.5.2 for the general case. Let by−i denote the prediction for the ☞ 38\\ni-th observation using all the data except yi. The error in the prediction, yi −by−i, is called a\\npredicted residual predicted\\nresidual\\n— in contrast to an ordinary residual,ei = yi −byi, which is the difference\\nbetween an observation and its fitted valuebyi = gτ(xi) obtained using the whole sample. In\\nthis way, we obtain the collection of predicted residuals {yi −by−i}n\\ni=1 and summarize them\\nthrough the predicted residual sum of squares (PRESS PRESS):\\nPRESS =\\nnX\\ni=1\\n(yi −by−i)2.\\nDividing the PRESS by n gives an estimate of the expected generalization risk.\\nIn general, computing the PRESS is computationally intensive as it involves training\\nand predicting n separate times. For linear models, however, the predicted residuals can be ☞ 171\\ncalculated quickly using only the ordinary residuals and the projection matrix P = XX+\\nonto the linear space spanned by the columns of the model matrix X (see (2.13)). The i-th ☞ 28\\ndiagonal element Pii of the projection matrix is called thei-th leverage leverage, and it can be shown\\nthat 0 ⩽Pii ⩽1 (see Exercise 10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 191, 'page_label': '174'}, page_content='174 Analysis via Linear Models\\nTheorem 5.1: PRESS for Linear Models\\nConsider the linear model (5.8), where then×p model matrix X is of full rank. Given\\nan outcome y = [y1,..., yn]⊤of Y, the fitted values can be obtained asby = Py,where\\nP = XX+ = X(X⊤X)−1X⊤is the projection matrix. If the leverage valuepi := Pii , 1\\nfor all i = 1,..., n, then the predicted residual sum of squares can be written as\\nPRESS =\\nnX\\ni=1\\n ei\\n1 −pi\\n!2\\n,\\nwhere ei = yi −byi = yi −(Xbβ)i is the i-th residual.\\nProof: It su ffices to show that the i-th predicted residual can be written as yi −by−i =\\nei/(1 −pi). Let X−i denote the model matrix X with the i-th row, x⊤\\ni , removed, and define\\ny−i similarly. Then, the least-squares estimate for β using all but the i-th observation is\\nbβ−i = (X⊤\\n−iX−i)−1X⊤\\n−i y−i. Writing X⊤X = X⊤\\n−iX−i + xi x⊤\\ni , we have by the Sherman–Morrison\\nformula☞371\\n(X⊤\\n−iX−i)−1 = (X⊤X)−1 + (X⊤X)−1 xi x⊤\\ni (X⊤X)−1\\n1 −x⊤\\ni (X⊤X)−1 xi\\n,\\nwhere x⊤\\ni (X⊤X)−1 xi = pi < 1. Also, X⊤\\n−i y−i = X⊤y −xiyi. Combining all these identities,\\nwe have\\nbβ−i = (X⊤\\n−iX−i)−1X⊤\\n−i y−i\\n=\\n \\n(X⊤X)−1 + (X⊤X)−1 xi x⊤\\ni (X⊤X)−1\\n1 −pi\\n!\\n(X⊤y −xiyi)\\n= bβ+ (X⊤X)−1 xi x⊤\\ni\\nbβ\\n1 −pi\\n−(X⊤X)−1 xiyi −(X⊤X)−1 xi piyi\\n1 −pi\\n= bβ+ (X⊤X)−1 xi x⊤\\ni\\nbβ\\n1 −pi\\n−(X⊤X)−1 xiyi\\n1 −pi\\n= bβ−(X⊤X)−1 xi(yi −x⊤\\ni\\nbβ)\\n1 −pi\\n= bβ−(X⊤X)−1 xiei\\n1 −pi\\n.\\nIt follows that the predicted value for the i-th observation is given by\\nby−i = x⊤\\ni bβ−i = x⊤\\ni bβ−x⊤\\ni (X⊤X)−1 xiei\\n1 −pi\\n= byi − piei\\n1 −pi\\n.\\nHence, yi −by−i = ei + piei/(1 −pi) = ei/(1 −pi). □\\nExample 5.2 (Polynomial Regression (cont.)) We return to Example 2.1, where we☞26\\nestimated the generalization risk for various polynomial prediction functions using inde-\\npendent validation data. Instead, let us estimate the expected generalization risk via cross-\\nvalidation (thus using only the training set) and apply Theorem 5.1 to compute the PRESS.\\n☞174'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 192, 'page_label': '175'}, page_content='Regression 175\\npolyregpress.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef generate_data(beta , sig, n):\\nu = np.random.rand(n, 1)\\ny = u ** np.arange(0, 4) @ beta.reshape(4,1) + (\\nsig * np.random.randn(n, 1))\\nreturn u, y\\nnp.random.seed(12)\\nbeta = np.array([[10.0, -140, 400, -250]]).T;\\nsig=5; n = 10**2;\\nu,y = generate_data(beta,sig,n)\\nX = np.ones((n, 1))\\nK = 12 #maximum number of parameters\\npress = np.zeros(K+1)\\nfor k in range (1,K):\\nif k > 1:\\nX = np.hstack((X, u**(k-1))) # add column to matrix\\nP = X @ np.linalg.pinv(X) # projection matrix\\ne = y - P @ y\\npress[k] = np. sum ((e/(1-np.diag(P).reshape(n,1)))**2)\\nplt.plot(press[1:K]/n)\\nThe PRESS values divided by n = 100 for the constant, linear, quadratic, cubic, and\\nquartic order polynomial regression models are, respectively, 152 .487,56.249,51.606,\\n30.999, and 31.634. Hence, the cubic polynomial regression model has the lowest PRESS,\\nindicating that it has the best predictive performance.\\n5.3.4 In-Sample Risk and Akaike Information Criterion\\nIn Section 2.5.1 we introduced the in-sample risk as a measure for the accuracy of the ☞ 35\\nprediction function. To recapitulate, given a fixed data setτwith associated response vector\\ny and n ×p matrix of explanatory variables X, the in-sample risk of a prediction function\\ng is defined as\\nℓin(g) := EX Loss(Y,g(X)), (5.11)\\nwhere EX signifies that the expectation is taken under a di fferent probability model, in\\nwhich X takes the values x1,..., xn with equal probability, and given X = xi the random\\nvariable Y is drawn from the conditional pdf f (y |xi). The difference between the in-sample\\nrisk and the training loss is called theoptimism. For the squared-error loss, Theorem 2.2 ex- ☞ 36\\npresses the expected optimism of a learnergT as two times the average covariance between\\nthe predicted values and the responses.\\nIf the conditional variance of the error Y −g∗(X) given X = x does not depend on x,\\nthen the expected in-sample risk of a learnergτ, averaged over all training sets, has a simple\\nexpression:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 193, 'page_label': '176'}, page_content='176 Analysis via Linear Models\\nTheorem 5.2: Expected In-Sample Risk for Linear Models\\nLet X be the model matrix for a linear model, of dimension n ×p. If Var[Y −\\ng∗(X) |X = x] =: v2 does not depend on x, then the expected in-sample risk (with\\nrespect to the squared-error loss) for a random learner gT is given by\\nEX ℓin(gT) = EX ℓT(gT) + 2ℓ∗p\\nn , (5.12)\\nwhere ℓ∗is the irreducible risk.\\nProof: The expected optimism is, by definition, EX[ℓin(gT) −ℓT(gT)] which, for the\\nsquared-error loss, is equal to 2ℓ∗p/n, using exactly the same reasoning as in Example 2.3.\\nNote that here ℓ∗= v2. □\\nEquation (5.12) is the basis of the following model comparison heuristic: Estimate the\\nirreducible risk ℓ∗= v2 via bv2, using a model with relatively high complexity. Then choose\\nthe linear model with the lowest value of\\n∥y −Xbβ∥2 + 2 bv2 p. (5.13)\\nWe can also use the Akaike information criterion (AIC) as a heuristic for model com-\\nparison. We discussed the AIC in the unsupervised learning setting in Section 4.2, but the☞122\\narguments used there can also be applied to the supervised case, under the in-sample model\\nfor the data. In particular, let Z = (X,Y). We wish to predict the joint density\\nf (z) = f (x,y) := 1\\nn\\nnX\\ni=1\\n1{x=xi} f (y |xi),\\nusing a prediction function g(z |θ) from a family G:= {g(z |θ),θ∈Rq}, where\\ng(z |θ) = g(x,y |θ) := 1\\nn\\nnX\\ni=1\\n1{x=xi}gi(y |θ).\\nNote that q is the number of parameters (typically larger than p for a linear model with a\\nn ×p design matrix).\\nFollowing Section 4.2, the in-sample cross-entropy risk in this case is\\nr(θ) := −EX ln g(Z |θ),\\nand to approximate the optimal parameter θ∗we minimize the corresponding training loss\\nrτn (θ) := −1\\nn\\nnX\\nj=1\\nln g(zj |θ).\\nThe optimal parameter bθn for the training loss is thus found by minimizing\\n−1\\nn\\nnX\\nj=1\\n\\x10\\n−ln n + ln gj(yj |θ)\\n\\x11\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 194, 'page_label': '177'}, page_content='Regression 177\\nThat is, it is the maximum likelihood estimate of θ:\\nbθn = argmax\\nθ\\nnX\\ni=1\\nln gi(yi |θ).\\nUnder the assumption that f = g(·|θ∗) for some parameter θ∗, we have from Theorem 4.1\\nthat the estimated in-sample generalization risk can be approximated as ☞ 125\\nEX r(bθn) ≈rTn (bθn) + q\\nn = ln n −1\\nn\\nnX\\nj=1\\nln gj(yj |bθn) + q\\nn.\\nThis leads to the heuristic of selecting the learner g(·|bθn) with the smallest value of the\\nAIC:\\n−2\\nnX\\ni=1\\nln gi(yi |bθn) + 2q. (5.14)\\nExample 5.3 (Normal Linear Model) For the normal linear model Y ∼ N(x⊤β,σ2)\\n(see (2.34)), with a p-dimensional vector β, we have ☞ 47\\ngi(yi |β,σ2\\n|{z}\\n= θ\\n) = 1√\\n2πσ2\\nexp\\n \\n−1\\n2\\n(yi −x⊤\\ni β)2\\nσ2\\n!\\n, i = 1,..., n,\\nso that the AIC is\\nn ln(2π) + n ln bσ2 + ∥y −Xbβ∥2\\nbσ2 + 2q, (5.15)\\nwhere (bβ,bσ2) is the maximum likelihood estimate andq = p+1 is the number of parameters\\n(including σ2). For model comparison we may remove the n ln(2π) term if all the models\\nare normal linear models.\\nCertain software packages report the AIC without the n ln bσ2 term in (5.15). This\\nmay lead to sub-optimal model selection if normal models are compared with non-\\nnormal ones.\\n5.3.5 Categorical Features\\nSuppose that, as described in Chapter 1, the data is given in the form of a spreadsheet or\\ndata frame with n rows and p + 1 columns, where the first element of row i is the response\\nvariable yi, and the remaining p elements form the vector of explanatory variables x⊤\\ni .\\nWhen all the explanatory variables (features, predictors) are quantitative, then the model\\nmatrix X can be directly read off from the data frame as the n ×p matrix with rows x⊤\\ni ,i =\\n1,..., n.\\nHowever, when some explanatory variables arequalitative (categorical), such a one-to-\\none correspondence between data frame and model matrix no longer holds. The solution is\\nto include indicator or dummy variables.\\nLinear models with continuous responses and categorical explanatory variables often\\narise in factorial experiments. These are controlled statistical experiments in which the factorial\\nexperiments'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 195, 'page_label': '178'}, page_content='178 Analysis via Linear Models\\naim is to assess how a response variable is affected by one or more factors tested at several\\nfactors levels. A typical example is an agricultural experiment where one wishes to investigate\\nlevels how the yield of a food crop depends on factors such as location, pesticide, and fertilizer.\\nExample 5.4 (Crop Yield) The data in Table 5.1 lists the yield of a food crop for four\\ndifferent crop treatments (e.g., strengths of fertilizer) on four different blocks (plots).\\nTable 5.1: Crop yield for different treatments and blocks.\\nTreatment\\nBlock 1 2 3 4\\n1 9.2988 9.4978 9.7604 10.1025\\n2 8.2111 8.3387 8.5018 8.1942\\n3 9.0688 9.1284 9.3484 9.5086\\n4 8.2552 7.8999 8.4859 8.9485\\nThe corresponding data frame, given in Table 5.2, has 16 rows and 3 columns: one\\ncolumn for the crop yield (the response variable), one column for the Treatment, with\\nlevels 1, 2, 3, 4, and one column for the Block, also with levels 1, 2, 3, 4. The values 1,\\n2, 3, and 4 have no quantitative meaning (it does not make sense to take their average, for\\nexample) — they merely identify the category of the treatment or block.\\nTable 5.2: Crop yield data organized as a data frame in standard format.\\nYield Treatment Block\\n9.2988 1 1\\n8.2111 1 2\\n9.0688 1 3\\n8.2552 1 4\\n9.4978 2 1\\n8.3387 2 2\\n... ... ...\\n9.5086 4 3\\n8.9485 4 4\\nIn general, suppose there are r factor (categorical) variables u1,..., ur, where the j-\\nth factor has pj mutually exclusive levels, denoted by 1 ,..., pj. In order to include these\\ncategorical variables in a linear model, a common approach is to introduce an indicator\\nfeatureindicator\\nfeature\\nxjk = 1{uj = k}for each factor j at level k. Thus, xjk = 1 if the value of factor j\\nis k and 0 otherwise. Since P\\nk 1{uj = k}= 1, it su ffices to consider only pj −1 of these\\nindicator features for each factor j (this prevents the model matrix from being rank defi-\\ncient). For a single responseY, the feature vector x⊤is thus a row vector of binary variables'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 196, 'page_label': '179'}, page_content='Regression 179\\nthat indicates which levels were observed for each factor. The model assumption is that Y\\ndepends in a linear way on the indicator features, apart from an error term. That is,\\nY = β0 +\\nrX\\nj=1\\npjX\\nk=2\\nβjk 1{uj = k}|     {z     }\\nxjk\\n+ ε,\\nwhere we have omitted one indicator feature (corresponding to level 1) for each factor\\nj. For independent responses Y1,..., Yn, where each Yi corresponds to the factor values\\nui1,..., uir, let xi jk = 1{ui j = k}. Then, the linear model for the data becomes\\nYi = β0 +\\nrX\\nj=1\\npjX\\nk=2\\nβjk xi jk + εi, (5.16)\\nwhere the {εi}are independent with expectation 0 and some variance σ2. By gathering the\\nβ0 and {βjk}into a vector β, and the {xi jk}into a matrix X, we have again a linear model of\\nthe form (5.8). The model matrix X has n rows and 1 + Pr\\nj=1(pj −1) columns. Using the\\nabove convention that the βj1 parameters are subsumed in the parameter β0 (correspond-\\ning to the “constant” feature), we can interpret β0 as a baseline response when using the\\nexplanatory vector x⊤for which xj1 = 1 for all factors j = 1,..., r. The other parameters\\n{βjk}can be viewed as incremental effects incremental\\neffects\\nrelative to this baseline effect. For example, β12\\ndescribes by how much the response is expected to change if level 2 is used instead of level\\n1 for factor 1.\\nExample 5.5 (Crop Yield (cont.)) In Example 5.4, the linear model (5.16) has eight\\nparameters: β0,β12,β13,β14,β22,β23,β24, and σ2. The model matrix X depends on how\\nthe crop yields are organized in a vector y and on the ordering of the factors. Let\\nus order y column-wise from Table 5.1, as in y = [9.2988,8.2111,9.0688,8.2552,\\n9.4978,..., 8.9485]⊤, and let Treatment be Factor 1 and Block be Factor 2. Then we can\\nwrite (5.16) as\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0 C\\n1 1 0 0 C\\n1 0 1 0 C\\n1 0 0 1 C\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|               {z               }\\nX\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ0\\nβ12\\nβ13\\nβ14\\nβ22\\nβ23\\nβ24\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ\\n+ ε, where C =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 0\\n1 0 0\\n0 1 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\nand with 1 = [1,1,1,1]⊤ and 0 = [0,0,0,0]⊤. Estimation of βand σ2, model selection,\\nand prediction can now be carried out in the usual manner for linear models.\\nIn the context of factorial experiments, the model matrix is often called the design\\nmatrix design matrix, as it specifies the design of the experiment; e.g., how many replications are taken\\nfor each combination of factor levels. The model (5.16) can be extended by adding products\\nof indicator variables as new features. Such features are called interaction interactionterms.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 197, 'page_label': '180'}, page_content='180 Analysis via Linear Models\\n5.3.6 Nested Models\\nLet X be a n ×p model matrix of the form X = [X1,X2], where X1 and X2 are model\\nmatrices of dimension n ×k and n ×(p −k), respectively. The linear modelsY = X1β1 + ε\\nand Y = X2β2 + εare said to benested models nested within the linear model Y = Xβ+ ε. This simply\\nmeans that certain features inX are ignored in each of the first two models. Note thatβ, β1,\\nand β2 are parameter vectors of dimension p, k, and p −k, respectively. In what follows,\\nwe assume that n ⩾p and that all model matrices are full-rank.\\nSuppose we wish to assess whether to use the full model matrixX or the reduced model\\nmatrix X1. Let bβbe the estimate of βunder the full model (that is, obtained via (5.9)), and\\nlet bβ1 denote the estimate of β1 for the reduced model. LetY(2) = Xbβbe the projection of Y\\nonto the space Span(X) spanned by the columns ofX; and let Y(1) = X1 bβ1 be the projection\\nof Y onto the space Span(X1) spanned by the columns of X1 only; see Figure 5.3. In order\\nto decide whether the features inX2 are needed, we may compare the estimated error terms\\nof the two models, as calculated by (5.10); that is, by the residual sum of squares divided\\nby the number of observations n. If the outcome of this comparison is that there is little\\ndifference between the model error for the full and reduced model, then it is appropriate to\\nadopt the reduced model, as it has fewer parameters than the full model, while explaining\\nthe data just as well. The comparison is thus between the squared norms ∥Y −Y(2)∥2 and\\n∥Y −Y(1)∥2. Because of the nested nature of the linear models, Span( X1) is a subspace of\\nSpan(X) and, consequently, the orthogonal projection of Y(2) onto Span(X1) is the same\\nas the orthogonal projection of Y onto Span(X1); that is, Y(1). By Pythagoras’ theorem, we\\nthus have the decomposition∥Y(2) −Y(1)∥2 +∥Y −Y(2)∥2 = ∥Y −Y(1)∥2. This is also illustrated\\nin Figure 5.3.\\nY\\nY −Y(1) Y −Y(2)\\nY(2)\\nO\\nSpan(X)\\nSpan(X1 )\\nY(2) −Y(1)\\nY(1)\\nFigure 5.3: The residual sum of squares for the full model corresponds to∥Y−Y(2)∥2 and for\\nthe reduced model it is∥Y −Y(1)∥2. By Pythagoras’s theorem, the difference is ∥Y(2) −Y(1)∥2.\\nThe above decomposition can be generalized to more than two model matrices. Sup-\\npose that the model matrix can be decomposed into d submatrices: X = [X1,X2,..., Xd],\\nwhere the matrix Xi has pi columns and n rows, i = 1,..., d. Thus, the number of columns2\\n2As always, we assume the columns are linearly independent.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 198, 'page_label': '181'}, page_content='Regression 181\\nin the full model matrix isp = p1 +···+ pd. This creates an increasing sequence of “nested”\\nmodel matrices: X1,[X1,X2],..., [X1,X2,..., Xd], from (say) the baseline normal model\\nmatrix X1 = 1 to the full model matrix X. Think of each model matrix corresponding to\\nspecific variables in the model.\\nWe follow a similar projection procedure as in Figure 5.3: First projectY onto Span(X)\\nto yield the vector Y(d), then project Y(d) onto Span([X1,..., Xd−1]) to obtain Y(d−1), and so\\non, until Y(2) is projected onto Span(X1) to yield Y(1) = Y1 (in the case that X1 = 1).\\nBy applying Pythagoras’ theorem, the total sum of squares can be decomposed as\\n∥Y −Y(1)∥2\\n|       {z       }\\ndf=n−p1\\n= ∥Y −Y(d)∥2\\n|       {z       }\\ndf=n−p\\n+ ∥Y(d) −Y(d−1)∥2\\n|            {z            }\\ndf=pd\\n+ ··· + ∥Y(2) −Y(1)∥2\\n|          {z          }\\ndf=p2\\n. (5.17)\\nSoftware packages typically report the sums of squares as well as the corresponding de-\\ngrees of freedom (df): n −p,pd,..., p2. degrees of\\nfreedom\\n5.3.7 Coefficient of Determination\\nTo assess how a linear model Y = Xβ+ εcompares to the default model Y = β01 + ε, we\\ncan compare the variance of the original data, estimated via P\\ni(Yi −Y)2/n = ∥Y −Y1∥2/n,\\nwith the variance of the fitted data; estimated via P\\ni(bYi −Y)2/n = ∥bY −Y1∥2/n, where\\nbY = Xbβ. The sum P\\ni(Yi −Y)2/n = ∥Y −Y1∥2 is sometimes called the total sum of squares total sum of\\nsquares(TSS), and the quantity\\nR2 = ∥bY −Y1∥2\\n∥Y −Y1∥2\\n(5.18)\\nis called the coefficient of determination coefficient of\\ndetermination\\nof the linear model. In the notation of Figure 5.3,\\nbY = Y(2) and Y1 = Y(1), so that\\nR2 = ∥Y(2) −Y(1)∥2\\n∥Y −Y(1)∥2 = ∥Y −Y(1)∥2 −∥Y −Y(2)∥2\\n∥Y −Y(1)∥2 = TSS −RSS\\nTSS .\\nNote that R2 lies between 0 and 1. An R2 value close to 1 indicates that a large propor-\\ntion of the variance in the data has been explained by the model.\\nMany software packages also give the adjusted coefficient of determination adjusted\\ncoefficient of\\ndetermination\\n, or simply\\nthe adjusted R2, defined by\\nR2\\nadjusted = 1 −(1 −R2) n −1\\nn −p.\\nThe regular R2 is always non-decreasing in the number of parameters (see Exercise 15),\\nbut this may not indicate better predictive power. The adjusted R2 compensates for this\\nincrease by decreasing the regular R2 as the number of variables increases. This heuristic\\nadjustment can make it easier to compare the quality of two competing models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 199, 'page_label': '182'}, page_content='182 Inference for Normal Linear Models\\n5.4 Inference for Normal Linear Models\\nSo far we have not assumed any distribution for the random vector of errors ε =\\n[ε1,...,ε n]⊤ in a linear model Y = Xβ+ ε. When the error terms {εi}are assumed to be\\nnormally distributed (that is, {εi}∼iid N(0,σ2)), whole new avenues open up for inference\\non linear models. In Section 2.8 we already saw that for suchnormal linear models, estim-☞47\\nation of βand σ2 can be carried out via maximum likelihood methods, yielding the same\\nestimators from (5.9) and (5.10).\\nThe following theorem lists the properties of these estimators. In particular, it shows\\nthat bβand cσ2n/(n −p) are independent and unbiased estimators of βand σ2, respectively.\\nTheorem 5.3: Properties of the Estimators for a Normal Linear Model\\nConsider the linear model Y = Xβ+ ε, with ε∼N(0,σ2In), where β is a p-\\ndimensional vector of parameters and σ2 a dispersion parameter. The following res-\\nults hold.\\n1. The maximum likelihood estimators bβand cσ2 are independent.\\n2. bβ∼N(β, σ2(X⊤X)+).\\n3. n cσ2/σ2 ∼χ2\\nn−p, where p = rank(X).\\nProof: Using the pseudo-inverse (Definition A.2), we can write the random vector bβ as☞360\\nX+Y, which is a linear transformation of a normal random vector. Consequently, bβhas a\\nmultivariate normal distribution; see Theorem C.6. The mean vector and covariance matrix☞435\\nfollow from the same theorem:\\nEbβ= X+ EY = X+X β= β\\nand\\nCov(bβ) = X+σ2In(X+)⊤= σ2(X⊤X)+.\\nTo show that bβand cσ2 are independent, define Y(2) = Xbβ. Note that Y/σ has a N(µ,In)\\ndistribution, with expectation vector µ = Xβ/σ. A direct application of Theorem C.10\\nnow shows that ( Y −Y(2))/σ is independent of Y(2)/σ. Since bβ = X+Xbβ = X+Y(2) and☞438\\ncσ2 = ∥Y −Y(2)∥2/n, it follows that cσ2 is independent of bβ. Finally, by the same theorem,\\nthe random variable ∥Y −Y(2)∥2/σ2 has a χ2\\nn−p distribution, as Y(2) has the same expectation\\nvector as Y. □\\nAs a corollary, we see that each estimatorbβi of βi has a normal distribution with expect-\\nation βi and variance σ2u⊤\\ni X+(X+)⊤ui = σ2∥u⊤\\ni X+∥2, where ui = [0,..., 0,1,0,..., 0]⊤ is\\nthe i-th unit vector; in other words, the variance is σ2[(X⊤X)+]ii.\\nIt is of interest to test whether certain regression parameters βi are 0 or not, since if\\nβi = 0, the i-th explanatory variable has no direct e ffect on the expected response and so\\ncould be removed from the model. A standard procedure is to conduct a hypothesis test\\n(see Section C.14 for a review of hypothesis testing) to test the null hypothesis H0 : βi = 0☞458'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 200, 'page_label': '183'}, page_content='Regression 183\\nagainst the alternative H1 : βi , 0, using the test statistic\\nT =\\nbβi/∥u⊤\\ni X+∥\\n√\\nRSE\\n, (5.19)\\nwhere RSE is the residual squared error; that is RSE = RSS/(n −p). This test statistic has\\na tn−p distribution under H0. To see this, write T = Z/\\np\\nV/(n −p), with\\nZ =\\nbβi\\nσ∥u⊤\\ni X+∥ and V = n cσ2/σ2.\\nThen, by Theorem 5.3, Z ∼N(0,1) under H0, V ∼χ2\\nn−p, and Z and V are independent. The\\nresult now follows directly from Corollary C.1. ☞ 439\\n5.4.1 Comparing Two Normal Linear Models\\nSuppose we have the following normal linear model for data Y = [Y1,..., Yn]⊤:\\nY = X1β1 + X2β2|          {z          }\\nXβ\\n+ε, ε∼N(0,σ2In), (5.20)\\nwhere β1 and β2 are unknown vectors of dimension k and p −k, respectively; and X1\\nand X2 are full-rank model matrices of dimensions n ×k and n ×(p −k), respectively.\\nAbove we implicitly defined X = [X1,X2] and β⊤ = [β⊤\\n1 ,β⊤\\n2 ]. Suppose we wish to test the\\nhypothesis H0 : β2 = 0 against H1 : β2 , 0. Following Section 5.3.6, the idea is to compare\\nthe residual sum of squares for both models, expressed as∥Y −Y(2)∥2 and ∥Y −Y(1)∥2. Using\\nPythagoras’ theorem we saw that ∥Y −Y(2)∥2 −∥Y −Y(1)∥2 = ∥Y(2) −Y(1)∥2, and so it makes\\nsense to base the decision whether to retain or reject H0 on the basis of the quotient of\\n∥Y(2) −Y(1)∥2 and ∥Y −Y(2)∥2. This leads to the following test statistics.\\nTheorem 5.4: Test Statistic for Comparing Two Normal Linear Models\\nFor the model (5.20), letY(2) and Y(1) be the projections of Y onto the space spanned\\nby the p columns of X and the k columns of X1, respectively. Then underH0 : β2 = 0\\nthe test statistic\\nT = ∥Y(2) −Y(1)∥2/(p −k)\\n∥Y −Y(2)∥2/(n −p) (5.21)\\nhas an F(p −k,n −p) distribution.\\nProof: Define X := Y/σwith expectation µ:= Xβ/σ, and Xj := Y( j)/σwith expectation\\nµj, j = k,p. Note that µp = µand, under H0, µk = µp. We can directly apply Theorem C.10\\nto find that ∥Y −Y(2)∥2/σ2 = ∥X −Xp∥2 ∼χ2\\nn−p and, under H0, ∥Y(2) −Y(1)∥2/σ2 = ∥Xp − ☞ 438\\nXk∥2 ∼χ2\\np−k. Moreover, these random variables are independent of each other. The proof\\nis completed by applying Theorem C.11. □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 201, 'page_label': '184'}, page_content='184 Inference for Normal Linear Models\\nNote that H0 is rejected for large values of T. The testing procedure thus proceeds as\\nfollows:\\n1. Compute the outcome, t say, of the test statistic T in (5.21).\\n2. Evaluate the P-value P(T ⩾t), with T ∼F(p −k,n −p).\\n3. Reject H0 if this P-value is too small, say less than 0.05.\\nFor nested models [X1,X2,..., Xi], i = 1,2,..., d, as in Section 5.3.6, theF test statistic\\nin Theorem 5.4 can now be used to test whether certain Xi are needed or not. In particular,☞183\\nsoftware packages will report the outcomes of\\nFi = ∥Y(i) −Y(i−1)∥2/pi\\n∥Y −Y(d)∥2/(n −p)\\n, (5.22)\\nin the order i = 2,3,..., d. Under the null hypothesis that Y(i) and Y(i−1) have the same ex-\\npectation (that is, adding Xi to Xi−1 has no additional effect on reducing the approximation\\nerror), the test statistic Fi has an F(pi,n −p) distribution, and the corresponding P-values\\nquantify the strength of the decision to include an additional variable in the model or not.\\nThis procedure is called analysis of variance (ANOV A).analysis of\\nvariance\\nNote that the output of an ANOV A table depends on the order in which the variables\\nare considered.\\nExample 5.6 (Crop Yield (cont.)) We continue Examples 5.4 and 5.5. Decompose the\\nlinear model as\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nX1\\nβ0|{z}\\nβ1\\n+\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 0\\n1 0 0\\n0 1 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|     {z     }\\nX2\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ12\\nβ13\\nβ14\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ2\\n+\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nC\\nC\\nC\\nC\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nX3\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ22\\nβ23\\nβ24\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|{z}\\nβ3\\n+ ε.\\nIs the crop yield dependent on treatment levels as well as blocks? We first test whether we\\ncan remove Block as a factor in the model against it playing a significant role in explain-\\ning the crop yields. Specifically, we test β3 = 0 versus β3 , 0 using Theorem 5.4. Now\\nthe vector Y(2) is the projection of Y onto the ( p = 7)-dimensional space spanned by the\\ncolumns of X = [X1,X2,X3]; and Y(1) is the projection of Y onto the (k = 4)-dimensional\\nspace spanned by the columns of X12 := [X1,X2]. The test statistic, T12 say, under H0 has\\nan F(3,9) distribution.\\nThe Python code below calculates the outcome of the test statistic T12 and the corres-\\nponding P-value. We find t12 = 34.9998, which gives a P-value 2 .73 ×10−5. This shows\\nthat the block effects are extremely important for explaining the data.\\nUsing the extended model (including the block effects), we can test whether β2 = 0 or\\nnot; that is, whether the treatments have a significant effect on the crop yield in the presence\\nof the Block factor. This is done in the last six lines of the code below. The outcome of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 202, 'page_label': '185'}, page_content='Regression 185\\nthe test statistic is 4 .4878, with a P-value of 0 .0346. By including the block e ffects, we\\neffectively reduce the uncertainty in the model and are able to more accurately assess the\\neffects of the treatments, to conclude that the treatment seems to have an effect on the crop\\nyield. A closer look at the data shows that within each block (row) the crop yield roughly\\nincreases with the treatment level.\\ncrop.py\\nimport numpy as np\\nfrom scipy.stats import f\\nfrom numpy.linalg import lstsq, norm\\nyy = np.array([9.2988, 9.4978, 9.7604, 10.1025,\\n8.2111, 8.3387, 8.5018, 8.1942,\\n9.0688, 9.1284, 9.3484, 9.5086,\\n8.2552, 7.8999, 8.4859, 8.9485]).reshape(4,4).T\\nnrow, ncol = yy.shape[0], yy.shape[1]\\nn = nrow * ncol\\ny = yy.reshape(16,)\\nX_1 = np.ones((n,1))\\nKM = np.kron(np.eye(ncol),np.ones((nrow,1)))\\nKM[:,0]\\nX_2 = KM[:,1:ncol]\\nIM = np.eye(nrow)\\nC = IM[:,1:nrow]\\nX_3 = np.vstack((C, C))\\nX_3 = np.vstack((X_3, C))\\nX_3 = np.vstack((X_3, C))\\nX = np.hstack((X_1,X_2))\\nX = np.hstack((X,X_3))\\np = X.shape[1] #number of parameters in full model\\nbetahat = lstsq(X, y,rcond=None)[0] #estimate under the full model\\nym = X @ betahat\\nX_12 = np.hstack((X_1, X_2)) #omitting the block effect\\nk = X_12.shape[1] #number of parameters in reduced model\\nbetahat_12 = lstsq(X_12, y,rcond=None)[0]\\ny_12 = X_12 @ betahat_12\\nT_12=(n-p)/(p-k)*(norm(y-y_12)**2 - norm(y-ym)**2)/norm(y-ym)**2\\npval_12 = 1 - f.cdf(T_12,p-k,n-p)\\nX_13 = np.hstack((X_1, X_3)) #omitting the treatment effect\\nk = X_13.shape[1] #number of parameters in reduced model\\nbetahat_13 = lstsq(X_13, y,rcond=None)[0]\\ny_13 = X_13 @ betahat_13\\nT_13=(n-p)/(p-k)*(norm(y-y_13)**2 - norm(y-ym)**2)/norm(y-ym)**2\\npval_13 = 1 - f.cdf(T_13,p-k,n-p)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 203, 'page_label': '186'}, page_content='186 Inference for Normal Linear Models\\n5.4.2 Confidence and Prediction Intervals\\nAs in all supervised learning settings, linear regression is most useful when we wish to\\npredict how a new response variable will behave on the basis of a new explanatory vector\\nx. For example, it may be di fficult to measure the response variable, but by knowing the\\nestimated regression line and the value for x, we will have a reasonably good idea what Y\\nor the expected value of Y is going to be.\\nThus, consider a new x and let Y ∼N(x⊤β,σ2), with β and σ2 unknown. First we\\nare going to look at the expected value of Y, that is EY = x⊤β. Since βis unknown, we\\ndo not know EY either. However, we can estimate it via the estimator bY = x⊤bβ, where\\nbβ ∼N(β, σ2(X⊤X)+), by Theorem 5.3. Being linear in the components of β, bY therefore\\nhas a normal distribution with expectation x⊤βand variance σ2∥x⊤X+∥2. Let Z ∼N(0,1)\\nbe the standardized version of bY and V = ∥Y −Xbβ∥2/σ2 ∼χ2\\nn−p. Then the random variable\\nT := (x⊤bβ−x⊤β) /∥x⊤X+∥\\n∥Y −Xbβ∥/\\np\\n(n −p)\\n= Zp\\nV/(n −p)\\n(5.23)\\nhas, by Corollary C.1, atn−p distribution. After rearranging the identityP(|T|⩽tn−p;1−α/2) =☞439\\n1 −α, where tn−p;1−α/2 is the (1 −α/2) quantile of the tn−p distribution, we arrive at the\\nstochastic confidence intervalconfidence\\ninterval\\nx⊤bβ±tn−p;1−α/2\\n√\\nRSE ∥x⊤X+∥, (5.24)\\nwhere we have identified ∥Y −Xbβ∥2/(n −p) with RSE. This confidence interval quantifies\\nthe uncertainty in the learner (regression surface).\\nA prediction intervalprediction\\ninterval\\nfor a new response Y is different from a confidence interval for\\nEY. Here the idea is to construct an interval such that Y lies in this interval with a certain\\nguaranteed probability. Note that now we havetwo sources of variation:\\n1. Y ∼N(x⊤β,σ2) itself is a random variable.\\n2. Estimating x⊤βvia bY brings another source of variation.\\nWe can construct a (1−α) prediction interval, by finding two random bounds such that\\nthe random variable Y lies between these bounds with probability 1 −α. We can reason as\\nfollows. Firstly, note thatY ∼N(x⊤β,σ2) and bY ∼N(x⊤β,σ2∥x⊤X+∥2) are independent. It\\nfollows that Y −bY has a normal distribution with expectation 0 and variance\\nσ2(1 + ∥x⊤X+∥2). (5.25)\\nSecondly, letting Z ∼ N(0,1) be the standardized version of Y −bY, and repeating the\\nsteps used for the construction of the confidence interval (5.24), we arrive at the prediction\\ninterval\\nx⊤bβ±tn−p;1−α/2\\n√\\nRSE\\np\\n1 + ∥x⊤X+∥2. (5.26)\\nThis prediction interval captures the uncertainty from an as-yet-unobserved response as\\nwell as the uncertainty in the parameters of the regression model itself.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 204, 'page_label': '187'}, page_content=\"Regression 187\\nExample 5.7 (Confidence Limits in Simple Linear Regression) The following pro-\\ngram draws n = 100 samples from a simple linear regression model with parameters\\nβ= [6,13]⊤ and σ= 2, where the x-coordinates are evenly spaced on the interval [0 ,1].\\nThe parameters are estimated in the third block of the code. Estimates for β and σ are\\n[6.03,13.09]⊤ and bσ= 1.60, respectively. The program then proceeds by calculating the\\n95% numeric confidence and prediction intervals for various values of the explanatory\\nvariable. Figure 5.4 shows the results.\\nconfpred.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import t\\nfrom numpy.linalg import inv, lstsq, norm\\nnp.random.seed(123)\\nn = 100\\nx = np.linspace(0.01,1,100).reshape(n,1)\\n# parameters\\nbeta = np.array([6,13])\\nsigma = 2\\nXmat = np.hstack((np.ones((n,1)), x)) #design matrix\\ny = Xmat @ beta + sigma*np.random.randn(n)\\n# solve the normal equations\\nbetahat = lstsq(Xmat, y,rcond=None)[0]\\n# estimate for sigma\\nsqMSE = norm(y - Xmat @ betahat)/np.sqrt(n-2)\\ntquant = t.ppf(0.975,n-2) # 0.975 quantile\\nucl = np.zeros(n) #upper conf. limits\\nlcl = np.zeros(n) #lower conf. limits\\nupl = np.zeros(n)\\nlpl = np.zeros(n)\\nrl = np.zeros(n) # (true) regression line\\nu = 0\\nfor i in range (n):\\nu = u + 1/n;\\nxvec = np.array([1,u])\\nsqc = np.sqrt(xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\\nsqp = np.sqrt(1 + xvec.T @ inv(Xmat.T @ Xmat) @ xvec)\\nrl[i] = xvec.T @ beta;\\nucl[i] = xvec.T @ betahat + tquant*sqMSE*sqc;\\nlcl[i] = xvec.T @ betahat - tquant*sqMSE*sqc;\\nupl[i] = xvec.T @ betahat + tquant*sqMSE*sqp;\\nlpl[i] = xvec.T @ betahat - tquant*sqMSE*sqp;\\nplt.plot(x,y, '.')\\nplt.plot(x,rl, 'b')\\nplt.plot(x,ucl, 'k:')\\nplt.plot(x,lcl, 'k:')\\nplt.plot(x,upl, 'r--')\\nplt.plot(x,lpl, 'r--')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 205, 'page_label': '188'}, page_content='188 Nonlinear Regression Models\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\n5\\n10\\n15\\n20\\nFigure 5.4: The true regression line (blue, solid) and the upper and lower 95% prediction\\ncurves (red, dashed) and confidence curves (dotted).\\n5.5 Nonlinear Regression Models\\nSo far we have been mostly dealing with linear regression models, in which the predic-\\ntion function is of the form g(x |β) = x⊤β. In this section we discuss some strategies for\\nhandling general prediction functions g(x |β), where the functional form is known up to an\\nunknown parameter vector β. So the regression model becomes\\nYi = g(xi |β) + εi, i = 1,..., n, (5.27)\\nwhere ε1,...,ε n are independent with expectation 0 and unknown variance σ2. The model\\ncan be further specified by assuming that the error terms have a normal distribution.\\nTable 5.3 gives some common examples of nonlinear prediction functions for data tak-\\ning values in R.\\nTable 5.3: Common nonlinear prediction functions for one-dimensional data.\\nName g(x |β) β\\nExponential a ebx a,b\\nPower law a xb a,b\\nLogistic (1 + ea+bx)−1 a,b\\nWeibull 1 −exp(−xb/a) a,b\\nPolynomial Pp−1\\nk=0 βk xk p, {βk}p−1\\nk=0\\nThe logistic and polynomial prediction functions in Table 5.3 can be readily gener-\\nalized to higher dimensions. For example, for x ∈R2 a general second-order polynomial\\nprediction function is of the form\\ng(x |β) = β0 + β1 x1 + β2 x2 + β11 x2\\n1 + β22 x2\\n2 + β12 x1 x2. (5.28)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 206, 'page_label': '189'}, page_content='Regression 189\\nThis function can be viewed as a second-order approximation to a general smooth predic-\\ntion function g(x1,x2); see also Exercise 4. Polynomial regression models are also called\\nresponse surface models. response\\nsurface model\\nThe generalization of the above logistic prediction to Rd is\\ng(x |β) = (1 + e−x⊤β)−1. (5.29)\\nThis function will make its appearance in Section 5.7 and later on in Chapters 7 and 9.\\nThe first strategy for performing regression with nonlinear prediction functions is to\\nextend the feature space to obtain a simpler (ideally linear) prediction function in the ex-\\ntended feature space. We already saw an application of this strategy in Example 2.1 for ☞ 26\\nthe polynomial regression model, where the original feature u was extended to the feature\\nvector x = [1,u,u2,..., up−1]⊤, yielding a linear prediction function. In a similar way, the\\nright-hand side of the polynomial prediction function in (5.28) can be viewed as a linear\\nfunction of the extended feature vector ϕ(x) = [1,x1,x2,x2\\n1,x2\\n2,x1 x2]⊤. The function ϕis\\ncalled a feature map feature map.\\nThe second strategy is to transform the response variable y and possibly also the ex-\\nplanatory variable x such that the transformed variablesey, ex are related in a simpler (ideally\\nlinear) way. For example, for the exponential prediction function y = a e−bx, we have\\nln y = ln a −bx, which is a linear relation between ln y and [1,x]⊤.\\nExample 5.8 (Chlorine) Table 5.4 lists the free chlorine concentration (in mg per liter)\\nin a swimming pool, recorded every 8 hours for 4 days. A simple chemistry-based model\\nfor the chlorine concentration y as a function of time t is y = a e−b t, where a is the initial\\nconcentration and b >0 is the reaction rate.\\nTable 5.4: Chlorine concentration (in mg/L) as a function of time (hours).\\nHours Concentration\\n0 1.0056\\n8 0.8497\\n16 0.6682\\n24 0.6056\\n32 0.4735\\n40 0.4745\\n48 0.3563\\nHours Concentration\\n56 0.3293\\n64 0.2617\\n72 0.2460\\n80 0.1839\\n88 0.1867\\n96 0.1688\\nThe exponential relationshipy = a e−bt suggests that a log transformation ofy will result\\nin a linear relationship between ln y and the feature vector [1,t]⊤. Thus, if for some given\\ndata (t1,y1),..., (tn,yn), we plot (t1,ln y1),..., (tn,ln yn), these points should approximately\\nlie on a straight line, and hence the simple linear regression model applies. The left panel of\\nFigure 5.5 illustrates that the transformed data indeed lie approximately on a straight line.\\nThe estimated regression line is also drawn here. The intercept and slope areβ0 = −0.0555\\nand β1 = −0.0190 here. The original (non-transformed) data is shown in the right panel\\nof Figure 5.5, along with the fitted curve y = ba e−bbt, where ba = exp(bβ0) = 0.9461 and\\nbb = −bβ1 = 0.0190.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 207, 'page_label': '190'}, page_content='190 Nonlinear Regression Models\\n0 50 100\\nt\\n-2\\n-1.5\\n-1\\n-0.5\\n0\\n0.5\\nlog y\\n0 50 100\\nt\\n0\\n0.5\\n1\\n1.5y\\nFigure 5.5: The chlorine concentration seems to have an exponential decay.\\nRecall that for a general regression problem the learner gτ(x) for a given training set τ\\nis obtained by minimizing the training (squared-error) loss\\nℓτ(g(·|β)) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi |β))2. (5.30)\\nThe third strategy for regression with nonlinear prediction functions is to directly minimize\\n(5.30) by any means possible, as illustrated in the next example.\\nExample 5.9 (Hougen Function) In [7] the reaction rate y of a certain chemical reac-\\ntion is posited to depend on three input variables: quantities of hydrogen x1, n-pentane x2,\\nand isopentane x3. The functional relationship is given by the Hougen function:\\ny = β1 x2 −x3/β5\\n1 + β2 x1 + β3 x2 + β4 x3\\n,\\nwhere β1,...,β 5 are the unknown parameters. The objective is to estimate the model para-\\nmeters {βi}from the data, as given in Table 5.5.\\nTable 5.5: Data for the Hougen function.\\nx1 x2 x3 y\\n470 300 10 8.55\\n285 80 10 3.79\\n470 300 120 4.82\\n470 80 120 0.02\\n470 80 10 2.75\\n100 190 10 14.39\\n100 80 65 2.54\\nx1 x2 x3 y\\n470 190 65 4.35\\n100 300 54 13.00\\n100 300 120 8.50\\n100 80 120 0.05\\n285 300 10 11.32\\n285 190 120 3.13\\nThe estimation is carried out via the least-squares method. The objective function to\\nminimize is thus\\nℓτ(g(·|β)) = 1\\n13\\n13X\\ni=1\\n \\nyi − β1 xi2 −xi3/β5\\n1 + β2 xi1 + β3 xi2 + β4 xi3\\n!2\\n, (5.31)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 208, 'page_label': '191'}, page_content='Regression 191\\nwhere the {yi}and {xi j}are given in Table 5.5.\\nThis is a highly nonlinear optimization problem, for which standard nonlinear least- ☞ 414\\nsquares methods do not work well. Instead, one can use global optimization methods such\\nas CE and SCO (see Sections 3.4.2 and 3.4.3). Using the CE method, we found the minimal ☞ 100\\nvalue 0.02299 for the objective function, which is attained at\\nbβ= [1.2526, 0.0628, 0.0400, 0.1124, 1.1914]⊤.\\n5.6 Linear Models in Python\\nIn this section we describe how to define and analyze linear models using Python and the\\ndata science module statsmodels. We encourage the reader to regularly refer back to\\nthe theory in the preceding sections of this chapter, so as to avoid using Python merely\\nas a black box without understanding the underlying principles. To run the code start by\\nimporting the following code snippet:\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\n5.6.1 Modeling\\nAlthough specifying a normal3 linear model in Python is relatively easy, it requires some\\nsubtlety. The main thing to realize is that Python treats quantitative and qualitative (that\\nis, categorical) explanatory variables differently. In statsmodels, ordinary least-squares\\nlinear models are specified via the function ols (short for ordinary least-squares). The\\nmain argument of this function is a formula of the form\\ny ∼x1 + x2 + ··· + xd, (5.32)\\nwhere y is the name of the response variable and x1, . . . ,xd are the names of the explan-\\natory variables. If all variables are quantitative, this describes the linear model\\nYi = β0 + β1 xi1 + β2 xi2 + ··· + βd xid + εi, i = 1,..., n, (5.33)\\nwhere xi j is the j-th explanatory variable for the i-th observation and the errors εi are\\nindependent normal random variables such that Eεi = 0 and Var εi = σ2. Or, in matrix\\nform: Y = Xβ+ ε, with\\nY =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nY1\\n...\\nYn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, X =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 x11 ··· x1d\\n1 x21 ··· x2d\\n... ... ... ...\\n1 xn1 ··· xnd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, β=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ0\\n...\\nβd\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, and ε=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nε1\\n...\\nεn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\n3For the rest of this section, we assume all linear models to be normal.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 209, 'page_label': '192'}, page_content='192 Linear Models in Python\\nThus, the first column is always taken as an “intercept” parameter, unless otherwise spe-\\ncified. To remove the intercept term, add -1to the olsformula, as in ols(’y∼x-1’).\\nFor any linear model, the model matrix can be retrieved via the construction:\\nmodel_matrix = pd.DataFrame(model.exog,columns=model.exog_names)\\nLet us look at some examples of linear models. In the first model the variables x1 and x2\\nare both considered (by Python) to be quantitative.\\nmyData = pd.DataFrame({ \\'y\\' : [10,9,4,2,4,9],\\n\\'x1\\' : [7.4,1.2,3.1,4.8,2.8,6.5],\\n\\'x2\\' : [1,1,2,2,3,3]})\\nmod = ols(\"y~x1+x2\", data=myData)\\nmod_matrix = pd.DataFrame(mod.exog,columns=mod.exog_names)\\nprint (mod_matrix)\\nIntercept x1 x2\\n0 1.0 7.4 1.0\\n1 1.0 1.2 1.0\\n2 1.0 3.1 2.0\\n3 1.0 4.8 2.0\\n4 1.0 2.8 3.0\\n5 1.0 6.5 3.0\\nSuppose the second variable is actually qualitative; e.g., it represents a color, and the\\nlevels 1, 2, and 3 stand for red, blue, and green. We can account for such a categorical\\nvariable by using the astype method to redefine the data type (see Section 1.2).☞3\\nmyData[ \\'x2\\'] = myData[ \\'x2\\'].astype( \\'category \\')\\nAlternatively, a categorical variable can be specified in the model formula by wrapping\\nit with C(). Observe how this changes the model matrix.\\nmod2 = ols(\"y~x1+C(x2)\", data=myData)\\nmod2_matrix = pd.DataFrame(mod2.exog,columns=mod2.exog_names)\\nprint (mod2_matrix)\\nIntercept C(x2)[T.2] C(x2)[T.3] x1\\n0 1.0 0.0 0.0 7.4\\n1 1.0 0.0 0.0 1.2\\n2 1.0 1.0 0.0 3.1\\n3 1.0 1.0 0.0 4.8\\n4 1.0 0.0 1.0 2.8\\n5 1.0 0.0 1.0 6.5\\nThus, if a statsmodels formula of the form (5.32) contains factor (qualitative) variables,\\nthe model is no longer of the form (5.33), but contains indicator variables for each level of\\nthe factor variable, except the first level.\\nFor the case above, the corresponding linear model is\\nYi = β0 + β1 xi1 + α2 1{xi2 = 2}+ α3 1{xi2 = 3}+ εi, i = 1,..., 6, (5.34)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 210, 'page_label': '193'}, page_content='Regression 193\\nwhere we have used parameters α2 and α3 to correspond to the indicator features of the\\nqualitative variable. The parameter α2 describes how much the response is expected to\\nchange if the factor x2 switches from level 1 to 2. A similar interpretation holds for α3.\\nSuch parameters can thus be viewed as incremental effects.\\nIt is also possible to model interaction interactionbetween two variables. For two continuous\\nvariables, this simply adds the products of the original features to the model matrix. Adding\\ninteraction terms in Python is achieved by replacing “ +” in the formula with “ *”, as the\\nfollowing example illustrates.\\nmod3 = ols(\"y~x1*C(x2)\", data=myData)\\nmod3_matrix = pd.DataFrame(mod3.exog,columns=mod3.exog_names)\\nprint (mod3_matrix)\\nIntercept C(x2)[T.2] C(x2)[T.3] x1 x1:C(x2)[T.2] x1:C(x2)[T.3]\\n0 1.0 0.0 0.0 7.4 0.0 0.0\\n1 1.0 0.0 0.0 1.2 0.0 0.0\\n2 1.0 1.0 0.0 3.1 3.1 0.0\\n3 1.0 1.0 0.0 4.8 4.8 0.0\\n4 1.0 0.0 1.0 2.8 0.0 2.8\\n5 1.0 0.0 1.0 6.5 0.0 6.5\\n5.6.2 Analysis\\nLet us consider some easy linear regression models by using the student survey data set\\nsurvey.csv from the book’s GitHub site, which contains measurements such as height,\\nweight, sex, etc., from a survey conducted amongn = 100 university students. Suppose we\\nwish to investigate the relation between the shoe size (explanatory variable) and the height\\n(response variable) of a person. First, we load the data and draw a scatterplot of the points\\n(height versus shoe size); see Figure 5.6 (without the fitted line).\\nsurvey = pd.read_csv( \\'survey.csv \\')\\nplt.scatter(survey.shoe, survey.height)\\nplt.xlabel(\"Shoe size\")\\nplt.ylabel(\"Height\")\\nWe observe a slight increase in the height as the shoe size increases, although this\\nrelationship is not very distinct. We analyze the data through the simple linear regression\\nmodel Yi = β0 + β1 xi + εi,i = 1,..., n. In statsmodels this is performed via the ols ☞ 169\\nmethod as follows:\\nmodel = ols(\"height~shoe\", data=survey) # define the model\\nfit = model.fit() #fit the model defined above\\nb0, b1 = fit.params\\nprint (fit.params)\\nIntercept 145.777570\\nshoe 1.004803\\ndtype: float64'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 211, 'page_label': '194'}, page_content='194 Linear Models in Python\\n15\\n 20\\n 25\\n 30\\n 35\\nShoe size\\n150\\n160\\n170\\n180\\n190\\n200Height\\nFigure 5.6: Scatterplot of height (cm) against shoe size (cm), with the fitted line.\\nThe above output gives the least-squares estimates of β0 and β1. For this example, we\\nhave bβ0 = 145.778 and bβ1 = 1.005. Figure 5.6, which includes the regression line, was\\nobtained as follows:\\nplt.plot(survey.shoe, b0 + b1*survey.shoe)\\nplt.scatter(survey.shoe, survey.height)\\nplt.xlabel(\"Shoe size\")\\nplt.ylabel(\"Height\")\\nAlthough ols performs a complete analysis of the linear model, not all its calculations\\nneed to be presented. A summary of the results can be obtained with the methodsummary.\\nprint (fit.summary())\\nDep. Variable: height R-squared: 0.178\\nModel: OLS Adj. R-squared: 0.170\\nMethod: Least Squares F-statistic: 21.28\\nNo. Observations: 100 Prob (F-statistic): 1.20e-05\\nDf Residuals: 98 Log-Likelihood: -363.88\\nDf Model: 1 AIC: 731.8\\nCovariance Type: nonrobust BIC: 737.0\\n=====================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n--------------------------------------------------------------------\\nIntercept 145.7776 5.763 25.296 0.000 134.341 157.214\\nshoe 1.0048 0.218 4.613 0.000 0.573 1.437\\n=====================================================================\\nOmnibus: 1.958 Durbin-Watson: 1.772\\nProb(Omnibus): 0.376 Jarque-Bera (JB): 1.459\\nSkew: -0.072 Prob(JB): 0.482\\nKurtosis: 2.426 Cond. No. 164.\\nThe main output items are the following:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 212, 'page_label': '195'}, page_content='Regression 195\\n• coef:Estimates of the parameters of the regression line.\\n• std error: Standard deviations of the estimators of the regression line. These are\\nthe square roots of the variances of the {bβi}obtained in (5.25). ☞ 186\\n• t:Realization of Student’s test statistics associated with the hypotheses H0 : βi = 0\\nand H1 : βi , 0, i = 0,1. In particular, the outcome of T in (5.19). ☞ 183\\n• P>|t|: P-value of Student’s test (two-sided test).\\n• [0.025 0.975]: 95% confidence intervals for the parameters.\\n• R-Squared: Coefficient of determination R2 (percentage of variation explained by\\nthe regression), as defined in (5.18). ☞ 181\\n• Adj. R-Squared:adjusted R2 (explained in Section 5.3.7).\\n• F-statistic: Realization of the F test statistic (5.21) associated with testing the ☞ 183\\nfull model against the default model. The associated degrees of freedom (Df Model\\n= 1 and Df Residuals= n−2) are given, as is the P-value:Prob (F-statistic).\\n• AIC:The AIC number in (5.15); that is, minus two times the log-likelihood plus two ☞ 177\\ntimes the number of model parameters (which is 3 here).\\nYou can access all the numerical values as they are attributes of the fit object. First\\ncheck which names are available, as in:\\ndir (fit)\\nThen access the values via the dot construction. For example, the following extracts the\\nP-value for the slope.\\nfit.pvalues[1]\\n1.1994e-05\\nThe results show strong evidence for a linear relationship between shoe size and height\\n(or, more accurately, strong evidence that the slope of the regression line is not zero), as\\nthe P-value for the corresponding test is very small (1 .2 ·10−5). The estimate of the slope\\nindicates that the di fference between the average height of students whose shoe size is\\ndifferent by one cm is 1.0048 cm.\\nOnly 17 .84% of the variability of student height is explained by the shoe size. We\\ntherefore need to add other explanatory variables to the model (multiple linear regression)\\nto increase the model’s predictive power.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 213, 'page_label': '196'}, page_content='196 Linear Models in Python\\n5.6.3 Analysis of Variance (ANOVA)\\nWe continue the student survey example of the previous section, but now add an extra\\nvariable, and also consider an analysis of variance of the model. Instead of “explaining”\\nthe student height via their shoe size, we include weight as an explanatory variable. The\\ncorresponding ols formula for this model is\\nheight∼shoe + weight,\\nmeaning that each random height, denoted by Height, satisfies\\nHeight = β0 + β1shoe + β2weight + ε,\\nwhere εis a normally distributed error term with mean 0 and varianceσ2. Thus, the model\\nhas 4 parameters. Before analyzing the model we present a scatterplot of all pairs of vari-\\nables, using scatter_matrix.\\nmodel = ols(\"height~shoe+weight\", data=survey)\\nfit = model.fit()\\naxes = pd.plotting.scatter_matrix(\\nsurvey[[ \\'height \\',\\'shoe \\',\\'weight \\']])\\nplt.show()\\n150\\n175\\nheight\\n20\\n30\\nshoe\\n150\\n175\\nheight\\n50\\n100\\nweight\\n20\\n30\\nshoe\\n50\\n100\\nweight\\nFigure 5.7: Scatterplot of all pairs of variables: height (cm), shoe (cm), and weight (kg).\\nAs for the simple linear regression model in the previous section, we can analyze the\\nmodel using the summary method (below we have omitted some output):\\nfit.summary()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 214, 'page_label': '197'}, page_content='Regression 197\\nDep. Variable: height R-squared: 0.430\\nModel: OLS Adj. R-squared: 0.418\\nMethod: Least Squares F-statistic: 36.61\\nNo. Observations: 100 Prob (F-statistic): 1.43e-12\\nDf Residuals: 97 Log-Likelihood: -345.58\\nDf Model: 2 AIC: 697.2\\nBIC: 705.0\\n======================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 132.2677 5.247 25.207 0.000 121.853 142.682\\nshoe 0.5304 0.196 2.703 0.008 0.141 0.920\\nweight 0.3744 0.057 6.546 0.000 0.261 0.488\\nThe F-statistic is used to test whether the full model (here with two explanatory\\nvariables) is better at “explaining” the height than the default model. The corresponding\\nnull hypothesis is H0 : β1 = β2 = 0. The assertion of interest isH1: at least one of the coeffi-\\ncients βj ( j = 1,2) is significantly different from zero. Given the result of this test (P-value\\n= 1.429·10−12), we can conclude that at least one of the explanatory variables is associated\\nwith height. The individual Student tests indicate that:\\n• shoe size is linearly associated with student height, after adjusting for weight, with\\nP-value 0.0081. At the same weight, an increase of one cm in shoe size corresponds\\nto an increase of 0.53 cm in average student height;\\n• weight is linearly associated with student height, after adjusting for shoe size (the\\nP-value is actually 2.82 ·10−09; the reported value of 0.000 should be read as “less\\nthan 0.001”). At the same shoe size, an increase of one kg in weight corresponds to\\nan increase of 0.3744 cm in average student height.\\nFurther understanding is extracted from the model by conducting an analysis of vari-\\nance. The standard statsmodels function is anova_lm. The input to this function is the\\nfit object (obtained from model.fit()) and the output is a DataFrameobject.\\ntable = sm.stats.anova_lm(fit)\\nprint (table)\\ndf sum_sq mean_sq F PR(>F)\\nshoe 1.0 1840.467359 1840.467359 30.371310 2.938651e-07\\nweight 1.0 2596.275747 2596.275747 42.843626 2.816065e-09\\nResidual 97.0 5878.091294 60.598879 NaN NaN\\nThe meaning of the columns is as follows.\\n• df: The degrees of freedom of the variables, according to the sum of squares decom-\\nposition (5.17). As both shoe and weight are quantitative variables, their degrees ☞ 181\\nof freedom are both 1 (each corresponding to a single column in the overall model\\nmatrix). The degrees of freedom for the residuals is n −p = 100 −3 = 97.\\n• sum sq: The sum of squares according to (5.17). The total sum of squares is the\\nsum of all the entries in this column. The residual error in the model that cannot be\\nexplained by the variables is RSS ≈5878.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 215, 'page_label': '198'}, page_content='198 Linear Models in Python\\n• mean sq: The sum of squares divided by their degrees of freedom. Note that the\\nresidual square error RSE = RSS/(n −p) = 60.6 is an unbiased estimate of the\\nmodel variance σ2; see Section 5.4.☞182\\n• F: These are the outcomes of the test statistic (5.22).☞184\\n• PR(>F): These are the P-values corresponding to the test statistic in the preceding\\ncolumn and are computed using an F distribution whose degrees of freedom are\\ngiven in the dfcolumn.\\nThe ANOV A table indicates that the shoe variable explains a reasonable amount of the\\nvariation in the model, as evidenced by a sum of squares contribution of 1840 out of 1840+\\n2596+5878 = 10314 and a very small P-value. Aftershoeis included in the model, it turns\\nout that the weightvariable explains even more of the remaining variability, with an even\\nsmaller P-value. The remaining sum of squares (5878) is 57% of the total sum of squares,\\nyielding a 43% reduction, in accordance with the R2 value reported in the summary for the\\nols method. As mentioned in Section 5.4.1, the order in which the ANOV A is conducted\\nis important. To illustrate this, consider the output of the following commands.\\nmodel = ols(\"height~weight+shoe\", data=survey)\\nfit = model.fit()\\ntable = sm.stats.anova_lm(fit)\\nprint (table)\\ndf sum_sq mean_sq F PR(>F)\\nweight 1.0 3993.860167 3993.860167 65.906502 1.503553e-12\\nshoe 1.0 442.882938 442.882938 7.308434 8.104688e-03\\nResidual 97.0 5878.091294 60.598879 NaN NaN\\nWe see that weight as a single model variable explains much more of the variability\\nthan shoe did. If we now also include shoe, we only obtain a small (but according to the\\nP-value still significant) reduction in the model variability.\\n5.6.4 Confidence and Prediction Intervals\\nIn statsmodels a method for computing confidence or prediction intervals from a dic-\\ntionary of explanatory variables is get_prediction. It simply executes formula (5.24) or\\n(5.26). A simpler version is predict, which only returns the predicted value.☞186\\nContinuing the student survey example, suppose we wish to predict the height of a\\nperson with shoe size 30 cm and weight 75 kg. Confidence and prediction intervals can\\nbe obtained as given in the code below. The new explanatory variable is entered as a dic-\\ntionary. Notice that the 95% prediction interval (for the corresponding random response) is\\nmuch wider than the 95% confidence interval (for the expectation of the random response).\\nx = { \\'shoe \\': [30.0], \\'weight \\': [75.0]} # new input (dictionary)\\npred = fit.get_prediction(x)\\npred.summary_frame(alpha=0.05).unstack()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 216, 'page_label': '199'}, page_content='Regression 199\\nmean 0 176.261722 # predicted value\\nmean_se 0 1.054015\\nmean_ci_lower 0 174.169795 # lower bound for CI\\nmean_ci_upper 0 178.353650 # upper bound for CI\\nobs_ci_lower 0 160.670610 # lower bound for PI\\nobs_ci_upper 0 191.852835 # upper bound for PI\\ndtype: float64\\n5.6.5 Model Validation\\nWe can perform an analysis of residuals to examine whether the underlying assumptions\\nof the (normal) linear regression model are verified. Various plots of the residuals can be\\nused to inspect whether the assumptions on the errors{εi}are satisfied. Figure 5.8 gives two\\nsuch plots. The first is a scatterplot of the residuals{ei}against the fitted valuesbyi. When the\\nmodel assumptions are valid, the residuals, as approximations of the model error, should\\nbehave approximately as iid normal random variables for each of the fitted values, with a\\nconstant variance. In this case we see no strong aberrant structure in this plot. The residuals\\nare fairly evenly spread and symmetrical about they = 0 line (not shown). The second plot\\nis a quantile–quantile (or qq) plot. This is a useful way to check for normality of the error\\nterms, by plotting the sample quantiles of the residuals against the theoretical quantiles\\nof the standard normal distribution. Under the model assumptions, the points should lie\\napproximately on a straight line. For the current case there does not seem to be an extreme\\ndeparture from normality. Drawing a histogram or density plot of the residuals will also\\nhelp to verify the normality assumption. The following code was used.\\nplt.plot(fit.fittedvalues,fit.resid, \\'.\\')\\nplt.xlabel(\"fitted values\")\\nplt.ylabel(\"residuals\")\\nsm.qqplot(fit.resid)\\n155\\n 160\\n 165\\n 170\\n 175\\n 180\\n 185\\n 190\\n 195\\nfitted values\\n25\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n20\\nresiduals\\n3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\nTheoretical Quantiles\\n25\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\n20\\nSample Quantiles\\nFigure 5.8: Left: residuals against fitted values. Right: a qq plot of the residuals. Neither\\nshows clear evidence against the model assumptions of constant variance and normality.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 217, 'page_label': '200'}, page_content='200 Linear Models in Python\\n5.6.6 Variable Selection\\nAmong the large number of possible explanatory variables, we wish to select those which\\nbest explain the observed responses. By eliminating redundant explanatory variables, we\\nreduce the statistical error without increasing the approximation error, and thus reduce the\\n(expected) generalization risk of the learner.\\nIn this section, we briefly present two methods for variable selection. They are illus-\\ntrated on a few variables from the data set birthwt discussed in Section 1.5.3.2. The data☞13\\nset contains information on the birth weights (masses) of babies, as well as various char-\\nacteristics of the mother, such as whether she smokes, her age, etc. We wish to explain\\nthe child’s weight at birth using various characteristics of the mother, her family history,\\nand her behavior during pregnancy. The response variable is weight at birth (quantitative\\nvariable bwt, expressed in grams); the explanatory variables are given below.\\nThe data can be obtained as explained in Section 1.5.3.2, or from statsmodels in the\\nfollowing way:\\nbwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\\nHere is some information about the explanatory variables that we will investigate.\\nage: mother \\'s age in years\\nlwt: mother \\'s weight in lbs\\nrace: mother \\'s race (1 = white, 2 = black, 3 = other)\\nsmoke: smoking status during pregnancy (0 = no, 1 = yes)\\nptl: no. of previous premature labors\\nht: history of hypertension (0 = no, 1 = yes)\\nui: presence of uterine irritability (0 = no, 1 = yes)\\nftv: no. of physician visits during first trimester\\nbwt: birth weight in grams\\nWe can see the structure of the variables via bwt.info(). Check yourself that all\\nvariables are defined as quantitative (int64). However, the variables race, smoke, ht,\\nand ui should really be interpreted as qualitative (factors). To fix this, we could redefine\\nthem with the methodastype, similar to what we did in Chapter 1. Alternatively, we could\\nuse the C() construction in a statsmodels formula to let the program know that certain\\nvariables are factors. We will use the latter approach.\\nFor binary features it does not matter whether the variables are interpreted as\\nfactorial or numerical as the numerical and summary results are identical.\\nWe consider the explanatory variableslwt, age, ui, smoke, ht, and two recoded binary\\nvariables ftv1and ptl1. We define ftv1= 1 if there was at least one visit to a physician,\\nand ftv1= 0 otherwise. Similarly, we defineptl1= 1 if there is at least one preterm birth\\nin the family history, and ptl1= 0 otherwise.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 218, 'page_label': '201'}, page_content='Regression 201\\nftv1 = (bwt[ \\'ftv\\']>=1).astype( int )\\nptl1 = (bwt[ \\'ptl\\']>=1).astype( int )\\n5.6.6.1 Forward Selection and Backward Elimination\\nThe forward selection forward\\nselection\\nmethod is an iterative method for variable selection. In the first\\niteration we consider which feature f1 is the most significant in terms of its P-value in the\\nmodels bwt∼f1, with f1 ∈{lwt,age,... }. This feature is then selected into the model. In\\nthe second iteration, the featuref2 that has the smallest P-value in the modelsbwt∼f1+f2\\nis selected, where f2 , f1, and so on. Usually only features are selected that have a P-\\nvalue of at most 0.05. The following Python program automates this procedure. Instead of\\nselecting on the P-value one could select on the AIC or BIC value.\\nforwardselection.py\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\nbwt = sm.datasets.get_rdataset(\"birthwt\",\"MASS\").data\\nftv1 = (bwt[ \\'ftv\\']>=1).astype( int )\\nptl1 = (bwt[ \\'ptl\\']>=1).astype( int )\\nremaining_features = { \\'lwt\\', \\'age\\', \\'C(ui) \\', \\'smoke \\',\\n\\'C(ht) \\', \\'ftv1 \\', \\'ptl1 \\'}\\nselected_features = []\\nwhile remaining_features:\\nPF = [] #list of (P value , feature)\\nfor f in remaining_features:\\ntemp = selected_features + [f] #temporary list of features\\nformula = \\'bwt~ \\' + \\'+\\'.join(temp)\\nfit = ols(formula,data=bwt).fit()\\npval= fit.pvalues[-1]\\nif pval < 0.05:\\nPF.append((pval,f))\\nif PF: #if not empty\\nPF.sort(reverse=True)\\n(best_pval, best_f) = PF.pop()\\nremaining_features.remove(best_f)\\nprint (\\'feature {} with P-value = {:.2E} \\'.\\nformat (best_f, best_pval))\\nselected_features.append(best_f)\\nelse :\\nbreak\\nfeature C(ui) with P-value = 7.52E-05\\nfeature C(ht) with P-value = 1.08E-02\\nfeature lwt with P-value = 6.01E-03\\nfeature smoke with P-value = 7.27E-03\\nIn backward elimination backward\\nelimination\\nwe start with the complete model (all features included) and\\nat each step, we remove the variable with the highest P-value, as long as it is not significant\\n(greater than 0.05). We leave it as an exercise to verify that the order in which the fea-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 219, 'page_label': '202'}, page_content=\"202 Linear Models in Python\\ntures are removed is: age, ftv1, and ptl1. In this case, forward selection and backward\\nelimination result in the same model, but this need not be the case in general.\\nThis way of model selection has the advantage of being easy to use and of treating the\\nquestion of variable selection in a systematic manner. The main drawback is that variables\\nare included or deleted based on purely statistical criteria, without taking into account the\\naim of the study. This usually leads to a model which may be satisfactory from a statistical\\npoint of view, but in which the variables are not necessarily the most relevant when it comes\\nto understanding and interpreting the data in the study.\\nOf course, we can choose to investigate any combination of features, not just the ones\\nsuggested by the above variable selection methods. For example, let us see if the mother’s\\nweight, her age, her race, and whether she smokes explain the baby’s birthweight.\\nformula = 'bwt~lwt+age+C(race)+ smoke '\\nbwt_model = ols(formula, data=bwt).fit()\\nprint (bwt_model.summary())\\nOLS Regression Results\\n======================================================================\\nDep. Variable: bwt R-squared: 0.148\\nModel: OLS Adj. R-squared: 0.125\\nMethod: Least Squares F-statistic: 6.373\\nNo. Observations: 189 Prob (F-statistic): 1.76e-05\\nDf Residuals: 183 Log-Likelihood: -1498.4\\nDf Model: 5 AIC: 3009.\\nBIC: 3028.\\n=====================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 2839.4334 321.435 8.834 0.000 2205.239 3473.628\\nC(race)[T.2] -510.5015 157.077 -3.250 0.001 -820.416 -200.587\\nC(race)[T.3] -398.6439 119.579 -3.334 0.001 -634.575 -162.713\\nsmoke -401.7205 109.241 -3.677 0.000 -617.254 -186.187\\nlwt 3.9999 1.738 2.301 0.022 0.571 7.429\\nage -1.9478 9.820 -0.198 0.843 -21.323 17.427\\n======================================================================\\nOmnibus: 3.916 Durbin-Watson: 0.458\\nProb(Omnibus): 0.141 Jarque-Bera (JB): 3.718\\nSkew: -0.343 Prob(JB): 0.156\\nKurtosis: 3.038 Cond. No. 899.\\nGiven the result of Fisher’s global test given byProb (F-Statistic)in the summary\\n(P-value = 1.76 ×10−5), we can conclude that at least one of the explanatory variables is\\nassociated with child weight at birth, after adjusting for the other variables. The individual\\nStudent tests indicate that:\\n• the mother’s weight is linearly associated with child weight, after adjusting for age,\\nrace, and smoking status (P-value = 0.022). At the same age, race, and smoking\\nstatus, an increase of one pound in the mother’s weight corresponds to an increase\\nof 4 g in the average child weight at birth;\\n• the age of the mother is not significantly linearly associated with child weight at\\nbirth, when mother weight, race, and smoking status are already taken into account\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 220, 'page_label': '203'}, page_content=\"Regression 203\\n(P-value = 0.843);\\n• weight at birth is significantly lower for a child born to a mother who smokes, com-\\npared to children born to non-smoking mothers of the same age, race, and weight,\\nwith a P-value of 0.00031 (to see this, inspect bwt_model.pvalues). At the same\\nage, race, and mother weight, the child’s weight at birth is 401.720 g less for a\\nsmoking mother than for a non-smoking mother;\\n• regarding the interpretation of the variable race, we note that the first level of this\\ncategorical variable corresponds to white mothers. The estimate of −510.501 g for\\nC(race)[T.2] represents the di fference in the child’s birth weight between black\\nmothers and white mothers (reference group), and this result is significantly different\\nfrom zero (P-value = 0.001) in a model adjusted for the mother’s weight, age, and\\nsmoking status.\\n5.6.6.2 Interaction\\nWe can also include interaction terms in the model. Let us see whether there is any inter-\\naction effect between smokeand agevia the model\\nBwt = β0 + β1age + β2smoke + β3age ×smoke + ε.\\nIn Pythonthis can be done as follows (below we have removed some output):\\nformula = 'bwt~age*smoke '\\nbwt_model = ols(formula, data=bwt).fit()\\nprint (bwt_model.summary())\\nOLS Regression Results\\n======================================================================\\nDep. Variable: bwt R-squared: 0.069\\nModel: OLS Adj. R-squared: 0.054\\nMethod: Least Squares F-statistic: 4.577\\nNo. Observations: 189 Prob (F-statistic): 0.00407\\nDf Residuals: 183 Log-Likelihood: -1506.8\\nDf Model: 5 AIC: 3009.\\nBIC: 3028.\\n======================================================================\\ncoef std err t P>|t| [0.025 0.975]\\n----------------------------------------------------------------------\\nIntercept 2406.1 292.190 8.235 0.000 1829.6 2982.5\\nsmoke 798.2 484.342 1.648 0.101 -157.4 1753.7\\nage 27.7 12.149 2.283 0.024 3.8 51.7\\nage:smoke -46.6 20.447 -2.278 0.024 -86.9 -6.2\\nWe observe that the estimate forβ3 (−46.6) is significantly different from zero (P-value\\n= 0.024). We therefore conclude that the e ffect of the mother’s age on the child’s weight\\ndepends on the smoking status of the mother. The results on association between mother\\nage and child weight must therefore be presented separately for the smoking and the non-\\nsmoking group. For non-smoking mothers ( smoke = 0), the mean child weight at birth\\nincreases on average by 27.7 grams for each year of the mother’s age. This is statistically\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 221, 'page_label': '204'}, page_content='204 Generalized Linear Models\\nsignificant, as can be seen from the 95% confidence intervals for the parameters (which\\ndoes not contain zero):\\nbwt_model.conf_int()\\n0 1\\nIntercept 1829.605754 2982.510194\\nage 3.762780 51.699977\\nsmoke -157.368023 1753.717779\\nage:smoke -86.911405 -6.232425\\nSimilarly, for smoking mothers, there seems to be a decrease in birthweight, bβ1 + bβ3 =\\n27.7 −46.6 = −18.9, but this is not statistically significant; see Exercise 6.\\n5.7 Generalized Linear Models\\nThe normal linear model in Section 2.8 deals with continuous response variables — such\\nas height and crop yield — and continuous or discrete explanatory variables. Given the\\nfeature vectors {xi}, the responses {Yi}are independent of each other, and each has a normal\\ndistribution with mean x⊤\\ni β, where x⊤\\ni is the i-th row of the model matrix X. Generalized\\nlinear models allow for arbitrary response distributions, including discrete ones.\\nDefinition 5.2: Generalized Linear Model\\nIn a generalized linear modelgeneralized\\nlinear model\\n(GLM) the expected response for a given feature vec-\\ntor x = [x1,..., xp]⊤is of the form\\nE[Y |X = x] = h(x⊤β) (5.35)\\nfor some function h, which is called the activation functionactivation\\nfunction\\n. The distribution of\\nY (for a given x) may depend on additional dispersion parameters that model the\\nrandomness in the data that is not explained by x.\\nThe inverse of function h is called the link functionlink function . As for the linear model, (5.35) is\\na model for a single pair ( x,Y). Using the model simplification introduced at the end of\\nSection 5.1, the corresponding model for a whole training set T= {(xi,Yi)}is that the {xi}\\nare fixed and that the {Yi}are independent; each Yi satisfying (5.35) with x = xi. Writing\\nY = [Y1,..., Yn]⊤and defining h as the multivalued function with components h, we have\\nEXY = h(Xβ),\\nwhere X is the (model) matrix with rows x⊤\\n1 ,..., x⊤\\nn . A common assumption is that\\nY1,..., Yn come from the same family of distributions, e.g., normal, Bernoulli, or Pois-\\nson. The central focus is the parameter vector β, which summarizes how the matrix of\\nexplanatory variables X affects the response vector Y. The class of generalized linear mod-\\nels can encompass a wide variety of models. Obviously the normal linear model (2.34) is\\na generalized linear model, with E[Y |X = x] = x⊤β, so that h is the identity function. In\\nthis case, Y ∼N(x⊤β,σ2), i = 1,..., n, where σ2 is a dispersion parameter.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 222, 'page_label': '205'}, page_content='Regression 205\\nExample 5.10 (Logistic Regression) In a logistic regression logistic\\nregression\\nor logit model , we as-\\nsume that the response variables Y1,..., Yn are independent and distributed according to\\nYi ∼Ber(h(x⊤\\ni β)),where h here is defined as the cdf of the logistic distribution logistic\\ndistribution\\n:\\nh(x) = 1\\n1 + e−x .\\nLarge values of x⊤\\ni βthus lead to a high probability that Yi = 1, and small (negative) values\\nof x⊤\\ni βcause Yi to be 0 with high probability. Estimation of the parameter vector βfrom\\nthe observed data is not as straightforward as for the ordinary linear model, but can be\\naccomplished via the minimization of a suitable training loss, as explained below.\\nAs the {Yi}are independent, the pdf of Y = [Y1,..., Yn]⊤is\\ng(y |β,X) =\\nnY\\ni=1\\n[h(x⊤\\ni β)]yi [1 −h(x⊤\\ni β)]1−yi .\\nMaximizing the log-likelihood ln g(y |β,X) with respect to β gives the maximum likeli-\\nhood estimator of β. In a supervised learning framework, this is equivalent to minimizing:\\n−1\\nn ln g(y |β,X) = −1\\nn\\nnX\\ni=1\\nln g(yi |β,xi)\\n= −1\\nn\\nnX\\ni=1\\n\\x02yi ln h(x⊤\\ni β) + (1 −yi) ln(1 −h(x⊤\\ni β))\\x03.\\n(5.36)\\nBy comparing (5.36) with (4.4), we see that we can interpret (5.36) as the cross-entropy ☞ 123\\ntraining loss associated with comparing a true conditional pdf f (y |x) with an approxima-\\ntion pdf g(y |β,x) via the loss function\\nLoss( f (y |x),g(y |β,x)) := −ln g(y |β,x) = −y ln h(x⊤β) −(1 −y) ln(1 −h(x⊤β)).\\nMinimizing (5.36) in terms of βactually constitutes a convex optimization problem. Since\\nln h(x⊤β) = −ln(1 + e−x⊤β) and ln(1 −h(x⊤β)) = −x⊤β−ln(1 + e−x⊤β), the cross-entropy\\ntraining loss (5.36) can be rewritten as\\nrτ(β) := 1\\nn\\nnX\\ni=1\\nh\\n(1 −yi)x⊤\\ni β+ ln\\n\\x10\\n1 + e−x⊤\\ni β\\x11i\\n.\\nWe leave it as Exercise 7 to show that the gradient ∇rτ(β) and Hessian H(β) of rτ(β) are\\ngiven by\\n∇rτ(β) = 1\\nn\\nnX\\ni=1\\n(µi −yi) xi (5.37)\\nand\\nH(β) = 1\\nn\\nnX\\ni=1\\nµi(1 −µi) xi x⊤\\ni , (5.38)\\nrespectively, where µi := h(x⊤\\ni β).\\nNotice that H(β) is a positive semidefinite matrix for all values of β, implying the ☞ 403'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 223, 'page_label': '206'}, page_content='206 Generalized Linear Models\\nconvexity of rτ(β). Consequently, we can find an optimal βefficiently; e.g., via Newton’s\\nmethod. Specifically, given an initial valueβ0, for t = 1,2,..., iteratively compute☞409\\nβt = βt−1 −H−1(βt−1) ∇rτ(βt−1), (5.39)\\nuntil the sequence β0,β1,β2,... is deemed to have converged, using some pre-fixed con-\\nvergence criterion.\\nFigure 5.9 shows the outcomes of 100 independent Bernoulli random variables, where\\neach success probability, (1+exp(−(β0 +β1 x)))−1, depends on x and β0 = −3, β1 = 10. The\\ntrue logistic curve is also shown (dashed line). The minimum training loss curve (red line)\\nis obtained via the Newton scheme (5.39), giving estimates bβ0 = −2.66 and bβ1 = 10.08.\\nThe Python code is given below.\\n-1 -0.5 0 0.5 1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 5.9: Logistic regression data (blue dots), fitted curve (red), and true curve (black\\ndashed).\\nlogreg1d.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom numpy.linalg import lstsq\\nn = 100 # sample size\\nx = (2*np.random.rand(n)-1).reshape(n,1) # explanatory variables\\nbeta = np.array([-3, 10])\\nXmat = np.hstack((np.ones((n,1)), x))\\np = 1/(1 + np.exp(-Xmat @ beta))\\ny = np.random.binomial(1,p,n) # response variables\\n# initial guess\\nbetat = lstsq((Xmat.T @ Xmat),Xmat.T @ y, rcond=None)[0]\\ngrad = np.array([2,1]) # gradient\\nwhile (np. sum (np. abs (grad)) > 1e-5) : # stopping criteria\\nmu = 1/(1+np.exp(-Xmat @ betat))\\n# gradient\\ndelta = (mu - y).reshape(n,1)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 224, 'page_label': '207'}, page_content=\"Regression 207\\ngrad = np. sum (np.multiply( np.hstack((delta,delta)),Xmat), axis\\n=0).T\\n# Hessian\\nH = Xmat.T @ np.diag(np.multiply(mu,(1-mu))) @ Xmat\\nbetat = betat - lstsq(H,grad,rcond=None)[0]\\nprint (betat)\\nplt.plot(x,y, '.') # plot data\\nxx = np.linspace(-1,1,40).reshape(40,1)\\nXXmat = np.hstack( (np.ones(( len (xx),1)), xx))\\nyy = 1/(1 + np.exp(-XXmat @ beta))\\nplt.plot(xx,yy, 'r-') #true logistic curve\\nyy = 1/(1 + np.exp(-XXmat @ betat));\\nplt.plot(xx,yy, 'k--')\\nFurther Reading\\nAn excellent overview of regression is provided in [33] and an accessible mathematical\\ntreatment of linear regression models can be found in [108]. For extensions to nonlinear\\nregression we refer the reader to [7]. A practical introduction to multilevel /hierarchical\\nmodels is given in [47]. For further discussion on regression with discrete responses (clas-\\nsification) we refer to Chapter 7 and the further reading therein. On the important question ☞ 251\\nof how to handle missing data, the classic reference is [80] (see also [85]) and a modern\\napplied reference is [120].\\nExercises\\n1. Following his mentor Francis Galton, the mathematician /statistician Karl Pearson con-\\nducted comprehensive studies comparing hereditary traits between members of the same\\nfamily. Figure 5.10 depicts the measurements of the heights of 1078 fathers and their\\nadult sons (one son per father). The data is available from the book’s GitHub site as\\npearson.csv.\\n(a) Show that sons are on average 1 inch taller than the fathers.\\n(b) We could try to “explain” the height of the son by taking the height of his father and\\nadding 1 inch. The prediction line y = x + 1 (red dashed) is given Figure 5.10. The\\nblack solid line is the fitted regression line. This line has a slope less than 1, and\\ndemonstrates Galton’s “regression” to the average. Find the intercept and slope of the\\nfitted regression line.\\n2. For the simple linear regression model, show that the values for bβ1 and bβ0 that solve the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 225, 'page_label': '208'}, page_content='208 Exercises\\n58 60 62 64 66 68 70 72 74 76\\nHeight Father (in)\\n60\\n65\\n70\\n75Height Son (in)\\nFigure 5.10: A scatterplot of heights from Pearson’s data.\\nequations (5.9) are:\\nbβ1 =\\nPn\\ni=1(xi −x)(yi −y)Pn\\ni=1(xi −x)2 (5.40)\\nbβ0 = y −bβ1 x, (5.41)\\nprovided that not all xi are the same.\\n3. Edwin Hubble discovered that the universe is expanding. If v is a galaxy’s recession ve-\\nlocity (relative to any other galaxy) and d is its distance (from that same galaxy), Hubble’s\\nlaw states that\\nv = Hd,\\nwhere H is known as Hubble’s constant. The following are distance (in millions of light-\\nyears) and velocity (thousands of miles per second) measurements made on five galactic\\nclusters.\\ndistance 68 137 315 405 700\\nvelocity 2.4 4.7 12.0 14.4 26.0\\nState the regression model and estimate H.\\n4. The multiple linear regression model (5.6) can be viewed as a first-order approximation\\nof the general model\\nY = g(x) + ε, (5.42)\\nwhere Eε= 0, Var ε= σ2, and g(x) is some known or unknown function of a d-\\ndimensional vector x of explanatory variables. To see this, replace g(x) with its first-order\\nTaylor approximation around some point x0 and write this as β0 + x⊤β. Express β0 and β\\nin terms of g and x0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 226, 'page_label': '209'}, page_content='Regression 209\\n5. Table 5.6 shows data from an agricultural experiment where crop yield was measured\\nfor two levels of pesticide and three levels of fertilizer. There are three responses for each\\ncombination.\\nTable 5.6: Crop yields for pesticide and fertilizer combinations.\\nFertilizer\\nPesticide Low Medium High\\nNo 3.23, 3.20, 3.16 2.99, 2.85, 2.77 5.72, 5.77, 5.62\\nYes 6.78, 6.73, 6.79 9.07, 9.09, 8.86 8.12, 8.04, 8.31\\n(a) Organize the data in standard form, where each row corresponds to a single meas-\\nurement and the columns correspond to the response variable and the two factor vari-\\nables.\\n(b) Let Yi jk be the response for the k-th replication at level i for factor 1 and level j\\nfor factor 2. To assess which factors best explain the response variable, we use the\\nANOV A model\\nYi jk = µ+ αi + βj + γi j + εi jk, (5.43)\\nwhere P\\ni αi = P\\nj βj = P\\ni γi j = P\\nj γi j = 0. Define β = [µ,α1,α2,β1,β2, β3,γ11,γ12,\\nγ13,γ21,γ22,γ23]⊤. Give the corresponding 18 ×12 model matrix.\\n(c) Note that the parameters are linearly dependent in this case. For example, α2 = −α1\\nand γ13 = −(γ11 + γ12). To retain only 6 linearly independent variables consider the\\n6-dimensional parameter vector eβ= [µ,α1,β1,β2,γ11,γ12]⊤. Find the matrix M such\\nthat Meβ= β.\\n(d) Give the model matrix corresponding to eβ.\\n6. Show that for the birthweight data in Section 5.6.6.2 there is no significant decrease\\nin birthweight for smoking mothers. [Hint: create a new variable nonsmoke = 1−smoke,\\nwhich reverses the encoding for the smoking and non-smoking mothers. Then, the para-\\nmeter β1 + β3 in the original model is the same as the parameter β1 in the model\\nBwt = β0 + β1age + β2nonsmoke + β3age ×nonsmoke + ε.\\nNow find a 95% for β3 and see if it contains zero.]\\n7. Prove (5.37) and (5.38).\\n8. In the Tobit regression Tobit\\nregression\\nmodel with normally distributed errors, the response is modeled\\nas:\\nYi =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nZi, if ui <Zi\\nui, if Zi ⩽ui\\n, Z ∼N(Xβ,σ2In),\\nwhere the model matrix X and the thresholds u1,..., un are given. Typically, ui = 0,i =\\n1,..., n. Suppose we wish to estimate θ:= (β,σ2) via the Expectation–Maximization\\nmethod, similar to the censored data Example 4.2. Let y = [y1,..., yn]⊤ be the vector ☞ 130\\nof observed data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 227, 'page_label': '210'}, page_content='210 Exercises\\n(a) Show that the likelihood of y is:\\ng(y |θ) =\\nY\\ni:yi>ui\\nφσ2 (yi −x⊤\\ni β) ×\\nY\\ni:yi=ui\\nΦ((ui −x⊤\\ni β)/σ),\\nwhere Φ is the cdf of the N(0,1) distribution and φσ2 the pdf of the N(0,σ2) distribu-\\ntion.\\n(b) Let y and y be vectors that collect all yi >ui and yi = ui, respectively. Denote the\\ncorresponding matrix of predictors by X and X, respectively. For each observation\\nyi = ui introduce a latent variable zi and collect these into a vector z. For the same\\nindices i collect the corresponding ui into a vector c. Show that the complete-data\\nlikelihood is given by\\ng(y,z |θ) = 1\\n(2πσ2)n/2 exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−∥y −Xβ∥2\\n2σ2 −∥z −Xβ∥2\\n2σ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f81{z ⩽c}.\\n(c) For the E-step, show that, for a fixed θ,\\ng(z |y,θ) =\\nY\\ni\\ng(zi |y,θ),\\nwhere each g(zi |y,θ) is the pdf of the N((Xβ)i,σ2) distribution, truncated to the in-\\nterval (−∞,ci].\\n(d) For the M-step, compute the expectation of the complete log-likelihood\\n−n\\n2 ln σ2 −n\\n2 ln(2π) −∥y −Xβ∥2\\n2σ2 −E∥Z −Xβ∥2\\n2σ2 .\\nThen, derive the formulas forβand σ2 that maximize the expectation of the complete\\nlog-likelihood.\\n9. Dowload data set WomenWage.csvfrom the book’s website. This data set is a tidied-up\\nversion of the women’s wages data set from [91]. The first column of the data ( hours) is\\nthe response variable Y. It shows the hours spent in the labor force by married women in\\nthe 1970s. We want to understand what factors determine the participation rate of women\\nin the labor force. The predictor variables are:\\nTable 5.7: Features for the women’s wage data set.\\nFeature Description\\nkidslt6 Number of children younger than 6 years.\\nkidsge6 Number of children older than 6 years.\\nage Age of the married woman.\\neduc Number of years of formal education.\\nexper Number of years of “work experience”.\\nnwifeinc Non-wife income, that is, the income of the husband.\\nexpersq The square of exper, to capture any nonlinear relationships.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 228, 'page_label': '211'}, page_content='Regression 211\\nWe observe that some of the responses areY = 0, that is, some women did not particip-\\nate in the labor force. For this reason, we model the data using the Tobit regression model,\\nin which the response Y is given as:\\nYi =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nZi, if Zi >0\\n0, if Zi ⩽0 , Z ∼N(Xβ,σ2In).\\nWith θ= (β,σ2), the likelihood of the data y = [y1,..., yn]⊤is:\\ng(y |θ) = Q\\ni:yi>0 φσ2 (yi −x⊤\\ni β) ×Q\\ni:yi=0 Φ((ui −x⊤\\ni β)/σ),\\nwhere Φ is the standard normal cdf. In Exercise 8, we derived the EM algorithm for max-\\nimizing the log-likelihood.\\n(a) Write down the EM algorithm in pseudo code as it applies to this Tobit regression.\\n(b) Implement the EM algorithm pseudo code in Python. Comment on which factor you\\nthink is important in determining the labor participation rate of women living in the\\nUSA in the 1970s.\\n10. Let P be a projection matrix. Show that the diagonal elements ofP all lie in the interval\\n[0,1]. In particular, for P = XX+ in Theorem 5.1, the leverage value pi := Pii satisfies\\n0 ⩽pi ⩽1 for all i.\\n11. Consider the linear model Y = Xβ+ εin (5.8), with X being the n ×p model matrix\\nand ε having expectation vector 0 and covariance matrix σ2In. Suppose that bβ−i is the\\nleast-squares estimate obtained by omitting the i-th observation, Yi; that is,\\nbβ−i = argmin\\nβ\\nX\\nj,i\\n(Yj −x⊤\\nj β)2,\\nwhere x⊤\\nj is the j-th row of X. Let bY−i = x⊤\\ni\\nbβ−i be the corresponding fitted value atxi. Also,\\ndefine Bi as the least-squares estimator of βbased on the response data\\nY(i) := [Y1,..., Yi−1,bY−i,Yi+1,..., Yn]⊤.\\n(a) Prove that bβ−i = Bi; that is, the linear model obtained from fitting all responses except\\nthe i-th is the same as the one obtained from fitting the data Y(i).\\n(b) Use the previous result to verify that\\nYi −bY−i = (Yi −bYi)/(1 −Pii),\\nwhere P = XX+ is the projection matrix onto the columns of X. Hence, deduce the\\nPRESS formula in Theorem 5.1. ☞ 174\\n12. Take the linear model Y = Xβ+ ε,where X is an n ×p model matrix, ε = 0, and\\nCov(ε) = σ2In. Let P = XX+ be the projection matrix onto the columns of X.\\n(a) Using the properties of the pseudo-inverse (see Definition A.2), show that PP⊤= P. ☞ 360'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 229, 'page_label': '212'}, page_content='212 Exercises\\n(b) Let E = Y −bY be the (random) vector of residuals, where bY = PY. Show that the i-th\\nresidual has a normal distribution with expectation 0 and varianceσ2(1 −Pii) (that is,\\nσ2 times 1 minus the i-th leverage).\\n(c) Show that σ2 can be unbiasedly estimated via\\nS 2 := 1\\nn −p∥Y −bY∥2 = 1\\nn −p∥Y −Xbβ∥2. (5.44)\\n[Hint: use the cyclic property of the trace as in Example 2.3.]\\n13. Consider a normal linear model Y = Xβ+ ε, where X is an n ×p model matrix and\\nε ∼N(0,σ2In). Exercise 12 shows that for any such model the i-th standardized residual\\nEi/(σ√1 −Pii) has a standard normal distribution. This motivates the use of the leverage\\nPii to assess whether the i-th observation is an outlier depending on the size of the i-th\\nresidual relative to √1 −Pii. A more robust approach is to include an estimate for σusing\\nall data except the i-th observation. This gives rise to the studentized residualstudentized\\nresidual\\nTi, defined\\nas\\nTi := Ei\\nS −i\\n√1 −Pii\\n,\\nwhere S −i is an estimate of σ obtained by fitting all the observations except the i-th and\\nEi = Yi −bYi is the i-th (random) residual. Exercise 12 shows that we can take, for example,\\nS 2\\n−i = 1\\nn −1 −p∥Y−i −X−ibβ−i∥2, (5.45)\\nwhere X−i is the model matrix X with the i-th row removed, is an unbiased estimator of\\nσ2. We wish to compute S 2\\n−i efficiently, using S 2 in (5.44), as the latter will typically be\\navailable once we have fitted the linear model. To this end, define ui as the i-th unit vector\\n[0,..., 0,1,0,..., 0]⊤, and let\\nY(i) := Y −(Yi −bY−i)ui = Y − Ei\\n1 −Pii\\nui,\\nwhere we have used the fact that Yi −bY−i = Ei/(1 −Pii), as derived in the proof of The-\\norem 5.1. Now apply Exercise 11 to prove that\\nS 2\\n−i = (n −p) S 2 −E2\\ni /(1 −Pii)\\nn −p −1 .\\n14. Using the notation from Exercises 11–13, Cook’s distanceCook’s distance for observation i is defined\\nas\\nDi := ∥bY −bY\\n(i)\\n∥2\\np S2 .\\nIt measures the change in the fitted values when thei-th observation is removed, relative to\\nthe residual variance of the model (estimated via S 2).\\nBy using similar arguments as those in Exercise 13, show that\\nDi = Pii E2\\ni\\n(1 −Pii)2 p S2 .\\nIt follows that there is no need to “omit and refit” the linear model in order to compute\\nCook’s distance for the i-th response.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 230, 'page_label': '213'}, page_content='Regression 213\\n15. Prove that if we add an additional feature to the general linear model, then R2, the\\ncoefficient of determination, is necessarily non-decreasing in value and hence cannot be\\nused to compare models with different numbers of predictors.\\n16. Let X := [X1,..., Xn]⊤ and µ := [µ1,...,µ n]⊤. In the fundamental Theorem C.9, we\\nuse the fact that ifXi ∼N(µi,1), i = 1,..., n are independent, then ∥X∥2 has (per definition)\\na noncentral χ2\\nn distribution. Show that ∥X∥2 has moment generating function\\net∥µ∥2/(1−2t)\\n(1 −2t)n/2 , t <1/2,\\nand so the distribution of ∥X∥2 depends on µonly through the norm ∥µ∥.\\n17. Carry out a logistic regression analysis on a (partial) wine data set classification prob-\\nlem. The data can be loaded using the following code.\\nfrom sklearn import datasets\\nimport numpy as np\\ndata = datasets.load_wine()\\nX = data.data[:, [9,10]]\\ny = np.array(data.target==1,dtype=np.uint)\\nX = np.append(np.ones( len (X)).reshape(-1,1),X,axis=1)\\nThe model matrix has three features, including the constant feature. Instead of using\\nNewton’s method (5.39) to estimate β, implement a simple gradient descent procedure\\nβt = βt−1 −α∇rτ(βt−1),\\nwith learning rate α= 0.0001, and run it for 106 steps. Your procedure should deliver three\\ncoefficients; one for the intercept and the rest for the explanatory variables. Solve the same\\nproblem using the Logit method of statsmodels.api and compare the results.\\n18. Consider again Example 5.10, where we train the learner via the Newton iteration\\n(5.39). If X⊤:= [x1,..., xn] defines the matrix of predictors and µt := h(Xβt), then the ☞ 206\\ngradient (5.37) and Hessian (5.38) for Newton’s method can be written as:\\n∇rτ(βt) = 1\\nnX⊤(µt −y) and H(βt) = 1\\nnX⊤DtX,\\nwhere Dt := diag(µt ⊙(1 −µt)) is a diagonal matrix. Show that the Newton iteration (5.39)\\ncan be written as the iterative reweighted least-squares iterative\\nreweighted\\nleast squares\\nmethod:\\nβt = argmin\\nβ\\n(eyt−1 −Xβ)⊤Dt−1(eyt−1 −Xβ),\\nwhere eyt−1 := Xβt−1 + D−1\\nt−1(y −µt−1) is the so-called adjusted response. [Hint: use the fact\\nthat (M⊤M)−1M⊤z is the minimizer of ∥Mβ−z∥2.]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 231, 'page_label': '214'}, page_content='214 Exercises\\n19. In multi-output linear regressionmulti-output\\nlinear\\nregression\\n, the response variable is a real-valued vector of di-\\nmension, say, m. Similar to (5.8), the model can be written in matrix notation:\\nY = XB +\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nε⊤\\n1\\n...\\nε⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nwhere:\\n• Y is an n ×m matrix of n independent responses (stored as row vectors of length m);\\n• X is the usual n ×p model matrix;\\n• B is an p ×m matrix of model parameters;\\n• ε1,..., εn ∈Rm are independent error terms with Eε= 0 and Eεε⊤= Σ.\\nWe wish to learn the matrix parameters B and Σ from the training set {Y,X}. To this end,\\nconsider minimizing the training loss:\\n1\\nntr\\n\\x10\\n(Y −XB) Σ−1 (Y −XB)⊤\\x11\\n,\\nwhere tr(·) is the trace of a matrix.☞357\\n(a) Show that the minimizer of the training loss, denoted bB, satisfies the normal equa-\\ntions:\\nX⊤X bB = X⊤Y.\\n(b) Noting that\\n(Y −XB)⊤(Y −XB) =\\nnX\\ni=1\\nεiε⊤\\ni ,\\nexplain why\\nbΣ := (Y −XbB)⊤(Y −XbB)\\nn\\nis a method-of-moments estimator of Σ, just like the one given in (5.10).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 232, 'page_label': '215'}, page_content='CHAPTER 6\\nREGULARIZATION AND KERNEL\\nMETHODS\\nThe purpose of this chapter is to familiarize the reader with two central concepts\\nin modern data science and machine learning: regularization and kernel methods. Reg-\\nularization provides a natural way to guard against overfitting and kernel methods of-\\nfer a broad generalization of linear models. Here, we discuss regularized regression\\n(ridge, lasso) as a bridge to the fundamentals of kernel methods. We introduce repro-\\nducing kernel Hilbert spaces and show that selecting the best prediction function in\\nsuch spaces is in fact a finite-dimensional optimization problem. Applications to spline\\nfitting, Gaussian process regression, and kernel PCA are given.\\n6.1 Introduction\\nIn this chapter we return to the supervised learning setting of Chapter 5 (regression) and ex-\\npand its scope. Given training data τ= {(x1,y1),..., (xn,yn)}, we wish to find a prediction\\nfunction (the learner) gτ that minimizes the (squared-error) training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(yi −g(xi))2\\nwithin a class of functions G. As noted in Chapter 2, ifGis the set of all possible functions\\nthen choosing any function g with the property thatg(xi) = yi for all i will give zero training\\nloss, but will likely have poor generalization performance (that is, suffer from overfitting).\\nRecall from Theorem 2.1 that the best possible prediction function (over all g) for ☞ 21\\nthe squared-error risk E(Y −g(X))2 is given by g∗(x) = E[Y |X = x]. The class Gshould\\nbe simple enough to permit theoretical understanding and analysis but, at the same time,\\nrich enough to contain the optimal function g∗ (or a function close to g∗). This ideal can\\nbe realized by taking Gto be a Hilbert space Hilbert space(i.e., a complete inner product space) of\\nfunctions; see Appendix A.7. ☞ 384\\nMany of the classes of functions that we have encountered so far are in fact Hilbert\\nspaces. In particular, the set Gof linear functions on Rp is a Hilbert space. To see this,\\n215'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 233, 'page_label': '216'}, page_content='216 Regularization\\nidentify with each element β ∈Rp the linear function gβ : x 7→x⊤βand define the inner\\nproduct on Gas ⟨gβ,gγ⟩:= β⊤γ. In this way, Gbehaves in exactly the same way as (is\\nisomorphic to) the space Rp equipped with the Euclidean inner product (dot product). The☞360\\nlatter is a Hilbert space, because it is completecomplete\\nvector space\\nwith respect to the Euclidean norm. See\\nExercise 12 for a further discussion.\\nLet us now turn to our “running” polynomial regression Example 2.1, where the feature☞26\\nvector x = [1,u,u2,..., up−1]⊤ =: ϕ(u) is itself a vector-valued function of another feature\\nu. Then, the space of functions hβ : u 7→ϕ(u)⊤βis a Hilbert space, through the identifica-\\ntion hβ ≡β. In fact, this is true for any feature mapping ϕ: u 7→[ϕ1(u),...,ϕ p(u)]⊤.\\nThis can be further generalized by considering feature maps u 7→κu, where each κufeature maps\\nis a real-valued function v 7→κu(v) on the feature space. As we shall soon see (in Sec-\\ntion 6.3), functions of the form u 7→P∞\\ni=1 βiκvi (u) live in a Hilbert space of functions called\\na reproducing kernel Hilbert space (RKHS).RKHS In Section 6.3 we introduce the notion of a\\nRKHS formally, give specific examples, including the linear and Gaussian kernels, and de-\\nrive various useful properties, the most important of which is the representer Theorem 6.6.\\nApplications of such spaces include the smoothing splines (Section 6.6), Gaussian pro-☞235\\ncess regression (Section 6.7), kernel PCA (Section 6.8), and support vector machines for\\nclassification (Section 7.7).☞269\\nThe RKHS formalism also makes it easier to treat the important topic ofregularization.regularization\\nThe aim of regularization is to improve the predictive performance of the best learner in\\nsome class of functions Gby adding a penalty term to the training loss that penalizes\\nlearners that tend to overfit the data. In the next section we introduce the main ideas behind\\nregularization, which then segues into a discussion of kernel methods in the subsequent\\nsections.\\n6.2 Regularization\\nLet Gbe the Hilbert space of functions over which we search for the minimizer, gτ, of the\\ntraining loss ℓτ(g). Often, the Hilbert space Gis rich enough so that we can find a learner\\ngτ within Gsuch that the training loss is zero or close to zero. Consequently, if the space of\\nfunctions Gis sufficiently rich, we run the risk of overfitting. One way to avoid overfitting\\nis to restrict attention to a subset of the space Gby introducing a non-negative functional\\nJ : G→ R+ which penalizes complex models (functions). In particular, we want to find\\nfunctions g ∈G such that J(g) <c for some “regularization” constant c >0. Thus we can\\nformulate the quintessential supervised learning problem as:\\nmin {ℓτ(g) : g ∈G,J(g) <c}, (6.1)\\nthe solution (argmin) of which is our learner. When this optimization problem is convex, it\\ncan be solved by first obtaining the Lagrangian dual function\\nL∗(λ) := min\\ng∈G\\n{ℓτ(g) + λ(J(g) −c)},\\nand then maximizing L∗(λ) with respect to λ⩾0; see Section B.2.3.☞407\\nIn order to introduce the overall ideas of kernel methods and regularization, we will\\nproceed by exploring (6.1) in the special case of ridge regressionridge\\nregression\\n, with the following run-\\nning example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 234, 'page_label': '217'}, page_content='Regularization and Kernel Methods 217\\nExample 6.1 (Ridge Regression) Ridge regression is simply linear regression with a\\nsquared-norm penalty functional (also called a regularization function, or regularizer regularizer).\\nSuppose we have a training set τ= {(xi,yi),i = 1,..., n}, with each xi ∈Rp and we use a\\nsquared-norm penalty with regularization parameter regularization\\nparameter\\nγ> 0. Then, the problem is to solve\\nmin\\ng∈G\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 + γ∥g∥2, (6.2)\\nwhere Gis the Hilbert space of linear functions on Rp. As explained in Section 6.1, we\\ncan identify each g ∈G with a vector β∈Rp and, consequently, ∥g∥2 = ⟨β,β⟩= ∥β∥2. The\\nabove functional optimization problem is thus equivalent to the parametric optimization\\nproblem\\nmin\\nβ∈Rp\\n1\\nn\\nnX\\ni=1\\n\\x00yi −x⊤\\ni β\\x012 + γ∥β∥2, (6.3)\\nwhich, in the notation of Chapter 5, further simplifies to\\nmin\\nβ∈Rp\\n1\\nn ∥y −Xβ∥2 + γ∥β∥2. (6.4)\\nIn other words, the solution to (6.2) is of the form x 7→x⊤β∗, where β∗ solves (6.3) (or\\nequivalently (6.4)). Observe that asγ→∞, the regularization term becomes dominant and\\nconsequently the optimal g becomes identically zero.\\nThe optimization problem in (6.4) is convex, and by multiplying by the constant n/2\\nand setting the gradient equal to zero, we obtain\\nX⊤(Xβ−y) + n γβ= 0. (6.5)\\nIf γ = 0 these are simply the normal equations, albeit written in a slightly di fferent form. ☞ 28\\nIf the matrix X⊤X + n γIp is invertible (which is the case for any γ >0; see Exercise 13),\\nthen the solution to these modified normal equations is\\nbβ= (X⊤X + n γIp)−1X⊤y.\\nWhen using regularization with respect to some Hilbert spaceG, it is sometimes useful\\nto decompose Ginto two orthogonal subspaces, Hand Csay, such that every g ∈G can\\nbe uniquely written as g = h + c, with h ∈H, c ∈C, and ⟨h,c⟩= 0. Such a Gis said to be\\nthe direct sum direct sumof Cand H, and we write G= H⊕C. Decompositions of this form become\\nuseful when functions in Hare penalized but functions in Care not. We illustrate this\\ndecomposition with the ridge regression example where one of the features is a constant\\nterm, which we do not wish to penalize.\\nExample 6.2 (Ridge Regression (cont.)) Suppose one of the features in Example 6.1\\nis the constant 1, which we do not wish to penalize. The reason for this is to ensure that\\nwhen γ → ∞, the optimal g becomes the “constant” model, g(x) = β0, rather than the\\n“zero” model, g(x) = 0. Let us alter the notation slightly by considering the feature vectors\\nto be of the formex = [1,x⊤]⊤, where x = [x1,..., xp]⊤. We thus have p + 1 features, rather'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 235, 'page_label': '218'}, page_content='218 Regularization\\nthan p. Let Gbe the space of linear functions of ex. Each linear function g of ex can be\\nwritten as g : ex 7→β0 + x⊤β, which is the sum of the constant function c : ex 7→β0 and\\nh : ex 7→x⊤β. Moreover, the two functions are orthogonal with respect to the inner product\\non G: ⟨c,h⟩= [β0,0⊤][0,β⊤]⊤= 0, where 0 is a column vector of zeros.\\nAs subspaces of G, both Cand Hare again Hilbert spaces, and their inner products and\\nnorms follow directly from the inner product on G. For example, each function h : ex 7→\\nx⊤βin Hhas norm ∥h∥H = ∥β∥, and the constant function c : ex 7→β0 in Chas norm |β0|.\\nThe modification of the regularized optimization problem (6.2) where the constant term\\nis not penalized can now be written as\\nmin\\ng∈H⊕C\\n1\\nn\\nnX\\ni=1\\n(yi −g(exi))2 + γ∥g∥2\\nH, (6.6)\\nwhich further simplifies to\\nmin\\nβ0,β\\n1\\nn ∥y −β01 −Xβ∥2 + γ∥β∥2, (6.7)\\nwhere 1 is the n×1 vector of 1s. Observe that, in this case, asγ→∞ the optimal g tends to\\nthe sample mean y of the {yi}; that is, we obtain the “default” regression model, without ex-\\nplanatory variables. Again, this is a convex optimization problem, and the solution follows\\nfrom\\nX⊤(β01 + Xβ−y) + n γβ= 0, (6.8)\\nwith\\nn β0 = 1⊤(y −Xβ). (6.9)\\nThis results in solving for βfrom\\n(X⊤X −n−1X⊤11⊤X + n γIp)β= (X⊤−n−1X⊤11⊤)y, (6.10)\\nand determining β0 from (6.9).\\nAs a precursor to the kernel methods in the following sections, let us assume thatn ⩾p\\nand that X has full (column) rank p. Then any vector β∈Rp can be written as a linear\\ncombination of the feature vectors {xi}; that is, as linear combinations of the columns of\\nthe matrix X⊤. In particular, letβ= X⊤α, where α= [α1,...,α n]⊤∈Rn. In this case (6.10)\\nreduces to\\n(XX⊤−n−111⊤XX⊤+ n γIn)α= (In −n−111⊤)y.\\nAssuming invertibility of (XX⊤−n−111⊤XX⊤+ n γIn), we have the solution\\nbα= (XX⊤−n−111⊤XX⊤+ n γIn)−1(In −n−111⊤)y,\\nwhich depends on the training feature vectors {xi}only through the n ×n matrix of inner\\nproducts: XX⊤ = [⟨xi,xj⟩]. This matrix is called the Gram matrixGram matrix of the {xi}. From (6.9),\\nthe solution for the constant term is bβ0 = n−11⊤(y −XX⊤bα). It follows that the learner is a\\nlinear combination of inner products {⟨xi,x⟩}plus a constant:\\ngτ(ex) = bβ0 + x⊤X⊤bα= bβ0 +\\nnX\\ni=1\\nbαi ⟨xi,x⟩,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 236, 'page_label': '219'}, page_content='Regularization and Kernel Methods 219\\nwhere the coefficients bβ0 and bαi only depend on the inner products {⟨xi,xj⟩}. We will see\\nshortly that the representer Theorem 6.6 generalizes this result to a broad class of regular- ☞ 231\\nized optimization problems.\\nWe illustrate in Figure 6.1 how the solutions of the ridge regression problems appearing\\nin Examples 6.1 and 6.2 are qualitatively a ffected by the regularization parameter γfor a\\nsimple linear regression model. The data was generated from the modelyi = −1.5 +0.5xi +\\nεi, i = 1,..., 100, where each xi is drawn independently and uniformly from the interval\\n[0,10] and each εi is drawn independently from the standard normal distribution.\\n. = 0:1\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n. = 1 . = 10\\n-2 0 2\\n-0\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n-2 0 2\\n-0\\n-2 0 2\\n-0\\nFigure 6.1: Ridge regression solutions for a simple linear regression problem. Each panel\\nshows contours of the loss function (log scale) and the effect of the regularization parameter\\nγ ∈{0.1,1,10}, appearing in (6.4) and (6.7). Top row: both terms are penalized. Bottom\\nrow: only the non-constant term is penalized. Penalized (plus) and unpenalized (diamond)\\nsolutions are shown in each case.\\nThe contours are those of the squared-error loss (actually the logarithm thereof), which\\nis minimized with respect to the model parameters β0 and β1. The diamonds all repres-\\nent the same minimizer of this loss. The plusses show each minimizer [ β∗\\n0,β∗\\n1]⊤ of the\\nregularized minimization problems (6.4) and (6.7) for three choices of the regularization\\nparameter γ. For the top three panels the regularization involves both β0 and β1, through\\nthe squared norm β2\\n0 + β2\\n1. The circles show the points that have the same squared norm as'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 237, 'page_label': '220'}, page_content='220 Regularization\\nthe optimal solution. For the bottom three panels only β1 is regularized; there, horizontal\\nlines indicate vectors [β0,β1]⊤for which |β1|= |β∗\\n1|.\\nThe problem of ridge regression discussed in Example 6.2 boils down to solving a\\nproblem of the form in (6.7), involving a squared 2-norm penalty ∥β∥2. A natural ques-\\ntion to ask is whether we can replace the squared 2-norm penalty by a di fferent penalty\\nterm. Replacing it with a 1-norm gives the lasso (least absolute shrinkage and selection☞408\\nlasso operator). The lasso equivalent of the ridge regression problem (6.7) is thus:\\nmin\\nβ0,β\\n1\\nn ∥y −β01 −Xβ∥2 + γ∥β∥1, (6.11)\\nwhere ∥β∥1 = Pp\\ni=1 |βi|.\\nThis is again a convex optimization problem. Unlike ridge regression, the lasso gener-\\nally does not have an explicit solution, and so numerical methods must be used to solve it.\\nNote that the problem (6.11) is of the form\\nmin\\nx,z\\nf (x) + g(z)\\nsubject to Ax + Bz = c,\\n(6.12)\\nwith x := [β0,β⊤]⊤, z := β, A := [0p,Ip], B := −Ip, and c := 0p (vector of zeros), and\\nconvex functions f (x) := 1\\nn ∥y −[1n,X] x ∥2 and g(z) := γ∥z∥1. There exist e fficient al-\\ngorithms for solving such problems, including the alternating direction method of mul-\\ntipliers (ADMM) [17]. We refer to Example B.11 for details on this algorithm.☞416\\nWe repeat the examples from Figure 6.1, but now using lasso regression and taking\\nthe square roots of the previous regularization parameters. The results are displayed in\\nFigure 6.2.\\n. =\\np\\n0:1\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n. = 1 . =\\np\\n10\\n-2 0 2\\n-0\\n-2\\n-1\\n0\\n1\\n2\\n-1\\n-2 0 2\\n-0\\n-2 0 2\\n-0\\nFigure 6.2: Lasso regression solutions. Compare with Figure 6.1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 238, 'page_label': '221'}, page_content='Regularization and Kernel Methods 221\\nOne advantage of using the lasso regularization is that the resulting optimal parameter\\nvector often has several components that are exactly 0. For example, in the top middle\\nand right panels of Figure 6.2, the optimal solution lies exactly at a corner point of the\\nsquare {[β0,β1]⊤ : |β0|+ |β1|= |β∗\\n0|+ |β∗\\n1|}; in this case β∗\\n0 = 0. For statistical models with\\nmany parameters, the lasso can provide a methodology for model selection. Namely, as the\\nregularization parameter increases (or, equivalently, as theL1 norm of the optimal solution\\ndecreases), the solution vector will have fewer and fewer non-zero parameters. By plotting\\nthe values of the parameters for eachγor L1 one obtains the so-called regularization paths regularization\\npaths(also called homotopy paths or coefficient profiles) for the variables. Inspection of such\\npaths may help assess which of the model parameters are relevant to explain the variability\\nin the observed responses {yi}.\\nExample 6.3 (Regularization Paths) Figure 6.3 shows the regularization paths forp =\\n60 coefficients from a multiple linear regression model ☞ 169\\nYi =\\n60X\\nj=1\\nβj xi j + εi, i = 1,..., 150,\\nwhere βj = 1 for j = 1,..., 10 and βj = 0 for j = 11,..., 60. The error terms {εi}are inde-\\npendent and standard normal. The explanatory variables{xi j}were independently generated\\nfrom a standard normal distribution. As it is clear from the figure, the estimates of the 10\\nnon-zero coefficients are first selected, as the L1 norm of the solutions increases. By the\\ntime the L1 norm reaches around 4, all 10 variables for which βj = 1 have been correctly\\nidentified and the remaining 50 parameters are estimated as exactly 0. Only after the L1\\nnorm reaches around 8, will these “spurious” parameters be estimated to be non-zero. For\\nthis example, the regularization parameter γvaried from 10−4 to 10.\\n0 5 10 15\\nL1 norm\\n-0.5\\n0\\n0.5\\n1\\n1.5\\nb-\\nFigure 6.3: Regularization paths for lasso regression solutions as a function of theL1 norm\\nof the solutions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 239, 'page_label': '222'}, page_content='222 Reproducing Kernel Hilbert Spaces\\n6.3 Reproducing Kernel Hilbert Spaces\\nIn this section, we formalize the idea outlined at the end of Section 6.1 of extending finite\\ndimensional feature maps to those that are functions by introducing a special type of Hil-\\nbert space of functions known as a reproducing kernel Hilbert space (RKHS). Although\\nthe theory extends naturally to Hilbert spaces of complex-valued functions, we restrict\\nattention to Hilbert spaces of real-valued functions here.\\nTo evaluate the loss of a learnerg in some class of functionsG, we do not need to expli-\\ncitly construct g — rather, it is only required that we can evaluateg at all the feature vectors\\nx1,..., xn of the training set. A defining property of an RKHS is that function evaluation\\nat a point x can be performed by simply taking the inner product of g with some feature\\nfunction κx associated with x. We will see that this property becomes particularly useful\\nin light of the representer theorem (see Section 6.5), which states that the learner g itself☞230\\ncan be represented as a linear combination of the set of feature functions{κxi ,i = 1,..., n}.\\nConsequently, we can evaluate a learner g at the feature vectors {xi}by taking linear com-\\nbinations of terms of the form κ(xi,xj) = ⟨κxi ,κxj ⟩G. Collecting these inner products into\\na matrix K = [κ(xi,xj),i, j = 1,..., n] (the Gram matrix of the {κxi }), we will see that the\\nfeature vectors {xi}only enter the loss minimization problem through K.\\nDefinition 6.1: Reproducing Kernel Hilbert Space\\nFor a non-empty set X, a Hilbert space Gof functions g : X→ Rwith inner product\\n⟨·,·⟩Gis called a reproducing kernel Hilbert spacereproducing\\nkernel Hilbert\\nspace\\n(RKHS) with reproducing kernel\\nκ: X×X→ Rif:\\n1. for every x ∈X, κx := κ(x,·) is in G,\\n2. κ(x,x) <∞for all x ∈X,\\n3. for every x ∈X and g ∈G, g(x) = ⟨g,κx⟩G.\\nThe reproducing kernel of a Hilbert space of functions, if it exists, is unique; see Exer-\\ncise 2. The main (third) condition in Definition 6.1 is known as the reproducing propertyreproducing\\nproperty\\n.\\nThis property allows us to evaluate any functiong ∈G at a point x ∈X by taking the inner\\nproduct of g and κx; as such, κx is called the representer of evaluation. Further, by taking\\ng = κx′ and applying the reproducing property, we have⟨κx′,κx⟩G= κ(x′,x), and so by sym-\\nmetry of the inner product it follows thatκ(x,x′) = κ(x′,x). As a consequence, reproducing\\nkernels are necessarily symmetric functions. Moreover, a reproducing kernelκis a positive\\nsemidefinitepositive\\nsemidefinite\\nfunction, meaning that for every n ⩾1 and every choice of α1,...,α n ∈Rand\\nx1,..., xn ∈X, it holds that\\nnX\\ni=1\\nnX\\nj=1\\nαi κ(xi,xj) αj ⩾0. (6.13)\\nIn other words, every Gram matrix K associated with κ is a positive semidefinite matrix;\\nthat is α⊤Kα⩾0 for all α. The proof is addressed in Exercise 1.\\nThe following theorem gives an alternative characterization of an RKHS. The proof\\nuses the Riesz representation Theorem A.17. Also note that in the theorem below we could☞390'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 240, 'page_label': '223'}, page_content='Regularization and Kernel Methods 223\\nhave replaced the word “bounded” with “continuous”, as the two are equivalent for linear\\nfunctionals; see Theorem A.16.\\nTheorem 6.1: Continuous Evaluation Functionals Characterize a RKHS\\nAn RKHS Gon a set Xis a Hilbert space in which every evaluation functional evaluation\\nfunctionalδx : g 7→g(x) is bounded. Conversely, a Hilbert space Gof functions X→ Rfor\\nwhich every evaluation functional is bounded is an RKHS.\\nProof: Note that, since evaluation functionals δx are linear operators, showing bounded-\\nness is equivalent to showing continuity. Given an RKHS with reproducing kernel κ, sup-\\npose that we have a sequence gn ∈G converging to g ∈G, that is ∥gn −g∥G→0. We apply\\nthe Cauchy–Schwarz inequality (Theorem A.15) and the reproducing property of κto find ☞ 389\\nthat for every x ∈X and any n:\\n|δxgn −δxg|= |gn(x) −g(x)|= |⟨gn −g,κx⟩G|⩽∥gn −g∥G∥κx∥G= ∥gn −g∥G\\np\\n⟨κx,κx⟩G\\n= ∥gn −g∥G\\np\\nκ(x,x).\\nNoting that √κ(x,x) <∞by definition for every x ∈X, and that ∥gn −g∥G→0 as n →∞,\\nwe have shown continuity of δx, that is |δxgn −δxg|→ 0 as n →∞ for every x ∈X.\\nConversely, suppose that evaluation functionals are bounded. Then from the Riesz\\nrepresentation Theorem A.17, there exists some gδx ∈G such that δxg = ⟨g,gδx ⟩G for all\\ng ∈G — the representer of evaluation. If we defineκ(x,x′) = gδx (x′) for all x,x′∈X, then\\nκx := κ(x,·) = gδx is an element of Gfor every x ∈X and ⟨g,κx⟩G= δxg = g(x), so that the\\nreproducing property in Definition 6.1 is verified. □\\nThe fact that an RKHS has continuous evaluation functionals means that if two func-\\ntions g,h ∈G are “close” with respect to ∥·∥ G, then their evaluations g(x),h(x) are close\\nfor every x ∈X. Formally, convergence in ∥·∥ G norm implies pointwise convergence for\\nall x ∈X.\\nThe following theorem shows that any finite function κ : X×X→ Rcan serve as a\\nreproducing kernel as long as it is finite, symmetric, and positive semidefinite. The cor-\\nresponding (unique!) RKHS Gis the completion of the set of all functions of the formPn\\ni=1 αi κxi where αi ∈Rfor all i = 1,..., n.\\nTheorem 6.2: Moore–Aronszajn\\nGiven a non-empty set Xand any finite symmetric positive semidefinite function\\nκ : X×X→ R, there exists an RKHS Gof functions g : X→ Rwith reproducing\\nkernel κ. Moreover, Gis unique.\\nProof: (Sketch) As the proof of uniqueness is treated in Exercise 2, the objective is to\\nprove existence. The idea is to construct a pre-RKHSG0 from the given function κthat has\\nthe essential structure and then to extend G0 to an RKHS G.\\nIn particular, define G0 as the set of finite linear combinations of functions κx, x ∈X:\\nG0 :=\\n\\x1a\\ng =\\nnX\\ni=1\\nαi κxi\\n\\x0c\\x0c\\x0c\\x0c\\x0c x1,..., xn ∈X, αi ∈R, n ∈N\\n\\x1b\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 241, 'page_label': '224'}, page_content='224 Construction of Reproducing Kernels\\nDefine on G0 the following inner product:\\n⟨f,g⟩G0 :=\\n* nX\\ni=1\\nαi κxi ,\\nmX\\nj=1\\nβj κx′\\nj\\n+\\nG0\\n:=\\nnX\\ni=1\\nmX\\nj=1\\nαi βj κ(xi,x′\\nj).\\nThen G0 is an inner product space. In fact,G0 has the essential structure we require, namely\\nthat (i) evaluation functionals are bounded /continuous (Exercise 4) and (ii) Cauchy se-\\nquences in G0 that converge pointwise also converge in norm (see Exercise 5).\\nWe then enlarge G0 to the set Gof all functions g : X→ Rfor which there exists a\\nCauchy sequence in G0 converging pointwise to g and define an inner product on Gas the\\nlimit\\n⟨f,g⟩G:= lim\\nn→∞\\n⟨fn,gn⟩G0 , (6.14)\\nwhere fn →f and gn →g. To show thatGis an RKHS it remains to be shown that (1) this\\ninner product is well defined; (2) evaluation functionals remain bounded; and (3) the space\\nGis complete. A detailed proof is established in Exercises 6 and 7. □\\n6.4 Construction of Reproducing Kernels\\nIn this section we describe various ways to construct a reproducing kernel κ : X×X→\\nR for some feature space X. Recall that κ needs to be a finite, symmetric, and positive\\nsemidefinite function (that is, it satisfies (6.13)). In view of Theorem 6.2, specifying the\\nspace Xand a reproducing kernel κ : X×X→ Rcorresponds to uniquely specifying an\\nRKHS.\\n6.4.1 Reproducing Kernels via Feature Mapping\\nPerhaps the most fundamental way to construct a reproducing kernel κ is via a feature\\nmap ϕ : X→ Rp. We define κ(x,x′) : = ⟨ϕ(x),ϕ(x′)⟩, where ⟨, ⟩denotes the Euclidean\\ninner product. The function is clearly finite and symmetric. To verify that κ is positive\\nsemidefinite, let Φ be the matrix with rows ϕ(x1)⊤,..., ϕ(xn)⊤and let α= [α1,...,α n]⊤ ∈\\nRn. Then,\\nnX\\ni=1\\nnX\\nj=1\\nαi κ(xi,xj) αj =\\nnX\\ni=1\\nnX\\nj=1\\nαi ϕ⊤(xi) ϕ(xj) αj = α⊤ΦΦ⊤α= ∥Φ⊤α∥2 ⩾0.\\nExample 6.4 (Linear Kernel) Taking the identity feature map ϕ(x) = x on X= Rp,\\ngives the linear kernellinear kernel\\nκ(x,x′) = ⟨x,x′⟩= x⊤x′.\\nAs can be seen from the proof of Theorem 6.2, the RKHS of functions corresponding to\\nthe linear kernel is the space of linear functions on Rp. This space is isomorphic to Rp\\nitself, as discussed in the introduction (see also Exercise 12).\\nIt is natural to wonder whether a given kernel function corresponds uniquely to a feature\\nmap. The answer is no, as we shall see by way of example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 242, 'page_label': '225'}, page_content='Regularization and Kernel Methods 225\\nExample 6.5 (Feature Maps and Kernel Functions) Let X= Rand consider feature\\nmaps ϕ1 : X→ Rand ϕ2 : X→ R2, with ϕ1(x) := x and ϕ2(x) := [x,x]⊤/\\n√\\n2. Then\\nκϕ1 (x,x′) = ⟨ϕ1(x),ϕ1(x′)⟩= xx′,\\nbut also\\nκϕ2 (x,x′) = ⟨ϕ2(x),ϕ2(x′)⟩= xx′.\\nThus, we arrive at the same kernel function defined for the same underlying set Xvia two\\ndifferent feature maps.\\n6.4.2 Kernels from Characteristic Functions\\nAnother way to construct reproducing kernels on X= Rp makes use of the properties of\\ncharacteristic functions. In particular, we have the following result. We leave its proof as ☞ 441\\nExercise 10.\\nTheorem 6.3: Reproducing Kernel from a Characteristic Function\\nLet X ∼µ be an Rp-valued random vector that is symmetric about the origin (that\\nis, X and −X are identically distributed), and let ψ be its characteristic function:\\nψ(t) = Eeit⊤X =\\nR\\neit⊤x µ(dx) for t ∈Rp. Then κ(x,x′) := ψ(x −x′) is a valid repro-\\nducing kernel on Rp.\\nExample 6.6 (Gaussian Kernel) The multivariate normal distribution with mean vec-\\ntor 0 and covariance matrix b2 Ip is clearly symmetric around the origin. Its characteristic\\nfunction is\\nψ(t) = exp\\n \\n−1\\n2b2 ∥t∥2\\n!\\n, t ∈Rp.\\nTaking b2 = 1/σ2, this gives the popular Gaussian kernel Gaussian\\nkernel\\non Rp:\\nκ(x,x′) = exp\\n \\n−1\\n2\\n∥x −x′∥2\\nσ2\\n!\\n. (6.15)\\nThe parameter σ is sometimes called the bandwidth bandwidth. Note that in the machine learning\\nliterature, the Gaussian kernel is sometimes referred to as “the” radial basis function (rbf)\\nkernel radial basis\\nfunction (rbf)\\nkernel\\n.1\\nFrom the proof of Theorem 6.2, we see that the RKHS Gdetermined by the Gaussian\\nkernel κis the space of pointwise limits of functions of the form\\ng(x) =\\nnX\\ni=1\\nαi exp\\n \\n−1\\n2\\n∥x −xi∥2\\nσ2\\n!\\n.\\nWe can think of each pointxi having a feature κxi that is a scaled multivariate Gaussian pdf\\ncentered at xi.\\n1The term radial basis function is sometimes used more generally to mean kernels of the formκ(x,x′) =\\nf (∥x −x′∥) for some function f : R→R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 243, 'page_label': '226'}, page_content='226 Construction of Reproducing Kernels\\nExample 6.7 (Sinc Kernel) The characteristic function of a Uniform[−1,1] random\\nvariable (which is symmetric around 0) isψ(t) = sinc(t) := sin(t)/t, so κ(x,x′) = sinc(x−x′)\\nis a valid kernel.\\nInspired by kernel density estimation (Section 4.4), we may be tempted to use the pdf☞131\\nof a random variable that is symmetric about the origin to construct a reproducing kernel.\\nHowever, doing so will not work in general, as the next example illustrates.\\nExample 6.8 (Uniform pdf Does not Construct a Valid Reproducing Kernel) Take\\nthe function ψ(t) = 1\\n2 1{|t|⩽1}, which is the pdf of X ∼Uniform[−1,1]. Unfortunately, the\\nfunction κ(x,x′) = ψ(x −x′) is not positive semidefinite, as can be seen for example by\\nconstructing the matrix A = [κ(ti,tj),i, j = 1,2,3] for the points t1 = 0, t2 = 0.75, and\\nt3 = 1.5 as follows:\\nA =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nψ(0) ψ(−0.75) ψ(−1.5)\\nψ(0.75) ψ(0) ψ(−0.75)\\nψ(1.5) ψ(0.75) ψ(0)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n0.5 0 .5 0\\n0.5 0 .5 0 .5\\n0 0 .5 0 .5\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nThe eigenvalues of A are {1/2 −√1/2,1/2,1/2 + √1/2}≈{− 0.2071,0.5,1.2071}and so\\nby Theorem A.9, A is not a positive semidefinite matrix, since it has a negative eigenvalue.☞367\\nConsequently, κis not a valid reproducing kernel.\\nOne of the reasons why the Gaussian kernel (6.15) is popular is that it enjoys the uni-\\nversal approximation propertyuniversal\\napproximation\\nproperty\\n[88]: the space of functions spanned by the Gaussian kernel\\nis dense in the space of continuous functions with support Z⊂ Rp. Naturally, this is a\\ndesirable property especially if there is little prior knowledge about the properties of g∗.\\nHowever, note that every function g in the RKHS Gassociated with a Gaussian kernel κis\\ninfinitely differentiable. Moreover, a Gaussian RKHS does not contain non-zero constant\\nfunctions. Indeed, if A ⊂Z is non-empty and open, then the only function of the form\\ng(x) = c 1{x ∈A}contained in Gis the zero function (c = 0).\\nConsequently, if it is known that g is differentiable only to a certain order, one may\\nprefer the Matérn kernelMat´ern kernel with parameters ν,σ> 0:\\nκν(x,x′) = 21−ν\\nΓ(ν)\\n\\x10√\\n2ν∥x −x′∥/σ\\n\\x11ν\\nKν\\n\\x10√\\n2ν∥x −x′∥/σ\\n\\x11\\n, (6.16)\\nwhich gives functions that are (weakly) di fferentiable to order ⌊ν⌋(but not necessarily to\\norder ⌈ν⌉). Here, Kν denotes the modified Bessel function of the second kind; see (4.49).\\nThe particular form of the Matérn kernel appearing in (6.16) ensures that limν→∞κν(x,x′) =☞164\\nκ(x,x′), where κis the Gaussian kernel appearing in (6.15).\\nWe remark that Sobolev spaces are closely related to the Matérn kernel. Up to constants\\n(which scale the unit ball in the space), in dimension p and for a parameter s > p/2, these\\nspaces can be identified with ψ(t) = 21−s\\nΓ(s) ∥t∥s−p/2Kp/2−s(∥t∥), which in turn can be viewed as\\nthe characteristic function corresponding to the (radially symmetric) multivariate Student’s\\nt distribution with s degrees of freedom: that is, with pdf f (x) ∝(1 + ∥x∥2)−s.☞162'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 244, 'page_label': '227'}, page_content='Regularization and Kernel Methods 227\\n6.4.3 Reproducing Kernels Using Orthonormal Features\\nWe have seen in Sections 6.4.1 and 6.4.2 how to construct reproducing kernels from feature\\nmaps and characteristic functions. Another way to construct kernels on a spaceXis to work\\ndirectly from the function class L2(X; µ); that is, the set of square-integrable 2 functions\\non Xwith respect to µ; see also Definition A.4. For simplicity, in what follows, we will ☞ 385\\nconsider µto be the Lebesgue measure, and will simply write L2(X) rather than L2(X; µ).\\nWe will also assume that X⊆ Rp.\\nLet {ξ1,ξ2,... }be an orthonormal basis of L2(X) and let c1,c2,... be a sequence of\\npositive numbers. As discussed in Section 6.4.1, the kernel corresponding to a feature map\\nϕ: X→ Rp is κ(x,x′) = ϕ(x)⊤ϕ(x′) = Pp\\ni=1 ϕi(x) ϕi(x′). Now consider a (possibly infinite)\\nsequence of feature functions ϕi = ci ξi,i = 1,2,... and define\\nκ(x,x′) :=\\nX\\ni⩾1\\nϕi(x) ϕi(x′) =\\nX\\ni⩾1\\nλi ξi(x) ξi(x′), (6.17)\\nwhere λi = c2\\ni ,i = 1,2,... . This is well-defined as long as P\\ni⩾1 λi < ∞, which we assume\\nfrom now on. Let Hbe the linear space of functions of the form f = P\\ni⩾1 αiξi, whereP\\ni⩾1 α2\\ni /λi <∞. As every function f ∈L2(X) can be represented as f = P\\ni⩾1⟨f,ξi⟩ξi, we\\nsee that His a linear subspace of L2(X). On Hdefine the inner product\\n⟨f,g⟩H :=\\nX\\ni⩾1\\n⟨f,ξi⟩⟨g,ξi⟩\\nλi\\n.\\nWith this inner product, the squared norm of f = P\\ni⩾1 αi ξi is ∥f ∥2\\nH = P\\ni⩾1 α2\\ni /λi < ∞.\\nWe show that His actually an RKHS with kernel κby verifying the conditions of Defini-\\ntion 6.1. First,\\nκx =\\nX\\ni⩾1\\nλi ξi(x) ξi ∈H,\\nas P\\ni λi < ∞by assumption, and so κ is finite. Second, the reproducing property holds.\\nNamely, let f = P\\ni⩾1 αi ξi. Then,\\n⟨κx, f ⟩H =\\nX\\ni⩾1\\n⟨κx,ξi⟩⟨f,ξi⟩\\nλi\\n=\\nX\\ni⩾1\\nλi ξi(x) αi\\nλi\\n=\\nX\\ni⩾1\\nαiξi(x) = f (x).\\nThe discussion above demonstrates that kernels can be constructed via (6.17). In fact,\\n(under mild conditions) any given reproducing kernel κcan be written in the form (6.17),\\nwhere this series representation enjoys desirable convergence properties. This result is\\nknown as Mercer’s theorem, and is given below. We leave the full proof including the\\nprecise conditions to, e.g., [40], but the main idea is that a reproducing kernel κ can be\\nthought of as a generalization of a positive semidefinite matrix K, and can also be writ-\\nten in spectral form (see also Section A.6.5). In particular, by Theorem A.9, we can write ☞ 367\\nK = VDV⊤, where V is a matrix of orthonormal eigenvectors [ vℓ] and D the diagonal\\nmatrix of the (positive) eigenvalues [λℓ]; that is,\\nK(i, j) =\\nX\\nℓ⩾1\\nλℓ vℓ(i) vℓ( j).\\n2A function f : X→ Ris said to be square-integrable if\\nR\\nf 2(x) µ(dx) <∞, where µis a measure on X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 245, 'page_label': '228'}, page_content='228 Construction of Reproducing Kernels\\nIn (6.18) below, x,x′play the role of i, j, and ξℓ plays the role of vℓ.\\nTheorem 6.4: Mercer\\nLet κ : X×X → R be a reproducing kernel for a compact set X ⊂Rp. Then\\n(under mild conditions) there exists a countable sequence of non-negative numbers\\n{λℓ}decreasing to zero and functions {ξℓ}orthonormal in L2(X) such that\\nκ(x,x′) =\\nX\\nℓ⩾1\\nλℓ ξℓ(x) ξℓ(x′) , for all x,x′∈X, (6.18)\\nwhere (6.18) converges absolutely and uniformly on X×X.\\nFurther, if λℓ >0, then (λℓ,ξℓ) is an (eigenvalue, eigenfunction) pair for the integral\\noperator K : L2(X) →L2(X) defined by [K f](x) :=\\nR\\nXκ(x,y) f (y) dy for x ∈X.\\nTheorem 6.4 holds if (i) the kernel κis continuous on X×X , (ii) the function eκ(x) :=\\nκ(x,x) defined for x ∈X is integrable. Extensions of Theorem 6.4 to more general spaces\\nXand measures µhold; see, e.g., [115] or [40].\\nThe key importance of Theorem 6.4 lies in the fact that the series representation (6.18)\\nconverges absolutely and uniformly onX×X. The uniform convergence is a much stronger\\ncondition than pointwise convergence, and means for instance that properties of the se-\\nquence of partial sums, such as continuity and integrability, are transferred to the limit.\\nExample 6.9 (Mercer) Suppose X= [−1,1] and the kernel is κ(x,x′) = 1 + xx′which\\ncorresponds to the RKHS Gof affine functions from X →R. To find the (eigenvalue,\\neigenfunction) pairs for the integral operator appearing in Theorem 6.4, we need to find\\nnumbers {λℓ}and orthonormal functions {ξℓ(x)}that solve\\nZ 1\\n−1\\n(1 + xx′) ξℓ(x′) dx′= λℓ ξℓ(x) , for all x ∈[−1,1].\\nConsider first a constant functionξ1(x) = c. Then, for allx ∈[−1,1], we have that 2c = λ1c,\\nand the normalization condition requires that\\nR 1\\n−1 c2 dx = 1. Together, these giveλ1 = 2 and\\nc = ±1/\\n√\\n2. Next, consider an affine function ξ2(x) = a + bx. Orthogonality requires that\\nZ 1\\n−1\\nc(a + bx) dx = 0,\\nwhich implies a = 0 (since c , 0). Moreover, the normalization condition then requires\\nZ 1\\n−1\\nb2 x2 dx = 1,\\nor, equivalently, 2b2/3 = 1, implying b = ±√3/2. Finally, the integral equation reads\\nZ 1\\n−1\\n(1 + xx′) bx′dx′= λ2 bx ⇐⇒ 2bx\\n3 = λ2bx,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 246, 'page_label': '229'}, page_content='Regularization and Kernel Methods 229\\nimplying that λ2 = 2/3. We take the positive solutions (i.e.,c >0 and b >0), and note that\\nλ1 ξ1(x) ξ1(x′) + λ2 ξ2(x) ξ2(x′) = 2 1√\\n2\\n1√\\n2\\n+ 2\\n3\\n√\\n3√\\n2\\nx\\n√\\n3√\\n2\\nx′= 1 + xx′= κ(x,x′),\\nand so we have found the decomposition appearing in (6.18). As an aside, observe that ξ1\\nand ξ2 are orthonormal versions of the first two Legendre polynomials. The corresponding ☞ 387\\nfeature map can be explicitly identified as ϕ1(x) =\\n√\\nλ1 ξ1(x) = 1 and ϕ2(x) = √λ2 ξ2(x) =\\nx.\\n6.4.4 Kernels from Kernels\\nThe following theorem lists some useful properties for constructing reproducing kernels\\nfrom existing reproducing kernels.\\nTheorem 6.5: Rules for Constructing Kernels from Other Kernels\\n1. If κ : Rp ×Rp →Ris a reproducing kernel and ϕ : X→ Rp is a function, then\\nκ(ϕ(x),ϕ(x′)) is a reproducing kernel from X×X→ R.\\n2. If κ : X×X→ R is a reproducing kernel and f : X→ R+ is a function, then\\nf (x)κ(x,x′) f (x′) is also a reproducing kernel from X×X→ R.\\n3. If κ1 and κ2 are reproducing kernels from X×X→ R, then so is their sum κ1 + κ2.\\n4. If κ1 and κ2 are reproducing kernels from X×X→ R, then so is their product\\nκ1κ2.\\n5. If κ1 and κ2 are reproducing kernels from X×X → R and Y×Y → R re-\\nspectively, then κ+((x,y),(x′,y′)) := κ1(x,x′) + κ2(y,y′) and κ×((x,y),(x′,y′)) :=\\nκ1(x,x′)κ2(y,y′) are reproducing kernels from (X×Y) ×(X×Y) →R.\\nProof: For Rules 1, 2, and 3 it is easy to verify that the resulting function is finite, sym-\\nmetric, and positive semidefinite, and so is a valid reproducing kernel by Theorem 6.2.\\nFor example, for Rule 1 we have Pn\\ni=1\\nPn\\nj=1 αi κ(yi,yj)αj ⩾0 for every choice of {αi}n\\ni=1\\nand {yi}n\\ni=1 ∈Rp, since κis a reproducing kernel. In particular, it holds true for yi = ϕ(xi),\\ni = 1,..., n. Rule 4 is easy to show for kernelsκ1,κ2 that admit a representation of the form\\n(6.17), since\\nκ1(x,x′) κ2(x,x′) =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\ni⩾1\\nϕ(1)\\ni (x) ϕ(1)\\ni (x′)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX\\nj⩾1\\nϕ(2)\\nj (x) ϕ(2)\\nj (x′)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n=\\nX\\ni,j⩾1\\nϕ(1)\\ni (x) ϕ(2)\\nj (x) ϕ(1)\\ni (x′) ϕ(2)\\nj (x′)\\n=\\nX\\nk⩾1\\nϕk(x) ϕk(x′) =: κ(x,x′),\\nshowing that κ= κ1κ2 also admits a representation of the form (6.17), where the new (pos-\\nsibly infinite) sequence of features (ϕk) is identified in a one-to-one way with the sequence\\n(ϕ(1)\\ni ϕ(2)\\nj ). We leave the proof of rule 5 as an exercise (Exercise 8). □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 247, 'page_label': '230'}, page_content='230 Representer Theorem\\nExample 6.10 (Polynomial Kernel) Consider x,x′∈R2 with\\nκ(x,x′) = (1 + ⟨x,x′⟩)2,\\nwhere ⟨x,x′⟩= x⊤x′. This is an example of a polynomial kernelpolynomial\\nkernel\\n. Combining the fact that\\nsums and products of kernels are again kernels (rules 3 and 4 of Theorem 6.5), we find that,\\nsince ⟨x,x′⟩and the constant function 1 are kernels, so are 1+ ⟨x,x′⟩and (1 + ⟨x,x′⟩)2. By\\nwriting\\nκ(x,x′) = (1 + x1 x′\\n1 + x2 x′\\n2)2\\n= 1 + 2x1 x′\\n1 + 2x2 x′\\n2 + 2x1 x2 x′\\n1 x′\\n2 + (x1 x′\\n1)2 + (x2 x′\\n2)2,\\nwe see that κ(x,x′) can be written as the inner product inR6 of the two feature vectorsϕ(x)\\nand ϕ(x′), where the feature map ϕ: R2 →R6 can be explicitly identified as\\nϕ(x) = [1,\\n√\\n2x1,\\n√\\n2x2,\\n√\\n2x1 x2,x2\\n1,x2\\n2]⊤.\\nThus, the RKHS determined by κ can be explicitly identified with the space of functions\\nx 7→ϕ(x)⊤βfor some β∈R6.\\nIn the above example we could explicitly identify the feature map. However, in general\\na feature map need not be explicitly available. Using a particular reproducing kernel cor-\\nresponds to using an implicit (possibly infinite dimensional!) feature map that never needs\\nto be explicitly computed.\\n6.5 Representer Theorem\\nRecall the setting discussed at the beginning of this chapter: we are given training data\\nτ = {(xi,yi)}n\\ni=1 and a loss function that measures the fit to the data, and we wish to find\\na function g that minimizes the training loss, with the addition of a regularization term,\\nas described in Section 6.2. To do this, we assume first that the class Gof prediction\\nfunctions can be decomposed as the direct sum of an RKHSH, defined by a kernel function\\nκ: X×X→ R, and another linear space of real-valued functions H0 on X; that is,\\nG= H⊕H 0,\\nmeaning that any element g ∈G can be written as g = h + h0, with h ∈H and h0 ∈H0.\\nIn minimizing the training loss we wish to penalize the h term of g but not the h0 term.\\nSpecifically, the aim is to solve the functional optimization problem\\nmin\\ng∈H⊕H0\\n1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) + γ∥g∥2\\nH. (6.19)\\nHere, we use a slight abuse of notation: ∥g∥H means ∥h∥H if g = h + h0, as above. In this\\nway, we can viewH0 as the null space of the functional g 7→∥g∥H. This null space may be\\nempty, but typically has a small dimensionm; for example it could be the one-dimensional\\nspace of constant functions, as in Example 6.2.☞217'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 248, 'page_label': '231'}, page_content='Regularization and Kernel Methods 231\\nExample 6.11 (Null Space) Consider again the setting of Example 6.2, for which we\\nhave feature vectorsex = [1,x⊤]⊤and Gconsists of functions of the formg : ex 7→β0 + x⊤β.\\nEach function g can be decomposed as g = h + h0, where h : ex 7→x⊤β, and h0 : ex 7→β0.\\nGiven g ∈G, we have∥g∥H = ∥β∥, and so the null spaceH0 of the functional g 7→∥g∥H\\n(that is, the set of all functions g ∈G for which ∥g∥H = 0) is the set of constant functions\\nhere, which has dimension m = 1.\\nRegularization favors elements in H0 and penalizes large elements in H. As the reg-\\nularization parameter γ varies between zero and infinity, solutions to (6.19) vary from\\n“complex” (g ∈H⊕H 0) to “simple” (g ∈H0).\\nA key reason why RKHSs are so useful is the following. By choosing Hto be an\\nRKHS in (6.19) this functional optimization problem e ffectively becomes a parametric\\noptimization problem. The reason is that any solution to (6.19) can be represented as a\\nfinite-dimensional linear combination of kernel functions, evaluated at the training sample.\\nThis is known as the kernel trick kernel trick.\\nTheorem 6.6: Representer Theorem\\nThe solution to the penalized optimization problem (6.19) is of the form\\ng(x) =\\nnX\\ni=1\\nαi κ(xi,x) +\\nmX\\nj=1\\nηj qj(x), (6.20)\\nwhere {q1,..., qm}is a basis of H0.\\nProof: Let F= Span \\x08κxi ,i = 1,..., n\\t. Clearly, F ⊆H. Then, the Hilbert space Hcan\\nbe represented as H= F⊕F ⊥, where F⊥ is the orthogonal complement of F. In other\\nwords, F⊥is the class of functions\\n{f ⊥∈H : ⟨f ⊥, f ⟩H = 0, f ∈F}≡{ f ⊥: ⟨f ⊥,κxi ⟩H = 0, ∀i}.\\nIt follows, by the reproducing kernel property, that for all f ⊥∈F⊥:\\nf ⊥(xi) = ⟨f ⊥,κxi ⟩H = 0, i = 1,..., n.\\nNow, take any g ∈H⊕H 0, and write it as g = f + f ⊥+ h0, with f ∈F, f ⊥∈F⊥, and\\nh0 ∈H0. By the definition of the null space H0, we have ∥g∥2\\nH = ∥f + f ⊥∥2\\nH. Moreover, by\\nPythagoras’ theorem, the latter is equal to ∥f ∥2\\nH + ∥f ⊥∥2\\nH. It follows that\\n1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) + γ∥g∥2\\nH = 1\\nn\\nnX\\ni=1\\nLoss(yi, f (xi) + h0(xi)) + γ\\n\\x10\\n∥f ∥2\\nH + ∥f ⊥∥2\\nH\\n\\x11\\n⩾1\\nn\\nnX\\ni=1\\nLoss(yi, f (xi) + h0(xi)) + γ∥f ∥2\\nH.\\nSince we can obtain equality by taking f ⊥= 0, this implies that the minimizer of the pen-\\nalized optimization problem (6.19) lies in the subspaceF⊕H 0 of G= H⊕H0, and hence\\nis of the form (6.20). □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 249, 'page_label': '232'}, page_content='232 Representer Theorem\\nSubstituting the representation (6.20) of g into (6.19) gives the finite-dimensional op-\\ntimization problem:\\nmin\\nα∈Rn,η∈Rm\\n1\\nn\\nnX\\ni=1\\nLoss(yi,(Kα+ Qη)i) + γα⊤Kα, (6.21)\\nwhere\\n• K is the n ×n (Gram) matrix with entries [κ(xi,xj),i = 1,..., n, j = 1,..., n].\\n• Q is the n ×m matrix with entries [qj(xi),i = 1,..., n, j = 1,..., m].\\nIn particular, for the squared-error loss we have\\nmin\\nα∈Rn,η∈Rm\\n1\\nn\\n\\r\\r\\r y −(Kα+ Qη)\\n\\r\\r\\r\\n2\\n+ γα⊤Kα. (6.22)\\nThis is a convex optimization problem, and its solution is found by di fferentiating (6.22)\\nwith respect to αand ηand equating to zero, leading to the following system of ( n + m)\\nlinear equations: \"KK⊤+ n γK KQ\\nQ⊤K⊤ Q⊤Q\\n#\"α\\nη\\n#\\n=\\n\"K⊤\\nQ⊤\\n#\\ny. (6.23)\\nAs long as Q is of full column rank, the minimizing function is unique.\\nExample 6.12 (Ridge Regression (cont.)) We return to Example 6.2 and identify that\\nHis the RKHS with linear kernel functionκ(x,x′) = x⊤x′and C= H0 is the linear space of\\nconstant functions. In this case,H0 is spanned by the functionq1 ≡1. Moreover,K = XX⊤\\nand Q = 1.\\nIf we appeal to the representer theorem directly, then the problem in (6.6) becomes, as\\na result of (6.21):\\nmin\\nα,η0\\n1\\nn\\n\\r\\r\\r y −η0 1 −XX⊤α\\n\\r\\r\\r\\n2\\n+ γ∥X⊤α∥2.\\nThis is a convex optimization problem, and so the solution follows by taking derivatives\\nand setting them to zero. This gives the equations\\nXX⊤\\x00(XX⊤+ n γIn) α+ η0 1 −y\\x01 = 0,\\nand\\nn η0 = 1⊤(y −XX⊤α).\\nNote that these are equivalent to (6.8) and (6.9) (once again assuming thatn ⩾p and X has\\nfull rank p). Equivalently, the solution is found by solving (6.23):\\n\"XX⊤XX⊤+ n γXX⊤ XX⊤1\\n1⊤XX⊤ n\\n#\" α\\nη0\\n#\\n=\\n\"XX⊤\\n1⊤\\n#\\ny.\\nThis is a system of (n + 1) linear equations, and is typically of much larger dimension than\\nthe (p + 1) linear equations given by (6.8) and (6.9). As such, one may question the prac-\\nticality of reformulating the problem in this way. However, the benefit of this formulation\\nis that the problem can be expressed entirely through the Gram matrix K, without having\\nto explicitly compute the feature vectors — in turn permitting the (implicit) use of infinite\\ndimensional feature spaces.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 250, 'page_label': '233'}, page_content='Regularization and Kernel Methods 233\\nExample 6.13 (Estimating the Peaks Function) Figure 6.4 shows the surface plot of\\nthe peaks function:\\nf (x1,x2) = 3(1 −x1)2e−x2\\n1−(x2+1)2\\n−10\\n\\x12x1\\n5 −x3\\n1 −x5\\n2\\n\\x13\\ne−x2\\n1−x2\\n2 −1\\n3e−(x1+1)2−x2\\n2 . (6.24)\\nThe goal is to learn the function y = f (x) based on a small set of training data (pairs of\\n(x,y) values). The red dots in the figure represent dataτ= {(xi,yi)}20\\ni=1, where yi = f (xi) and\\nthe {xi}have been chosen in a quasi-random quasi-randomway, using Hammersley points (with bases 2\\nand 3) on the square [−3,3]2. Quasi-random point sets have better space-filling properties\\nthan either a regular grid of points or a set of pseudo-random points. We refer to [71] for\\ndetails. Note that there is no observation noise in this particular problem.\\n-5\\n2-2\\n0\\n00\\n5\\n-22\\nFigure 6.4: Peaks function sampled at 20 Hammersley points.\\nThe purpose of this example is to illustrate how, using the small data set of sizen = 20,\\nthe entire peaks function can be approximated well using kernel methods. In particular, we\\nuse the Gaussian kernel (6.15) on R2, and denote by Hthe unique RKHS corresponding\\nto this kernel. We omit the regularization term in (6.19), and thus our objective is to find\\nthe solution to\\nmin\\ng∈H\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2.\\nBy the representer theorem, the optimal function is of the form\\ng(x) =\\nnX\\ni=1\\nαi exp\\n \\n−1\\n2\\n∥x −xi∥2\\nσ2\\n!\\n,\\nwhere α:= [α1,...,α n]⊤is, by (6.23), the solution to the set of linear equations KK⊤α=\\nKy.\\nNote that we are performing regression over the class of functions Hwith an implicit\\nfeature space. Due to the representer theorem, the solution to this problem coincides with\\nthe solution to the linear regression problem for which the i-th feature (for i = 1,..., n) is\\nchosen to be the vector [κ(x1,xi),...,κ (xn,xi)]⊤.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 251, 'page_label': '234'}, page_content=\"234 Representer Theorem\\nThe following code performs these calculations and gives the contour plots of g and\\nthe peaksfunctions, shown in Figure 6.5. We see that the two are quite close. Code for the\\ngeneration of Hammersley points is available from the book’s GitHub site asgenham.py.\\npeakskernel.py\\nfrom genham import hammersley\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfrom matplotlib import cm\\nfrom numpy.linalg import norm\\nimport numpy as np\\ndef peaks(x,y):\\nz = (3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2)\\n- 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2)\\n- 1/3 * np.exp(-(x+1)**2 - y**2))\\nreturn (z)\\nn = 20\\nx = -3 + 6*hammersley([2,3],n)\\nz = peaks(x[:,0],x[:,1])\\nxx, yy = np.mgrid[-3:3:150j,-3:3:150j]\\nzz = peaks(xx,yy)\\nplt.contour(xx,yy,zz,levels=50)\\nfig=plt.figure()\\nax = fig.add_subplot(111,projection= '3d')\\nax.plot_surface(xx,yy,zz,rstride=1,cstride=1,color= 'c',alpha=0.3,\\nlinewidth=0)\\nax.scatter(x[:,0],x[:,1],z,color= 'k',s=20)\\nplt.show()\\nsig2 = 0.3 # kernel parameter\\ndef k(x,u):\\nreturn (np.exp(-0.5*norm(x- u)**2/sig2))\\nK = np.zeros((n,n))\\nfor i in range (n):\\nfor j in range (n):\\nK[i,j] = k(x[i,:],x[j])\\nalpha = np.linalg.solve(K@K.T, K@z)\\nN, = xx.flatten().shape\\nKx = np.zeros((n,N))\\nfor i in range (n):\\nfor j in range (N):\\nKx[i,j] = k(x[i,:],np.array([xx.flatten()[j],yy.flatten()[j\\n]]))\\ng = Kx.T @ alpha\\ndim = np.sqrt(N).astype( int )\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 252, 'page_label': '235'}, page_content='Regularization and Kernel Methods 235\\nyhat = g.reshape(dim,dim)\\nplt.contour(xx,yy,yhat,levels=50)\\n-2 0 2\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-2 0 2\\nFigure 6.5: Contour plots for the prediction function g (left) and the peaks function given\\nin (6.24) (right).\\n6.6 Smoothing Cubic Splines\\nA striking application of kernel methods is to fitting “well-behaved” functions to data.\\nKey examples of “well-behaved” functions are those that do not have large second-\\norder derivatives. Consider functionsg : [0,1] →Rthat are twice differentiable and define\\n∥g′′∥2 :=\\nR 1\\n0 (g′′(x))2 dx as a measure of the size of the second derivative.\\nExample 6.14 (Behavior of ∥g′′∥2) Intuitively, the larger ∥g′′∥2 is, the more “wiggly”\\nthe function g will be. As an explicit example, considerg(x) = sin(ωx) for x ∈[0,1], where\\nωis a free parameter. We can explicitly computeg′′(x) = −ω2 sin(ωx), and consequently\\n∥g′′∥2 =\\nZ 1\\n0\\nω4 sin2(ωx) dx = ω4\\n2 (1 −sinc(2ω)) .\\nAs |ω|→∞ , the frequency of g increases and we have ∥g′′∥2 →∞.\\nNow, in the context of data fitting, consider the following penalized least-squares op-\\ntimization problem on [0,1]:\\nmin\\ng∈G\\n1\\nn\\nnX\\ni=1\\n(yi −g(xi))2 + γ∥g′′∥2, (6.25)\\nwhere we will specify Gin what follows. In order to apply the kernel machinery, we want\\nto write this in the form (6.19), for some RKHSHand null space H0. Clearly, the norm on\\nHshould be of the form ∥g∥H = ∥g′′∥and should be well-defined (i.e., finite and ensuring\\ng and g′are absolutely continuous). This suggests that we take\\nH= {g ∈L2[0,1] : ∥g′′∥<∞,g,g′absolutely continuous, g(0) = g′(0) = 0},'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 253, 'page_label': '236'}, page_content='236 Smoothing Cubic Splines\\nwith inner product\\n⟨f,g⟩H :=\\nZ 1\\n0\\nf ′′(x) g′′(x) dx.\\nOne rationale for imposing the boundary conditions g(0) = g′(0) = 0 is as follows: when\\nexpanding g about the point x = 0, Taylor’s theorem (with integral remainder term) states\\nthat\\ng(x) = g(0) + g′(0) x +\\nZ x\\n0\\ng′′(s) (x −s) ds.\\nImposing the condition that g(0) = g′(0) = 0 for functions in Hwill ensure that G=\\nH⊕H 0 where the null space H0 contains only linear functions, as we will see.\\nTo see that this His in fact an RKHS, we derive its reproducing kernel. Using integra-\\ntion by parts (or directly from the Taylor expansion above), write\\ng(x) =\\nZ x\\n0\\ng′(s) ds =\\nZ x\\n0\\ng′′(s) (x −s) ds =\\nZ 1\\n0\\ng′′(s) (x −s)+ ds.\\nIf κis a kernel, then by the reproducing property it must hold that\\ng(x) = ⟨g,κx⟩H =\\nZ 1\\n0\\ng′′(s) κ′′\\nx (s) ds,\\nso that κ must satisfy ∂2\\n∂s2 κ(x,s) = (x −s)+, where y+ := max{y,0}. Therefore, noting that\\nκ(x,u) = ⟨κx,κu⟩H, we have (see Exercise 15)\\nκ(x,u) =\\nZ 1\\n0\\n∂2κ(x,s)\\n∂s2\\n∂2κ(u,s)\\n∂s2 ds = max{x,u}min{x,u}2\\n2 −min{x,u}3\\n6 .\\nThe last expression is a cubic function with quadratic and cubic terms that misses the\\nconstant and linear monomials. This is not surprising considering the Taylor’s theorem\\ninterpretation of a function g ∈H . If we now take H0 as the space of functions of the\\nfollowing form (having zero second derivative):\\nh0 = η1 + η2 x, x ∈[0,1],\\nthen (6.25) is exactly of the form (6.19).\\nAs a consequence of the representer Theorem 6.6, the optimal solution to (6.25) is a\\nlinear combination of piecewise cubic functions:\\ng(x) = η1 + η2 x +\\nnX\\ni=1\\nαi κ(xi,x). (6.26)\\nSuch a function is called a cubic splinecubic spline with n knots (with one knot at each data point xi)\\n— so called, because the piecewise cubic function between knots is required to be “tied\\ntogether” at the knots. The parameters α,η are determined from (6.21) for instance by\\nsolving (6.23) with matrices K = [κ(xi,xj)]n\\ni,j=1 and Q with i-th row of the form [1,xi] for\\ni = 1,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 254, 'page_label': '237'}, page_content='Regularization and Kernel Methods 237\\nExample 6.15 (Smoothing Spline) Figure 6.6 shows various cubic smoothing splines\\nfor the data (0 .05,0.4),(0.2,0.2),(0.5,0.6),(0.75,0.7),(1,1). In the figure, we use the re-\\nparameterization r = 1/(1 + n γ) for the smoothing parameter. Thusr ∈[0,1], where r = 0\\nmeans an infinite penalty for curvature (leading to the ordinary linear regression solution)\\nand r = 1 does not penalize curvature at all and leads to a perfect fit via the so-called nat-\\nural spline. Of course the latter will generally lead to overfitting. Forr from 0 up to 0.8 the\\nsolutions will be close to the simple linear regression line, while only for r very close to 1,\\nthe shape of the curve changes significantly.\\n0 0.2 0.4 0.6 0.8 1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 6.6: Various cubic smoothing splines for smoothing parameter r = 1/(1 + n γ) ∈\\n{0.8,0.99,0.999,0.999999}. For r = 1, the natural spline through the data points is ob-\\ntained; for r = 0, the simple linear regression line is found.\\nThe following code first computes the matrices K and Q, and then solves the linear\\nsystem (6.23). Finally, the smoothing curve is determined via (6.26), for selected points,\\nand then plotted. Note that the code plots only a single curve corresponding to the specified\\nvalue of p.\\nsmoothspline.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.array([[0.05, 0.2, 0.5, 0.75, 1.]]).T\\ny = np.array([[0.4, 0.2, 0.6, 0.7, 1.]]).T\\nn = x.shape[0]\\nr = 0.999\\nngamma = (1-r)/r\\nk = lambda x1, x2 : (1/2)* np. max ((x1,x2)) * np. min ((x1,x2)) ** 2 \\\\\\n- ((1/6)* np. min ((x1,x2))**3)\\nK = np.zeros((n,n))\\nfor i in range (n):\\nfor j in range (n):'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 255, 'page_label': '238'}, page_content=\"238 Gaussian Process Regression\\nK[i,j] = k(x[i], x[j])\\nQ = np.hstack((np.ones((n,1)), x))\\nm1 = np.hstack((K @ K.T + (ngamma * K), K @ Q))\\nm2 = np.hstack((Q.T @ K.T, Q.T @ Q))\\nM = np.vstack((m1,m2))\\nc = np.vstack((K, Q.T)) @ y\\nad = np.linalg.solve(M,c)\\n# plot the curve\\nxx = np.arange(0,1+0.01,0.01).reshape(-1,1)\\ng = np.zeros_like(xx)\\nQx = np.hstack((np.ones_like(xx), xx))\\ng = np.zeros_like(xx)\\nN = np.shape(xx)[0]\\nKx = np.zeros((n,N))\\nfor i in range (n):\\nfor j in range (N):\\nKx[i,j] = k(x[i], xx[j])\\ng = g + np.hstack((Kx.T, Qx)) @ ad\\nplt.ylim((0,1.15))\\nplt.plot(xx, g, label = 'r = {} '.format (r), linewidth = 2)\\nplt.plot(x,y, 'b.', markersize=15)\\nplt.xlabel( '$x$')\\nplt.ylabel( '$y$')\\nplt.legend()\\n6.7 Gaussian Process Regression\\nAnother application of the kernel machinery is to Gaussian process regression. AGaussian\\nprocessGaussian\\nprocess\\n(GP) on a space Xis a stochastic process {Zx,x ∈X} where, for any choice of\\nindices x1,..., xn, the vector [ Zx1 ,... Zxn ]⊤ has a multivariate Gaussian distribution. As\\nsuch, the distribution of a GP is completely specified by its mean and covariance functions\\nµ: X→ Rand κ : X×X→ R, respectively. The covariance function is a finite positive\\nsemidefinite function, and hence, in view of Theorem 6.2, can be viewed as a reproducing\\nkernel on X.\\nAs for ordinary regression, the objective of GP regression is to learn a regression func-☞168\\ntion g that predicts a responsey = g(x) for each feature vectorx. This is done in a Bayesian\\nfashion, by establishing (1) a prior pdf for g and (2) the likelihood of the data, for a given\\ng. From these two we then derive, via Bayes’ formula, the posterior distribution ofg given\\nthe data. We refer to Section 2.9 for the general Bayesian framework.☞48\\nA simple Bayesian model for GP regression is as follows. First, the prior distribution of\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 256, 'page_label': '239'}, page_content='Regularization and Kernel Methods 239\\ng is taken to be the distribution of a GP with some known mean function µand covariance\\nfunction (that is, kernel) κ. Most often µ is taken to be a constant, and for simplicity of\\nexposition, we take it to be 0. The Gaussian kernel (6.15) is often used for the covariance\\nfunction. For radial basis function kernels (including the Gaussian kernel), points that are\\ncloser will be more highly correlated or “similar” [97], independent of translations in space.\\nSecond, similar to standard regression, we view the observed feature vectorsx1,..., xn\\nas fixed and the responses y1,..., yn as outcomes of random variables Y1,..., Yn. Specific-\\nally, given g, we model the {Yi}as\\nYi = g(xi) + εi , i = 1,..., n, (6.27)\\nwhere {εi}\\niid\\n∼N(0,σ2). To simplify the analysis, let us assume thatσ2 is known, so no prior\\nneeds to be specified for σ2. Let g = [g(x1),..., g(xn)]⊤ be the (unknown) vector of re-\\ngression values. Placing a GP prior on the functiong is equivalent to placing a multivariate\\nGaussian prior on the vector g:\\ng ∼N(0,K), (6.28)\\nwhere the covariance matrix K of g is a Gram matrix (implicitly associated with a feature\\nmap through the kernel κ), given by:\\nK =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ(x1,x1) κ(x1,x2) ... κ (x1,xn)\\nκ(x2,x1) κ(x2,x2) ... κ (x2,xn)\\n... ... ... ...\\nκ(xn,x1) κ(xn,x2) ... κ (xn,xn)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (6.29)\\nThe likelihood of our data given g, denoted p(y |g), is obtained directly from the model\\n(6.27):\\n(Y |g) ∼N(g,σ2In). (6.30)\\nSolving this Bayesian problem involves deriving the posterior distribution of ( g |Y). To\\ndo so, we first note that since Y has covariance matrix K + σ2In (which can be seen from\\n(6.27)), the joint distribution of Y and g is again normal, with mean 0 and covariance\\nmatrix:\\nKy,g =\\n\"K + σ2In K\\nK K\\n#\\n. (6.31)\\nThe posterior can then be found by conditioning on Y = y, via Theorem C.8, giving ☞ 436\\n(g |y) ∼N\\n\\x10\\nK⊤(K + σ2In)−1 y, K −K⊤(K + σ2In)−1K\\n\\x11\\n.\\nThis only gives information aboutg at the observed points x1,..., xn. It is more interesting\\nto consider the posterior predictive distribution ofeg := g(ex) for a new input ex. We can find\\nthe corresponding posterior predictive pdf p(eg |y) by integrating out the joint posterior pdf\\np(eg,g |y), which is equivalent to taking the expectation of p(eg |g) when g is distributed\\naccording to the posterior pdf p(g |y); that is,\\np(eg |y) =\\nZ\\np(eg |g) p(g |y) dg.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 257, 'page_label': '240'}, page_content='240 Gaussian Process Regression\\nTo do so more easily than direct evaluation via the above integral representation ofp(eg |y),\\nwe can begin with the joint distribution of [y⊤,eg]⊤, which is multivariate normal with mean\\n0 and covariance matrix\\neK =\\n\"K + σ2In κ\\nκ⊤ κ(ex,ex)\\n#\\n, (6.32)\\nwhere κ= [κ(ex,x1),...,κ (ex,xn)]⊤. It now follows, again by using Theorem C.8, that (eg |y)\\nhas a normal distribution with mean and variance given respectively by\\nµ(ex) = κ⊤(K + σ2In)−1 y (6.33)\\nand\\nσ2(ex) = κ(ex,ex) −κ⊤(K + σ2In)−1κ. (6.34)\\nThese are sometimes called the predictivepredictive mean and variance. It is important to note that\\nwe are predicting the expected response EeY = g(ex) here, and not the actual response eY.\\nExample 6.16 (GP Regression) Suppose the regression function is\\ng(x) = 2 sin(2πx), x ∈[0,1].\\nWe use GP regression to estimateg, using a Gaussian kernel of the form (6.15) with band-\\nwidth parameter 0 .2. The explanatory variables x1,..., x30 were drawn uniformly on the\\ninterval [0,1], and the responses were obtained from (6.27), with noise level σ= 0.5. Fig-\\nure 6.7 shows 10 samples from the prior distribution for g as well as the data points and\\nthe true sinusoidal regression function g.\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure 6.7: Left: samples drawn from the GP prior distribution. Right: the true regression\\nfunction with the data points.\\nAgain assuming that the variance σ2, is known, the predictive distribution as determ-\\nined by (6.33) and (6.34) is shown in Figure 6.8 for bandwidth 0 .2 (left) and 0.02 (right).\\nClearly, decreasing the bandwidth leads to the covariance between pointsx and x′decreas-\\ning at a faster rate with respect to the squared distance ∥x −x′∥2, leading to a predictive\\nmean that is less smooth.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 258, 'page_label': '241'}, page_content='Regularization and Kernel Methods 241\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\ng(x)\\nPredictive Mean\\n0 0.2 0.4 0.6 0.8 1\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\ng(x)\\nPredictive Mean\\nFigure 6.8: GP regression of synthetic data set with bandwidth 0 .2 (left) and 0.02 (right).\\nThe black dots represent the data and the blue curve is the latent functiong(x) = 2 sin(2πx).\\nThe red curve is the mean of the GP predictive distribution given by (6.33), and the shaded\\nregion is the 95% confidence band, corresponding to the predictive variance given in (6.34).\\nIn the above exposition, we have taken the mean function for the prior distribution\\nof g to be identically zero. If instead we have a general mean function m and write\\nm = [m(x1),..., m(xn)]⊤ then the predictive variance (6.34) remains unchanged, and the\\npredictive mean (6.33) is modified to read\\nµ(ex) = m(ex) + κ⊤(K + σ2In)−1 (y −m) . (6.35)\\nTypically, the variance σ2 appearing in (6.27) is not known, and the kernel κ itself\\ndepends on several parameters — for instance a Gaussian kernel (6.15) with an unknown\\nbandwidth parameter. In the Bayesian framework, one typically specifies a hierarchical\\nmodel by introducing a prior p(θ) for the vector θ of such hyperparameters hyperparamet-\\ners\\n. Now, the\\nGP prior ( g |θ) (equivalently, specifying p(g |θ)) and the model for the likelihood of the\\ndata given Y|g,θ, namely p(y |g,θ), are both dependent on θ. The posterior distribution of\\n(g |y,θ) is as before.\\nOne approach to setting the hyperparameter θis to determine its posterior p(θ|y) and\\nobtain a point estimate, for instance via its maximum a posteriori estimate. However, this\\ncan be a computationally demanding exercise. What is frequently done in practice is to\\nconsider instead the marginal likelihood p(y |θ) and maximize this with respect to θ. This\\nprocedure is called empirical Bayes empirical Bayes.\\nConsidering again the mean function m to be identically zero, from (6.31), we have\\nthat ( Y |θ) is multivariate normal with mean 0 and covariance matrix Ky = K + σ2In,\\nimmediately giving an expression for the marginal log-likelihood:\\nln p(y |θ) = −n\\n2 ln(2π) −1\\n2 ln |det(Ky)|− 1\\n2 y⊤K−1\\ny y. (6.36)\\nWe notice that only the second and third terms in (6.36) depend onθ. Considering a partial'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 259, 'page_label': '242'}, page_content='242 Kernel PCA\\nderivative of (6.36) with respect to a single elementθof the hyperparameter vectorθyields\\n∂\\n∂θln p(y |θ) = −1\\n2tr\\n \\nK−1\\ny\\n\" ∂\\n∂θKy\\n#!\\n+ 1\\n2 y⊤K−1\\ny\\n\" ∂\\n∂θKy\\n#\\nK−1\\ny y, (6.37)\\nwhere\\nh\\n∂\\n∂θKy\\ni\\nis the element-wise derivative of matrix Ky with respect to θ. If these partial\\nderivatives can be computed for each hyperparameterθ, gradient information could be used\\nwhen maximizing (6.36).\\nExample 6.17 (GP Regression (cont.)) Continuing Example 6.16, we plot in Fig-\\nure 6.9 the marginal log-likelihood as a function of the noise level σand bandwidth para-\\nmeter.\\n10-2 10-1 10010-1\\n100\\nFigure 6.9: Contours of the marginal log-likelihood for the GP regression example. The\\nmaximum is denoted by a cross.\\nThe maximum is attained for a bandwidth parameter around 0.20 and σ≈0.44, which\\nis very close to the left panel of Figure 6.8 for the case where σwas assumed to be known\\n(and equal to 0.5). We note here that the marginal log-likelihood is extremely flat, perhaps\\nowing to the small number of points.\\n6.8 Kernel PCA\\nIn its basic form, kernel PCA (principal component analysis) can be thought of as PCA in\\nfeature space. The main motivation for PCA introduced in Section 4.8 was as a dimension-☞153\\nality reduction technique. There, the analysis rested on an SVD of the matrix bΣ = 1\\nn X⊤X,\\nwhere the data in X was first centered via x′\\ni,j = xi,j −xj where xi = 1\\nn\\nPn\\ni=1 xi,j.\\nWhat we shall do is to first re-cast the problem in terms of the Gram matrixK = XX⊤=\\n[⟨xi,xj⟩] (note the different order of X and X⊤), and subsequently replace the inner product\\n⟨x,x′⟩with κ(x,x′) for a general reproducing kernel κ. To make the link, let us start with\\nan SVD of X⊤:\\nX⊤= UDV⊤. (6.38)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 260, 'page_label': '243'}, page_content='Regularization and Kernel Methods 243\\nThe dimensions of X⊤, U, D, and V are d ×n, d ×d, d ×n, and n ×n, respectively. Then an\\nSVD of X⊤X is\\nX⊤X = (UDV⊤)(UDV⊤)⊤= U(DD⊤)U⊤\\nand an SVD of K is\\nK = (UDV⊤)⊤(UDV⊤) = V(D⊤D)V⊤.\\nLet λ1 ⩾··· ⩾λr >0 denote the non-zero eigenvalues of X⊤X (or, equivalently, ofK) and\\ndenote the corresponding r ×r diagonal matrix by Λ. Without loss of generality we can\\nassume that the eigenvector of X⊤X corresponding to λk is the k-th column of U and that\\nthe k-th column of V is an eigenvector of K. Similar to Section 4.8, let Uk and Vk contain ☞ 153\\nthe first k columns of U and V, respectively, and letΛk be the correspondingk×k submatrix\\nof Λ, k = 1,..., r.\\nBy the SVD (6.38), we haveX⊤Vk = UDV⊤Vk = UkΛ1/2\\nk . Next, consider the projection\\nof a point x onto the k-dimensional linear space spanned by the columns of Uk — the first\\nk principal components. We saw in Section 4.8 that this projection simply is the linear\\nmapping x 7→U⊤\\nk x. Using the fact that Uk = X⊤VkΛ−1/2, we find that x is projected to a\\npoint z given by\\nz = Λ−1/2\\nk V⊤\\nk Xx = Λ−1/2\\nk V⊤\\nk κx,\\nwhere we have (suggestively) defined κx := [⟨x1,x⟩,..., ⟨xn,x⟩]⊤. The important point\\nis that z is completely determined by the vector of inner products κx and the k principal\\neigenvalues and (right) eigenvectors of the Gram matrix K. Note that each component zm\\nof z is of the form\\nzm =\\nnX\\ni=1\\nαm,i κ(xi,x), m = 1,..., k. (6.39)\\nThe preceding discussion assumed centering of the columns of X. Consider now an\\nuncentered data matrix eX. Then the centered data can be written as X = eX −1\\nn EneX, where\\nEn is the n ×n matrix of ones. Consequently,\\nXX⊤= eXeX\\n⊤\\n−1\\nnEneXeX\\n⊤\\n−1\\nn\\neXeX\\n⊤\\nEn + 1\\nn2 EneXeX\\n⊤\\nEn,\\nor, more compactly,XX⊤= H eXeX\\n⊤\\nH, where H = In −1\\nn 1n1⊤\\nn , In is the n×n identity matrix,\\nand 1n is the n ×1 vector of ones.\\nTo generalize to the kernel setting, we replace eXeX\\n⊤\\nby K = [κ(xi,xj),i, j = 1,..., n]\\nand set κx = [κ(x1,x),...,κ (xn,x)]⊤, so thatΛk is the diagonal matrix of thek largest eigen-\\nvalues of HKH and Vk is the corresponding matrix of eigenvectors. Note that the “usual”\\nPCA is recovered when we use the linear kernelκ(x,y) = x⊤y. However, instead of having\\nonly kernels that are explicitly inner products of feature vectors, we are now permitted to\\nimplicitly use infinite feature maps (functions) by using kernels.\\nExample 6.18 (Kernel PCA) We simulated 200 points, x1,..., x200, from the uniform\\ndistribution on the set B1 ∪(B4 ∩Bc\\n3), where Br := {(x,y) ∈R2 : x2 + y2 ⩽r2}(disk with\\nradius r). We apply kernel PCA with Gaussian kernel κ(x,x′) = exp\\n\\x10\\n−∥x −x′∥2\\n\\x11\\nand\\ncompute the functions zm(x),m = 1,..., 9 in (6.39). Their density plots are shown in Fig-\\nure 6.10. The data points are superimposed in each plot. From this we see that the principal\\ncomponents identify the radial structure present in the data. Finally, Figure 6.11 shows'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 261, 'page_label': '244'}, page_content='244 Kernel PCA\\nthe projections [z1(xi),z2(xi)]⊤,i = 1,..., 200 of the original data points onto the first two\\nprincipal components. We see that the projected points can be separated by a straight line,\\nwhereas this is not possible for the original data; see also, Example 7.6 for a related prob-☞272\\nlem.\\nFigure 6.10: First nine eigenfunctions using a Gaussian kernel for the two-dimensional\\ndata set formed by the red and cyan points.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 262, 'page_label': '245'}, page_content='Regularization and Kernel Methods 245\\n-0.4 -0.2 0 0.2 0.4 0.6 0.8\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0\\n0.2\\n0.4\\n0.6\\nFigure 6.11: Projection of the data onto the first two principal components. Observe that\\nalready the projections of the inner and outer points are well separated.\\nFurther Reading\\nFor a good overview of the ridge regression and the lasso, we refer the reader to [36, 56].\\nFor overviews of the theory of RKHS we refer to [3, 115, 126], and for in-depth background\\non splines and their connection to RKHSs we refer to [123]. For further details on GP\\nregression we refer to [97] and for kernel PCA in particular we refer to [12, 92]. Finally,\\nmany facts about kernels and their corresponding RKHSs can be found in [115].\\nExercises\\n1. Let Gbe an RKHS with reproducing kernel κ. Show that κis a positive semidefinite\\nfunction.\\n2. Show that a reproducing kernel, if it exists, is unique.\\n3. Let Gbe a Hilbert space of functions g : X→ R. Recall that the evaluation func-\\ntional is the map δx : g 7→g(x) for a given x ∈X. Show that evaluation functionals\\nare linear operators.\\n4. Let G0 be the pre-RKHS G0 constructed in the proof of Theorem 6.2. Thus, g ∈G0\\nis of the form g = Pn\\ni=1 αi κxi and\\n⟨g,κx⟩G0 =\\nnX\\ni=1\\nαi ⟨κxi ,κx⟩G0 =\\nnX\\ni=1\\nαi κ(xi,x) = g(x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 263, 'page_label': '246'}, page_content='246 Exercises\\nTherefore, we may write the evaluation functional of g ∈G0 at x as δxg := ⟨g,κx⟩G0 .\\nShow that δx is bounded on G0 for every x; that is, |δx f |<γ ∥f ∥G0 , for some γ< ∞.\\n5. Continuing Exercise 4, let ( fn) be a Cauchy sequence in G0 such that |fn(x)|→ 0 for\\nall x. Show that ∥fn∥G0 →0.\\n6. Continuing Exercises 5 and 4, to show that the inner product (6.14) is well defined,\\na number of facts have to be checked.\\n(a) Verify that the limit converges.\\n(b) Verify that the limit is independent of the Cauchy sequences used.\\n(c) Verify that the properties of an inner product are satisfied. The only non-trivial\\nproperty to verify is that ⟨f, f ⟩G= 0 if and only if f = 0.\\n7. Exercises 4–6 show that Gdefined in the proof of Theorem 6.2 is an inner product\\nspace. It remains to prove thatGis an RKHS. This requires us to prove that the inner\\nproduct space Gis complete (and thus Hilbert), and that its evaluation functionals\\nare bounded and hence continuous (see Theorem A.16). This is done in a number of☞389\\nsteps.\\n(a) Show that G0 is dense in Gin the sense that every f ∈G is a limit point (with\\nrespect to the norm on G) of a Cauchy sequence ( fn) in G0.\\n(b) Show that every evaluation functional δx on Gis continuous at the 0 function.\\nThat is,\\n∀ε> 0 : ∃δ> 0 : ∀f ∈G : ∥f ∥G<δ ⇒|f (x)|<ε. (6.40)\\nContinuity of δx at all functionsg ∈Gthen follows automatically from linearity.\\n(c) Show that Gis complete; that is, every Cauchy sequence (fn) ∈G converges in\\nthe norm ||·||G.\\n8. If κ1 and κ2 are kernels on Xand Y, then κ+((x,y),(x′,y′)) : = κ1(x,x′) + κ2(y,y′)\\nand κ×((x,y),(x′,y′) := κ1(x,x′)κ2(y,y′) are kernels on the Cartesian productX×Y.\\nProve this.\\n9. An RKHS enjoys the following desirable smoothness property: if ( gn) is a sequence\\nbelonging to RKHS Gon X, and ∥gn −g∥G→0, then g(x) = limn gn(x) for all x ∈X.\\nProve this, using Cauchy–Schwarz.\\n10. Let X be an Rd-valued random variable that is symmetric about the origin (that is,\\nX and (−X) are identically distributed). Denote by µ is its distribution and ψ(t) =\\nEeit⊤X =\\nR\\neit⊤x µ(dx) for t ∈Rd is its characteristic function. Verify that κ(x,x′) =\\nψ(x −x′) is a real-valued positive semidefinite function.\\n11. Suppose an RKHS Gof functions from X→ R(with kernel κ) is invariant under a\\ngroup Tof transformations T : X→X ; that is, for all f,g ∈G and T ∈T, we have\\n(i) f ◦T ∈G and (ii) ⟨f ◦T,g ◦T⟩G = ⟨f,g⟩G. Show that κ(T x,T x′) = κ(x,x′) for\\nall x,x′∈X and T ∈T.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 264, 'page_label': '247'}, page_content='Regularization and Kernel Methods 247\\n12. Given two Hilbert spaces Hand G, we call a mapping A : H→G a Hilbert space\\nisomorphism Hilbert space\\nisomorphism\\nif it is\\n(i) a linear map; that is, A(a f +bg) = aA( f ) +bA(g) for any f,g ∈H and a,b ∈R.\\n(ii) a surjective map; and\\n(iii) an isometry; that is, for all f,g ∈H, it holds that ⟨f,g⟩H = ⟨A f,Ag⟩G.\\nLet H= Rp (equipped with the usual Euclidean inner product) and construct its\\n(continuous) dual space G, consisting of all continuous linear functions from Rp to\\nR, as follows: (a) For each β∈Rp, define gβ : Rp →Rvia gβ(x) = ⟨β,x⟩= β⊤x, for\\nall x ∈Rp. (b) Equip Gwith the inner product ⟨gβ,gγ⟩G:= β⊤γ.\\nShow that A : H→G defined by A(β) = gβ for β∈Rp is a Hilbert space isomorph-\\nism.\\n13. Let X be an n ×p model matrix. Show that X⊤X + n γIp for γ> 0 is invertible.\\n14. As Example 6.8 clearly illustrates, the pdf of a random variable that is symmetric\\nabout the origin is not in general a valid reproducing kernel. Take two such iid ran-\\ndom variables X and X′ with common pdf f , and define Z = X + X′. Denote by ψZ\\nand fZ the characteristic function and pdf of Z, respectively.\\nShow that if ψZ is in L1(R), fZ is a positive semidefinite function. Use this to show\\nthat κ(x,x′) = fZ(x −x′) = 1{|x −x′|⩽2}(1 −|x −x′|/2) is a valid reproducing kernel.\\n15. For the smoothing cubic spline of Section 6.6, show that κ(x,u) = max{x,u}min{x,u}2\\n2 −\\nmin{x,u}3\\n6 .\\n16. Let X be an n ×p model matrix and let u ∈Rp be the unit-length vector with k-th\\nentry equal to one (uk = ∥u∥= 1). Suppose that the k-th column of X is v and that it\\nis replaced with a new predictor w, so that we obtain the new model matrix:\\neX = X + (w −v)u⊤.\\n(a) Denoting\\nδ:= X⊤(w −v) + ∥w −v∥2\\n2 u,\\nshow that\\neX\\n⊤eX = X⊤X + uδ⊤+ δu⊤= X⊤X + (u + δ)(u + δ)⊤\\n2 −(u −δ)(u −δ)⊤\\n2 .\\nIn other words, eX\\n⊤eX differs from X⊤X by a symmetric matrix of rank two.\\n(b) Suppose that B := (X⊤X + n γIp)−1 is already computed. Explain how the\\nSherman–Morrison formulas in Theorem A.10 can be applied twice to com- ☞ 371\\npute the inverse and log-determinant of the matrix eX\\n⊤eX + n γIp in O((n + p)p)\\ncomputing time, rather than the usual O((n + p2)p) computing time.3\\n3This Sherman–Morrison updating is not always numerically stable. A more numerically stable method\\nwill perform two consecutive rank-one updates of the Cholesky decomposition of X⊤X + n γIp.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 265, 'page_label': '248'}, page_content='248 Exercises\\n(c) Write a Python program for updating a matrix B = (X⊤X + n γIp)−1 when we\\nchange the k-th column of X, as shown in the following pseudo-code.\\nAlgorithm 6.8.1: Updating via Sherman–Morrison Formula\\ninput: Matrices X and B, index k, and replacement w for the k-th column of X.\\noutput: Updated matrices X and B.\\n1 Set v ∈Rn to be the k-th column of X.\\n2 Set u ∈Rp to be the unit-length vector such that uk = ∥u∥= 1.\\n3 B ←B − Buδ⊤B\\n1 + δ⊤Bu\\n4 B ←B − Bδu⊤B\\n1 + u⊤Bδ\\n5 Update the k-th column of X with w.\\n6 return X,B\\n17. Use Algorithm 6.8.1 from Exercise 16 to write Python code that computes the ridge\\nregression coefficient βin (6.5) and use it to replicate the results on Figure 6.1. The☞217\\nfollowing pseudo-code (with running cost ofO((n+ p)p2)) may help with the writing\\nof the Python code.\\nAlgorithm 6.8.2: Ridge Regression Coefficients via Sherman–Morrison Formula\\ninput: Training set {X,y}and regularization parameter γ> 0.\\noutput: Solution bβ= (n γIp + X⊤X)−1X⊤y.\\n1 Set A to be an n ×p matrix of zeros and B ←(n γIp)−1.\\n2 for j = 1,..., p do\\n3 Set w to be the j-th column of X.\\n4 Update {A,B}via Algorithm 6.8.1 with inputs {A,B, j,w}.\\n5 bβ←B(X⊤y)\\n6 return bβ\\n18. Consider Example 2.10 with D = diag(λ1,...,λ p) for some nonnegative vector λ ∈☞56\\nRp, so that twice the negative logarithm of the model evidence can be written as\\n−2 lng(y) = l(λ) := n ln[y⊤(I −XΣX⊤)y] + ln |D|−ln |Σ|+ c,\\nwhere c is a constant that depends only on n.\\n(a) Use the Woodbury identities (A.15) and (A.16) to show that☞371\\nI −XΣX⊤= (I + XDX⊤)−1\\nln |D|−ln |Σ|= ln |I + XDX⊤|.\\nDeduce that l(λ) = n ln[y⊤Cy] −ln |C|+ c, where C := (I + XDX⊤)−1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 266, 'page_label': '249'}, page_content='Regularization and Kernel Methods 249\\n(b) Let [ v1,..., vp] := X denote the p columns/predictors of X. Show that\\nC−1 = I +\\npX\\nk=1\\nλkvkv⊤\\nk .\\nExplain why setting λk = 0 has the effect of excluding the k-th predictor from\\nthe regression model. How can this observation be used for model selection?\\n(c) Prove the following formulas for the gradient and Hessian elements of l(λ):\\n∂l\\n∂λi\\n= v⊤\\ni Cvi −n(v⊤\\ni Cy)2\\ny⊤Cy\\n∂2l\\n∂λi ∂λj\\n= (n −1)(v⊤\\ni Cvj)2 −n\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0v⊤\\ni Cvj −\\n(v⊤\\ni Cy)(v⊤\\nj Cy)\\ny⊤Cy\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n2\\n.\\n(6.41)\\n(d) One method to determine which predictors in X are important is to compute\\nλ∗:= argmin\\nλ⩾0\\nl(λ)\\nusing, for example, the interior-point minimization Algorithm B.4.1 with gradi- ☞ 419\\nent and Hessian computed from (6.41). Write Python code to compute λ∗ and\\nuse it to select the best polynomial model in Example 2.10.\\n19. (Exercise 18 continued.) Consider again Example 2.10 with D = diag(λ1,...,λ p) for ☞ 56\\nsome nonnegative model-selection parameter λ∈Rp. A Bayesian choice for λis the\\nmaximizer of the marginal likelihood g(y |λ); that is,\\nλ∗= argmax\\nλ⩾0\\n\"\\ng(β,σ2,y |λ) dβdσ2,\\nwhere\\nln g(β,σ2,y |λ) = −∥y −Xβ∥2 + β⊤D−1β\\n2σ2 −1\\n2 ln |D|− n + p\\n2 ln(2πσ2) −ln σ2.\\nTo maximize g(y |λ), one can use the EM algorithm with βand σ2 acting as latent ☞ 128\\nvariables in the complete-data log-likelihood ln g(β,σ2,y |λ). Define\\nΣ := (D−1 + X⊤X)−1\\nβ:= ΣX⊤y\\nbσ2 :=\\n\\x10\\n∥y∥2 −y⊤Xβ\\n\\x11\\x0en.\\n(6.42)\\n(a) Show that the conditional density of the latent variables βand σ2 is such that\\n\\x10\\nσ−2 \\x0c\\x0c\\x0cλ,y\\n\\x11\\n∼Gamma\\n\\x12n\\n2, n\\n2 bσ2\\n\\x13\\n\\x10\\nβ\\n\\x0c\\x0c\\x0cλ,σ2,y\\n\\x11\\n∼N\\n\\x10\\nβ, σ2Σ\\n\\x11\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 267, 'page_label': '250'}, page_content='250 Exercises\\n(b) Use Theorem C.2 to show that the expected complete-data log-likelihood is☞430\\n−β\\n⊤\\nD−1β\\n2bσ2 −tr(D−1Σ) + ln |D|\\n2 + c1,\\nwhere c1 is a constant that does not depend on λ.\\n(c) Use Theorem A.2 to simplify the expected complete-data log-likelihood and to☞359\\nshow that it is maximized at λi = Σii + (βi/bσ)2 for i = 1,..., p.Hence, deduce\\nthe following E and M steps in the EM algorithm:\\nE-step. Given λ, update (Σ,β,bσ2) via the formulas (6.42).\\nM-step. Given (Σ,β,bσ2), update λvia λi = Σii + (βi/bσ)2, i = 1,..., p.\\n(d) Write Python code to compute λ∗ via the EM algorithm, and use it to select\\nthe best polynomial model in Example 2.10. A possible stopping criterion is to\\nterminate the EM iterations when\\nln g(y |λt+1) −ln g(y |λt) <ε\\nfor some small ε> 0, where the marginal log-likelihood is\\nln g(y |λ) = −n\\n2 ln(nπbσ2) −1\\n2 ln |D|+ 1\\n2 ln |Σ|+ ln Γ(n/2).\\n20. In this exercise we explore how the early stopping of the gradient descent iterations\\n(see Example B.10),☞412\\nxt+1 = xt −α∇f (xt), t = 0,1,...,\\nis (approximately) equivalent to the global minimization of f (x) + 1\\n2 γ∥x∥2 for certain\\nvalues of the ridge regularization parameter γ >0 (see Example 6.1). We illustrate\\nthe early stoppingearly stopping idea on the quadratic function f (x) = 1\\n2 (x −µ)⊤H(x −µ), where\\nH ∈Rn×n is a symmetric positive-definite (Hessian) matrix with eigenvalues {λk}n\\nk=1.\\n(a) Verify that for a symmetric matrix A ∈Rn such that I −A is invertible, we have\\nI + A + ··· + At−1 = (I −At)(I −A)−1.\\n(b) Let H = QΛQ⊤ be the diagonalization of H as per Theorem A.8. If x0 = 0,☞366\\nshow that the formula for xt is\\nxt = µ−Q(I −αΛ)tQ⊤µ.\\nHence, deduce that a necessary condition for xt to converge is α< 2/maxk λk.\\n(c) Show that the minimizer of f (x) + 1\\n2 γ∥x∥2 can be written as\\nx∗= µ−Q(I + γ−1Λ)−1Q⊤µ.\\n(d) For a fixed value of t, let the learning rate α↓0. Using part (b) and (c), show\\nthat if γ≃1/(t α) as α↓0, then xt ≃x∗. In other words, xt is approximately\\nequal to x∗for small α, provided that γis inversely proportional to t α.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 268, 'page_label': '251'}, page_content='CHAPTER 7\\nCLASSIFICATION\\nThe purpose of this chapter is to explain the mathematical ideas behind well-known\\nclassification techniques such as the naïve Bayes method, linear and quadratic discrim-\\ninant analysis, logistic /softmax classification, the K-nearest neighbors method, and\\nsupport vector machines.\\n7.1 Introduction\\nClassification methods are supervised learning methods in which a categorical response\\nvariable Y takes one of c possible values (for example whether a person is sick or healthy),\\nwhich is to be predicted from a vector X of explanatory variables (for example, the blood\\npressure, age, and smoking status of the person), using a prediction function g . In this\\nsense, g classifies the input X into one of the classes, say in the set {0,..., c −1}. For this\\nreason, we will call g a classification function or simply classifier classifier. As with any supervised\\nlearning technique (see Section 2.3), the goal is to minimize the expected loss or risk\\nℓ(g) = ELoss(Y,g(X)) (7.1)\\nfor some loss function, Loss(y,by), that quantifies the impact of classifying a response y via\\nby = g(x). The natural loss function is the zero–one (also written 0–1) or indicator loss indicator loss:\\nLoss(y,by) : = 1{y , by}; that is, there is no loss for a correct classification ( y = by) and a\\nunit loss for a misclassification (y , by). In this case the optimal classifier g∗is given in the\\nfollowing theorem.\\nTheorem 7.1: Optimal classifier\\nFor the loss function Loss(y,by) = 1{y , by}, an optimal classification function is\\ng∗(x) = argmax\\ny∈{0,...,c−1}\\nP[Y = y |X = x]. (7.2)\\nProof: The goal is to minimize ℓ(g) = E1{Y , g(X)}over all functions g taking values in\\n{0,..., c −1}. Conditioning on X gives, by the tower property,ℓ(g) = E(P[Y , g(X) |X] ), ☞ 431\\nand so minimizing ℓ(g) with respect to g can be accomplished by maximizing P[Y =\\n251'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 269, 'page_label': '252'}, page_content='252 Introduction\\ng(x) |X = x] with respect to g(x), for every fixed x. In other words, take g(x) to be equal\\nto the class label y for which P[Y = y |X = x] is maximal. □\\nThe formulation (7.2) allows for “ties”, when there is an equal probability between\\noptimal classes for a feature vector x. Assigning one of these tied classes arbitrarily (or\\nrandomly) to x does not affect the loss function and so we assume for simplicity that g∗(x)\\nis always a scalar value.\\nNote that, as was the case for the regression (see, e.g., Theorem 2.1), the optimal pre-☞21\\ndiction function depends on the conditional pdf f (y |x) = P[Y = y |X = x]. However, since\\nwe assign x to class y if f (y |x) ⩾f (z |x) for all z, we do not need to learn the entire sur-\\nface of the function f (y |x); we only need to estimate it well enough near the decision\\nboundary {x : f (y |x) = f (z |x)}for any choice of classes y and z. This is because the as-\\nsignment (7.2) divides the feature space into c regions, Ry = {x : f (y |x) = maxz f (z |x)},\\ny = 0,..., c −1.\\nRecall that for any supervised learning problem the smallest possible expected loss\\n(that is, the irreducible risk) is given by ℓ∗ = ℓ(g∗). For the indicator loss, the irreducible\\nrisk is equal to P[Y , g∗(X)]. This smallest possible probability of misclassification is\\noften called the Bayes error rateBayes error\\nrate\\n.\\nFor a given training set τ, a classifier is often derived from apre-classifier gτ, which\\nis a prediction function (learner) that can take any real value, rather than only values\\nin the set of class labels. A typical situation is the case of binary classification with\\nlabels −1 and 1, where the prediction function gτ is a function taking values in the\\ninterval [−1,1] and the actual classifier is given by sign( gτ). It will be clear from\\nthe context whether a prediction function gτ should be interpreted as a classifier or\\npre-classifier.\\nThe indicator loss function may not always be the most appropriate choice of loss\\nfunction for a given classification problem. For example, when diagnosing an illness, the\\nmistake in misclassifying a person as being sick when in fact the person is healthy may\\nbe less serious than classifying the person as healthy when in fact the person is sick. In\\nSection 7.2 we consider various classification metrics.\\nThere are many ways to fit a classifier to a training set τ= {(x1,y1),..., (xn,yn)}. The\\napproach taken in Section 7.3 is to use a Bayesian framework for classification. Here the\\nconditional pdf f (y |x) is viewed as a posterior pdf f (y |x) ∝f (x |y) f (y) for a given class\\nprior f (y) and likelihood f (x |y). Section 7.4 discusses linear and quadratic discriminant\\nanalysis for classification, which assumes that the class of approximating functions for the\\nconditional pdf f (x |y) is a parametric class Gof Gaussian densities. As a result of this\\nchoice of G, the marginal f (x) is approximated via a Gaussian mixture density.\\nIn contrast, in the logistic or soft-max classification in Section 7.5, the conditional\\npdf f (y |x) is approximated using a more flexible class of approximating functions. As a\\nresult of this, the approximation to the marginal density f (x) does not belong to a simple\\nparametric class (such as a Gaussian mixture). As in unsupervised learning, the cross-\\nentropy loss is the most common choice for training the learner.\\nThe K-nearest neighbors method, discussed in Section 7.6, is yet another approach to\\nclassification that makes minimal assumptions on the class G. Here the aim is to directly'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 270, 'page_label': '253'}, page_content='Classification 253\\nestimate the conditional pdf f (y |x) from the training data, using only feature vectors in\\nthe neighborhood of x. In Section 7.7 we explain the support vector methodology for clas-\\nsification; this is based on the same Reproducing Kernel Hilbert Space ideas that proved\\nsuccessful for regression analysis in Section 6.3. Finally, a versatile way to do both clas- ☞ 222\\nsification and regression is to use classification and regression trees. This is the topic of\\nChapter 8. Neural networks (Chapter 9) provide yet another way to perform classification. ☞ 287\\n☞ 323\\n7.2 Classification Metrics\\nThe effectiveness of a classifierg is, theoretically, measured in terms of the risk (7.1), which\\ndepends on the loss function used. Fitting a classifier to iid training data τ= {(xi,yi)}n\\ni=1 is\\nestablished by minimizing the training loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) (7.3)\\nover some class of functions G. As the training loss is often a poor estimator of the risk,\\nthe risk is usually estimated as in (7.3), using instead a test set τ′ = {(x′\\ni ,y′\\ni )}n′\\ni=1}that is\\nindependent of the training set, as explained in Section 2.3. To measure the performance ☞ 23\\nof a classifier on a training or test set, it is convenient to introduce the notion of a loss\\nmatrix loss matrix. Consider a classification problem with classifier g, loss function Loss, and classes\\n0,..., c −1. If an input feature vector x is classified as by = g(x) when the observed class\\nis y, the loss incurred is, by definition, Loss( y,by). Consequently, we may identify the loss\\nfunction with a matrix L = [Loss( j,k), j,k ∈{0,..., c −1}]. For the indicator loss function,\\nthe matrix L has 0s on the diagonal and 1s everywhere else. Another useful matrix is the\\nconfusion matrix confusion\\nmatrix\\n, denoted by M, where the ( j,k)-th element of M counts the number of\\ntimes that, for the training or test data, the actual (observed) class isj whereas the predicted\\nclass is k. Table 7.1 shows the confusion matrix of some Dog/Cat/Possum classifier.\\nTable 7.1: Confusion matrix for three classes.\\nPredicted\\nActual Dog Cat Possum\\nDog 30 2 6\\nCat 8 22 15\\nPossum 7 4 41\\nWe can now express the classifier performance (7.3) in terms of L and M as\\n1\\nn\\nX\\nj,k\\n[L ⊙M]jk, (7.4)\\nwhere L ⊙M is the elementwise product of L and M. Note that for the indicator loss, (7.4)\\nis simply 1−tr(M)/n, and is called the misclassification error. The expression (7.4) makes misclassification\\nerrorit clear that both the counts and the loss are important in determining the performance of a\\nclassifier.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 271, 'page_label': '254'}, page_content='254 Classification Metrics\\nIn the spirit of Table C.4 for hypothesis testing, it is sometimes useful to divide the☞459\\nelements of a confusion matrix into four groups. The diagonal elements are thetrue positivetrue positive\\ncounts; that is, the numbers of correct classifications for each class. The true positive counts\\nfor the Dog, Cat, and Possum classes in Table 7.1 are 30,22, and 41, respectively. Similarly,\\nthe true negativetrue negative count for a class is the sum of all matrix elements that do not belong to the\\nrow or the column of this particular class. For the Dog class it is 22+15 +4 +41 = 82. The\\nfalse positivefalse positive count for a class is the sum of the corresponding column elements without\\nthe diagonal element. For the Dog class it is 8 + 7 = 15. Finally, the false negativefalse negative count\\nfor a specific class, can be calculated by summing over the corresponding row elements\\n(again, without counting the diagonal element). For the Dog class it is 2 + 6 = 8.\\nIn terms of the elements of the confusion matrix, we have the following counts for class\\nj = 0,..., c −1:\\nTrue positive tp j = Mj j,\\nFalse positive fp j =\\nX\\nk,j\\nMk j, (column sum)\\nFalse negative fn j =\\nX\\nk,j\\nMjk, (row sum)\\nTrue negative tn j = n −fnj −fpj −tpj.\\nNote that in the binary classification case (c = 2), and using the indicator loss function,\\nthe misclassification error (7.4) can be written as\\nerrorj =\\nfpj + fnj\\nn . (7.5)\\nThis does not depend on which of the two classes is considered, as fp 0 + fn0 = fp1 + fn1.\\nSimilarly, the accuracyaccuracy measures the fraction of correctly classified objects:\\naccuracyj = 1 −errorj =\\ntpj + tnj\\nn . (7.6)\\nIn some cases, classification error (or accuracy) alone is not su fficient to adequately\\ndescribe the effectiveness of a classifier. As an example, consider the following two classi-\\nfication problems based on a fingerprint detection system:\\n1. Identification of authorized personnel in a top-secret military facility.\\n2. Identification to get an online discount for some retail chain.\\nBoth problems are binary classification problems. However, a false positive in the first\\nproblem is extremely dangerous, while a false positive in the second problem will make\\na customer happy. Let us examine a classifier in the top-secret facility. The corresponding\\nconfusion matrix is given in Table 7.2.\\nTable 7.2: Confusion matrix for authorized personnel classification.\\nPredicted\\nActual authorized non-authorized\\nauthorized 100 400\\nnon-authorized 50 100,000'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 272, 'page_label': '255'}, page_content='Classification 255\\nFrom (7.6), we conclude that the accuracy of classification is equal to\\naccuracy = tp + tn\\ntp + tn + fp + fn = 100 + 100,000\\n100 + 100,000 + 50 + 400 ≈99.55%.\\nHowever, we can see that in this particular case, accuracy is a problematic metric, since the\\nalgorithm allowed 50 non-authorized personnel to enter the facility. One way to deal with\\nthis issue is to modify the loss function to give a much higher loss to non-authorized access.\\nThus, instead of an (indicator) loss matrix, we could for example take the loss matrix\\nL =\\n 0 1\\n1000 0\\n!\\n.\\nAn alternative approach is to keep the indicator loss function and consider additional clas-\\nsification metrics. Below we give a list of commonly used metrics. For simplicity we call\\nan object whose actual class is j a “ j-object”.\\n• The precision precision(also called positive predictive value ) is the fraction of all objects\\nclassified as j that are actually j-objects. Specifically,\\nprecisionj =\\ntpj\\ntpj + fpj\\n.\\n• The recall recall(also called sensitivity) is the fraction of all j-objects that are correctly\\nclassified as such. That is,\\nrecallj =\\ntpj\\ntpj + fnj\\n.\\n• The specificity specificitymeasures the fraction of all non-j-objects that are correctly classified\\nas such. Specifically,\\nspecificityj = tnj\\nfpj + tnj\\n.\\n• The Fβ score Fβ scoreis a combination of the precision and the recall and is used as a single\\nmeasurement for a classifier’s performance. The Fβ score is given by\\nFβ,j =\\n(β2 + 1) tpj\\n(β2 + 1) tpj + β2 fnj + fpj\\n.\\nFor β= 0 we obtain the precision and for β→∞ we obtain the recall.\\nThe particular choice of metric is clearly application dependent. For example, in the\\nclassification of authorized personnel in a top-secret military facility, suppose we have\\ntwo classifiers. The first (Classifier 1) has a confusion matrix given in Table 7.2, and the\\nsecond (Classifier 2) has a confusion matrix given in Table 7.3. Various metrics for these\\ntwo classifiers are show in Table 7.4. In this case we prefer Classifier 1, which has a much\\nhigher precision.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 273, 'page_label': '256'}, page_content='256 Classification Metrics\\nTable 7.3: Confusion matrix for authorized personnel classification, using a different clas-\\nsifier (Classifier 2).\\nPredicted\\nActual Authorized Non-Authorized\\nauthorized 50 10\\nnon-authorized 450 100,040\\nTable 7.4: Comparing the metrics for the confusion matrices in Tables 7.2 and 7.3.\\nMetric Classifier 1 Classifier 2\\naccuracy 9 .955 ×10−1 9.954 ×10−1\\nprecision 6 .667 ×10−1 1.000 ×10−1\\nrecall 2 .000 ×10−1 8.333 ×10−1\\nspecificity 9 .995 ×10−1 9.955 ×10−1\\nF1 3.077 ×10−1 1.786 ×10−1\\nRemark 7.1 (Multilabel and Hierarchical Classification) In standard classification\\nthe classes are assumed to be mutually exclusive. For example a satellite image could\\nbe classified as “cloudy”, “clear”, or “foggy”. Inmultilabel classificationmultilabel\\nclassification\\nthe classes (often\\ncalled labels) do not have to be mutually exclusive. In this case the response is a subset\\nYof some collection of labels {0,..., c −1}. Equivalently, the response can be viewed as\\na binary vector of length c, where the y-th element is 1 if the response belongs to label y\\nand 0 otherwise. Again, consider the satellite image example and add two labels, such as\\n“road” and “river” to the previous three labels. Clearly, an image can contain both a road\\nand a river. In addition, the image can be clear, cloudy, or foggy.\\nIn hierarchical classificationhierarchical\\nclassification\\na hierarchical relation between classes/labels is taken into\\naccount during the classification process. Usually, the relations are modeled via a tree or a\\ndirected acyclic graph. A visual comparison between the hierarchical and non-hierarchical\\n(flat) classification tasks for satellite image data is presented in Figure 7.1.\\nroot\\nrural\\nfarm barn\\nurban\\nskyscraper\\nroot\\nrural barn farm urban skyscraper\\nFigure 7.1: Hierarchical (left) and non-hierarchical (right) classification schemes. Barns\\nand farms are common in rural areas, while skyscrapers are generally located in cities.\\nWhile this relation can be clearly observed in the hierarchical model scheme, the connec-\\ntion is missing in the non-hierarchical design.\\nIn multilabel classification, both the prediction bY:= g(x) and the true response Yare\\nsubsets of the label set{0,..., c−1}. A reasonable metric is the so-calledexact match ratioexact match\\nratio\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 274, 'page_label': '257'}, page_content='Classification 257\\ndefined as\\nexact match ratio =\\nPn\\ni=1 1{bYi = Yi}\\nn .\\nThe exact match ratio is rather stringent, as it requires a full match. In order to consider\\npartial correctness, the following metrics could be used instead.\\n• The accuracy is defined as the ratio of correctly predicted labels and the total number\\nof predicted and actual labels. The formula is given by\\naccuracy =\\nPn\\ni=1 |Yi ∩bYi|\\nPn\\ni=1 |Yi ∪bYi|\\n.\\n• The precision is defined as the ratio of correctly predicted labels and the total number\\nof predicted labels. Specifically,\\nprecision =\\nPn\\ni=1 |Yi ∩bYi|\\nPn\\ni=1 |bYi|\\n. (7.7)\\n• The recall is defined as the ratio of correctly predicted labels and the total number of\\nactual labels. Specifically,\\nrecall =\\nPn\\ni=1 |Yi ∩bYi|Pn\\ni=1 |Yi| . (7.8)\\n• The Hamming loss counts the average number of incorrect predictions for all classes,\\ncalculated as\\nHamming = 1\\nn c\\nnX\\ni=1\\nc−1X\\ny=0\\n1{y ∈bYi}1{y < Yi}+ 1{y < bYi}1{y ∈Yi}.\\n7.3 Classification via Bayes’ Rule\\nWe saw from Theorem 7.1 that the optimal classifier for classes 0 ,..., c −1 divides the\\nfeature space into c regions, depending on f (y |x): the conditional pdf of the response Y\\ngiven the feature vector X = x. In particular, if f (y |x) > f (z |x) for all z , y, the feature\\nvector x is classified as y. Classifying feature vectors on the basis of their conditional class\\nprobabilities is a natural thing to do, especially in a Bayesian learning context; see Sec-\\ntion 2.9 for an overview of Bayesian terminology and usage. Specifically, the conditional ☞ 48\\nprobability f (y |x) is interpreted as a posterior probability, of the form\\nf (y |x) ∝f (x |y) f (y), (7.9)\\nwhere f (x |y) is the likelihood of obtaining feature vector x from class y and f (y) is the\\nprior probability1 of class y. By making various modeling assumptions about the prior\\n1Here we have used the Bayesian notation convention of “overloading” the notation f .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 275, 'page_label': '258'}, page_content='258 Classification via Bayes’ Rule\\n(e.g., all classes are a priori equally likely) and the likelihood function, one obtains the\\nposterior pdf via Bayes’ formula (7.9). A class by is then assigned to a feature vector x\\naccording to the highest posterior probability; that is, we classify according to the Bayes\\noptimal decision ruleBayes optimal\\ndecision rule\\n:\\nby = argmax\\ny\\nf (y |x), (7.10)\\nwhich is exactly (7.2). Since the discrete density f (y |x), y = 0,..., c −1 is usually not\\nknown, the aim is to approximate it well with a function g(y |x) from some class of func-\\ntions G. Note that in this context, g(·|x) refers to a discrete density (a probability mass\\nfunction) for a given x.\\nSuppose a feature vector x = [x1,..., xp]⊤of p features has to be classified into one of\\nthe classes 0,..., c −1. For example, the classes could be different people and the features\\ncould be various facial measurements, such as the width of the eyes divided by the distance\\nbetween the eyes, or the ratio of the nose height and mouth width. In the naïve Bayesna¨ive Bayes\\nmethod, the class of approximating functions Gis chosen such that g(x |y) = g(x1 |y) ···\\ng(xp |y), that is, conditional on the label, all features are independent. Assuming a uniform\\nprior for y, the posterior pdf can thus be written as\\ng(y |x) ∝\\npY\\nj=1\\ng(xj |y),\\nwhere the marginal pdfs g(xj |y), j = 1,..., p belong to a given class of approximating\\nfunctions G. To classify x, simply take the y that maximizes the unnormalized posterior\\npdf.\\nFor instance, suppose that the approximating class Gis such that (Xj |y) ∼N(µy j,σ2),\\ny = 0,..., c −1, j = 1,..., p. The corresponding posterior pdf is then\\ng(y |θ,x) ∝exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\npX\\nj=1\\n(xj −µy j)2\\nσ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 = exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\n∥x −µy∥2\\nσ2\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere µy := [µy1,...,µ yp ]⊤ and θ := {µ0,..., µc−1,σ2}collects all model parameters. The\\nprobability g(y |θ,x) is maximal when ∥x −µy∥is minimal. Thus by = argminy ∥x −µy∥is\\nthe classifier that maximizes the posterior probability. That is, classify x as y when µy is\\nclosest to x in Euclidean distance. Of course, the parameters (here, the {µy}and σ2) are\\nunknown and have to be estimated from the training data.\\nWe can extend the above idea to the case where also the variance σ2 depends on the\\nclass y and feature j, as in the next example.\\nExample 7.1 (Naïve Bayes Classification) Table 7.5 lists the meansµand standard de-\\nviations σof p = 3 normally distributed features, for c = 4 different classes. How should\\na feature vector x = [1.67,2.00,4.23]⊤be classified? The posterior pdf is\\ng(y |θ,x) ∝(σy1σy2σy3)−1 exp\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed−1\\n2\\n3X\\nj=1\\n(xj −µy j)2\\nσ2\\ny j\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere θ:= {σj,µj}c−1\\nj=0 again collects all model parameters. The (unscaled) values for\\ng(y |θ,x), y = 0,1,2,3 are 53.5, 0.24, 8.37, and 3.5 ×10−6, respectively. Hence, the feature\\nvector should be classified as 0. The code follows.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 276, 'page_label': '259'}, page_content=\"Classification 259\\nTable 7.5: Feature parameters.\\nFeature 1 Feature 2 Feature 3\\nClass µ σ µ σ µ σ\\n0 1.6 0.1 2.4 0.5 4.3 0.2\\n1 1.5 0.2 2.9 0.6 6.1 0.9\\n2 1.8 0.3 2.5 0.3 4.2 0.3\\n3 1.1 0.2 3.1 0.7 5.6 0.3\\nnaiveBayes.py\\nimport numpy as np\\nx = np.array([1.67,2,4.23]).reshape(1,3)\\nmu = np.array([1.6, 2.4, 4.3,\\n1.5, 2.9, 6.1,\\n1.8, 2.5, 4.2,\\n1.1, 3.1, 5.6]).reshape(4,3)\\nsig = np.array([0.1, 0.5, 0.2,\\n0.2, 0.6, 0.9,\\n0.3, 0.3, 0.3,\\n0.2, 0.7, 0.3]).reshape(4,3)\\ng = lambda y: 1/np.prod(sig[y,:]) * np.exp(\\n-0.5*np. sum ((x-mu[y,:])**2/sig[y,:]**2));\\nfor y in range (0,4):\\nprint ('{:3.2e} '.format (g(y)))\\n5.35e+01\\n2.42e-01\\n8.37e+00\\n3.53e-06\\n7.4 Linear and Quadratic Discriminant Analysis\\nThe Bayesian viewpoint for classification of the previous section (not limited to naïve\\nBayes) leads in a natural way to the well-established technique of discriminant analysis discriminant\\nanalysis\\n.\\nWe discuss the binary classification case first, with classes 0 and 1.\\nWe consider a class of approximating functions Gsuch that, conditional on the class\\ny ∈{0,1}, the feature vector X = [X1,..., Xp]⊤has a N(µy,Σy) distribution (see (2.33)): ☞ 46\\ng(x |θ,y) = 1p(2π)p |Σy|\\ne−1\\n2 (x−µy)⊤Σ−1\\ny (x−µy), x ∈Rp, y ∈{0,1}, (7.11)\\nwhere θ = {αj,µj,Σj}c−1\\nj=0 collects all model parameters, including the probability vector α\\n(that is, P\\ni αi = 1 and αi ⩾0) which helps define the prior density: g(y |θ) = αy, y ∈{0,1}.\\nThen, the posterior density is\\ng(y |θ,x) ∝αy ×g(x |θ,y),\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 277, 'page_label': '260'}, page_content='260 Linear and Quadratic Discriminant Analysis\\nand, according to the Bayes optimal decision rule (7.10), we classify x to come from class\\n0 if α0g(x |θ,0) >α1g(x |θ,1) or, equivalently (by taking logarithms) if,\\nln α0 −1\\n2 ln |Σ0|− 1\\n2(x −µ0)⊤Σ−1\\n0 (x −µ0) >ln α1 −1\\n2 ln |Σ1|− 1\\n2(x −µ1)⊤Σ−1\\n1 (x −µ1).\\nThe function\\nδy(x) = ln αy −1\\n2 ln |Σy|− 1\\n2(x −µy)⊤Σ−1\\ny (x −µy), x ∈Rp (7.12)\\nis called the quadratic discriminant functionquadratic\\ndiscriminant\\nfunction\\nfor class y = 0,1. A point x is classified to\\nclass y for which δy(x) is largest. The function is quadratic in x and so the decision bound-\\nary {x ∈Rp : δ0(x) = δ1(x)}is quadratic as well. An important simplification arises for the\\ncase where the assumption is made that Σ0 = Σ1 = Σ. Now, the decision boundary is the\\nset of x for which\\nln α0 −1\\n2(x −µ0)⊤Σ−1(x −µ0) = ln α1 −1\\n2(x −µ1)⊤Σ−1(x −µ1).\\nExpanding the above expression shows that the quadratic term in x is eliminated, giving a\\nlinear decision boundary in x:\\nln α0 −1\\n2µ⊤\\n0 Σ−1µ0 + x⊤Σ−1µ0 = ln α1 −1\\n2µ⊤\\n1 Σ−1µ1 + x⊤Σ−1µ1.\\nThe corresponding linear discriminant functionlinear\\ndiscriminant\\nfunction\\nfor class y is\\nδy(x) = ln αy −1\\n2µ⊤\\ny Σ−1µy + x⊤Σ−1µy, x ∈Rp. (7.13)\\nExample 7.2 (Linear Discriminant Analysis) Consider the case whereα0 = α1 = 1/2\\nand\\nΣ =\\n\" 2 0 .7\\n0.7 2\\n#\\n, µ0 =\\n\"0\\n0\\n#\\n, µ1 =\\n\"2\\n4\\n#\\n.\\nThe distribution of X is a mixture of two bivariate normal distributions. Its pdf,☞135\\n1\\n2g(x |θ,y = 0) + 1\\n2g(x |θ,y = 1),\\nis depicted in Figure 7.2.\\nFigure 7.2: A Gaussian mixture density where the two mixture components have the same\\ncovariance matrix.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 278, 'page_label': '261'}, page_content=\"Classification 261\\nWe used the following Python code to make this figure.\\nLDAmixture.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom scipy.stats import multivariate_normal\\nfrom mpl_toolkits.mplot3d import Axes3D\\nfrom matplotlib.colors import LightSource\\nmu0, mu1 = np.array([0,0]), np.array([2,4])\\nSigma = np.array([[2,0.7],[0.7, 2]])\\nx, y = np.mgrid[-4:6:150j,-5:8:150j]\\nmvn0 = multivariate_normal( mu0, Sigma )\\nmvn1 = multivariate_normal( mu1, Sigma )\\nxy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\\nz = 0.5*mvn0.pdf(xy).reshape(x.shape) + 0.5*mvn1.pdf(xy).reshape(x.\\nshape)\\nfig = plt.figure()\\nax = fig.gca(projection= '3d')\\nls = LightSource(azdeg=180, altdeg=65)\\ncols = ls.shade(z, plt.cm.winter)\\nsurf = ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,\\nantialiased=False, facecolors=cols)\\nplt.show()\\nThe following Python code, which imports the previous code, draws a contour plot of\\nthe mixture density, simulates 1000 data points from the mixture density, and draws the\\ndecision boundary. To compute and display the linear decision boundary, let [ a1,a2]⊤ =\\n2Σ−1(µ1 −µ0) and b = µ⊤\\n0 Σ−1µ0 −µ⊤\\n1 Σ−1µ1. Then, the decision boundary can be written\\nas a1 x1 + a2 x2 + b = 0 or, equivalently, x2 = −(a1 x1 + b)/a2. We see in Figure 7.3 that the\\ndecision boundary nicely separates the two modes of the mixture density.\\nLDA.py\\nfrom LDAmixture import *\\nfrom numpy.random import rand\\nfrom numpy.linalg import inv\\nfig = plt.figure()\\nplt.contourf(x, y,z, cmap=plt.cm.Blues, alpha= 0.9,extend= 'both ')\\nplt.ylim(-5.0,8.0)\\nplt.xlim(-4.0,6.0)\\nM = 1000\\nr = (rand(M,1) < 0.5)\\nfor i in range (0,M):\\nif r[i]:\\nu = np.random.multivariate_normal(mu0,Sigma,1)\\nplt.plot(u[0][0],u[0][1], '.r',alpha = 0.4)\\nelse :\\nu = np.random.multivariate_normal(mu1,Sigma,1)\\nplt.plot(u[0][0],u[0][1], '+k',alpha = 0.6)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 279, 'page_label': '262'}, page_content=\"262 Linear and Quadratic Discriminant Analysis\\na = 2*inv(Sigma) @ (mu1-mu0);\\nb = ( mu0.reshape(1,2) @ inv(Sigma) @ mu0.reshape(2,1)\\n- mu1.reshape(1,2) @ inv(Sigma) @mu1.reshape(2,1) )\\nxx = np.linspace(-4,6,100)\\nyy = (-(a[0]*xx +b)/a[1])[0]\\nplt.plot(xx,yy, 'm')\\nplt.show()\\n4\\n 2\\n 0\\n 2\\n 4\\n 6\\n4\\n2\\n0\\n2\\n4\\n6\\n8\\nFigure 7.3: The linear discriminant boundary lies between the two modes of the mixture\\ndensity and is linear.\\nTo illustrate the difference between the linear and quadratic case, we specify di fferent\\ncovariance matrices for the mixture components in the next example.\\nExample 7.3 (Quadratic Discriminant Analysis) As in Example 7.2 we consider a\\nmixture of two Gaussians, but now with di fferent covariance matrices. Figure 7.4 shows\\nthe quadratic decision boundary. The Python code follows.\\n2\\n 1\\n 0\\n 1\\n 2\\n 3\\n 4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nFigure 7.4: A quadratic decision boundary.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 280, 'page_label': '263'}, page_content=\"Classification 263\\nQDA.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.stats import multivariate_normal\\nmu1 = np.array([0,0])\\nmu2 = np.array([2,2])\\nSigma1 = np.array([[1,0.3],[0.3, 1]])\\nSigma2 = np.array([[0.3,0.3],[0.3, 1]])\\nx, y = np.mgrid[-2:4:150j,-3:5:150j]\\nmvn1 = multivariate_normal( mu1, Sigma1 )\\nmvn2 = multivariate_normal( mu2, Sigma2 )\\nxy = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\\nz = ( 0.5*mvn1.pdf(xy).reshape(x.shape) +\\n0.5*mvn2.pdf(xy).reshape(x.shape) )\\nplt.contour(x,y,z)\\nz1 = ( 0.5*mvn1.pdf(xy).reshape(x.shape) -\\n0.5*mvn2.pdf(xy).reshape(x.shape))\\nplt.contour(x,y,z1, levels=[0],linestyles = 'dashed ',\\nlinewidths = 2, colors = 'm')\\nplt.show()\\nOf course, in practice the true parameter θ = {αj,Σj,µj}c\\nj=1 is not known and must be\\nestimated from the training data — for example, by minimizing the cross-entropy training\\nloss (4.4) with respect to θ: ☞ 123\\n1\\nn\\nnX\\ni=1\\nLoss( f (xi,yi),g(xi,yi |θ)) = −1\\nn\\nnX\\ni=1\\nln g(xi,yi |θ),\\nwhere\\nln g(x,y |θ) = ln αy −1\\n2 ln |Σy|− 1\\n2 (x −µy)⊤Σ−1\\ny (x −µy) −p\\n2 ln(2π).\\nThe corresponding estimates of the model parameters (see Exercise 2) are:\\nbαy = ny\\nn\\nbµy = 1\\nny\\nX\\ni:yi=y\\nxi\\nbΣy = 1\\nny\\nX\\ni:yi=y\\n(xi −bµy)(xi −bµy)⊤\\n(7.14)\\nfor y = 0,..., c −1, where ny := Pn\\ni=1 1{yi = y}. For the case where Σy = Σ for all y, we\\nhave bΣ = P\\ny bαy bΣy.\\nWhen c >2 classes are involved, the classification procedure carries through in exactly\\nthe same way, leading to quadratic and linear discriminant functions (7.12) and (7.13) for\\neach class. The space Rp now is partitioned into c regions, determined by the linear or\\nquadratic boundaries determined by each pair of Gaussians.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 281, 'page_label': '264'}, page_content='264 Linear and Quadratic Discriminant Analysis\\nFor the linear discriminant case (that is, when Σy = Σ for all y), it is convenient to first\\n“whiten” or sphere the datasphere the data as follows. Let B be an invertible matrix such that Σ = BB⊤,\\nobtained, for example, via the Cholesky method. We linearly transform each data point x☞373\\nto x′ := B−1 x and each mean µy to µ′\\ny := B−1µy, y = 0,..., c −1. Let the random vector X\\nbe distributed according to the mixture pdf\\ngX(x |θ) :=\\nX\\ny\\nαy\\n1p(2π)p |Σy|\\ne−1\\n2 (x−µy)⊤Σ−1\\ny (x−µy).\\nThen, by the transformation Theorem C.4, the vector X′= B−1 X has density☞433\\ngX′(x′|θ) = gX(x |θ)\\n|B−1| =\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 (x−µy)⊤(BB⊤)−1(x−µy)\\n=\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 (x′−µ′\\ny)⊤(x′−µ′\\ny) =\\nc−1X\\ny=0\\nαy\\n√(2π)p e−1\\n2 ∥x′−µ′\\ny∥2\\n.\\nThis is the pdf of a mixture of standard p-dimensional normal distributions. The name\\n“sphering” derives from the fact that the contours of each mixture component are perfect\\nspheres. Classification of the transformed data is now particularly easy: classify x as by :=\\nargminy{∥x′−µ′\\ny∥2 −2 lnαy}. Note that this rule only depends on the prior probabilities and\\nthe distance from x′to the transformed means {µ′\\ny}. This procedure can lead to a significant\\ndimensionality reduction of the data. Namely, the data can be projected onto the space\\nspanned by the di fferences between the mean vectors {µ′\\ny}. When there are c classes, this\\nis a (c −1)-dimensional space, as opposed to the p-dimensional space of the original data.\\nWe explain the precise ideas via an example.\\nExample 7.4 (Classification after Data Reduction) Consider an equal mixture of\\nthree 3-dimensional Gaussian distributions with identical covariance matrices. After spher-\\ning the data, the covariance matrices are all equal to the identity matrix. Suppose the mean\\nvectors of the sphered data are µ1 = [2,1,−3]⊤, µ2 = [1,−4,0]⊤, and µ3 = [2,4,6]⊤. The\\nleft panel of Figure 7.5 shows the 3-dimensional (sphered) data from each of the three\\nclasses.\\n4 2 0 2 4 6\\n4\\n2\\n0\\n2\\n4\\n5\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n6\\n 4\\n 2\\n 0\\n 2\\n 4\\n 6\\n4\\n2\\n0\\n2\\n4\\n6\\nFigure 7.5: Left: original data. Right: projected data.\\nThe data are stored in three 1000×3 matrices X1, X2, and X3. Here is how the data was\\ngenerated and plotted.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 282, 'page_label': '265'}, page_content=\"Classification 265\\ndatared.py\\nimport numpy as np\\nfrom numpy.random import randn\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nn=1000\\nmu1 = np.array([2,1,-3])\\nmu2 = np.array([1,-4,0])\\nmu3 = np.array([2,4,0])\\nX1 = randn(n,3) + mu1\\nX2 = randn(n,3) + mu2\\nX3 = randn(n,3) + mu3\\nfig = plt.figure()\\nax = fig.gca(projection= '3d',)\\nax.plot(X1[:,0],X1[:,1],X1[:,2], 'r.',alpha=0.5,markersize=2)\\nax.plot(X2[:,0],X2[:,1],X2[:,2], 'b.',alpha=0.5,markersize=2)\\nax.plot(X3[:,0],X3[:,1],X3[:,2], 'g.',alpha=0.5,markersize=2)\\nax.set_xlim3d(-4,6)\\nax.set_ylim3d(-5,5)\\nax.set_zlim3d(-5,2)\\nplt.show()\\nSince we have equal mixtures, we classify each data point x according to the closest\\ndistance to µ1, µ2, or µ3. We can achieve a reduction in the dimensionality of the data by\\nprojecting the data onto the two-dimensional a ffine space spanned by the {µi}; that is, all\\nvectors are of the form\\nµ1 + β1(µ2 −µ1) + β2(µ3 −µ1), β 1,β2 ∈R.\\nIn fact, one may just as well project the data onto the subspace spanned by the vectors\\nµ21 = µ2 −µ1 and µ31 = µ3 −µ1. Let W = [µ21,µ31] be the 3 ×2 matrix whose columns\\nare µ21 and µ31. The orthogonal projection matrix onto the subspace Wspanned by the\\ncolumns of W is (see Theorem A.4): ☞ 362\\nP = WW+ = W(W⊤W)−1W⊤.\\nLet UDV⊤be the singular value decomposition of W. Then P can also be written as\\nP = UD(D⊤D)−1D⊤U⊤.\\nNote that D has dimension 3 ×2, so is not square. The first two columns of U, say u1\\nand u2, form an orthonormal basis of the subspace W. What we want to do is rotate this\\nsubspace to the x−y plane, mapping u1 and u2 to [1,0,0]⊤and [0,1,0]⊤, respectively. This\\nis achieved via the rotation matrix U−1 = U⊤, giving the skewed projection matrix\\nR = U⊤P = D(D⊤D)−1D⊤U⊤,\\nwhose 3rd row only contains zeros. Applying R to all the data points, and ignoring the\\n3rd component of the projected points (which is 0), gives the right panel of Figure 7.5.\\nWe see that the projected points are much better separated than the original ones. We have\\nachieved dimensionality reduction of the data while retaining all the necessary information\\nrequired for classification. Here is the rest of the Python code.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 283, 'page_label': '266'}, page_content=\"266 Logistic Regression and Softmax Classification\\ndataproj.py\\nfrom datared import *\\nfrom numpy.linalg import svd, pinv\\nmu21 = (mu2 - mu1).reshape(3,1)\\nmu31 = (mu3 - mu1).reshape(3,1)\\nW = np.hstack((mu21, mu31))\\nU,_,_ = svd(W) # we only need U\\nP = W @ pinv(W)\\nR = U.T @ P\\nRX1 = (R @ X1.T).T\\nRX2 = (R @ X2.T).T\\nRX3 = (R @ X3.T).T\\nplt.plot(RX1[:,0],RX1[:,1], 'b.',alpha=0.5,markersize=2)\\nplt.plot(RX2[:,0],RX2[:,1], 'g.',alpha=0.5,markersize=2)\\nplt.plot(RX3[:,0],RX3[:,1], 'r.',alpha=0.5,markersize=2)\\nplt.show()\\n7.5 Logistic Regression and Softmax Classification\\nIn Example 5.10 we introduced the logistic (logit) regression model as a generalized linear☞205\\nmodel where, conditional on a p-dimensonal feature vector x, the random response Y has\\na Ber(h(x⊤β)) distribution with h(u) = 1/(1 + e−u). The parameter βwas then learned from\\nthe training data by maximizing the likelihood of the training responses or, equivalently,\\nby minimizing the supervised version of the cross-entropy training loss (4.4):☞123\\n−1\\nn\\nnX\\ni=1\\nln g(yi |β,xi),\\nwhere g(y = 1 |β,x) = 1/(1 + e−x⊤β) and g(y = 0 |β,x) = e−x⊤β/(1 + e−x⊤β). In particular,\\nwe have\\nln g(y = 1 |β,x)\\ng(y = 0 |β,x) = x⊤β. (7.15)\\nIn other words, the log-odds ratiolog-odds ratio is a linear function of the feature vector. As a con-\\nsequence, the decision boundary {x : g(y = 0 |β,x) = g(y = 1 |β,x)} is the hyperplane\\nx⊤β= 0. Note that x typically includes the constant feature. If the constant feature is con-\\nsidered separately, that is x = [1,ex⊤]⊤, then the boundary is an affine hyperplane in ex.\\nSuppose that training on τ= {(xi,yi)}yields the estimate bβ with the corresponding\\nlearner gτ(y = 1 |x) = 1/(1 + e−x⊤bβ). The learner can be used as a pre-classifier from which\\nwe obtain the classifier 1{gτ(y = 1 |x) >1/2}or, equivalently,\\nby := argmax\\nj∈{0,1}\\ngτ(y = j |x),\\nin accordance with the fundamental classification rule (7.2).\\nThe above classification methodology for the logit model can be generalized to the\\nmulti-logitmulti-logit model where the response takes values in the set {0,..., c −1}. The key idea is\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 284, 'page_label': '267'}, page_content='Classification 267\\nto replace (7.15) with\\nln g(y = j |W,b,x)\\ng(y = 0 |W,b,x) = x⊤βj, j = 1,..., c −1, (7.16)\\nwhere the matrix W ∈R(c−1)×(p−1) and vector b ∈Rc−1 reparameterize all βj ∈Rp such that\\n(recall x = [1,ex⊤]⊤):\\nWex + b = [β1,..., βc−1]⊤x.\\nObserve that the random response Y is assumed to have a conditional probability distri-\\nbution for which the log-odds ratio with respect to class j and a “reference” class (in this\\ncase 0) is linear. The separating boundaries between two pairs of classes are again a ffine\\nhyperplanes.\\nThe model (7.16) completely specifies the distribution of Y, namely:\\ng(y |W,b,x) = exp(zy+1)Pc\\nk=1 exp(zk), y = 0,..., c −1,\\nwhere z1 is an arbitrary constant, say 0, corresponding to the “reference” class y = 0, and\\n[z2,..., zc]⊤:= Wex + b.\\nNote that g(y |W,b,x) is the (y + 1)-st component of a = softmax(z), where\\nsoftmax : z 7→ exp(z)P\\nk exp(zk)\\nis the softmax softmaxfunction and z = [z1,..., zc]⊤. Finally, we can write the classifier as\\nby = argmax\\nj∈{0,...,c−1}\\naj+1.\\nIn summary, we have the sequence of mappings transforming the inputx into the outputby:\\nx →Wex + b →softmax(z) → argmax\\nj ∈{0,...,c−1}\\naj+1 →by.\\nIn Example 9.4 we will revisit the multi-logit model and reinterpret this sequence of map- ☞ 334\\npings as a neural network. In the context of neural networks, W is called a weight matrix\\nand b is called a bias vector.\\nThe parameters W and b have to be learned from the training data, which involves\\nminimization of the supervised version of the cross-entropy training loss (4.4): ☞ 123\\n1\\nn\\nnX\\ni=1\\nLoss( f (yi |xi),g(yi |W,b,xi)) = −1\\nn\\nnX\\ni=1\\nln g(yi |W,b,xi).\\nUsing the softmax function, the cross-entropy loss can be simplified to:\\nLoss( f (y |x),g(y |W,b,x)) = −zy+1 + ln\\ncX\\nk=1\\nexp(zk). (7.17)\\nThe discussion on training is postponed until Chapter 9, where we reinterpret the multi-\\nlogit model as a neural net, which can be trained using the limited-memory BFGS method\\n(Exercise 11). Note that in the binary case ( c = 2), where there is only one vector β to ☞ 353\\nbe estimated, Example 5.10 already established that minimization of the cross-entropy\\ntraining loss is equivalent to likelihood maximization.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 285, 'page_label': '268'}, page_content='268 K-Nearest Neighbors Classification\\n7.6 K-Nearest Neighbors Classification\\nLet τ = {(xi,yi)}n\\ni=1 be the training set, with yi ∈{0,..., c −1}, and let x be a new feature\\nvector. Define x(1),x(2),..., x(n) as the feature vectors ordered by closeness tox in some dis-\\ntance dist(x,xi), e.g., the Euclidean distance∥x−x′∥. Let τ(x) := {(x(1),y(1)) ..., (x(K),y(K))}\\nbe the subset ofτthat contains K feature vectors xi that are closest to x. Then the K-nearest\\nneighborsK-nearest\\nneighbors\\nclassification rule classifies x according to the most frequently occurring class\\nlabels in τ(x). If two or more labels receive the same number of votes, the feature vector\\nis classified by selecting one of these labels randomly with equal probability. For the case\\nK = 1 the set τ(x) contains only one element, say ( x′,y′), and x is classified as y′. This\\ndivides the space into n regions\\nRi = {x : dist(x,xi) ⩽dist(x,xj), j , i}, i = 1,..., n.\\nFor a feature space Rp with the Euclidean distance, this gives a V oronoi tessellation of the\\nfeature space, similar to what was done for vector quantization in Section 4.6.☞142\\nExample 7.5 (Nearest Neighbor Classification) The Python program below simulates\\n80 random points above and below the line x2 = x1. Points above the line x2 = x1 have\\nlabel 0 and points below this line have label 1. Figure 7.6 shows the V oronoi tessellation\\nobtained from the 1-nearest neighbor classification.\\n2\\n 1\\n 0\\n 1\\n 2\\n 3\\n4\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\nFigure 7.6: The 1-nearest neighbor algorithm divides up the space into V oronoi cells.\\nnearestnb.py\\nimport numpy as np\\nfrom numpy.random import rand,randn\\nimport matplotlib.pyplot as plt\\nfrom scipy.spatial import Voronoi, voronoi_plot_2d'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 286, 'page_label': '269'}, page_content=\"Classification 269\\nnp.random.seed(12345)\\nM = 80\\nx = randn(M,2)\\ny = np.zeros(M) # pre -allocate list\\nfor i in range (M):\\nif rand()<0.5:\\nx[i,1], y[i] = x[i,0] + np. abs (randn()), 0\\nelse :\\nx[i,1], y[i] = x[i,0] - np. abs (randn()), 1\\nvor = Voronoi(x)\\nplt_options = { 'show_vertices ':False, 'show_points ':False,\\n'line_alpha ':0.5}\\nfig = voronoi_plot_2d(vor, **plt_options)\\nplt.plot(x[y==0,0], x[y==0,1], 'bo',\\nx[y==1,0], x[y==1,1], 'rs', markersize=3)\\n7.7 Support Vector Machine\\nSuppose we are given the training set τ= {(xi,yi)}n\\ni=1, where each response2 yi takes either\\nthe value −1 or 1, and we wish to construct a classifier taking values in {−1,1}. As this\\nmerely involves a relabeling of the 0–1 classification problem in Section 7.1, the optimal\\nclassification function for the indicator loss, 1{y , by}, is, by Theorem 7.1, equal to\\ng∗(x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1 if P[Y = 1 |X = x] ⩾1/2,\\n−1 if P[Y = 1 |X = x] <1/2.\\nIt is not difficult to show, see Exercise 5, that the functiong∗can be viewed as the minimizer\\nof the risk for the hinge loss hinge lossfunction, Loss(y,by) = (1 −yby)+ := max{0, 1 −yby}, over all\\nprediction functions g (not necessarily taking values only in the set {−1,1}). That is,\\ng∗= argmin\\ng\\nE(1 −Y g(X))+. (7.18)\\nGiven the training set τ, we can approximate the risk ℓ(g) = E(1 −Y g(X))+ with the train-\\ning loss\\nℓτ(g) = 1\\nn\\nnX\\ni=1\\n(1 −yi g(xi))+,\\nand minimize this over a (smaller) class of functions to obtain the optimal prediction func-\\ntion gτ. Finally, as the prediction functiongτ generally is not a classifier by itself (it usually\\ndoes not only take values −1 or 1), we take the classifier\\nsign gτ(x).\\n2The reason why we use responses −1 and 1 here, instead of 0 and 1, is that the notation becomes easier.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 287, 'page_label': '270'}, page_content='270 Support Vector Machine\\nTherefore, a feature vector x is classified according to 1 or −1 depending on whether\\ngτ(x) ⩾0 or < 0, respectively. The optimal decision boundaryoptimal decision\\nboundary\\nis given by the set of x for\\nwhich gτ(x) = 0.\\nSimilar to the cubic smoothing spline or RKHS setting in (6.19), we can consider find-\\ning the best classifier, given the training data, via the penalized goodness-of-fit optimiza-\\ntion:\\nmin\\ng∈H⊕H0\\n1\\nn\\nnX\\ni=1\\n[1 −yi g(xi)]+ + eγ∥g∥2\\nH,\\nfor some regularization parameter eγ. It will be convenient to define γ := 2neγand to solve\\nthe equivalent problem\\nmin\\ng∈H⊕H0\\nnX\\ni=1\\n[1 −yi g(xi)]+ + γ\\n2 ∥g∥2\\nH.\\nWe know from the Representer Theorem 6.6 that if κ is the reproducing kernel cor-☞231\\nresponding to H, then the solution is of the form (assuming that the null space H0 has a\\nconstant term only):\\ng(x) = α0 +\\nnX\\ni=1\\nαi κ(xi,x). (7.19)\\nSubstituting into the minimization expression yields the analogue of (6.21):☞232\\nmin\\nα,α0\\nnX\\ni=1\\n[1 −yi(α0 + {Kα}i)]+ + γ\\n2 α⊤Kα, (7.20)\\nwhere K is the Gram matrix. This is a convex optimization problem, as it is the sum of a\\nconvex quadratic and piecewise linear term in α. Defining λi := γαi/yi, i = 1,..., n and\\nλ := [λ1,...,λ n]⊤, we show in Exercise 10 that the optimal α and α0 in (7.20) can be\\nobtained by solving the “dual” convex optimization problem\\nmax\\nλ\\nnX\\ni=1\\nλi − 1\\n2γ\\nnX\\ni=1\\nnX\\nj=1\\nλiλjyiyj κ(xi,xj)\\nsubject to: λ⊤y = 0, 0 ⩽λ⩽1,\\n(7.21)\\nand α0 = yj −P\\ni=1 αi κ(xi,xj) for any j for which λj ∈(0,1). In view of (7.19), the optimal\\nprediction function (pre-classifier) gτ is then given by\\ngτ(x) = α0 +\\nnX\\ni=1\\nαi κ(xi,x) = α0 + 1\\nγ\\nnX\\ni=1\\nyiλi κ(xi,x). (7.22)\\nTo mitigate possible numerical problems in the calculation ofα0 it is customary to take\\nan overall average:\\nα0 = 1\\n|J|\\nX\\nj∈J\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3yj −\\nnX\\ni=1\\nαi κ(xi,xj)\\n\\uf8fc\\uf8f4\\uf8f4\\uf8fd\\uf8f4\\uf8f4\\uf8fe,\\nwhere J:= {j : λj ∈(0,1)}.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 288, 'page_label': '271'}, page_content='Classification 271\\nNote that, from (7.22), the optimal pre-classifier g(x) and the classifier sign g(x) only\\ndepend on vectors xi for which λi , 0. These vectors are called the support vectors support vectorsof the\\nsupport vector machine. It is also important to note that the quadratic function in (7.21)\\ndepends on the regularization parameter γ. By defining νi := λi/γ, i = 1,..., n, we can\\nrewrite (7.21) as\\nmin\\nν\\n1\\n2\\nX\\ni,j\\nνiνjyiyj κ(xi,xj) −\\nnX\\ni=1\\nνi\\nsubject to:\\nnX\\ni=1\\nνiyi = 0, 0 ⩽νi ⩽1/γ=: C, i = 1,..., n.\\n(7.23)\\nFor perfectly separable data, that is, data for which an affine plane can be drawn to perfectly\\nseparate the two classes, we may take C = ∞, as explained below. Otherwise, C needs to\\nbe chosen via cross-validation or a test data set, for example.\\nGeometric interpretation\\nFor the linear kernel function κ(x,x′) = x⊤x′, we have\\ngτ(x) = β0 + β⊤x,\\nwith β0 = α0 and β= γ−1 Pn\\ni=1 λiyi xi = Pn\\ni=1 αi xi, and so the decision boundary is an affine\\nplane. The situation is illustrated in Figure 7.7. The decision boundary is formed by the\\npoints x such that gτ(x) = 0. The two sets {x : gτ(x) = −1}and {x : gτ(x) = 1}are called\\nthe margins. The distance from the points on a margin to the decision boundary is 1/∥β∥.\\n1 2\\n3\\nFigure 7.7: Classifying two classes (red and blue) using SVM.\\nBased on the “multipliers” {λi}, we can divide the training samples {(xi,yi)}into three\\ncategories (see Exercise 11):\\n• Points for which λi ∈(0,1). These are the support vectors on the margins (green\\nencircled in the figure) and are correctly classified.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 289, 'page_label': '272'}, page_content='272 Support Vector Machine\\n• Points for which λi = 1. These points, which are also support vectors, lie strictly\\ninside the margins (points 1, 2, and 3 in the figure). Such points may or may not be\\ncorrectly classified.\\n• Points for which λi = 0. These are the non-support vectors, which all lie outside the\\nmargins. Every such point is correctly classified.\\nIf the classes of points {xi : yi = 1}and {xi : yi = −1}are perfectly separable by some\\naffine plane, then there will be no points strictly inside the margins, so all support vectors\\nwill lie exactly on the margins. In this case (7.20) reduces to\\nmin\\nβ,β0\\n∥β∥2\\nsubject to: yi(β0 + x⊤\\ni β) ⩾1, i = 1,..., n,\\n(7.24)\\nusing the fact thatα0 = β0 and Kα= XX⊤α= Xβ. We may replace min∥β∥2 in (7.24) with\\nmax 1/∥β∥, as this gives the same optimal solution. As 1 /∥β∥is equal to half the margin\\nwidth, the latter optimization problem has a simple interpretation: separate the points via\\nan affine hyperplane such that the margin width is maximized.\\nExample 7.6 (Support Vector Machine) The data in Figure 7.8 was uniformly gener-\\nated on the unit disc. Class-1 points (blue dots) have a radius less than 1/2 (y-values 1) and\\nclass-2 points (red crosses) have a radius greater than 1/2 (y-values −1).\\n-1 -0.5 0 0.5 1\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nFigure 7.8: Separate the two classes.\\nOf course it is not possible to separate the two groups of points via a straight line in\\nR2. However, it is possible to separate them inR3 by considering three-dimensional feature\\nvectors z = [z1,z2,z3]⊤= [x1,x2,x2\\n1 + x2\\n2]⊤. For any x ∈R2, the corresponding feature vec-\\ntor z lies on a quadratic surface. In this space it is possible to separate the {zi}points into\\ntwo groups by means of a planar surface, as illustrated in Figure 7.9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 290, 'page_label': '273'}, page_content='Classification 273\\nFigure 7.9: In feature space R3 the points can be separated by a plane.\\nWe wish to find a separating plane inR3 using the transformed features. The following\\nPython code uses the SVC function of the sklearn module to solve the quadratic optimiz-\\nation problem (7.23) (with C = ∞). The results are summarized in Table 7.6. The data is\\navailable from the book’s GitHub site as svmcirc.csv.\\nsvmquad.py\\nimport numpy as np\\nfrom numpy import genfromtxt\\nfrom sklearn.svm import SVC\\ndata = genfromtxt( \\'svmcirc.csv \\', delimiter= \\',\\')\\nx = data[:,[0,1]] #vectors are rows\\ny = data[:,[2]].reshape( len (x),) #labels\\ntmp = np. sum (np.power(x,2),axis=1).reshape( len (x),1)\\nz = np.hstack((x,tmp))\\nclf = SVC(C = np.inf, kernel= \\'linear \\')\\nclf.fit(z,y)\\nprint (\"Support Vectors \\\\n\", clf.support_vectors_)\\nprint (\"Support Vector Labels \",y[clf.support_])\\nprint (\"Nu\",clf.dual_coef_)\\nprint (\"Bias\",clf.intercept_)\\nSupport Vectors\\n[[ 0.038758 0.53796 0.29090314]\\n[-0.49116 -0.20563 0.28352184]\\n[-0.45068 -0.04797 0.20541358]\\n[-0.061107 -0.41651 0.17721465]]\\nSupport Vector Labels [-1. -1. 1. 1.]\\nNu [[ -46.49249413 -249.01807328 265.31805855 30.19250886]]\\nBias [5.617891]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 291, 'page_label': '274'}, page_content='274 Support Vector Machine\\nTable 7.6: Optimal support vector machine parameters for the R3 data.\\nz⊤ y α= νy\\n0.0388 0.5380 0.2909 −1 −46.4925\\n−0.4912 −0.2056 0.2835 −1 −249.0181\\n−0.4507 −0.0480 0.2054 1 265.3181\\n−0.0611 −0.4165 0.1772 1 30.1925\\nIt follows that the normal vector of the plane is\\nβ=\\nX\\ni∈S\\nαi zi = [−0.9128,0.8917,−24.2764]⊤,\\nwhere Sis the set of indices of the support vectors. We see that the plane is almost per-\\npendicular to the z1,z2 plane. The bias term β0 can also be found from the table above. In\\nparticular, for any x⊤and y in Table 7.6, we have y −β⊤z = β0 = 5.6179.\\nTo draw the separating boundary in R2 we need to project the intersection of the sep-\\narating plane with the quadratic surface onto the z1,z2 plane. That is, we need to find all\\npoints (z1,z2) such that\\n5.6179 −0.9128z1 + 0.8917z2 = 24.2764 (z2\\n1 + z2\\n2). (7.25)\\nThis is the equation of a circle with (approximate) center (0.019,−0.018) and radius 0.48,\\nwhich is very close to the true circular boundary between the two groups, with center (0,0)\\nand radius 0.5. This circle is drawn in Figure 7.10.\\n-1 0 1\\n-1\\n0\\n1\\nFigure 7.10: The circular decision boundary can be viewed equivalently as (a) the pro-\\njection onto the x1,x2 plane of the intersection of the separating plane with the quadratic\\nsurface (both in R3), or (b) the set of points x = (x1,x2) for which gτ(x) = β0 +β⊤ϕ(x) = 0.\\nAn equivalent way to derive this circular separating boundary is to consider the feature\\nmap ϕ(x) = [x1,x2,x2\\n1 + x2\\n2]⊤on R2, which defines a reproducing kernel\\nκ(x,x′) = ϕ(x)⊤ϕ(x′),'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 292, 'page_label': '275'}, page_content='Classification 275\\non R2, which in turn gives rise to a (unique) RKHS H. The optimal prediction function\\n(7.19) is now of the form\\ngτ(x) = α0 + 1\\nγ\\nnX\\ni=1\\nyi λi ϕ(xi)⊤ϕ(x) = β0 + β⊤ϕ(x), (7.26)\\nwhere α0 = β0 and\\nβ= 1\\nγ\\nnX\\ni=1\\nyi λi ϕ(xi).\\nThe decision boundary, {x : gτ(x) = 0}, is again a circle in R2. The following code de-\\ntermines the fitted model parameters and the decision boundary. Figure 7.10 shows the\\noptimal decision boundary, which is identical to (7.25). The function mykernel specifies\\nthe custom kernel above.\\nsvmkern.py\\nimport numpy as np, matplotlib.pyplot as plt\\nfrom numpy import genfromtxt\\nfrom sklearn.svm import SVC\\ndef mykernel(U,V):\\ntmpU = np. sum (np.power(U,2),axis=1).reshape( len (U),1)\\nU = np.hstack((U,tmpU))\\ntmpV = np. sum (np.power(V,2),axis=1).reshape( len (V),1)\\nV = np.hstack((V,tmpV))\\nK = U @ V.T\\nprint (K.shape)\\nreturn K\\n# read in the data\\ninp = genfromtxt( \\'svmcirc.csv \\', delimiter= \\',\\')\\ndata = inp[:,[0,1]] #vectors are rows\\ny = inp[:,[2]].reshape( len (data),) #labels\\nclf = SVC(C = np.inf, kernel=mykernel, gamma= \\'auto \\') # custom kernel\\n# clf = SVC(C = np.inf , kernel=\"rbf\", gamma= \\'scale \\') # inbuilt\\nclf.fit(data,y)\\nprint (\"Support Vectors \\\\n\", clf.support_vectors_)\\nprint (\"Support Vector Labels \",y[clf.support_])\\nprint (\"Nu \",clf.dual_coef_)\\nprint (\"Bias \",clf.intercept_)\\n# plot\\nd = 0.001\\nx_min, x_max = -1,1\\ny_min, y_max = -1,1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, d), np.arange(y_min,\\ny_max, d))\\nplt.plot(data[clf.support_,0],data[clf.support_,1], \\'go\\')\\nplt.plot(data[y==1,0],data[y==1,1], \\'b.\\')'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 293, 'page_label': '276'}, page_content='276 Support Vector Machine\\nplt.plot(data[y==-1,0],data[y==-1,1], \\'rx\\')\\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\nplt.contour(xx, yy, Z,colors =\"k\")\\nplt.show()\\nFinally, we illustrate the use of the Gaussian kernel\\nκ(x,x′) = e−c ∥x−x′∥2\\n, (7.27)\\nwhere c >0 is some tuning constant. This is an example of a radial basis function kernel,\\nwhich are reproducing kernels of the form κ(x,x′) = f (∥x −x′∥), for some positive real-\\nvalued function f . Each feature vector x is now transformed to a function κx = κ(x,·). We\\ncan think of it as the (unnormalized) pdf of a Gaussian distribution centered around x, and\\ngτ is a (signed) mixture of these pdfs, plus a constant; that is,\\ngτ(x) = α0 +\\nnX\\ni=1\\nαi e−c ∥xi−x∥2\\n.\\nReplacing in Line 2 of the previous code mykernel with ’rbf’ produces the SVM\\nparameters given in Table 7.7. Figure 7.11 shows the decision boundary, which is not ex-\\nactly circular, but is close to the true (circular) boundary {x : ∥x∥= 1/2}. There are now\\nseven support vectors, rather than the four in Figure 7.10.\\nTable 7.7: Optimal support vector machine parameters for the Gaussian kernel case.\\nx⊤ y α(×109)\\n0.0388 0.5380 −1 −0.0635\\n−0.4912 −0.2056 −1 −9.4793\\n0.5086 0.1576 −1 −0.5240\\n−0.4507 −0.0480 1 5.5405\\nx⊤ y α(×109)\\n−0.4374 0.3854 −1 −1.4399\\n0.3402 −0.5740 −1 −0.1000\\n−0.4098 −0.1763 1 6.0662\\n-1 0 1\\n-1\\n0\\n1\\nFigure 7.11: Left: The decision boundary {x : gτ(x) = 0}is roughly circular, and separates\\nthe two classes well. There are seven support vectors, indicated by green circles. Right:\\nThe graph of gτ is a scaled mixture of Gaussian pdfs plus a constant.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 294, 'page_label': '277'}, page_content='Classification 277\\nRemark 7.2 (Scaling and Penalty Parameters) When using a radial basis function in\\nSVC in sklearn, the scaling c (7.27) can be set via the parameter gamma. Note that large\\nvalues of gammalead to highly peaked predicted functions, and small values lead to highly\\nsmoothed predicted functions. The parameter Cin SVC refers C = 1/γin (7.23).\\n7.8 Classification with Scikit-Learn\\nIn this section we apply several classification methods to a real-world data set, using the\\nPython module sklearn (the package name is Scikit-Learn). Specifically, the data is ob-\\ntained from UCI’s Breast Cancer Wisconsin data set. This data set, first published and\\nanalyzed in [118], contains the measurements related to 569 images of 357 benign and\\n212 malignant breast masses. The goal is to classify a breast mass as benign or malig-\\nnant based on 10 features: Radius, Texture, Perimeter, Area, Smoothness, Compactness,\\nConcavity, Concave Points, Symmetry, and Fractal Dimension of each mass. The mean,\\nstandard error, and “worst” of these attributes were computed for each image, resulting in\\n30 features. For instance, feature 1 is Mean Radius, feature 11 is Radius SE, feature 21 is\\nWorst Radius.\\nThe following Python code reads the data, extracts the response vector and model (fea-\\nture) matrix and divides the data into a training and test set.\\nskclass1.py\\nfrom numpy import genfromtxt\\nfrom sklearn.model_selection import train_test_split\\nurl1 = \"http://mlr.cs.umass.edu/ml/machine-learning-databases/\"\\nurl2 = \"breast-cancer-wisconsin/\"\\nname = \"wdbc.data\"\\ndata = genfromtxt(url1 + url2 + name, delimiter= \\',\\', dtype= str )\\ny = data[:,1] #responses\\nX = data[:,2:].astype( \\'float \\') #features as an ndarray matrix\\nX_train , X_test , y_train , y_test = train_test_split(\\nX, y, test_size = 0.4, random_state = 1234)\\nTo visualize the data we create a 3D scatterplot for the features mean radius, mean\\ntexture, and mean concavity, which correspond to the columns 0, 1, and 6 of the model\\nmatrix X. Figure 7.12 suggests that the malignant and benign breast masses could be well\\nseparated using these three features.\\nskclass2.py\\nfrom skclass1 import X, y\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nimport numpy as np\\nBidx = np.where(y == \\'B\\')\\nMidx= np.where(y == \\'M\\')'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 295, 'page_label': '278'}, page_content='278 Classification with Scikit-Learn\\n# plot features Radius (column 0), Texture (1), Concavity (6)\\nfig = plt.figure()\\nax = fig.gca(projection = \\'3d\\')\\nax.scatter(X[Bidx,0], X[Bidx,1], X[Bidx,6],\\nc=\\'r\\', marker= \\'^\\', label= \\'Benign \\')\\nax.scatter(X[Midx,0], X[Midx,1], X[Midx,6],\\nc=\\'b\\', marker= \\'o\\', label= \\'Malignant \\')\\nax.legend()\\nax.set_xlabel( \\'Mean Radius \\')\\nax.set_ylabel( \\'Mean Texture \\')\\nax.set_zlabel( \\'Mean Concavity \\')\\nplt.show()\\nMean Radius\\n10\\n15\\n20\\n25 Mean Texture10\\n15\\n20\\n25\\n30\\n35\\n40\\nMean Concavity\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nBenign\\nMalignant\\nFigure 7.12: Scatterplot of three features of the benign and malignant breast masses.\\nThe following code uses various classifiers to predict the category of breast masses\\n(benign or malignant). In this case the training set has 341 elements and the test set has 228\\nelements. For each classifier the percentage of correct predictions (that is, the accuracy) in\\nthe test set is reported. We see that in this case quadratic discriminant analysis gives the\\nhighest accuracy (0.956). Exercise 18 explores the question whether this metric is the most\\nappropriate for these data.\\nskclass3.py\\nfrom skclass1 import X_train, y_train, X_test, y_test\\nfrom sklearn.metrics import accuracy_score\\nimport sklearn.discriminant_analysis as DA\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nnames = [\"Logit\",\"NBayes\", \"LDA\", \"QDA\", \"KNN\", \"SVM\"]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 296, 'page_label': '279'}, page_content=\"Classification 279\\nclassifiers = [LogisticRegression(C=1e5),\\nGaussianNB(),\\nDA.LinearDiscriminantAnalysis(),\\nDA.QuadraticDiscriminantAnalysis(),\\nKNeighborsClassifier(n_neighbors=5),\\nSVC(kernel= 'rbf', gamma = 1e-4)]\\nprint ('Name Accuracy\\\\n '+14* '-')\\nfor name, clf in zip (names, classifiers):\\nclf.fit(X_train, y_train)\\ny_pred = clf.predict(X_test)\\nprint ('{:6} {:3.3f} '.format (name, accuracy_score(y_test,y_pred)))\\nName Accuracy\\n--------------\\nLogit 0.943\\nNBayes 0.908\\nLDA 0.943\\nQDA 0.956\\nKNN 0.925\\nSVM 0.939\\nFurther Reading\\nAn excellent source for understanding various pattern recognition techniques is the book\\n[35] by Duda et al. Theoretical foundations of classification, including the Vapnik–\\nChernovenkis dimension and the fundamental theorem of learning, are discussed in\\n[109, 121, 122]. A popular measure for characterizing the performance of a binary classi-\\nfier is the receiver operating characteristic (ROC) curve [38]. The naïve Bayes classific-\\nation paradigm can be extended to handle explanatory variable dependency via graphical\\nmodels such as Bayesian networks and Markov random fields [46, 66, 69]. For a detailed\\ndiscussion on Bayesian decision theory, see [8].\\nExercises\\n1. Let 0 ⩽w ⩽1. Show that the solution to the convex optimization problem\\nmin\\np1,...,pn\\nnX\\ni=1\\np2\\ni\\nsubject to:\\nn−1X\\ni−1\\npi = w and\\nnX\\ni=1\\npi = 1,\\n(7.28)\\nis given by pi = w/(n −1),i = 1,..., n −1 and pn = 1 −w.\\n2. Derive the formulas (7.14) by minimizing the cross-entropy training loss:\\n−1\\nn\\nnX\\ni=1\\nln g(xi,yi |θ),\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 297, 'page_label': '280'}, page_content='280 Exercises\\nwhere g(x,y |θ) is such that:\\nln g(x,y |θ) = ln αy −1\\n2 ln |Σy|− 1\\n2 (x −µy)⊤Σ−1\\ny (x −µy) −p\\n2 ln(2π).\\n3. Adapt the code in Example 7.2 to plot the estimated decision boundary instead of the\\ntrue one in Figure 7.3. Compare the true and estimated decision boundaries.\\n4. Recall from equation (7.16) that the decision boundaries of the multi-logit classifier are\\nlinear, and that the pre-classifier can be written as a conditional pdf of the form:\\ng(y |W,b,x) = exp(zy+1)Pc\\ni=1 exp(zi), y ∈{0,..., c −1},\\nwhere x⊤= [1,ex⊤] and z = Wex + b.\\n(a) Show that the linear discriminant pre-classifier in Section 7.4 can also be written as a\\nconditional pdf of the form (θ= {αy,Σy,µy}c−1\\ny=0):\\ng(y |θ,x) = exp(zy+1)Pc\\ni=1 exp(zi), y ∈{0,..., c −1},\\nwhere x⊤ = [1,ex⊤] and z = Wex + b. Find formulas for the corresponding b and W\\nin terms of the linear discriminant parameters {αy,µy,Σy}c−1\\ny=0, where Σy = Σ for all y.\\n(b) Explain which pre-classifier has smaller approximation error: the linear discriminant\\nor multi-logit one? Justify your answer by proving an inequality between the two\\napproximation errors.\\n5. Consider a binary classification problem where the response Y takes values in {−1,1}.\\nShow that optimal prediction function for the hinge loss Loss(y,by) = (1−yby)+ := max{0,1−\\nyby}is the same as the optimal prediction function g∗for the indicator loss:\\ng∗(x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1 if P[Y = 1 |X = x] >1/2,\\n−1 if P[Y = 1 |X = x] <1/2.\\nThat is, show that\\nE(1 −Y h(X))+ ⩾E(1 −Y g∗(X))+ (7.29)\\nfor all functions h.\\n6. In Example 4.12, we applied a principal component analysis (PCA) to the iris data,☞159\\nbut refrained from classifying the flowers based on their feature vectors x. Implement a\\n1-nearest neighbor algorithm, using a training set of 50 randomly chosen data pairs ( x,y)\\nfrom the iris data set. How many of the remaining 100 flowers are correctly classified?\\nNow classify these entries with an o ff-the-shelf multi-logit classifier, e.g., such as can be\\nfound in the sklearn and statsmodels packages.\\n7. Figure 7.13 displays two groups of data points, given in Table 7.8. The convex hulls\\nhave also been plotted. It is possible to separate the two classes of points via a straight line.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 298, 'page_label': '281'}, page_content='Classification 281\\nIn fact, many such lines are possible. SVM gives the best separation, in the sense that the\\ngap (margin) between the points is maximal.\\n-3 -2 -1 0 1 2 3 4\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nFigure 7.13: Separate the points by a straight line so that the separation between the two\\ngroups is maximal.\\nTable 7.8: Data for Figure 7.13.\\nx1 x2 y\\n2.4524 5 .5673 −1\\n1.2743 0 .8265 1\\n0.8773 −0.5478 1\\n1.4837 3 .0464 −1\\n0.0628 4 .0415 −1\\n−2.4151 −0.9309 1\\n1.8152 3 .9202 −1\\n1.8557 2 .7262 −1\\n−0.4239 1 .8349 1\\n1.9630 0 .6942 1\\nx1 x2 y\\n0.5819 −1.0156 1\\n1.2065 3 .2984 −1\\n2.6830 0 .4216 1\\n−0.0734 1 .3457 1\\n0.0787 0 .6363 1\\n0.3816 5 .2976 −1\\n0.3386 0 .2882 1\\n−0.1493 −0.7095 1\\n1.5554 4 .9880 −1\\n3.2031 4 .4614 −1\\n(a) Identify from the figure the three support vectors.\\n(b) For a separating boundary (line) given by β0 + β⊤x = 0, show that the margin width\\nis 2/∥β∥.\\n(c) Show that the parameters β0 and βthat solve the convex optimization problem (7.24)\\nprovide the maximal width between the margins.\\n(d) Solve (7.24) using a penalty approach; see Section B.4. In particular, minimize the ☞ 415'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 299, 'page_label': '282'}, page_content='282 Exercises\\npenalty function\\nS (β,β0) = ∥β∥2 −C\\nnX\\ni=1\\nmin\\nn\\n(β0 + β⊤xi) yi −1, 0\\no\\nfor some positive penalty constant C.\\n(e) Find the solution the dual optimization problem (7.21) by using sklearn’s SCV\\nmethod. Note that, as the two point sets are separable, the constraint λ ⩽1 may\\nbe removed, and the value of γcan be set to 1.\\n8. In Example 7.6 we used the feature map ϕ(x) = [x1,x2,x2\\n1 + x2\\n2]⊤to classify the points.\\nAn easier way is to map the points intoR1 via the feature map ϕ(x) = ∥x∥or any monotone\\nfunction thereof. Translated back into R2 this yields a circular separating boundary. Find\\nthe radius and center of this circle, using the fact that here the sorted norms for the two\\ngroups are ..., 0.4889,0.5528,... .\\n9. Let Y ∈{0,1}be a response variable and let h(x) be the regression function\\nh(x) := E[Y |X = x] = P[Y = 1 |X = x].\\nRecall that the Bayes classifier is g∗(x) = 1{h(x) > 1/2}. Let g : R → {0,1}be any\\nother classifier function. Below, we denote all probabilities and expectations conditional\\non X = x as Px[·] and Ex[·].\\n(a) Show that\\nPx[g(x) , Y] =\\nirreducible error\\nz           }|           {\\nPx[g∗(x) , Y] +|2h(x) −1|1{g(x) , g∗(x)}.\\nHence, deduce that for a learner gT constructed from a training set T, we have\\nE[Px[gT(x) , Y |T]] = Px[g∗(x) , Y] + |2h(x) −1|P[gT(x) , g∗(x)],\\nwhere the first expectation and last probability operations are with respect to T.\\n(b) Using the previous result, deduce that for the unconditional error (that is, we no longer\\ncondition on X = x), we have\\nP[g∗(X) , Y] ⩽P[gT(X) , Y].\\n(c) Show that, if gT := 1{hT(x) >1/2}is a classifier function such that as n →∞\\nhT(x)\\nd\\n−→Z ∼N(µ(x),σ2(x))\\nfor some mean and variance functions µ(x) and σ2(x), respectively, then\\nPx[gT(x) , g∗(x)] −→Φ\\n sign(1 −2h(x))(2µ(x) −1)\\n2σ(x)\\n!\\n,\\nwhere Φ is the cdf of a standard normal random variable.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 300, 'page_label': '283'}, page_content='Classification 283\\n10. The purpose of this exercise is to derive the dual program (7.21) from the primal\\nprogram (7.20). The starting point is to introduce a vector of auxiliary variables ξ :=\\n[ξ1,...,ξ n]⊤and write the primal program as\\nmin\\nα,α0,ξ\\nnX\\ni=1\\nξi + γ\\n2α⊤Kα\\nsubject to: ξ⩾0,\\nyi(α0 + {Kα}i) ⩾1 −ξi, i = 1,..., n.\\n(7.30)\\n(a) Apply the Lagrangian optimization theory from Section B.2.2 to obtain the Lag- ☞ 406\\nrangian function L({α0,α,ξ},{λ,µ}), where µand λare the Lagrange multipliers cor-\\nresponding to the first and second inequality constraints, respectively.\\n(b) Show that the Karush–Kuhn–Tucker (see Theorem B.2) conditions for optimizing L ☞ 407\\nare:\\nλ⊤y = 0\\nα= y ⊙λ/γ\\n0 ⩽λ⩽1\\n(1 −λ) ⊙ξ= 0, λ i (yig(xi) −1 + ξi) = 0, i = 1,..., n\\nξ⩾0, yig(xi) −1 + ξi ⩾0, i = 1,..., n.\\n(7.31)\\nHere ⊙stands for componentwise multiplication; e.g., y ⊙λ= [y1λ1,..., ynλn]⊤, and\\nwe have abbreviated α0 + {Kα}i to g(xi), in view of (7.19). [Hint: one of the KKT\\nconditions is λ= 1 −µ; thus we can eliminate µ.]\\n(c) Using the KKT conditions (7.31), reduce the Lagrange dual function L∗(λ) : =\\nminα0,α,ξ L({α0,α,ξ},{λ,1 −λ}) to\\nL∗(λ) =\\nnX\\ni=1\\nλi − 1\\n2γ\\nnX\\ni=1\\nnX\\nj=1\\nλiλjyiyj κ(xi,xj). (7.32)\\n(d) As a consequence of (7.19) and (a)–(c), show that the optimal prediction function gτ\\nis given by\\ngτ(x) = α0 + 1\\nγ\\nnX\\ni=1\\nyiλi κ(xi,x), (7.33)\\nwhere λis the solution to\\nmax\\nλ\\nL∗(λ)\\nsubject to: λ⊤y = 0, 0 ⩽λ⩽1,\\n(7.34)\\nand α0 = yj −1\\nγ\\nPn\\ni=1 yiλi κ(xi,xj) for any j such that λj ∈(0,1).\\n11. Consider SVM classification as illustrated in Figure 7.7. The goal of this exercise is to\\nclassify the training points{(xi,yi)}based on the value of the multipliers{λi}in Exercise 10.\\nLet ξi be the auxiliary variable in Exercise 10, i = 1,..., n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 301, 'page_label': '284'}, page_content='284 Exercises\\n(a) For λi ∈(0,1) show that (xi,yi) lies exactly on the decision border.\\n(b) For λi = 1, show that (xi,yi) lies strictly inside the margins.\\n(c) Show that for λi = 0 the point (xi,yi) lies outside the margins and is correctly classi-\\nfied.\\n12. A well-known data set is the MNIST handwritten digit database, containing many\\nthousands of digitalized numbers (from 0 to 9), each described by a 28×28 matrix of gray\\nscales. A similar but much smaller data set is described in [63]. Here, each handwritten\\ndigit is summarized by a 8 ×8 matrix with integer entries from 0 (white) to 15 (black).\\nFigure 7.14 shows the first 50 digitized images. The data set can be accessed with Python\\nusing the sklearn package as follows.\\nfrom sklearn import datasets\\ndigits = datasets.load_digits()\\nx_digits = digits.data # explanatory variables\\ny_digits = digits.target # responses\\nFigure 7.14: Classify the digitized images.\\n(a) Divide the data into a 75% training set and 25% test set.\\n(b) Compare the e ffectiveness of the K-nearest neighbors and naïve Bayes method to\\nclassify the data.\\n(c) Assess which K to use in the K-nearest neighbors classification.\\n13. Download the winequality-red.csv data set from UCI’s wine-quality website.\\nThe response here is the wine quality (from 0 to 10) as specified by a wine “expert”\\nand the explanatory variables are various characteristics such as acidity and sugar con-\\ntent. Use the SVC classifier of sklearn.svm with a linear kernel and penalty para-\\nmeter C = 1 (see Remark 7.2) to fit the data. Use the method cross_val_score from'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 302, 'page_label': '285'}, page_content='Classification 285\\nsklearn.model_selection to obtain a five-fold cross-validation score as an estimate of\\nthe probability that the predicted class matches the expert’s class.\\n14. Consider the credit approval data set crx.data from UCI’s credit approval website.\\nThe data set is concerned with credit card applications. The last column in the data set\\nindicates whether the application is approved ( +) or not (−). With the view of preserving\\ndata privacy, all 15 explanatory variables were anonymized. Note that some explanatory\\nvariables are continuous and some are categorical.\\n(a) Load and prepare the data for analysis with sklearn. First, eliminate data\\nrows with missing values. Next, encode categorical explanatory variables using a\\nOneHotEncoder object from sklearn.preprocessing to create a model matrix X\\nwith indicator variables for the categorical variables, as described in Section 5.3.5. ☞ 177\\n(b) The model matrix should contain 653 rows and 46 columns. The response variable\\nshould be a 0 /1 variable (reject/approve). We will consider several classification al-\\ngorithms and test their performance (using a zero-one loss) via ten-fold cross valida-\\ntion.\\ni. Write a function which takes 3 parameters: X,y, and a model, and returns the\\nten-fold cross-validation estimate of the expected generalization risk.\\nii. Consider the following sklearn classifiers: KNeighborsClassifier (k = 5),\\nLogisticRegression, and MPLClassifier (multilayer perceptron). Use the\\nfunction from (i) to identify the best performing classifier.\\n15. Consider a synthetic data set that was generated in the following fashion. The explan-\\natory variable follows a standard normal distribution. The response label is 0 if the explan-\\natory variable is between the 0.95 and 0.05 quantiles of the standard normal distribution,\\nand 1, otherwise. The data set was generated using the following code.\\nimport numpy as np\\nimport scipy.stats\\n# generate data\\nnp.random.seed(12345)\\nN = 100\\nX = np.random.randn(N)\\nq = scipy.stats.norm.ppf(0.95)\\ny = np.zeros(N)\\ny[X>=q] = 1\\ny[X<=-q] = 1\\nX = X.reshape(-1,1)\\nCompare the K-nearest neighbors classifier with K = 5 and logistic regression classi-\\nfier. Without computation, which classifier is likely to be better for these data? Verify your\\nanswer by coding both classifiers and printing the corresponding training 0–1 loss.\\n16. Consider the digits data set from Exercise 12. In this exercise, we would like to train\\na binary classifier for the identification of digit 8.\\n(a) Divide the data such that the first 1000 rows are used as the training set and the rest\\nare used as the test set.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 303, 'page_label': '286'}, page_content=\"286 Exercises\\n(b) Train the LogisticRegression classifier from the sklearn.linear_model pack-\\nage.\\n(c) “Train” a naïve classifier that always returns 0. That is, the naïve classifier identifies\\neach instance as being not 8.\\n(d) Compare the zero-one test losses of the logistic regression and the naïve classifiers.\\n(e) Find the confusion matrix, the precision, and the recall of the logistic regression clas-\\nsifier.\\n(f) Find the fraction of eights that are correctly detected by the logistic regression clas-\\nsifier.\\n17. Repeat Exercise 16 with the original MNIST data set. Use the first 60,000 rows as the\\ntrain set and the remaining 10,000 rows as the test set. The original data set can be obtained\\nusing the following code.\\nfrom sklearn.datasets import fetch_openml\\nX, y = fetch_openml( 'mnist_784 ', version=1, return_X_y=True)\\n18. For the breast cancer data in Section 7.8, investigate and discuss whether accuracy is☞277\\nthe relevant metric to use or if other metrics discussed in Section 7.2 are more appropriate.☞253\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 304, 'page_label': '287'}, page_content='CHAPTER 8\\nDECISION TREES AND ENSEMBLE\\nMETHODS\\nStatistical learning methods based on decision trees have gained tremendous pop-\\nularity due to their simplicity, intuitive representation, and predictive accuracy. This\\nchapter gives an introduction to the construction and use of such trees. We also dis-\\ncuss two key ensemble methods, namely bootstrap aggregation and boosting, which\\ncan further improve the efficiency of decision trees and other learning methods.\\n8.1 Introduction\\nTree-based methods provide a simple, intuitive, and powerful mechanism for both regres-\\nsion and classification. The main idea is to divide a (potentially complicated) feature space\\nXinto smaller regions and fit a simple prediction function to each region. For example,\\nin a regression setting, one could take the mean of the training responses associated with\\nthe training features that fall in that specific region. In the classification setting, a com-\\nmonly used prediction function takes the majority vote among the corresponding response\\nvariables. We start with a simple classification example.\\nExample 8.1 (Decision Tree for Classification) The left panel of Figure 8.1 shows a\\ntraining set of 15 two-dimensional points (features) falling into two classes (red and blue).\\nHow should the new feature vector (black point) be classified?\\n20 10 0 10 20\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\n20 10 0 10 20\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\nFigure 8.1: Left: training data and a new feature. Right: a partition of the feature space.\\n287'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 305, 'page_label': '288'}, page_content='288 Introduction\\nIt is not possible to linearly separate the training set, but we can partition the feature\\nspace X= R2 into rectangular regions and assign a class (color) to each region, as shown\\nin the right panel of Figure 8.1. Points in these regions are classified accordingly as blue\\nor red. The partition thus defines a classifier (prediction function) g that assigns to each\\nfeature vector x a class “red” or “blue”. For example, forx = [−15,0]⊤(solid black point),\\ng(x) = “blue”, since it belongs to a blue region of the feature space.\\nBoth the classification procedure and the partitioning of the feature space can be con-\\nveniently represented by a binary decision treedecision tree . This is a tree where each node v corres-\\nponds to a region (subset) Rv of the feature space X— the root node corresponding to the\\nfeature space itself.\\nx2 ≤ 12.0\\nx1 ≤ −20.5\\nx1 ≤ 20.0\\nx1 ≤ 2.5\\nx1 ≤ −5.0\\nTrue\\nTrue False\\nTrue\\nTrue\\nTrue False\\nFalse\\nFalse\\nFalse\\nFigure 8.2: The decision-\\ntree that corresponds to the\\npartition in Figure 8.1.\\nEach internal node v contains a logical condition that di-\\nvides Rv into two disjoint subregions. The leaf nodes (the ter-\\nminal nodes of the tree) are not subdivided, and their corres-\\nponding regions form a partition ofX, as they are disjoint and\\ntheir union is X. Associated with each leaf node w is also a\\nregional prediction function gw on Rw.\\nThe partitioning of Figure 8.1 was obtained from\\nthe decision tree shown in Figure 8.2. As an illustra-\\ntion of the decision procedure, consider again the input\\nx = [x1,x2]⊤= [−15,0]⊤. The classification process starts\\nfrom the tree root, which contains the conditionx2 ⩽12.0. As\\nthe second component of x is 0, the root condition is satisfied\\nand we proceed to the left child, which contains the condition\\nx1 ⩽−20.5. The next step is similar. As −15 > −20.5, the\\ncondition is not satisfied and we proceed to the right child.\\nSuch an evaluation of logical conditions along the tree path\\nwill eventually bring us to a leaf node and its associated re-\\ngion. In this case the process terminates in a leaf that corres-\\nponds to the left blue region in the right-hand panel of Fig-\\nure 8.1.\\nMore generally, a binary tree Twill partition the feature space Xinto as many regions\\nas there are leaf nodes. Denote the set of leaf nodes byW. The overall prediction function\\ng that corresponds to the tree can then be written as\\ng(x) =\\nX\\nw∈W\\ngw(x) 1{x ∈Rw}, (8.1)\\nwhere 1 denotes the indicator function. The representation (8.1) is very general and de-\\npends on (1) how the regions{Rw}are constructed via the logical conditions in the decision\\ntree, as well as (2) how the regional prediction functionsregional\\nprediction\\nfunctions\\nof the leaf nodes are defined.\\nSimple logical conditions of the form xj ⩽ξ split a Euclidean feature space into rect-\\nangles aligned with the axes. For example, Figure 8.2 partitions the feature space into six\\nrectangles: two blue and four red rectangles.\\nIn a classification setting, the regional prediction function gw corresponding to a leaf\\nnode w takes values in the set of possible class labels. In most cases, as in Example 8.1, it'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 306, 'page_label': '289'}, page_content='Decision Trees and Ensemble Methods 289\\nis taken to be constant on the corresponding region Rw. In a regression setting, gw is real-\\nvalued and also usually takes only one value. That is, every feature vector in Rw leads to\\nthe same predicted value. Of course, different regions will usually have different predicted\\nvalues.\\nConstructing a tree with a training set τ = {(xi,yi)}}n\\ni=1 amounts to minimizing the\\ntraining loss\\nℓτ (g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) (8.2)\\nfor some loss function; see Chapter 2. With g of the form (8.1), we can write ☞ 19\\nℓτ (g) = 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi)) = 1\\nn\\nnX\\ni=1\\nX\\nw∈W\\n1{xi ∈Rw}Loss(yi,g(xi)) (8.3)\\n=\\nX\\nw∈W\\n1\\nn\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n|                                    {z                                    }\\n(∗)\\n, (8.4)\\nwhere (∗) is the contribution by the regional prediction function gw to the overall training\\nloss. In the case where all {xi}are different, finding a decision tree T that gives a zero\\nsquared-error or zero–one training loss is easy, see Exercise 1, but such an “overfitted” tree\\nwill have poor predictive behavior, expressed in terms of the generalization risk. Instead\\nwe consider a restricted class of decision trees and aim to minimize the training loss within\\nthat class. It is common to use a top-down greedy approach, which can only achieve an\\napproximate minimization of the training loss.\\n8.2 Top-Down Construction of Decision Trees\\nLet τ = {(xi,yi)}n\\ni=1 be the training set. The key to constructing a binary decision tree T\\nis to specify a splitting rule splitting rulefor each node v, which can be defined as a logical function\\ns : X→{ False,True}or, equivalently, a binary function s : X→{ 0,1}. For example,\\nin the decision tree of Figure 8.2 the root node has splitting rule x 7→1{x2 ⩽12.0}, in\\ncorrespondence with the logical condition {x2 ⩽12.0}. During the construction of the tree,\\neach node v is associated with a specific region Rv ⊆ Xand therefore also the training\\nsubset {(x,y) ∈τ: x ∈Rv}⊆ τ. Using a splitting rule s, we can divide any subset σof the\\ntraining set τinto two sets:\\nσT := {(x,y) ∈σ : s(x) = True}and σF := {(x,y) ∈σ : s(x) = False}. (8.5)\\nStarting from an empty tree and the initial data set τ, a generic decision tree con-\\nstruction takes the form of the recursive Algorithm 8.2.1. Here we use the notation\\nTv for a subtree of T starting from node v. The final tree T is thus obtained via T =\\nConstruct_Subtree(v0,τ), where v0 is the root of the tree.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 307, 'page_label': '290'}, page_content='290 Top-Down Construction of Decision Trees\\nAlgorithm 8.2.1: Construct_Subtree\\nInput: A node v and a subset of the training data: σ⊆τ.\\nOutput: A (sub) decision tree Tv.\\n1 if termination criterion is met then // v is a leaf node\\n2 Train a regional prediction function gv using the training data σ.\\n3 else // split the node\\n4 Find the best splitting rule sv for node v.\\n5 Create successors vT and vF of v.\\n6 σT ←{(x,y) ∈σ : sv(x) = True}\\n7 σF ←{(x,y) ∈σ : sv(x) = False}\\n8 TvT ←Construct_Subtree(vT,σT) // left branch\\n9 TvF ←Construct_Subtree(vF,σF) // right branch\\n10 return Tv\\nThe splitting rule sv divides the region Rv into two disjoint parts, say RvT and RvF . The\\ncorresponding prediction functions, gT and gF, satisfy\\ngv(x) = gT(x) 1{x ∈RvT}+ gF(x) 1{x ∈RvF}, x ∈Rv.\\nIn order to implement the procedure described in Algorithm 8.2.1, we need to address\\nthe construction of the regional prediction functions gv at the leaves (Line 2), the specific-\\nation of the splitting rule (Line 4), and the termination criterion (Line 1). These important\\naspects are detailed in the following Sections 8.2.1, 8.2.2, and 8.2.3, respectively.\\n8.2.1 Regional Prediction Functions\\nIn general, there is no restriction on how to choose the prediction function gw for a leaf\\nnode v = w in Line 2 of Algorithm 8.2.1. In principle we can train any model from the\\ndata; e.g., via linear regression. However, in practice very simple prediction functions are\\nused. Below, we detail a popular choice for classification, as well as one for regression.\\n1. In the classification setting with class labels 0 ,..., c −1, the regional prediction\\nfunction gw for leaf node w is usually chosen to be constant and equal to the most\\ncommon class label of the training data in the associated region Rw (ties can be\\nbroken randomly). More precisely, let nw be the number of feature vectors in region\\nRw and let\\npw\\nz = 1\\nnw\\nX\\n{(x,y)∈τ: x∈Rw}\\n1{y=z},\\nbe the proportion of feature vectors in Rw that have class label z = 0,..., c −1. The\\nregional prediction function for node w is chosen to be the constant\\ngw(x) = argmax\\nz∈{0,...,c−1}\\npw\\nz . (8.6)\\n2. In the regression setting, gw is usually chosen as the mean response in the region;\\nthat is,\\ngw(x) = yRw := 1\\nnw\\nX\\n{(x,y)∈τ: x∈Rw}\\ny, (8.7)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 308, 'page_label': '291'}, page_content='Decision Trees and Ensemble Methods 291\\nwhere nw is again the number of feature vectors in Rw. It is not difficult to show that\\ngw(x) = yRw minimizes the squared-error loss with respect to all constant functions,\\nin the region Rw; see Exercise 2.\\n8.2.2 Splitting Rules\\nIn Line 4 in Algorithm 8.2.1, we divide region Rv into two sets, using a splitting rule\\n(function) sv. Consequently, the data set σassociated with node v (that is, the subset of the\\noriginal data set τwhose feature vectors lie in Rv), is also split — into σT and σF. What is\\nthe benefit of such a split in terms of a reduction in the training loss? If v were set to a leaf\\nnode, its contribution to the training loss would be (see (8.4)):\\n1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σ}Loss(yi,gv(xi)). (8.8)\\nIf v were to be split instead, its contribution to the overall training loss would be:\\n1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σT}Loss(yi,gT(xi)) + 1\\nn\\nnX\\ni=1\\n1{(xi,yi)∈σF}Loss(yi,gF(xi)), (8.9)\\nwhere gT and gF are the prediction functions belonging to the child nodes vT and vF. A\\ngreedy heuristic is to pretend that the tree construction algorithm immediately terminates\\nafter the split, in which case vT and vF are leaf nodes, and gT and gF are readily evaluated\\n— e.g., as in Section 8.2.1. Note that for any splitting rule the contribution (8.8) is always\\ngreater than or equal to (8.9). It therefore makes sense to choose the splitting rule such that\\n(8.9) is minimized. Moreover, the termination criterion may involve comparing (8.9) with\\n(8.8). If their difference is too small it may not be worth further splitting the feature space.\\nAs an example, suppose the feature space is X= Rp and we consider splitting rules of\\nthe form\\ns(x) = 1{xj ⩽ξ}, (8.10)\\nfor some 1 ⩽j ⩽p and ξ∈R, where we identify 0 withFalseand 1 with True. Due to the\\ncomputational and interpretative simplicity, such binary splitting rules are implemented in\\nmany software packages and are considered to be the de facto standard. As we have seen,\\nthese rules divide up the feature space into rectangles, as in Figure 8.1. It is natural to ask\\nhow j and ξ should be chosen so as to minimize (8.9). For a regression problem, using a\\nsquared-error loss and a constant regional prediction function as in (8.7), the sum (8.9) is\\ngiven by\\n1\\nn\\nX\\n(x,y)∈τ:xj⩽ξ\\n\\x00y −yT\\n\\x012 + 1\\nn\\nX\\n(x,y)∈τ:xj>ξ\\n\\x00y −yF\\n\\x012 , (8.11)\\nwhere yT and yF are the average responses for theσT and σF data, respectively. Let{xj,k}m\\nk=1\\ndenote the possible values of xj, j = 1,..., p within the training subset σ (with m ⩽n\\nelements). Note that, for a fixed j, (8.11) is a piecewise constant function of ξ, and that its\\nminimal value is attained at some value xj,k. As a consequence, to minimize (8.11) over\\nall j and ξ, it suffices to evaluate (8.11) for each of the m ×p values xj,k and then take the\\nminimizing pair ( j,xj,k).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 309, 'page_label': '292'}, page_content='292 Top-Down Construction of Decision Trees\\nFor a classification problem, using the indicator loss and a constant regional prediction\\nfunction as in (8.6), the aim is to choose a splitting rule that minimizes\\n1\\nn\\nX\\n(x,y)∈σT\\n1{y , y∗\\nT}+ 1\\nn\\nX\\n(x,y)∈σF\\n1{y , y∗\\nF}, (8.12)\\nwhere y∗\\nT = gT(x) is the most prevalent class (majority vote) in the data set σT and y∗\\nF\\nis the most prevalent class in σF. If the feature space is X= Rp and the splitting rules\\nare of the form (8.10), then the optimal splitting rule can be obtained in the same way as\\ndescribed above for the regression case; the only di fference is that (8.11) is replaced with\\n(8.12).\\nWe can view the minimization of (8.12) as minimizing a weighted average of “impur-\\nities” of nodes σT and σF. Namely, for an arbitrary training subset σ⊆τ, if y∗is the most\\nprevalent label, then\\n1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y , y∗}= 1 − 1\\n|σ|\\nX\\n(x,y)∈σ\\n1{y = y∗}= 1 −py∗ = 1 − max\\nz∈{0,...,c−1}\\npz,\\nwhere pz is the proportion of data points in σthat have class label z, z = 0,..., c −1. The\\nquantity\\n1 − max\\nz∈{0,...,c−1}\\npz\\nmeasures the diversity of the labels in σand is called the misclassification impurity. Con-misclassification\\nimpurity sequently, (8.12) is the weighted sum of the misclassification impurities of σT and σF,\\nwith weights by |σT|/n and |σF|/n, respectively. Note that the misclassification impurity\\nonly depends on the label proportions rather than on the individual responses. Instead of\\nusing the misclassification impurity to decide if and how to split a data set σ, we can use\\nother impurity measures that only depend on the label proportions. Two popular choices\\nare the entropy impurityentropy\\nimpurity\\n:\\n−\\nc−1X\\nz=0\\npz log2(pz)\\nand the Gini impurityGini impurity :\\n1\\n2\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed1 −\\nc−1X\\nz=0\\np2\\nz\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\nAll of these impurities are maximal when the label proportions are equal to 1 /c. Typical\\nshapes of the above impurity measures are illustrated in Figure 8.3 for the two-label case,\\nwith class probabilities p and 1 −p. We see here the similarity of the di fferent impurity\\nmeasures. Note that impurities can be arbitrarily scaled, and so using ln(pz) = log2(pz) ln(2)\\ninstead of log2(pz) above gives an equivalent entropy impurity.\\n8.2.3 Termination Criterion\\nWhen building a tree, one can define various types of termination conditions. For example,\\nwe might stop when the number of data points in the tree node (the size of the input σset\\nin Algorithm 8.2.1) is less than or equal to some predefined number. Or we might choose'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 310, 'page_label': '293'}, page_content='Decision Trees and Ensemble Methods 293\\n0 0 .2 0 .4 0 .6 0 .8 10\\n0.2\\n0.4\\n0.6\\np\\nimpurity\\ncross-entropy\\nGini index\\nmisclassiﬁcation\\nFigure 8.3: Entropy, Gini, and misclassification impurities for binary classification, with\\nclass frequencies p1 = p and p2 = 1 −p. The entropy impurity was normalized (divided by\\n2), to ensure that all impurity measures attain the same maximum value of 1/2 at p = 1/2.\\nthe maximal depth of the tree in advance. Another possibility is to stop when there is no\\nsignificant advantage, in terms of training loss, to split regions. Ultimately, the quality of a\\ntree is determined by its predictive performance (generalization risk) and the termination\\ncondition should aim to strike a balance between minimizing the approximation error and\\nminimizing the statistical error, as discussed in Section 2.4. ☞ 31\\nExample 8.2 (Fixed Tree Depth) To illustrate how the tree depth impacts on the gener-\\nalization risk, consider Figure 8.4, which shows the typical behavior of the cross-validation\\nloss as a function of the tree depth. Recall that the cross-validation loss is an estimate of the\\nexpected generalization risk. Complicated (deep) trees tend to overfit the training data by\\nproducing many divisions of the feature space. As we have seen, this overfitting problem is\\ntypical of all learning methods; see Chapter 2 and in particular Example 2.1. To conclude, ☞ 26\\nincreasing the maximal depth does not necessarily result in better performance.\\n0 5 10 15 20 25 30\\n0.3\\n0.35\\n0.4\\n0.45\\ntree depth\\nloss\\nFigure 8.4: The ten-fold cross-validation loss as a function of the maximal tree depth for a\\nclassification problem. The optimal maximal tree depth is here 6.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 311, 'page_label': '294'}, page_content=\"294 Top-Down Construction of Decision Trees\\nTo create Figure 8.4 we used 1 the Python method make_blobs from the sklearn\\nmodule to produce a training set of size n = 5000 with ten-dimensional feature vectors☞491\\n(thus, p = 10 and X= R10), each of which is classified into one of c = 3 classes. The full\\ncode is given below.\\nTreeDepthCV.py\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import zero_one_loss\\nimport matplotlib.pyplot as plt\\ndef ZeroOneScore(clf, X, y):\\ny_pred = clf.predict(X)\\nreturn zero_one_loss(y, y_pred)\\n# Construct the training set\\nX, y = make_blobs(n_samples=5000, n_features=10, centers=3,\\nrandom_state=10, cluster_std=10)\\n# construct a decision tree classifier\\nclf = DecisionTreeClassifier(random_state=0)\\n# Cross -validation loss as a function of tree depth (1 to 30)\\nxdepthlist = []\\ncvlist = []\\ntree_depth = range (1,30)\\nfor d in tree_depth:\\nxdepthlist.append(d)\\nclf.max_depth=d\\ncv = np.mean(cross_val_score(clf, X, y, cv=10, scoring=\\nZeroOneScore))\\ncvlist.append(cv)\\nplt.xlabel( 'tree depth ', fontsize=18, color= 'black ')\\nplt.ylabel( 'loss ', fontsize=18, color= 'black ')\\nplt.plot(xdepthlist, cvlist, '-*' , linewidth=0.5)\\nThe code above relies heavily on sklearn and hides the implementation details. To\\nshow how decision trees are actually constructed using the previous theory, we proceed\\nwith a very basic implementation.\\n8.2.4 Basic Implementation\\nIn this section we implement a regression tree, step by step. To run the program, amalgam-\\nate the code snippets below into one file, in the order presented. First, we import various\\npackages and define a function to generate the training and test data.\\n1The data used for Figure 8.1 was produced in a similar way.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 312, 'page_label': '295'}, page_content='Decision Trees and Ensemble Methods 295\\nBasicTree.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.model_selection import train_test_split\\ndef makedata():\\nn_points = 500 # number of samples\\nX, y = make_friedman1(n_samples=n_points, n_features=5,\\nnoise=1.0, random_state=100)\\nreturn train_test_split(X, y, test_size=0.5, random_state=3)\\nThe “main” method calls themakedata method, uses the training data to build a regres-\\nsion tree, and then predicts the responses of the test set and reports the mean squared-error\\nloss.\\ndef main():\\nX_train, X_test, y_train, y_test = makedata()\\nmaxdepth = 10 # maximum tree depth\\n# Create tree root at depth 0\\ntreeRoot = TNode(0, X_train,y_train)\\n# Build the regression tree with maximal depth equal to max_depth\\nConstruct_Subtree(treeRoot, maxdepth)\\n# Predict\\ny_hat = np.zeros( len (X_test))\\nfor i in range (len (X_test)):\\ny_hat[i] = Predict(X_test[i],treeRoot)\\nMSE = np.mean(np.power(y_hat - y_test,2))\\nprint (\"Basic tree: tree loss = \", MSE)\\nThe next step is to specify a tree node as a Python class. Each node has a number of\\nattributes, including the features and the response data ( X and y) and the depth at which\\nthe node is placed in the tree. The root node has depth 0. Each node w can calculate its\\ncontribution to the squared-error training loss Pn\\ni=1 1{xi ∈Rw}(yi −gw(xi))2. Note that we\\nhave omitted the constant 1 /n term when training the tree, which simply scales the loss\\n(8.2).\\nclass TNode:\\ndef __init__(self, depth, X, y):\\nself.depth = depth\\nself.X = X # matrix of features\\nself.y = y # vector of response variables\\n# initialize optimal split parameters\\nself.j = None\\nself.xi = None\\n# initialize children to be None\\nself.left = None\\nself.right = None'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 313, 'page_label': '296'}, page_content='296 Top-Down Construction of Decision Trees\\n# initialize the regional predictor\\nself.g = None\\ndef CalculateLoss(self):\\nif (len (self.y)==0):\\nreturn 0\\nreturn np.sum (np.power(self.y - self.y.mean(),2))\\nThe function below implements the training (tree-building) Algorithm 8.2.1.\\ndef Construct_Subtree(node, max_depth):\\nif (node.depth == max_depth or len (node.y) == 1):\\nnode.g = node.y.mean()\\nelse :\\nj, xi = CalculateOptimalSplit(node)\\nnode.j = j\\nnode.xi = xi\\nXt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)\\nif (len (yt)>0):\\nnode.left = TNode(node.depth+1,Xt,yt)\\nConstruct_Subtree(node.left, max_depth)\\nif (len (yf)>0):\\nnode.right = TNode(node.depth+1, Xf,yf)\\nConstruct_Subtree(node.right, max_depth)\\nreturn node\\nThis requires an implementation of the CalculateOptimalSplit function. To start,\\nwe implement a function DataSplit that splits the data according to s(x) = 1{xj ⩽ξ}.\\ndef DataSplit(X,y,j,xi):\\nids = X[:,j]<=xi\\nXt = X[ids == True,:]\\nXf = X[ids == False,:]\\nyt = y[ids == True]\\nyf = y[ids == False]\\nreturn Xt, yt, Xf, yf\\nThe CalculateOptimalSplit method runs through the possible splitting thresholds\\nξfrom the set {xj,k}and finds the optimal split.\\ndef CalculateOptimalSplit(node):\\nX = node.X\\ny = node.y\\nbest_var = 0\\nbest_xi = X[0,best_var]\\nbest_split_val = node.CalculateLoss()'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 314, 'page_label': '297'}, page_content='Decision Trees and Ensemble Methods 297\\nm, n = X.shape\\nfor j in range (0,n):\\nfor i in range (0,m):\\nxi = X[i,j]\\nXt, yt, Xf, yf = DataSplit(X,y,j,xi)\\ntmpt = TNode(0, Xt, yt)\\ntmpf = TNode(0, Xf, yf)\\nloss_t = tmpt.CalculateLoss()\\nloss_f = tmpf.CalculateLoss()\\ncurr_val = loss_t + loss_f\\nif (curr_val < best_split_val):\\nbest_split_val = curr_val\\nbest_var = j\\nbest_xi = xi\\nreturn best_var, best_xi\\nFinally, we implement the recursive method for prediction.\\ndef Predict(X,node):\\nif (node.right == None and node.left != None):\\nreturn Predict(X,node.left)\\nif (node.right != None and node.left == None):\\nreturn Predict(X,node.right)\\nif (node.right == None and node.left == None):\\nreturn node.g\\nelse :\\nif (X[node.j] <= node.xi):\\nreturn Predict(X,node.left)\\nelse :\\nreturn Predict(X,node.right)\\nRunning the main function defined above gives a similar 2 result to what one would\\nachieve with the sklearn package, using the DecisionTreeRegressor method.\\nmain() # run the main program\\n# compare with sklearn\\nfrom sklearn.tree import DecisionTreeRegressor\\nX_train, X_test, y_train, y_test = makedata() # use the same data\\nregTree = DecisionTreeRegressor(max_depth = 10, random_state=0)\\nregTree.fit(X_train,y_train)\\ny_hat = regTree.predict(X_test)\\nMSE2 = np.mean(np.power(y_hat - y_test,2))\\nprint (\"DecisionTreeRegressor: tree loss = \", MSE2)\\nBasic tree: tree loss = 9.067077996170276\\nDecisionTreeRegressor: tree loss = 10.197991295531748\\n2After establishing a best split ξ = xj,k, sklearn assigns the corresponding feature vector randomly to\\none of the two child nodes, rather than to the Truechild.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 315, 'page_label': '298'}, page_content='298 Additional Considerations\\n8.3 Additional Considerations\\n8.3.1 Binary Versus Non-Binary Trees\\nWhile it is possible to split a tree node into more than two groups (multiway splits), it\\ngenerally produces inferior results compared to the simple binary split. The major reason\\nis that multiway splits can lead to too many nodes near the tree root that have only a\\nfew data points, thus leaving insu fficient data for later splits. As multiway splits can be\\nrepresented as several binary splits, the latter is preferred [55].\\n8.3.2 Data Preprocessing\\nSometimes, it can be beneficial to preprocess the data prior to the tree construction. For\\nexample, PCA can be used with a view to identify the most important dimensions, which☞153\\nin turn will lead to simpler and possibly more informative splitting rules in the internal\\nnodes.\\n8.3.3 Alternative Splitting Rules\\nWe restricted our attention to splitting rules of the type s(x) = 1{xj ⩽ξ}, where j ∈\\n{1,..., p}and ξ ∈ R. These types of rules may not always result in a simple partition\\nof the feature space, as illustrated by the binary data in Figure 8.5. In this case, the feature\\nspace could have been partitioned into just two regions, separated by a straight line.\\nFigure 8.5: The two groups of points can here be separated by a straight line. Instead, the\\nclassification tree divides up the space into many rectangles, leading to an unnecessarily\\ncomplicated classification procedure.\\nIn this case many classification methods discussed in Chapter 7, such as linear discrim-\\ninant analysis (Section 7.4), will work very well, whereas the classification tree is rather☞259\\nelaborate, dividing the feature set into too many regions. An obvious remedy is to use\\nsplitting rules of the form\\ns(x) = 1{a⊤x ⩽ξ}.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 316, 'page_label': '299'}, page_content='Decision Trees and Ensemble Methods 299\\nIn some cases, such as the one just discussed, it may be useful to use a splitting rule\\nthat involves several variables, as opposed to a single one. The decision regarding the split\\ntype clearly depends on the problem domain. For example, for logical (binary) variables\\nour domain knowledge may indicate that a different behavior is expected when both xi and\\nxj (i , j) are True. In this case, we will naturally introduce a decision rule of the form:\\ns(x) = 1{xi = T and xj = T}.\\n8.3.4 Categorical Variables\\nWhen an explanatory variable is categorical with labels (levels) say {1,..., k}, the split-\\nting rule is generally defined via a partition of the label set {1,..., k}into two subsets.\\nSpecifically, let L and R be a partition of {1,..., k}. Then, the splitting rule is defined via\\ns(x) = 1{xj ∈L}.\\nFor the general supervised learning case, finding the optimal partition in the sense of min-\\nimal loss requires one to consider 2 k subsets of {1,..., k}. Consequently, finding a good\\nsplitting rule for categorical variables can be challenging when the number of labels k is\\nlarge.\\n8.3.5 Missing Values\\nMissing data is present in many real-life problems. Generally, when working with incom-\\nplete feature vectors, where one or more values are missing, it is typical to either com-\\npletely delete the feature vector from the data (which may distort the data) or to impute\\n(guess) its missing values from the available data; see e.g., [120]. Tree methods, however,\\nallow an elegant approach for handling missing data. Specifically, in the general case, the\\nmissing data problem can be handled via surrogate splitting rules [20].\\nWhen dealing with categorical (factor) features, we can introduce an additional cat-\\negory “missing” for the absent data.\\nThe main idea of surrogate rules is as follows. First, we construct a decision (regression\\nor a classification) tree via Algorithm 8.2.1. During this construction process, the solution\\nof the optimization problem (8.9) is calculated only over the observations that are not\\nmissing a particular variable. Suppose that a tree nodev has a splitting rule s∗(x) = 1{xj∗ ⩽\\nξ∗}for some 1 ⩽j∗⩽k and threshold ξ∗.\\nFor the node v we can introduce a set of alternative splitting rules that resemble the\\noriginal splitting rule, sometimes called theprimary splitting rule, using different variables\\nand thresholds. Namely, we look for a binary splitting rule s(x |j,ξ), j , j∗ such that the\\ndata split introduced by s will be similar to the original data split from s∗. The similarity is\\ngenerally measured via a binary misclassification loss, where the true classes of observa-\\ntions are determined by the primary splitting rule and the surrogate splitting rules serve as\\nclassifiers. Consider, for example, the data in Table 8.1 and suppose that the primary split-\\nting rule at node v is 1{Age ⩽25}. That is, the five data points are split such that the left\\nand the right child ofv contains two and three data points, respectively. Next, the following\\nsurrogate splitting rules can be considered:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 317, 'page_label': '300'}, page_content='300 Controlling the Tree Shape\\n1. 1{Salary ⩽1500}, and\\n2. 1{Height ⩽173}.\\nTable 8.1: Example data with three variables (Age, Height, and Salary).\\nId Age Height Salary\\n1 20 173 1000\\n2 25 168 1500\\n3 38 191 1700\\n4 49 170 1900\\n5 62 182 2000\\nThe 1{Salary ⩽1500}surrogate rule completely mimics the primary rule, in the sense\\nthat the data splits induced by these rules are identical. Namely, both rules partition the\\ndata into two sets (by Id) {1,2}and {3,4,5}. On the other hand, the 1{Height ⩽173}rule\\nis less similar to the primary rule, since it causes the different partition {1,2,4}and {3,5}.\\nIt is up to the user to define the number of surrogate rules for each tree node. As soon as\\nthese surrogate rules are available, we can use them to handle a new data point, even if the\\nmain rule cannot be applied due to a missing value of the primary variablexj∗. Specifically,\\nif the observation is missing the primary split variable, we apply the first (best) surrogate\\nrule. If the first surrogate variable is also missing, we apply the second best surrogate rule,\\nand so on.\\n8.4 Controlling the Tree Shape\\nEventually, we are interested in getting the right-size tree. Namely, a tree that shows good\\ngeneralization properties. It was already discussed in Section 8.2.3 (Figure 8.4) that shal-\\nlow trees tend to underfit and deep trees tend to overfit the data. Basically, a shallow tree\\ndoes not produce a sufficient number of splits and a deep tree will produce many partitions\\nand thus many leaf nodes. If we grow the tree to a su fficient depth, each training sample\\nwill occupy a separate leaf and we will observe a zero loss with respect to the training data.\\nThe above phenomenon is illustrated in Figure 8.6, which presents the cross-validation loss\\nand the training loss as a function of the tree depth.\\nIn order to overcome the under- and the overfitting problem, Breiman et al. [20] ex-\\namined the possibility of stopping the tree from growing as soon as the decrease in loss\\ndue to a split of node v, as expressed in the di fference of (8.8) and (8.9), is smaller than\\nsome predefined parameter δ ∈R. Under this setting, the tree construction process will\\nterminate when no leaf node can be split such that the contribution to the training loss after\\nthis split is greater than δ.\\nThe authors found that this approach was unsatisfactory. Specifically, it was noted that a\\nvery smallδleads to an excessive amount of splitting and thus causes overfitting. Increasing\\nδdid not work either. The problem is that the nature of the proposed rule is one-step-look-\\nahead. To see this, consider a tree node for which the best possible decrease in loss is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 318, 'page_label': '301'}, page_content='Decision Trees and Ensemble Methods 301\\n0 5 10 15 20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\ntree depth\\nloss\\ntrain\\nCV\\nFigure 8.6: The cross-validation and the training loss as a function of the tree depth for a\\nbinary classification problem.\\nsmaller than δ. According to the proposed procedure, this node will not be split further. This\\nmay, however, be sub-optimal, because it could happen that one of the node’sdescendants,\\nif split, could lead to a major decrease in loss.\\nTo address these issues, a so-called pruning tree pruningroutine can be employed. The idea is as\\nfollows. We first grow a very deep tree and then prune (remove nodes) it upwards until we\\nreach the root node. Consequently, the pruning process causes the number of tree nodes\\nto decrease. While the tree is being pruned, the generalization risk gradually decreases up\\nto the point where it starts increasing again, at which point the pruning is stopped. This\\ndecreasing/increasing behavior is due to the bias–variance tradeoff (2.22).\\nWe next describe the details. To start with, let v and v′be tree nodes. We say that v′is\\na descendant of v if there is a path down the tree, which leads from v to v′. If such a path\\nexists, we also say that v is an ancestor of v′. Consider the tree in Figure 8.7.\\nTo formally define pruning, we will require the following Definition 8.1. An example\\nof pruning is demonstrated in Figure 8.8.\\nDefinition 8.1: Branches and Pruning\\n1. A tree branch tree branchTv of the tree Tis a sub-tree of Trooted at node v ∈T.\\n2. The pruning of branch Tv from a tree Tis performed via deletion of the entire\\nbranch Tv from Texcept the branch’s root nodev. The resulting pruned tree is\\ndenoted by T−Tv.\\n3. A sub-tree T−Tv is called a pruned sub-tree of T. We indicate this with the\\nnotation T−Tv ≺Tor T≻T−Tv.\\nA basic decision tree pruning procedure is summarized in Algorithm 8.4.1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 319, 'page_label': '302'}, page_content='302 Controlling the Tree Shape\\nv1\\nv2\\nv4\\nv7\\nv5\\nv8 v9\\nv3\\nv6\\nv10 v11\\nFigure 8.7: The node v9 is a descendant of v2, and v2 is an ancestor of {v4,v5,v7,v8,v9}, but\\nv6 is not a descendant of v2.\\nv1\\nv2\\nv4\\nv7\\nv5\\nv8 v9\\nv3\\nv6\\nv10 v11\\n(a) T\\nv2\\nv4\\nv7\\nv5\\nv8 v9 (b) Tv2\\nv1\\nv2 v3\\nv6\\nv10 v11 (c) T−Tv2\\nFigure 8.8: The pruned tree T−Tv2 in (c) is the result of pruning theTv2 branch in (b) from\\nthe original tree Tin (a).\\nAlgorithm 8.4.1: Decision Tree Pruning\\nInput: Training set τ.\\nOutput: Sequence of decision trees T0 ≻T1 ≻···\\n1 Build a large decision tree T0 via Algorithm 8.2.1. [A possible termination\\ncriterion for that algorithm is to have some small predetermined number of data\\npoints at each terminal node of T0.]\\n2 T′←T0\\n3 k ←0\\n4 while T′has more than one node do\\n5 k ←k + 1\\n6 Choose v ∈T′.\\n7 Prune the branch rooted at v from T′.\\n8 Tk ←T′−Tv and T′←Tk.\\n9 return T0,T1,..., Tk'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 320, 'page_label': '303'}, page_content='Decision Trees and Ensemble Methods 303\\nLet T0 be the initial (deep) tree and let Tk be the tree obtained after the k-th pruning\\noperation, for k = 1,..., K. As soon as the sequence of trees T0 ≻T1 ≻···≻ TK is avail-\\nable, one can choose the best tree of {Tk}K\\nk=1 according to the smallest generalization risk.\\nSpecifically, we can split the data into training and validation sets. In this case, Algorithm\\n8.4.1 is executed using the training set and the generalization risks of{Tk}K\\nk=1 are estimated\\nvia the validation set.\\nWhile Algorithm 8.4.1 and the corresponding best tree selection process look appeal-\\ning, there is still an important question to consider; namely, how to choose the node v and\\nthe corresponding branch Tv in Line 6 of the algorithm. In order to overcome this problem,\\nBreiman proposed a method called cost complexity pruning, which we discuss next.\\n8.4.1 Cost-Complexity Pruning\\nLet T ≺T0 be a tree obtained via pruning of a tree T0. Denote the set of leaf (terminal)\\nnodes of Tby W. The number of leaves |W|is a measure for the complexity of the tree;\\nrecall that |W|is the number of regions {Rw}in the partition of X. Corresponding to each\\ntree Tis a prediction function g, as in (8.1). In cost-complexity pruning cost-complexity\\npruning\\nthe objective is to\\nfind a prediction function g (or, equivalently, tree T) that minimizes the training loss ℓτ(g)\\nwhile taking into account the complexity of the tree. The idea is to regularize the training\\nloss, similar to what was done in Chapter 6, by adding a penalty term for the complexity\\nof the tree. This leads to the following definition.\\nDefinition 8.2: Cost-Complexity Measure\\nLet τ = {(xi,yi)}n\\ni=1 be a data set and γ ⩾0 be a real number. For a given tree T, the\\ncost-complexity measure cost-complexity\\nmeasure\\nCτ(γ,T) is defined as:\\nCτ(γ,T) := 1\\nn\\nX\\nw∈W\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 + γ|W| (8.13)\\n= ℓτ (g) + γ|W|,\\nwhere ℓτ (g) is the training loss (8.2).\\nSmall values of γresult in a small penalty for the tree complexity |W|, and thus large\\ntrees (that fit the entiretraining data well) will minimize the measureCτ(γ,T). In particular,\\nfor γ = 0, T = T0 will be the minimizer of Cτ(γ,T). On the other hand, large values of γ\\nwill prefer smaller trees or, more precisely, trees with fewer leaves. For su fficiently large\\nγ, the solution Twill collapse to a single (root) node.\\nIt can be shown that, for every value of γ, there exists a smallest minimizing sub-tree\\nof T0 with respect to the cost-complexity measure. In practice, a suitable γis selected via\\nobserving the performance of the learner on the validation set or by cross-validation.\\nThese advantages and the corresponding limitations are detailed next.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 321, 'page_label': '304'}, page_content='304 Controlling the Tree Shape\\n8.4.2 Advantages and Limitations of Decision Trees\\nWe list a number of advantages and disadvantages of decision trees, as compared with\\nother supervised learning methods such as were discussed in Chapters 5, 6, and 7.\\nAdvantages\\n1. The tree structure can handle both categorical and numerical features in a natural\\nand straightforward way. Specifically, there is no need to pre-process categorical\\nfeatures, say via the introduction of dummy variables.\\n2. The final tree obtained after the training phase can be compactly stored for the pur-\\npose of making predictions for new feature vectors. The prediction process only\\ninvolves a single tree traversal from the tree root to a leaf.\\n3. The hierarchical nature of decision trees allows for an e fficient encoding of the fea-\\nture’s conditional information. Specifically, after an internal split of a feature xj via\\nthe standard splitting rule (8.10), Algorithm 8.2.1 will only consider such subsets of\\ndata that were constructed based on this split, thus implicitly exploiting the corres-\\nponding conditional information from the initial split of xj.\\n4. The tree structure can be easily understood and interpreted by domain experts with\\nlittle statistical knowledge, since it is essentially a logical decision flow diagram.\\n5. The sequential decision tree growth procedure in Algorithm 8.2.1, and in particular\\nthe fact that the tree has been split using the most important features, provides an\\nimplicit step-wise variable elimination procedure. In addition, the partition of the\\nvariable space into smaller regions results in simpler prediction problems in these\\nregions.\\n6. Decision trees are invariant under monotone transformations of the data. To see this,\\nconsider the (optimal) splitting rule s(x) = 1{x3 ⩽2}, where x3 is a positive feature.\\nSuppose that x3 is transformed to x′\\n3 = x2\\n3. Now, the optimal splitting rule will take\\nthe form s(x) = 1{x′\\n3 ⩽4}.\\n7. In the classification setting, it is common to report not only the predicted value of a\\nfeature vector, e.g., as in (8.6), but also the respective class probabilities. Decision\\ntrees handle this task without any additional effort. Specifically, consider a new fea-\\nture vector. During the estimation process, we will perform a tree traversal and the\\npoint will end up in a certain leaf w. The probability of this feature vector lying in\\nclass z can be estimated as the proportion of training points in w that are in class z.\\n8. As each training point is treated equally in the construction of a tree, the structure of\\nthe tree will be relatively robust to outliers. In a way, trees exhibit a similar kind of\\nrobustness as the sample median does for real-valued data.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 322, 'page_label': '305'}, page_content='Decision Trees and Ensemble Methods 305\\nLimitations\\nDespite the fact that the decision trees are extremely interpretable, the predictive accuracy\\nis generally inferior to other established statistical learning methods. In addition, decision\\ntrees, and in particular very deep trees that were not subject to pruning, are heavily reliant\\non their training set. A small change in the training set can result in a dramatic change of the\\nresulting decision tree. Their inferior predictive accuracy, however, is a direct consequence\\nof the bias–variance tradeoff. Specifically, a decision tree model generally exhibits a high\\nvariance. To overcome the above limitations, several promising approaches such as bag-\\nging, random forest, and boosting are introduced below.\\nThe bagging approach was initially introduced in the context of an ensemble of\\ndecision trees. However, both the bagging and the boosting methods can be applied\\nto improve the accuracy of general prediction functions.\\n8.5 Bootstrap Aggregation\\nThe major idea of the bootstrap aggregation or bagging baggingmethod is to combine prediction\\nfunctions learned from multiple data sets, with a view to improving overall prediction\\naccuracy. Bagging is especially beneficial when dealing with predictors that tend to overfit\\nthe data, such as in decision trees, where the (unpruned) tree structure is very sensitive to\\nsmall changes in the training set [37, 55].\\nTo start with, consider an idealized setting for a regression tree, where we have access\\nto B iid copies3 T1,..., TB of a training set T. Then, we can train B separate regression\\nmodels (B different decision trees) using these sets, giving learners gT1 ,..., gTB , and take\\ntheir average:\\ngavg(x) = 1\\nB\\nBX\\nb=1\\ngTb (x). (8.14)\\nBy the law of large numbers, as B → ∞, the average prediction function converges to ☞ 445\\nthe expected prediction function g† := EgT. The following result shows that using g† as\\na prediction function (if it were known) would result in an expected squared-error gen-\\neralization risk that is less than or equal to the expected generalization risk for a general ☞ 24\\nprediction function gT. It thus suggests that taking an average of prediction functions may\\nlead to a better expected squared-error generalization risk.\\nTheorem 8.1: Expected Squared-Error Generalization Risk\\nLet Tbe a random training set and let X,Y be a random feature vector and response\\nthat are independent of T. Then,\\nE\\n\\x12\\nY −gT(X)\\n\\x132\\n⩾E\\n\\x10\\nY −g†(X)\\n\\x112\\n.\\n3In this section Tk means the k-th training set, not a training set of size k.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 323, 'page_label': '306'}, page_content='306 Bootstrap Aggregation\\nProof: We have\\nE\\n\"\\x12\\nY −gT(X)\\n\\x132 \\x0c\\x0c\\x0c\\x0c\\x0c X,Y\\n#\\n⩾\\n\\x12\\nE[Y |X,Y] −E[gT(X) |X,Y]\\n\\x132\\n=\\n\\x12\\nY −g†(X)\\n\\x132\\n,\\nwhere the inequality follows from EU2 ⩾(EU)2 for any (conditional) expectation. Con-\\nsequently, by the tower property,☞431\\nE\\n\\x12\\nY −gT(X)\\n\\x132\\n= E\\nh\\nE\\nh\\x00Y −gT(X)\\x012 |X,Y\\nii\\n⩾E\\n\\x12\\nY −g†(X)\\n\\x132\\n.\\n□\\nUnfortunately, multiple independent data sets are rarely available. But we can substi-\\ntute them by bootstrapped ones. Specifically, instead of the T1,..., TB sets, we can obtain\\nrandom training sets T∗\\n1 ,..., T∗\\nB by resampling them from a single (fixed) training set τ,☞76\\nsimilar to Algorithm 3.2.6, and use them to train B separate models. By model averaging\\nas in (8.14) we obtain the bootstrapped aggregated estimator or bagged estimatorbagged\\nestimator\\nof the\\nform:\\ngbag(x) = 1\\nB\\nBX\\nb=1\\ngT∗\\nb\\n(x). (8.15)\\nAlgorithm 8.5.1: Bootstrap Aggregation Sampling\\nInput: Training set τ= {(xi,yi)}n\\ni=1 and resample size B.\\nOutput: Bootstrapped data sets.\\n1 for b = 1 to B do\\n2 T∗\\nb ←∅\\n3 for i = 1 to n do\\n4 Draw U ∼U(0,1)\\n5 I ←⌈nU⌉ // select random index\\n6 T∗\\nb ←T∗\\nb ∪{(xI,yI)}.\\n7 return T∗\\nb ,b = 1,..., B.\\nRemark 8.1 (Bootstrap Aggregation for Classification Problems) Note that (8.15)\\nis suitable for handling regression problems. However, the bagging idea can be readily\\nextended to handle classification settings as well. For example, gbag can take the majority\\nvote among {gT∗\\nb\\n},b = 1,..., B; that is, to accept the most frequent class among B predict-\\nors.\\nWhile bagging can be applied for any statistical model (such as decision trees, neural\\nnetworks, linear regression, K-nearest neighbors, and so on), it is most e ffective for pre-\\ndictors that are sensitive to small changes in the training set. The reason becomes clear\\nwhen we decompose the expected generalization risk as\\nEℓ(gT) = ℓ∗+ E(E[gT(X) |X] −g∗(X))2\\n|                           {z                           }\\nexpected squared bias\\n+ E[Var[gT(X) |X]]|                 {z                 }\\nexpected variance\\n, (8.16)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 324, 'page_label': '307'}, page_content='Decision Trees and Ensemble Methods 307\\nsimilar to (2.22). Compare this with the same decomposition for the average prediction ☞ 35\\nfunction gbag in (8.14). As Egbag(x) = EgT(x), we see that any possible improvement in\\nthe generalization risk must be due to the expected variance term. Averaging and bagging\\nare thus only useful for predictors with a large expected variance, relative to the other two\\nterms. Examples of such “unstable” predictors include decision trees, neural networks, and\\nsubset selection in linear regression [22]. On the other hand, “stable” predictors are in-\\nsensitive to small data changes, an example being the K-nearest neighbors method. Note\\nthat for independent training sets T1,..., TB a reduction of the variance by a factor B is\\nachieved: Var gbag(x) = B−1Var gT(x). Again, it depends on the squared bias and irredu-\\ncible loss how significant this reduction is for the generalization risk.\\nRemark 8.2 (Limitations of Bagging) It is important to remember that gbag is not ex-\\nactly equal to gavg, which in turn is not exactlyg†. Specifically, gbag is constructed from the\\nbootstrap approximation of the sampling pdf f . As a consequence, for stable predictors,\\nit can happen that gbag will perform worse than gT. In addition to the deterioration of the\\nbagging performance for stable procedures, it can also happen thatgT has already achieved\\na near optimal predictive accuracy given the available training data. In this case, bagging\\nwill not introduce a significant improvement.\\nThe bagging process provides an opportunity to estimate the generalization risk of\\nthe bagged model without an additional test set. Specifically, recall that we obtain the\\nT∗\\n1 ,..., T∗\\nB sets from a single training set τby sampling via Algorithm 8.5.1, and use them\\nto train B separate models. It can be shown (see Exercise 8) that, for large sample sizes, on\\naverage about a third (more precisely, a fraction e−1 ≈0.37) of the original sample points\\nare not included in bootstrapped set T∗\\nb for 1 ⩽b ⩽B. Therefore, these samples can be\\nused for the loss estimation. These samples are called out-of-bag out-of-bag(OOB) observations.\\nSpecifically, for each sample from the original data set, we calculate the OOB loss using\\npredictors that were trained without this particular sample. The estimation procedure is\\nsummarized in Algorithm 8.5.2. Hastie et al. [55] observe that, under certain conditions, the\\nOOB loss is almost identical to the n-fold cross-validation loss. In addition, the OOB loss\\ncan be used to determine the number of trees required. Specifically, we can train predictors\\nuntil the OOB loss stops changing. Namely, decision trees are added until the OOB loss\\nstabilizes.\\nAlgorithm 8.5.2: Out-of-Bag Loss Estimation\\nInput: The original data set τ= {(x1,y1),..., (xn,yn)}, the bootstrapped data sets\\n{T∗\\n1 ,..., T∗\\nB}, and the trained predictors\\nn\\ngT∗\\n1 ,..., gT∗\\nB\\no\\n.\\nOutput: Out-of-bag loss for the averaged model.\\n1 for i = 1 to n do\\n2 Ci ←∅ // Indices of predictors not depending on (xi,yi)\\n3 for b = 1 to B do\\n4 if (xi,yi) < T∗\\nb then Ci ←Ci ∪{b}\\n5 Y′\\ni ←|Ci|−1 P\\nb∈Ci gT∗\\nb\\n(xi)\\n6 Li ←Loss\\n\\x10\\nyi,Y′\\ni\\n\\x11\\n7 LOOB ←1\\nn\\nPn\\ni=1 Li\\n8 return LOOB.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 325, 'page_label': '308'}, page_content='308 Bootstrap Aggregation\\nExample 8.3 (Bagging for a Regression Tree) We next proceed with a basic bagging\\nexample for a regression tree, in which we compare the decision tree estimator with the\\ncorresponding bagged estimator. We use the R2 metric (coefficient of determination) for\\ncomparison.\\nBaggingExample.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\nnp.random.seed(100)\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n# training\\nregTree = DecisionTreeRegressor(random_state=100)\\nregTree.fit(x_train,y_train)\\n# test\\nyhat = regTree.predict(x_test)\\n# Bagging construction\\nn_estimators=500\\nbag = np.empty((n_estimators), dtype= object )\\nbootstrap_ds_arr = np.empty((n_estimators), dtype= object )\\nfor i in range (n_estimators):\\n# sample bootstrapped data set\\nids = np.random.choice( range (0,len (x_train)),size= len (x_train),\\nreplace=True)\\nx_boot = x_train[ids]\\ny_boot = y_train[ids]\\nbootstrap_ds_arr[i] = np.unique(ids)\\nbag[i] = DecisionTreeRegressor()\\nbag[i].fit(x_boot,y_boot)\\n# bagging prediction\\nyhatbag = np.zeros( len (y_test))\\nfor i in range (n_estimators):\\nyhatbag = yhatbag + bag[i].predict(x_test)\\nyhatbag = yhatbag/n_estimators\\n# out of bag loss estimation'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 326, 'page_label': '309'}, page_content='Decision Trees and Ensemble Methods 309\\noob_pred_arr = np.zeros( len (x_train))\\nfor i in range (len (x_train)):\\nx = x_train[i].reshape(1, -1)\\nC = []\\nfor b in range (n_estimators):\\nif (np.isin(i, bootstrap_ds_arr[b])==False):\\nC.append(b)\\nfor pred in bag[C]:\\noob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/ len (C))\\nL_oob = r2_score(y_train, oob_pred_arr)\\nprint (\"DecisionTreeRegressor R^2 score = \",r2_score(y_test, yhat),\\n\"\\\\nBagging R^2 score = \", r2_score(y_test, yhatbag),\\n\"\\\\nBagging OOB R^2 score = \",L_oob)\\nDecisionTreeRegressor R^2 score = 0.575438224929718\\nBagging R^2 score = 0.7612121189201985\\nBagging OOB R^2 score = 0.7758253149069059\\nThe decision tree bagging improves the test-set R2 score by about 32% (from 0 .575\\nto 0.761). Moreover, the OOB score (0 .776) is very close to the true generalization risk\\n(0.761) of the bagged estimator.\\nThe bagging procedure can be further enhanced by introducing random forests, which\\nis discussed next.\\n8.6 Random Forests\\nIn Section 8.5, we discussed the intuition behind the prediction averaging procedure. Spe-\\ncifically, for some feature vector x let Zb = gTb (x),b = 1,2,..., B be iid prediction val-\\nues, obtained from independent training sets T1,..., TB. Suppose that Var Zb = σ2 for all\\nb = 1,..., B. Then the variance of the average prediction value ZB is equal to σ2/B. How-\\never, if bootstrapped data sets {T∗\\nb }are used instead, the corresponding random variables\\n{Zb}will be correlated. In particular, Zb = gT∗\\nb\\n(x) for b = 1,..., B are identically distrib-\\nuted (but not independent) with some positive pairwise correlationϱ. It then holds that (see\\nExercise 9)\\nVar ZB = ϱσ2 + σ2 (1 −ϱ)\\nB . (8.17)\\nWhile the second term of (8.17) goes to zero as the number of observationB increases, the\\nfirst term remains constant.\\nThis issue is particularly relevant for bagging with decision trees. For example, con-\\nsider a situation in which there exists a feature that provides a very good split of the data.\\nSuch a feature will be selected and split for every {gT∗\\nb\\n}B\\nb=1 at the root level and we will\\nconsequently end up with highly correlated predictions. In such a situation, prediction\\naveraging will not introduce the desired improvement in the performance of the bagged\\npredictor.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 327, 'page_label': '310'}, page_content='310 Random Forests\\nThe major idea of random forests is to perform bagging in combination with a “decor-\\nrelation” of the trees by including only a subset of features during the tree construction. For\\neach bootstrapped training setT∗\\nb we build a decision tree using a randomly selected subset\\nof m ⩽p features for the splitting rules. This simple but powerful idea will decorrelate the\\ntrees, since strong predictors will have a smaller chance to be considered at the root levels.\\nConsequentially, we can expect to improve the predictive performance of the bagged\\nestimator. The resulting predictor (random forest) construction is summarized in Algorithm\\n8.6.1.\\nAlgorithm 8.6.1: Random Forest Construction\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of trees in the forest B, and the\\nnumber m ⩽p of features to be included, where p is the total number of\\nfeatures in x.\\nOutput: Ensemble of trees.\\n1 Generate bootstrapped training sets {T∗\\n1 ,..., T∗\\nB}via Algorithm 8.5.1.\\n2 for b = 1 to B do\\n3 Train a decision tree gT∗\\nb\\nvia Algorithm 8.2.1, where each split is performed\\nusing m randomly selected features out of p.\\n4 return {gT∗\\nb\\n}B\\nb=1.\\nFor regression problems, the output of Algorithm 8.6.1 is combined to yield the random\\nforest prediction function:\\ngRF(x) = 1\\nB\\nBX\\nb=1\\ngT∗\\nb\\n(x).\\nIn the classification setting, similar to Remark 8.1, we take instead the majority vote from\\nthe {gT∗\\nb\\n}.\\nExample 8.4 (Random Forest for a Regression Tree) We continue with the basic\\nbagging Example 8.3 for a regression tree, in which we compared the decision tree es-\\ntimator with the corresponding bagged estimator. Here, however, we use the random forest\\nwith B = 500 trees and a subset sizem = 8. It can be seen that the random forest’sR2 score\\nis outperforming that of the bagged estimator.\\nBaggingExampleRF.py\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.ensemble import RandomForestRegressor\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 328, 'page_label': '311'}, page_content='Decision Trees and Ensemble Methods 311\\nrf = RandomForestRegressor(n_estimators=500, oob_score = True,\\nmax_features=8,random_state=100)\\nrf.fit(x_train,y_train)\\nyhatrf = rf.predict(x_test)\\nprint (\"RF R^2 score = \", r2_score(y_test, yhatrf),\\n\"\\\\nRF OOB R^2 score = \", rf.oob_score_)\\nRF R^2 score = 0.8106589580845707\\nRF OOB R^2 score = 0.8260541058404149\\nRemark 8.3 (The Optimal Number of Subset Features m) The default values for m\\nare ⌊p/3⌋and\\nj√p\\nk\\nfor regression and classification setting, respectively. However, the\\nstandard practice is to treat m as a hyperparameter that requires tuning, depending on the\\nspecific problem at hand [55].\\nNote that the procedure of bagging decision trees is a special case of a random forest\\nconstruction (see Exercise 11). Consequently, the OOB loss is readily available for random\\nforests.\\nWhile the advantage of bagging in the sense of enhanced accuracy is clear, we should\\nalso consider its negative aspects and, in particular, the loss of interpretability. Specifically\\na random forest consists of many trees, thus making the prediction process both hard to\\nvisualize and interpret. For example, given a random forest, it is not easy to determine a\\nsubset of features that are essential for accurate prediction.\\nThe feature importance measure intends to address this issue. The idea is as follows.\\nEach internal node of a decision tree induces a certain decrease in the training loss; see\\n(8.9). Let us denote this decrease in the training loss by ∆Loss(v), where v is not a leaf node\\nof T. In addition, recall that for splitting rules of the type 1{xj ⩽ξ}(1 ⩽j ⩽p), each node\\nv is associated with a feature xj that determines the split. Using the above definitions, we\\ncan define the feature importance feature\\nimportance\\nof xj as\\nIT(xj) =\\nX\\nv internal ∈T\\n∆Loss(v) 1{xj is associated with v}, 1 ⩽j ⩽p. (8.18)\\nWhile (8.18) is defined for a single tree, it can be readily extended to random forests.\\nSpecifically, the feature importance in that case will be averaged over all trees of the forest;\\nthat is, for a forest consisting of B trees {T1,..., TB}, the feature importance measure is:\\nIRF(xj) = 1\\nB\\nBX\\nb=1\\nITb (xj), 1 ⩽j ⩽p. (8.19)\\nExample 8.5 (Feature Importance) We consider a classification problem with 15 fea-\\ntures. The data is specifically designed to contain only 5 informative features out of 15.\\nIn the code below, we apply the random forest procedure and calculate the corresponding\\nfeature importance measures, which are summarized in Figure 8.9.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 329, 'page_label': '312'}, page_content='312 Random Forests\\nVarImportance.py\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport matplotlib.pyplot as plt, pylab\\nn_points = 1000 # create regression data with 1000 data points\\nx, y = make_classification(n_samples=n_points, n_features=15,\\nn_informative=5, n_redundant=0, n_repeated=0, random_state=100,\\nshuffle=False)\\nrf = RandomForestClassifier(n_estimators=200, max_features=\"log2\")\\nrf.fit(x,y)\\nimportances = rf.feature_importances_\\nindices = np.argsort(importances)[::-1]\\nfor f in range (15):\\nprint (\"Feature %d (%f)\" % (indices[f]+1, importances[indices[f\\n]]))\\nstd = np.std([rf.feature_importances_ for tree in rf.estimators_],\\naxis=0)\\nf = plt.figure()\\nplt.bar( range (x.shape[1]), importances[indices],\\ncolor=\"b\", yerr=std[indices], align=\"center\")\\nplt.xticks( range (x.shape[1]), indices+1)\\nplt.xlim([-1, x.shape[1]])\\npylab.xlabel(\"feature index\")\\npylab.ylabel(\"importance\")\\nplt.show()\\n5 1 2 4 3 7 11 13 6 9 15 8 14 10 12\\n0\\n0.1\\n0.2\\nfeature index\\nimportance\\nFigure 8.9: Importance measure for the 15-feature data set with only 5 informative features\\nx1,x2,x3,x4, and x5.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 330, 'page_label': '313'}, page_content='Decision Trees and Ensemble Methods 313\\nClearly, it is hard to visualize and understand the prediction process based on 200 trees.\\nHowever, Figure 8.9 shows that the features x1,x2,x3,x4,and x5 were correctly identified\\nas being important.\\n8.7 Boosting\\nBoosting is a powerful idea that aims to improve the accuracy of any learning algorithm,\\nespecially when involving weak learners weak learners— simple prediction functions that exhibit per-\\nformance slightly better than random guessing. Shallow decision trees typically yield weak\\nlearners.\\nOriginally, boosting was developed for binary classification tasks, but it can be readily\\nextended to handle general classification and regression problems. The boosting approach\\nhas some similarity with the bagging method in the sense that boosting uses an ensemble of\\nprediction functions. Despite this similarity, there exists a fundamental difference between\\nthese methods. Specifically, while bagging involves the fitting of prediction functions to\\nbootstrapped data, the predicting functions in boosting are learned sequentially. That is,\\neach learner uses information from previous learners.\\nThe idea is to start with a simple model (weak learner) g0 for the data τ = {(xi,yi)}n\\ni=1\\nand then to improve or “boost” this learner to a learner g1 := g0 + h1. Here, the function h1\\nis found by minimizing the training loss for g0 + h1 over all functions h in some class of\\nfunctions H. For example, Hcould be the set of prediction functions that can be obtained\\nvia a decision tree of maximal depth 2. Given a loss function Loss, the function h1 is thus\\nobtained as the solution to the optimization problem\\nh1 = argmin\\nh∈H\\n1\\nn\\nnX\\ni=1\\nLoss (yi,g0(xi) + h (xi)). (8.20)\\nThis process can be repeated for g1 to obtain g2 = g1 + h2, and so on, yielding the boosted\\nprediction function\\ngB(x) = g0(x) +\\nBX\\nb=1\\nhb(x). (8.21)\\nInstead of using the updating step gb = gb−1 + hb, one prefers to use the smooth updating\\nstep gb = gb−1 + γhb, for some suitably chosen step-size parameter γ. As we shall see\\nshortly, this helps reduce overfitting.\\nBoosting can be used for regression and classification problems. We start with a simple\\nregression setting, using the squared-error loss; thus, Loss( y,by) = (y −by)2. In this case, it\\nis common to start with g0(x) = n−1 Pn\\ni=1 yi, and each hb for b = 1,..., B is chosen as a\\nlearner for the data set τb of residuals corresponding to gb−1. That is, τb :=\\nn\\x10\\nxi,e(b)\\ni\\n\\x11on\\ni=1,\\nwith\\ne(b)\\ni := yi −gb−1(xi). (8.22)\\nThis leads to the following boosting procedure for regression with squared-error loss.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 331, 'page_label': '314'}, page_content='314 Boosting\\nAlgorithm 8.7.1: Regression Boosting with Squared-Error Loss\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of boosting rounds B, and a\\nshrinkage step-size parameter γ.\\nOutput: Boosted prediction function.\\n1 Set g0(x) ←n−1 Pn\\ni=1 yi.\\n2 for b = 1 to B do\\n3 Set e(b)\\ni ←yi −gb−1(xi) for i = 1,..., n, and let τb ←\\nn\\x10\\nxi,e(b)\\ni\\n\\x11on\\ni=1.\\n4 Fit a prediction function hb on the training data τb.\\n5 Set gb(x) ←gb−1(x) + γhb(x).\\n6 return gB.\\nThe step-size parameter γstep-size\\nparameter γ\\nintroduced in Algorithm 8.7.1 controls the speed of the\\nfitting process. Specifically, for small values of γ, boosting takes smaller steps to-\\nwards the training loss minimization. The step-size γ is of great practical import-\\nance, since it helps the boosting algorithm to avoid overfitting. This phenomenon is\\ndemonstrated in Figure 8.10.\\n−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 1\\n−2 −1 0 1 2\\n−50\\n0\\n50\\ntrain data\\ng1000, γ = 0. 005\\nFigure 8.10: The left and the right panels show the fitted boosting regression model g1000\\nwith γ= 1.0 and γ= 0.005, respectively. Note the overfitting on the left.\\nA very basic implementation of Algorithm 8.7.1 which reproduces Figure 8.10 is\\nprovided below.\\nRegressionBoosting.py\\nimport numpy as np\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import make_regression\\nimport matplotlib.pyplot as plt'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 332, 'page_label': '315'}, page_content=\"Decision Trees and Ensemble Methods 315\\ndef TrainBoost(alpha,BoostingRounds,x,y):\\ng_0 = np.mean(y)\\nresiduals = y-alpha*g_0\\n# list of basic regressor\\ng_boost = []\\nfor i in range (BoostingRounds):\\nh_i = DecisionTreeRegressor(max_depth=1)\\nh_i.fit(x,residuals)\\nresiduals = residuals - alpha*h_i.predict(x)\\ng_boost.append(h_i)\\nreturn g_0, g_boost\\ndef Predict(g_0, g_boost,alpha, x):\\nyhat = alpha*g_0*np.ones( len (x))\\nfor j in range (len (g_boost)):\\nyhat = yhat+alpha*g_boost[j].predict(x)\\nreturn yhat\\nnp.random.seed(1)\\nsz = 30\\n# create data set\\nx,y = make_regression(n_samples=sz, n_features=1, n_informative=1,\\nnoise=10.0)\\n# boosting algorithm\\nBoostingRounds = 1000\\nalphas = [1, 0.005]\\nfor alpha in alphas:\\ng_0, g_boost = TrainBoost(alpha,BoostingRounds,x,y)\\nyhat = Predict(g_0, g_boost, alpha, x)\\n# plot\\ntmpX = np.reshape(np.linspace(-2.5,2,1000),(1000,1))\\nyhatX = Predict(g_0, g_boost, alpha, tmpX)\\nf = plt.figure()\\nplt.plot(x,y, '*')\\nplt.plot(tmpX,yhatX)\\nplt.show()\\nThe parameter γ can be viewed as a step size made in the direction of the negative\\ngradient of the squared-error training loss. To see this, note that the negative gradient\\n−∂Loss (yi,z)\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\n= −∂ (yi −z)2\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\n= 2(yi −gb−1(xi))\\nis two times the residual e(b)\\ni given in (8.22) that is used in Algorithm 8.7.1 to fit the pre-\\ndiction function hb.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 333, 'page_label': '316'}, page_content='316 Boosting\\nIn fact, one of the major advances in the theory of boosting was the recognition that\\none can use a similar gradient descent method for any di fferentiable loss function. The\\nresulting algorithm is called gradient boostinggradient\\nboosting\\n. The general gradient boosting algorithm is\\nsummarized in Algorithm 8.7.2. The main idea is to mimic a gradient descent algorithm\\nin the following sense. At each stage of the boosting procedure, we calculate a negative☞412\\ngradient on n training points x1,..., xn (Lines 3–4). Then, we fit a simple model (such as\\na shallow decision tree) to approximate the gradient (Line 5) for any feature x. Finally,\\nsimilar to the gradient descent method, we make a γ-sized step in the direction of the\\nnegative gradient (Line 6).\\nAlgorithm 8.7.2: Gradient Boosting\\nInput: Training set τ= {(xi,yi)}n\\ni=1, the number of boosting rounds B, a\\ndifferentiable loss function Loss(y,by), and a gradient step-size parameter γ.\\nOutput: Gradient boosted prediction function.\\n1 Set g0(x) ←0.\\n2 for b = 1 to B do\\n3 for i = 1 to n do\\n4 Evaluate the negative gradient of the loss at (xi,yi) via\\nr(b)\\ni ←− ∂Loss (yi,z)\\n∂z\\n\\x0c\\x0c\\x0c\\x0c\\x0cz=gb−1(xi)\\ni = 1,..., n.\\n5 Approximate the negative gradient by solving\\nhb = argmin\\nh∈H\\n1\\nn\\nnX\\ni=0\\n\\x10\\nr(b)\\ni −h (xi)\\n\\x112\\n. (8.23)\\n6 Set gb(x) ←gb−1(x) + γhb(x).\\n7 return gB\\nExample 8.6 (Gradient Boosting for a Regression Tree) Let us continue with the ba-\\nsic bagging and random forest examples for a regression tree (Examples 8.3 and 8.4), where\\nwe compared the standard decision tree estimator with the corresponding bagging and ran-\\ndom forest estimators. Now, we use the gradient boosting estimator from Algorithm 8.7.2,\\nas implemented in sklearn. We use γ = 0.1 and perform B = 100 boosting rounds. As\\na prediction function hb for b = 1,..., B we use small regression trees of depth at most\\n3. Note that such individual trees do not usually give good performance; that is, they are\\nweak prediction functions. We can see that the resulting boosting prediction function gives\\nthe R2 score equal to 0.899, which is better than R2 scores of simple decision tree (0.5754),\\nthe bagged tree (0.761), and the random forest (0.8106).\\nGradientBoostingRegression.py\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 334, 'page_label': '317'}, page_content='Decision Trees and Ensemble Methods 317\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n# boosting sklearn\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nbreg = GradientBoostingRegressor(learning_rate=0.1,\\nn_estimators=100, max_depth =3, random_state=100)\\nbreg.fit(x_train,y_train)\\nyhat = breg.predict(x_test)\\nprint (\"Gradient Boosting R^2 score = \",r2_score(y_test, yhat))\\nGradient Boosting R^2 score = 0.8993055635639531\\nWe proceed with the classification setting and consider the original boosting algorithm:\\nAdaBoost AdaBoost. The inventors of the AdaBoost method considered a binary classification prob-\\nlem, where the response variable belongs to the{−1,1}set. The idea of AdaBoost is similar\\nto the one presented in the regression setting, that is, AdaBoost fits a sequence of prediction\\nfunctions g0,g1 = g0 + h1,g2 = g0 + h1 + h2,... with final prediction function\\ngB(x) = g0(x) +\\nBX\\nb=1\\nhb(x), (8.24)\\nwhere each function hb is of the form hb(x) = αb cb(x), with αb ∈R+ and where cb is a\\nproper (but weak) classifier in some classC. Thus, cb(x) ∈{−1,1}. Exactly as in (8.20), we\\nsolve at each boosting iteration the optimization problem\\n(αb,cb) = argmin\\nα⩾0,c∈C\\n1\\nn\\nnX\\ni=1\\nLoss (yi,gb−1(xi) + αc(xi)). (8.25)\\nHowever, in this case the loss function is defined as Loss(y,by) = e−yby. The algorithm starts\\nwith a simple model g0 := 0 and for each successive iteration b = 1,..., B solves (8.25).\\nThus,\\n(αb,cb) = argmin\\nα⩾0,c∈C\\nnX\\ni=1\\ne−yi gb−1(xi)\\n|    {z    }\\nw(b)\\ni\\ne−yi αc(xi) = argmin\\nα⩾0,c∈C\\nnX\\ni=1\\nw(b)\\ni e−yiαc(xi),\\nwhere w(b)\\ni := exp{−yi gb−1(xi)}does not depend on αor c. It follows that\\n(αb,cb) = argmin\\nα⩾0,c∈C\\ne−α\\nnX\\ni=1\\nw(b)\\ni 1{c(xi) = yi}+ eα\\nnX\\ni=1\\nw(b)\\ni 1{c(xi) , yi}\\n= argmin\\nα⩾0,c∈C\\n(eα −e−α) ℓ(b)\\nτ (c) + e−α, (8.26)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 335, 'page_label': '318'}, page_content='318 Boosting\\nwhere\\nℓ(b)\\nτ (c) :=\\nPn\\ni=1 w(b)\\ni 1{c(xi) , yi}\\nPn\\ni=1 w(b)\\ni\\ncan be interpreted as the weighted zero–one training loss at iteration b.\\nFor any α ⩾0, the program (8.26) is minimized by a classifier c ∈C that minimizes\\nthis weighted training loss; that is,\\ncb(x) = argmin\\nc∈C\\nℓ(b)\\nτ . (8.27)\\nSubstituting (8.27) into (8.26) and solving for the optimal αgives\\nαb = 1\\n2 ln\\n 1 −ℓ(b)\\nτ (cb)\\nℓ(b)\\nτ (cb)\\n!\\n. (8.28)\\nThis gives the AdaBoost algorithm, summarized below.\\nAlgorithm 8.7.3: AdaBoost\\nInput: Training set τ= {(xi,yi)}n\\ni=1, and the number of boosting rounds B.\\nOutput: AdaBoost prediction function.\\n1 Set g0(x) ←0.\\n2 for i = 1 to n do\\n3 w(1)\\ni ←1/n\\n4 for b = 1 to B do\\n5 Fit a classifier cb on the training set τby solving\\ncb = argmin\\nc∈C\\nℓ(b)\\nτ (c) = argmin\\nc∈C\\nPn\\ni=1 w(b)\\ni 1{c(xi) , yi}\\nPn\\ni=1 w(b)\\ni\\n.\\n6 Set αb ←1\\n2 ln\\n 1 −ℓ(b)\\nτ (cb)\\nℓ(b)\\nτ (cb)\\n!\\n. // Update weights\\n7 for i = 1 to n do\\n8 w(b+1)\\ni ←w(b)\\ni exp{−yi αb cb(xi)}.\\n9 return gB(x) := PB\\nb=1 αb cb(x).\\nAlgorithm 8.7.3 is quite intuitive. At the first step ( b = 1), AdaBoost assigns an equal\\nweight w(1)\\ni = 1/n to each training sample ( xi,yi) in the set τ = {(xi,yi)}n\\ni=1. Note that, in\\nthis case, the weighted zero–one training loss is equal to the regular zero–one training loss.\\nAt each successive step b >1, the weights of observations that were incorrectly classified\\nby the previous boosting prediction function gb are increased, and the weights of correctly\\nclassified observations are decreased. Due to the use of the weighted zero–one loss, the set\\nof incorrectly classified training samples will receive an extra weight and thus have a better\\nchance of being classified correctly by the next classifier cb+1. As soon as the AdaBoost\\nalgorithm finds the prediction function gB, the final classification is delivered via\\nsign\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nBX\\nb=1\\nαb cb(x)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 336, 'page_label': '319'}, page_content='Decision Trees and Ensemble Methods 319\\nThe step-size parameter αb found by the AdaBoost algorithm in Line 6 can be\\nviewed as an optimal step-size in the sense of training loss minimization. How-\\never, similar to the regression setting, one can slow down the AdaBoost algorithm\\nby setting αb to be a fixed (small) value αb = γ. As usual, when the latter is done in\\npractice, it is tackling the problem of overfitting.\\nWe consider an implementation of Algorithm 8.7.3 for a binary classification problem.\\nSpecifically, during all boosting rounds, we use simple decision trees of depth 1 (also called\\ndecision tree stumps stumps) as weak learners. The exponential and zero–one training losses as a\\nfunction of the number of boosting rounds are presented in Figure 8.11.\\nAdaBoost.py\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import zero_one_loss\\nimport numpy as np\\ndef ExponentialLoss(y,yhat):\\nn = len (y)\\nloss = 0\\nfor i in range (n):\\nloss = loss+np.exp(-y[i]*yhat[i])\\nloss = loss/n\\nreturn loss\\n# create binary classification problem\\nnp.random.seed(100)\\nn_points = 100 # points\\nx, y = make_blobs(n_samples=n_points, n_features=5, centers=2,\\ncluster_std=20.0, random_state=100)\\ny[y==0]=-1\\n# AdaBoost implementation\\nBoostingRounds = 1000\\nn = len (x)\\nW = 1/n*np.ones(n)\\nLearner = []\\nalpha_b_arr = []\\nfor i in range (BoostingRounds):\\nclf = DecisionTreeClassifier(max_depth=1)\\nclf.fit(x,y, sample_weight=W)\\nLearner.append(clf)\\ntrain_pred = clf.predict(x)\\nerr_b = 0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 337, 'page_label': '320'}, page_content='320 Boosting\\nfor i in range (n):\\nif (train_pred[i]!=y[i]):\\nerr_b = err_b+W[i]\\nerr_b = err_b/np. sum (W)\\nalpha_b = 0.5*np.log((1-err_b)/err_b)\\nalpha_b_arr.append(alpha_b)\\nfor i in range (n):\\nW[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])\\nyhat_boost = np.zeros( len (y))\\nfor j in range (BoostingRounds):\\nyhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)\\nyhat = np.zeros(n)\\nyhat[yhat_boost>=0] = 1\\nyhat[yhat_boost<0] = -1\\nprint (\"AdaBoost Classifier exponential loss = \", ExponentialLoss(y,\\nyhat_boost))\\nprint (\"AdaBoost Classifier zero--one loss = \",zero_one_loss(y,yhat))\\nAdaBoost Classifier exponential loss = 0.004224013663777142\\nAdaBoost Classifier zero--one loss = 0.0\\n200 400 600 800 1 ,000\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nB\\nloss\\nexponential loss\\nzero-one loss\\nFigure 8.11: Exponential and zero–one training loss as a function of the number of boosting\\nrounds B for a binary classification problem.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 338, 'page_label': '321'}, page_content='Decision Trees and Ensemble Methods 321\\nFurther Reading\\nBreiman’s book on decision trees, [20], serves as a great starting point. Some additional\\nadvances can be found in [62, 96]. From the computational point of view, there exists\\nan efficient recursive procedure for tree pruning; see Chapters 3 and 10 in [20]. Several\\nadvantages and disadvantages of using decision trees are debated in [37, 55]. A detailed\\ndiscussion on bagging and random forests can be found in [21] and [23], respectively.\\nFreund and Schapire [44] provide the first boosting algorithm, the AdaBoost. While Ad-\\naBoost was developed in the context of the computational complexity of learning, it was\\nlater discovered by Friedman [45] that AdaBoost is a special case of an additive model.\\nIn addition, it was shown that for any di fferentiable loss function, there exists an e fficient\\nboosting procedure which mimics the gradient descent algorithm. The foundation of the\\nresulting gradient boosting method is detailed in [45]. Python packages that implement\\ngradient boosting include XGBoost and LightGBM.\\nExercises\\n1. Show that any training set τ = {(x,yi),i = 1,..., n}can be fitted via a tree with zero\\ntraining loss.\\n2. Suppose during the construction of a decision tree we wish to specify a constant re-\\ngional prediction function gw on the region Rw, based on the training data in Rw, say\\n{(x1,y1),..., (xk,yk)}. Show that gw(x) := k−1 Pk\\ni=1 yi minimizes the squared-error loss.\\n3. Using the program from Section 8.2.4, write a basic implementation of a decision tree\\nfor a binary classification problem. Implement the misclassification, Gini index, and en-\\ntropy impurity criteria to split nodes. Compare the results.\\n4. Suppose in the decision tree of Example 8.1, there are 3 blue and 2 red data points in\\na certain tree region. Calculate the misclassification impurity, the Gini impurity, and the\\nentropy impurity. Repeat these calculations for 2 blue and 3 red data points.\\n5. Consider the procedure of finding the best splitting rule for a categorical variable with\\nk labels from Section 8.3.4. Show that one needs to consider 2k subsets of {1,..., k}to find\\nthe optimal partition of labels.\\n6. Reproduce Figure 8.6 using the following classification data.\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs(n_samples=5000, n_features=10, centers=3,\\nrandom_state=10, cluster_std=10)\\n7. Prove (8.13); that is, show that\\nX\\nw∈W\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nnX\\ni=1\\n1{xi ∈Rw}Loss(yi,gw(xi))\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8 = n ℓτ (g) .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 339, 'page_label': '322'}, page_content='322 Exercises\\n8. Suppose τ is a training set with n elements and τ∗, also of size n, is obtained from τ\\nby bootstrapping; that is, resampling with replacement. Show that for large n, τ∗does not\\ncontain a fraction of about e−1 ≈0.37 of the points from τ.\\n9. Prove Equation (8.17).\\n10. Consider the following training /test split of the data. Construct a random forest re-\\ngressor and identify the optimal subset size m in the sense of R2 score (see Remark 8.3).\\nimport numpy as np\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n# create regression problem\\nn_points = 1000 # points\\nx, y = make_friedman1(n_samples=n_points, n_features=15,\\nnoise=1.0, random_state=100)\\n# split to train/test set\\nx_train, x_test, y_train, y_test = \\\\\\ntrain_test_split(x, y, test_size=0.33, random_state=100)\\n11. Explain why bagging decision trees are a special case of random forests.\\n12. Show that (8.28) holds.\\n13. Consider the following classification data and module imports:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.metrics import zero_one_loss\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX_train, y_train = make_blobs(n_samples=5000, n_features=10,\\ncenters=3, random_state=10, cluster_std=5)\\nUsing the gradient boosting algorithm with B = 100 rounds, plot the training loss as a\\nfunction of γ, for γ = 0.1,0.3,0.5,0.7,1. What is your conclusion regarding the relation\\nbetween B and γ?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 340, 'page_label': '323'}, page_content='CHAPTER 9\\nDEEP LEARNING\\nIn this chapter, we show how one can construct a rich class of approximating func-\\ntions called neural networks. The learners belonging to the neural-network class of\\nfunctions have attractive properties that have made them ubiquitous in modern machine\\nlearning applications — their training is computationally feasible and their complexity\\nis easy to control and fine-tune.\\n9.1 Introduction\\nIn Chapter 2 we described the basic supervised learning task; namely, we wish to predict a\\nrandom output Y from a random inputX, using a prediction functiong : x 7→y that belongs\\nto a suitably chosen class of approximating functions G. More generally, we may wish to\\npredict a vector-valued output y using a prediction function g : x 7→y from class G.\\nIn this chapter y denotes the vector-valued output for a given input x. This di ffers\\nfrom our previous use (e.g., in Table 2.1), wherey denotes a vector of scalar outputs.\\nIn the machine learning context, the class Gis sometimes referred to as the hypothesis\\nspace or the universe of possible models, and the representational capacity of a hypothesis representational\\ncapacityspace Gis simply its complexity.\\nSuppose that we have a class of functions GL, indexed by a parameter L that controls\\nthe complexity of the class, so that GL ⊂ GL+1 ⊂ GL+2 ⊂ ···. In selecting a suitable\\nclass of functions, we have to be mindful of theapproximation–estimation tradeoff. On the ☞ 31\\none hand, the class GL must be complex (rich) enough to accurately represent the optimal\\nunknown prediction function g∗, which may require a very large L. On the other hand, the\\nlearners in the class GL must be simple enough to train with small estimation error and\\nwith minimal demands on computer memory, which may necessitate a small L.\\nIn balancing these competing objectives, it helps if the more complex class GL+1 is\\neasily constructed from an already existing and simpler GL. The simpler class of functions\\nGL may itself be constructed by modifying an even simpler class GL−1, and so on.\\nA class of functions that permits such a natural hierarchical construction is the class of\\nneural networks. Conceptually, a neural network with L layers is a nonlinear parametric neural\\nnetworksregression model whose representational capacity can easily be controlled by L.\\n☞ 188323'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 341, 'page_label': '324'}, page_content='324 Introduction\\nAlternatively, in (9.3) we will define the output of a neural network as the repeated\\ncomposition of linear and (componentwise) nonlinear functions. As we shall see, this rep-\\nresentation of the output will provide a flexible class of nonlinear functions that can be\\neasily differentiated. As a result, the training of learners via gradient optimization methods\\ninvolves mostly standard matrix operations that can be performed very efficiently.☞412\\nHistorically, neural networks were originally intended to mimic the workings of the\\nhuman brain, with the network nodes modeling neurons and the network links modeling\\nthe axons connecting neurons. For this reason, rather than using the terminology of the\\nregression models in Chapter 5, we prefer to use a nomenclature inspired by the apparent\\nresemblance of neural networks to structures in the human brain.\\nWe note, however, that the attempts at building efficient machine learning algorithms by\\nmimicking the functioning of the human brain have been as unsuccessful as the attempts\\nat building flying aircraft by mimicking the flapping of birds’ wings. Instead, many ef-\\nfective machine algorithms have been inspired by age-old mathematical ideas for function\\napproximation. One such idea is the following fundamental result (see [119] for a proof).\\nTheorem 9.1: Kolmogorov (1957)\\nEvery continuous function g∗: [0,1]p 7→Rwith p ⩾2 can be written as\\ng∗(x) =\\n2p+1X\\nj=1\\nhj\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\npX\\ni=1\\nhi j(xi)\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\nwhere {hj,hi j}is a set of univariate continuous functions that depend on g∗.\\nThis result tells us that any continuous high-dimensional map can be represented as\\nthe function composition of much simpler (one-dimensional) maps. The composition of\\nthe maps needed to compute the output g∗(x) for a given input x ∈Rp are depicted in\\nFigure 9.1, showing a directed graph or neural network with three layers, denoted as l =\\n0,1,2.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 342, 'page_label': '325'}, page_content='Deep Learning 325\\nhij\\nxi\\nxp\\nz j\\nh j\\n→ a j\\nz1\\nh1\\n→ a1\\nzq\\nhq\\n→ aq\\ng∗(x)\\nx1\\nhpq\\nh11\\nFigure 9.1: Every continuous function g∗: [0,1]p 7→Rcan be represented by a neural net-\\nwork with one hidden layer (l = 1), an input layer (l = 0), and an output layer (l = 2).\\nIn particular, each of the p components of the input x is represented as a node in the\\ninput layer (l = 0). In the hidden layer (l = 1) there are q := 2p + 1 nodes, each of which hidden layer\\nis associated with a pair of variables (z,a) with values\\nzj :=\\npX\\ni=1\\nhi j(xi) and aj := hj(zj).\\nA link between nodes ( zj,aj) and xi with weight hi j signifies that the value of zj depends\\non the value of xi via the function hi j. Finally, the output layer (l = 2) represents the value\\ng∗(x) = Pq\\nj=1 aj. Note that the arrows on the graph remind us that the sequence of the\\ncomputations is executed from left to right, or from the input layer l = 0 through to the\\noutput layer l = 2.\\nIn practice, we do not know the collection of functions {hj,hi j}, because they depend\\non the unknown g∗. In the unlikely event that g∗ is linear, then all of the (2 p + 1)(p + 1)\\none-dimensional functions will be linear as well. However, in general, we should expect\\nthat each of the functions in {hj,hi j}is nonlinear.\\nUnfortunately, Theorem 9.1 only asserts the existence of {hj,hi j}, and does not tell us\\nhow to construct these nonlinear functions. One way out of this predicament is to replace\\nthese (2p + 1)(p + 1) unknown functions with a much larger number of known nonlinear\\nfunctions called activation functions.1 For example, a logistic activation function is activation\\nfunctions\\nS (z) = (1 + exp(−z))−1.\\nWe then hope that such a network, built from a su fficiently large number of activation\\nfunctions, will have similar representational capacity as the neural network in Figure 9.1\\nwith (2p + 1)(p + 1) functions.\\nIn general, we wish to use the simplest activation functions that will allow us to build\\na learner with large representational capacity and low training cost. The logistic function\\n1Activation functions derive their name from models of a neuron’s response when exposed to chemical\\nor electric stimuli.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 343, 'page_label': '326'}, page_content='326 Feed-Forward Neural Networks\\nis merely one possible choice for an activation function from among infinite possibilit-\\nies. Figure 9.2 shows a small selection of activation functions with di fferent regularity or\\nsmoothness properties.\\nHeaviside or unit step rectified linear unit (ReLU) logistic\\n-2 0 2\\n0\\n0.5\\n1\\n-2 0 2\\n0\\n1\\n2\\n3\\n-2 0 2\\n0\\n0.5\\n1\\n1{z ⩾0} z ×1{z ⩾0} (1 + exp(−z))−1\\nFigure 9.2: Some common activation functionsS (z) with their defining formulas and plots.\\nThe logistic function is an example of a sigmoid (that is, an S-shaped) function. Some\\nbooks define the logistic function as 2S (z) −1 (in terms of our definition).\\nIn addition to choosing the type and number of activation functions in a neural network,\\nwe can improve its representational capacity in another important way: introduce more\\nhidden layers. In the next section we explore this possibility in detail.\\n9.2 Feed-Forward Neural Networks\\nIn a neural network withL+1 layers, the zero or input layer (l = 0) encodes the input feature\\nvector x, and the last or output layer (l = L) encodes the (multivalued) output functiong(x).\\nThe remaining layers are called hidden layers. Each layer has a number of nodes, say pl\\nnodes for layer l = 0,..., L. In this notation, p0 is the dimension of the input feature vector\\nx and, for example, pL = 1 signifies that g(x) is a scalar output. All nodes in the hidden\\nlayers (l = 1,..., L −1) are associated with a pair of variables ( z,a), which we gather\\ninto pl-dimensional column vectors zl and al. In the so-called feed-forwardfeed-forward networks, the\\nvariables in any layer l are simple functions of the variables in the preceding layer l −1. In\\nparticular, zl and al−1 are related via the linear relation zl = Wl al−1 + bl, for some weight\\nmatrix Wl and bias vector bl.weight matrix\\nbias vector Within any hidden layer l = 1,..., L −1, the components of the vectors zl and al\\nare related via al = Sl(zl), where Sl : Rpl 7→Rpl is a nonlinear multivalued function. All of\\nthese multivalued functions are typically of the form\\nSl(z) = [S (z1),..., S (zdim(z))]⊤, l = 1,..., L −1, (9.1)\\nwhere S is an activation function common to all hidden layers. The function SL : RpL−1 7→\\nRpL in the output layer is more general and its specification depends, for example, on\\nwhether the network is used for classification or for the prediction of a continuous output\\nY. A four-layer (L = 3) network is illustrated in Figure 9.3.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 344, 'page_label': '327'}, page_content='Deep Learning 327\\nInput layer Hidden layers Output layer\\nbias\\nweight\\nb1, j\\nz3,m\\nS3w1, ji\\nz2,k\\nS\\n→ a2,k\\nb2,k b3,m\\nz1, j\\nS\\n→ a1, j\\nw2,k j w3,mk g(x)xi\\nFigure 9.3: A neural network with L = 3: the l = 0 layer is the input layer, followed by two\\nhidden layers, and the output layer. Hidden layers may have different numbers of nodes.\\nThe output of this neural network is determined by the input vector x, (nonlinear)\\nfunctions {Sl}, as well as weight matrices Wl = [wl,i j] and bias vectors bl = [bl,j] for\\nl = 1,2,3.\\nHere, the (i, j)-th element of the weight matrix Wl = [wl,i j] is the weight that con-\\nnects the j-th node in the (l −1)-st layer with the i-th node in the l-th layer.\\nThe name given toL (the number of layers without the input layer) is thenetwork depth network depth\\nand maxl pl is called thenetwork width. While we mostly study networks that have an equal network width\\nnumber of nodes in the hidden layers ( p1 = ··· = pL−1), in general there can be di fferent\\nnumbers of nodes in each hidden layer.\\nThe output g(x) of a multiple-layer neural network is obtained from the input x via the\\nfollowing sequence of computations:\\nx|{z}\\na0\\n→W1 a0 + b1|       {z       }\\nz1\\n→S1(z1)|{z}\\na1\\n→W2 a1 + b2|       {z       }\\nz2\\n→S2(z2)|{z}\\na2\\n→···\\n→WL aL−1 + bL|          {z          }\\nzL\\n→SL(zL)|{z}\\naL\\n= g(x). (9.2)\\nDenoting the function z 7→Wl z + bl by Ml, the output g(x) can thus be written as the\\nfunction composition\\ng(x) = SL ◦ML ◦···◦ S2 ◦M2 ◦S1 ◦M1(x). (9.3)\\nThe algorithm for computing the output g(x) for an input x is summarized next. Note\\nthat we leave open the possibility that the activation functions{Sl}have different definitions'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 345, 'page_label': '328'}, page_content='328 Feed-Forward Neural Networks\\nfor each layer. In some cases, Sl may even depend on some or all of the already computed\\nz1,z2,... and a1,a2,... .\\nAlgorithm 9.2.1: Feed-Forward Propagation for a Neural Network\\ninput: Feature vector x; weights {wl,i j}, biases {bl,i}for each layer l = 1,..., L.\\noutput: The value of the prediction function g(x).\\n1 a0 ←x // the zero or input layer\\n2 for l = 1 to L do\\n3 Compute the hidden variable zl,i for each node i in layer l:\\nzl ←Wl al−1 + bl\\n4 Compute the activation function al,i for each node i in layer l:\\nal ←Sl(zl)\\n5 return g(x) ←aL // the output layer'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 346, 'page_label': '329'}, page_content='Deep Learning 329\\nExample 9.1 (Nonlinear Multi-Output Regression) Given the input x ∈Rp0 and an\\nactivation functionS : R7→R, the output g(x) := [g1(x),..., gp2 (x)]⊤of a nonlinear multi-\\noutput regression model can be computed via a neural network with: ☞ 214\\nz1 = W1 x + b1, where W1 ∈Rp1×p0 ,b1 ∈Rp1 ,\\na1,k = S (z1,k), k = 1,..., p1,\\ng(x) = W2 a1 + b2, where W2 ∈Rp2×p1 ,b2 ∈Rp2 ,\\nwhich is a neural network with one hidden layer and output function S2(z) = z. In the\\nspecial case where p1 = p2 = 1, b2 = 0,W2 = 1, and we collect all parameters into the\\nvector θ⊤= [b1,W1] ∈Rp0+1, the neural network can be interpreted as ageneralized linear\\nmodel with E[Y |X = x] = h([1,x⊤] θ) for some activation function h. ☞ 204\\nExample 9.2 (Multi-Logit Classification) Suppose that, for a classification problem,\\nan input x has to be classified into one ofc classes, labeled 0,..., c−1. We can perform the\\nclassification via a neural network with one hidden layer, with p1 = c nodes. In particular,\\nwe have\\nz1 = W1 x + b1, a1 = S1(z1),\\nwhere S1 is the softmax function: softmax\\nsoftmax : z 7→ exp(z)P\\nk exp(zk).\\nFor the output, we take g(x) = [g1(x),..., gc(x)]⊤ = a1, which can then be used as a\\npre-classifier of x. The actual classifier of x into one of the categories 0,1,..., c −1 is then ☞ 252\\nargmax\\nk ∈{0,...,c−1}\\ngk+1(x).\\nThis is equivalent to the multi-logit classifier in Section 7.5. Note, however, that there we ☞ 266\\nused a slightly di fferent notation, with ex instead of x and we have a reference class; see\\nExercise 13.\\nIn practical implementations, the softmax function can cause numerical over- and\\nunder-flow errors when either one of the exp( zk) happens to be extremely large orP\\nk exp(zk) happens to be very small. In such cases we can exploit the invariance\\nproperty (Exercise 1):\\nsoftmax(z) = softmax(z + c ×1) for any constant c.\\nUsing this property, we can compute softmax(z) with greater numerical stability via\\nsoftmax(z −maxk{zk}×1).\\nWhen neural networks are used for classification into c classes and the number of out-\\nput nodes is c −1, then the gi(x) may be viewed as nonlinear discriminant functions. ☞ 260'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 347, 'page_label': '330'}, page_content='330 Feed-Forward Neural Networks\\nExample 9.3 (Density Estimation) Estimating the density f of some random feature\\nX ∈Ris the prototypical unsupervised learning task, which we tackled in Section 4.5.2 us-\\ning Gaussian mixture models. We can view a Gaussian mixture model withp1 components☞137\\nand a common scale parameter σ >0 as a neural network with two hidden layers, similar\\nto the one on Figure 9.3. In particular, if the activation function in the first hidden layer,\\nS1, is of the form (9.1) with S (z) := exp(−z2/(2σ2))/\\n√\\n2πσ2, then the density value g(x) is\\ncomputed via:\\nz1 = W1 x + b1, a1 = S1(z1),\\nz2 = W2 a1 + b2, a2 = S2(z2),\\ng(x) = a⊤\\n1 a2,\\nwhere W1 = 1 is a p1 ×1 column vector of ones,W2 = O is a p1 ×p1 matrix of zeros, andS2\\nis the softmax function. We identify the column vector b1 with the p1 location parameters,\\n[µ1,...,µ p1 ]⊤ of the Gaussian mixture and b2 ∈Rp1 with the p1 weights of the mixture.\\nNote the unusual activation function of the output layer — it requires the value of a1 from\\nthe first hidden layer and a2 from the second hidden layer.\\nThere are a number of key design characteristics of a feed-forward network. First, we\\nneed to choose the activation function(s). Second, we need to choose the loss function for\\nthe training of the network. As we shall explain in the next section, the most common\\nchoices are the ReLU activation function and the cross-entropy loss. Crucially, we need\\nto carefully construct the network architecture — the number of connections among thenetwork\\narchitecture nodes in different layers and the overall number of layers of the network.\\nFor example, if the connections from one layer to the next are pruned (called sparse\\nconnectivity) and the links share the same weight values {wl,i j}(called parameter sharing)\\nfor all {(i, j) : |i −j|= 0,1,... }, then the weight matrices will be sparse and Toeplitz.☞379\\nIntuitively, the parameter sharing and sparse connectivity can speed up the training of\\nthe network, because there are fewer parameters to learn, and the Toeplitz structure permits\\nquick computation of the matrix-vector products in Algorithm 9.2.1. An important example\\nof such a network is the convolution neural network (CNN), in which some or all of theconvolution\\nneural\\nnetwork\\nnetwork layers encode the linear operation of convolution:\\nWl al−1 = wl ∗al−1,\\nwhere [ x ∗y]i := P\\nk xkyi−k+1. As discussed in Example A.10, a convolution matrix is a☞380\\nspecial type of sparse Toeplitz matrix, and its action on a vector of learning parameters can\\nbe evaluated quickly via the fast Fourier transform.☞394\\nCNNs are particularly suited to image processing problems, because their convolution\\nlayers closely mimic the neurological properties of the visual cortex. In particular, the\\ncortex partitions the visual field into many small regions and assigns a group of neurons to\\nevery such region. Moreover, some of these groups of neurons respond only to the presence\\nof particular features (for example, edges).\\nThis neurological property is naturally modeled via convolution layers in the neural\\nnetwork. Specifically, suppose that the input image is given by anm1 ×m2 matrix of pixels.\\nNow, define a k ×k matrix (sometimes called a kernel, where k is generally taken to be 3\\nor 5). Then, the convolution layer output can be calculated using the discrete convolution'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 348, 'page_label': '331'}, page_content='Deep Learning 331\\nof all possible k ×k input matrix regions and the kernel matrix; (see Example A.10). In\\nparticular, by noting that there are (m1 −k +1) ×(m2 −k +1) possible regions in the original\\nimage, we conclude that the convolution layer output size is ( m1 −k + 1) ×(m2 −k + 1).\\nIn practice, we frequently define several kernel matrices, giving an output layer of size\\n(m1 −k + 1) ×(m2 −k + 1) ×(the number of kernels). Figure 9.4 shows a 5×5 input image\\nand a 2 ×2 kernel with a 4 ×4 output matrix. An example of using a CNN for image\\nclassification is given in Section 9.5.2.\\nFigure 9.4: An example 5×5 input image and a 2×2 kernel. The kernel is applied to every\\n2 ×2 region of the original image.\\n9.3 Back-Propagation\\nThe training of neural networks is a major challenge that requires both ingenuity and much\\nexperimentation. The algorithms for training neural networks with great depth are collect-\\nively referred to asdeep learning methods. One of the simplest and most effective methods deep learning\\nfor training is via steepest descent and its variations. ☞ 412Steepest descent requires computation of the gradient with respect to all bias vectors\\nand weight matrices. Given the potentially large number of parameters (weight and bias\\nterms) in a neural network, we need to find an efficient method to calculate this gradient.\\nTo illustrate the nature of the gradient computations, let θ= {Wl,bl}be a column vec-\\ntor of length dim( θ) = PL\\nl=1(pl−1 pl + pl) that collects all the weight parameters (number-\\ning PL\\nl=1 pl−1 pl) and bias parameters (numbering PL\\nl=1 pl) of a multiple-layer network with\\ntraining loss:\\nℓτ(g(·|θ)) := 1\\nn\\nnX\\ni=1\\nLoss(yi,g(xi |θ)).\\nWriting Ci(θ) := Loss(yi,g(xi |θ)) for short (using C for cost), we have\\nℓτ(g(·|θ)) = 1\\nn\\nnX\\ni=1\\nCi(θ), (9.4)\\nso that obtaining the gradient of ℓτ requires computation of ∂Ci/∂θ for every i. For ac-\\ntivation functions of the form (9.1), define Dl as the diagonal matrix with the vector of\\nderivatives\\nS′(z) := [S ′(zl,1),..., S ′(zl,pl )]⊤\\ndown its main diagonal; that is,\\nDl := diag(S ′(zl,1),..., S ′(zl,pl )), l = 1,..., L −1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 349, 'page_label': '332'}, page_content='332 Back-Propagation\\nThe following theorem provides us with the formulas needed to compute the gradient of a\\ntypical Ci(θ).\\nTheorem 9.2: Gradient of Training Loss\\nFor a given (input, output) pair ( x,y), let g(x |θ) be the output of Algorithm 9.2.1,\\nand let C(θ) = Loss(y,g(x |θ)) be an almost-everywhere di fferentiable loss func-\\ntion. Suppose {zl,al}L\\nl=1 are the vectors obtained during the feed-forward propagation\\n(a0 = x,aL = g(x |θ)). Then, we have for l = 1,..., L:\\n∂C\\n∂Wl\\n= δl a⊤\\nl−1 and ∂C\\n∂bl\\n= δl,\\nwhere δl := ∂C/∂zl is computed recursively for l = L,..., 2:\\nδl−1 = Dl−1W⊤\\nl δl with δL = ∂SL\\n∂zL\\n∂C\\n∂g . (9.5)\\nProof: The scalar value C is obtained from the transitions (9.2), followed by the mapping\\ng(x |θ) 7→Loss(y,g(x |θ)). Using the chain rule (see Appendix B.1.2), we have☞400\\nδL = ∂C\\n∂zL\\n= ∂g(x)\\n∂zL\\n∂C\\n∂g(x) = ∂SL\\n∂zL\\n∂C\\n∂g .\\nRecall that the vector/vector derivative of a linear mapping z 7→Wz is given by W⊤; see\\n(B.5). It follows that, since zl = Wl al−1 + bl and al = S(zl), the chain rule gives☞399\\n∂zl\\n∂zl−1\\n= ∂al−1\\n∂zl−1\\n∂zl\\n∂al−1\\n= Dl−1W⊤\\nl .\\nHence, the recursive formula (9.5):\\nδl−1 = ∂C\\n∂zl−1\\n= ∂zl\\n∂zl−1\\n∂C\\n∂zl\\n= Dl−1W⊤\\nl δl, l = L,..., 3,2.\\nUsing the {δl}, we can now compute the derivatives with respect to the weight matrices\\nand the biases. In particular, applying the “scalar /matrix” di fferentiation rule (B.10) to\\nzl = Wl al−1 + bl gives:\\n∂C\\n∂Wl\\n= ∂C\\n∂zl\\n∂zl\\n∂Wl\\n= δl a⊤\\nl−1, l = 1,..., L\\nand\\n∂C\\n∂bl\\n= ∂zl\\n∂bl\\n∂C\\n∂zl\\n= δl, l = 1,..., L.\\n□\\nFrom the theorem we can see that for each pair (x,y) in the training set, we can compute the\\ngradient ∂C/∂θin a sequential manner, by computing δL,..., δ1. This procedure is called\\nback-propagationback-\\npropagation\\n. Since back-propagation mostly involves simple matrix multiplication, it'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 350, 'page_label': '333'}, page_content='Deep Learning 333\\ncan be efficiently implemented using dedicated computing hardware such as graphical pro-\\ncessor units (GPUs) and other parallel computing architecture. Note also that many matrix\\ncomputations that run in quadratic time can be replaced with linear-time componentwise\\nmultiplication. Specifically, multiplication of a vector with a diagonal matrix is equivalent\\nto componentwise multiplication:\\nA|{z}\\ndiag(a)\\nb = a ⊙b.\\nConsequently, we can write δl−1 = Dl−1W⊤\\nl δl as: δl−1 = S′(zl−1) ⊙W⊤\\nl δl, l = L,..., 3,2.\\nWe now summarize the back-propagation algorithm for the computation of a typical\\n∂C/∂θ. In the following algorithm, Lines 1 to 5 are the feed-forward part of the algorithm,\\nand Lines 7 to 10 are the back-propagation part of the algorithm.\\nAlgorithm 9.3.1: Computing the Gradient of a Typical C(θ)\\ninput: Training example (x,y), weight matrices and bias vectors {Wl,bl}L\\nl=1 =: θ,\\nactivation functions {Sl}L\\nl=1.\\noutput: The derivatives with respect to all weight matrices and bias vectors.\\n1 a0 ←x\\n2 for l = 1,..., L do // feed-forward\\n3 zl ←Wl al−1 + bl\\n4 al ←Sl(zl)\\n5 δL ←∂SL\\n∂zL\\n∂C\\n∂g\\n6 z0 ←0 // arbitrary assignment needed to finish the loop\\n7 for l = L,..., 1 do // back-propagation\\n8 ∂C\\n∂bl\\n←δl\\n9 ∂C\\n∂Wl\\n←δl a⊤\\nl−1\\n10 δl−1 ←S′(zl−1) ⊙W⊤\\nl δl\\n11 return ∂C\\n∂Wl\\nand ∂C\\n∂bl\\nfor all l = 1,..., L and the value g(x) ←aL (if needed)\\nNote that for the gradient of C(θ) to exist at every point, we need the activation func-\\ntions to be differentiable everywhere. This is the case, for example, for the logistic activa-\\ntion function in Figure 9.2. It is not the case for the ReLU function, which is differentiable\\neverywhere, except at z = 0. However, in practice, the kink of the ReLU function at z = 0\\nis unlikely to trip the back-propagation algorithm, because rounding errors and the finite-\\nprecision computer arithmetic make it extremely unlikely that we will need to evaluate the\\nReLU at precisely z = 0. This is the reason why in Theorem 9.2 we merely required that\\nC(θ) is almost-everywhere differentiable.\\nIn spite of its kink at the origin, the ReLU has an important advantage over the logistic\\nfunction. While the derivative of the logistic function decays exponentially fast to zero as\\nwe move away from the origin, a phenomenon referred to as saturation, the derivative of saturation\\nthe ReLU function is always unity for positivez. Thus, for large positivez, the derivative of\\nthe logistic function does not carry any useful information, but the derivative of the ReLU\\ncan help guide a gradient optimization algorithm. The situation for the Heaviside function\\nin Figure 9.2 is even worse, because its derivative is completely noninformative for any\\nz , 0. In this respect, the lack of saturation of the ReLU function for z > 0 makes it a\\ndesirable activation function for training a network via back-propagation.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 351, 'page_label': '334'}, page_content='334 Back-Propagation\\nFinally, note that to obtain the gradient ∂ℓτ/∂θof the training loss, we simply need to\\nloop Algorithm 9.3.1 over all the n training examples, as follows.\\nAlgorithm 9.3.2: Computing the Gradient of the Training Loss\\ninput: Training set τ= {(xi,yi)}n\\ni=1, weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ, activation functions {Sl}L\\nl=1.\\noutput: The gradient of the training loss.\\n1 for i = 1,..., n do // loop over all training examples\\n2 Run Algorithm 9.3.1 with input (xi,yi) to compute\\nn ∂Ci\\n∂Wl\\n,∂Ci\\n∂bl\\noL\\nl=1\\n3 return ∂C\\n∂Wl\\n= 1\\nn\\nPn\\ni=1\\n∂Ci\\n∂Wl\\nand ∂C\\n∂bl\\n= 1\\nn\\nPn\\ni=1\\n∂Ci\\n∂bl\\nfor all l = 1,..., L\\nExample 9.4 (Squared-Error and Cross-Entropy Loss) The back-propagation Al-\\ngorithm 9.3.1 requires a formula for δL in line 5. In particular, to execute line 5 we need to\\nspecify both a loss function and an SL that defines the output layer: g(x |θ) = aL = SL(zL).\\nFor instance, in the multi-logit classification of inputs x into pL categories labeled☞266\\n0,1,..., (pL −1), the output layer is defined via the softmax function:\\nSL : zL 7→ exp(zL)PpL\\nk=1 exp(zL,k).\\nIn other words, g(x |θ) is a probability vector such that its (y+1)-st component gy+1(x |θ) =\\ng(y |θ,x) is the estimate or prediction of the true conditional probability f (y |x). Combin-\\ning the softmax output with the cross-entropy loss, as was done in (7.17), yields:☞267\\nLoss( f (y |x),g(y |θ,x)) = −ln g(y |θ,x)\\n= −ln gy+1(x |θ)\\n= −zy+1 + ln PpL\\nk=1 exp(zk).\\nHence, we obtain the vector δL with components (k = 1,..., pL)\\nδL,k = ∂\\n∂zk\\n\\x10\\n−zy+1 + ln PpL\\nk=1 exp(zk)\\n\\x11\\n= gk(x |θ) −1{y = k −1}.\\nNote that we can remove a node from the final layer of the multi-logit network, be-\\ncause g1(x |θ) (which corresponds to the y = 0 class) can be eliminated, using the fact\\nthat g1(x |θ) = 1 −PpL\\nk=2 gk(x |θ). For a numerical comparison, see Exercise 13.\\nAs another example, in nonlinear multi-output regression (see Example 9.1), the out-\\nput function SL is typically of the form (9.1), so that ∂SL/∂z = diag(S ′\\nL(z1),..., S ′\\nL(zpL )).\\nCombining the output g(x |θ) = SL(zL) with the squared-error loss yields:\\nLoss(y,g(x |θ)) = ∥y −g(x |θ)∥2 =\\npLX\\nj=1\\n(yj −gj(x |θ))2.\\nHence, line 5 in Algorithm 9.3.1 simplifies to:\\nδL = ∂SL\\n∂z\\n∂C\\n∂g = S′\\nL(zL) ⊙2(g(x |θ) −y).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 352, 'page_label': '335'}, page_content='Deep Learning 335\\n9.4 Methods for Training\\nNeural networks have been studied for a long time, yet it is only recently that there have\\nbeen sufficient computational resources to train them e ffectively. The training of neural\\nnetworks requires minimization of a training loss, ℓτ(g(·|θ)) = 1\\nn\\nPn\\ni=1 Ci(θ), which is typ-\\nically a di fficult high-dimensional optimization problem with multiple local minima. We\\nnext consider a number of simple training methods.\\nIn this section, the vectors δt and 1t use the notation of Section B.3.2 and should not\\nbe confused with the derivative δand the prediction function g, respectively.\\n9.4.1 Steepest Descent\\nIf we can compute the gradient of ℓτ(g(·|θ)) via back-propagation, then we can apply the\\nsteepest descent algorithm, which reads as follows. Starting from a guessθ1, we iterate the ☞ 412\\nfollowing step until convergence:\\nθt+1 = θt −αt ut, t = 1,2,..., (9.6)\\nwhere ut := ∂ℓτ\\n∂θ (θt) and αt is the learning rate learning rate.\\nObserve that, rather than operating directly on the weights and biases, we operate in-\\nstead on θ := {Wl,bl}L\\nl=1 — a column vector of length PL\\nl=1(pl−1 pl + pl) that stores all the\\nweight and bias parameters. The advantage of organizing the computations in this way is\\nthat we can easily compute the learning rate αt; for example, via the Barzilai–Borwein\\nformula in (B.26). ☞ 413\\nAlgorithm 9.4.1: Training via Steepest Descent\\ninput: Training set τ= {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ1, activation functions {Sl}L\\nl=1.\\noutput: The parameters of the trained learner.\\n1 t ←1, δ←0.1 ×1, ut−1 ←0, α←0.1 // initialization\\n2 while stopping condition is not met do\\n3 compute the gradient ut = ∂ℓτ\\n∂θ (θt) using Algorithm 9.3.2\\n4 1←ut −ut−1\\n5 if δ⊤1>0 then // check if Hessian is positive-definite\\n6 α←δ⊤1\\x0e∥1∥2 // Barzilai-Borwein\\n7 else\\n8 α←2 ×α // failing positivity, do something heuristic\\n9 δ←−αut\\n10 θt+1 ←θt + δ\\n11 t ←t + 1\\n12 return θt as the minimizer of the training loss\\nTypically, we initialize the algorithm with small random values for θ1, while being\\ncareful to avoid saturating the activation function. For example, in the case of the ReLU'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 353, 'page_label': '336'}, page_content='336 Methods for Training\\nactivation function, we will use small positive values to ensure that its derivative is not\\nzero. A zero derivative of the activation function prevents the propagation of information\\nuseful for computing a good search direction.\\nRecall that computation of the gradient of the training loss via Algorithm 9.3.2 requires\\naveraging over all training examples. When the size n of the training set τn is too large,\\ncomputation of the gradient ∂ℓτn /∂θvia Algorithm 9.3.2 may be too costly. In such cases,\\nwe may employ the stochastic gradient descentstochastic\\ngradient\\ndescent\\nalgorithm. In this algorithm, we view the\\ntraining loss as an expectation that can be approximated via Monte Carlo sampling. In\\nparticular, if K is a random variable with distributionP[K = k] = 1/n for k = 1,..., n, then\\nwe can write\\nℓτ(g(·|θ)) = 1\\nn\\nnX\\nk=1\\nLoss(yk,g(xk |θ)) = ELoss(yK,g(xK |θ)).\\nWe can thus approximate ℓτ(g(·|θ)) via a Monte Carlo estimator using N iid copies of K:\\nbℓτ(g(·|θ)) := 1\\nN\\nNX\\ni=1\\nLoss(yKi ,g(xKi |θ)).\\nThe iid Monte Carlo sample K1,..., KN is called a minibatchminibatch (see also Exercise 3). Typic-\\nally, n ≫N so that the probability of observing ties in a minibatch of size N is negligible.\\nFinally, note that if the learning rate of the stochastic gradient descent algorithm sat-\\nisfies the conditions in (3.30), then the stochastic gradient descent algorithm is simply a☞107\\nversion of the stochastic approximation Algorithm 3.4.5.\\n9.4.2 Levenberg–Marquardt Method\\nSince a neural network with squared-error loss is a special type of nonlinear regression\\nmodel, it is possible to train it using classical nonlinear least-squares minimization meth-\\nods, such as the Levenberg–Marquardtalgorithm.☞415\\nFor simplicity of notation, suppose that the output of the net for an input x is a scalar\\ng(x). For a given input parameter θ of dimension d = dim(θ), the Levenberg–Marquardt\\nAlgorithm B.3.3 requires computation of the following vector of outputs:\\ng(τ|θ) := [g(x1 |θ),..., g(xn |θ)]⊤,\\nas well as the n ×d matrix of Jacobi, G, of g at θ. To compute these quantities, we can\\nagain use the back-propagation Algorithm 9.3.1, as follows.\\nAlgorithm 9.4.2: Output for Training via Levenberg–Marquardt\\ninput: Training set τ= {(xi,yi)}n\\ni=1, parameter θ.\\noutput: Vector g(τ|θ) and matrix of Jacobi G for use in Algorithm B.3.3.\\n1 for i = 1,..., n do // loop over all training examples\\n2 Run Algorithm 9.3.1 with input (xi,yi) (using ∂C\\n∂g = 1 in line 5) to compute\\ng(xi |θ) and ∂g(xi |θ)\\n∂θ .\\n3 g(τ|θ) ←[g(x1 |θ),..., g(xn |θ)]⊤\\n4 G ←\\nh∂g(x1 |θ)\\n∂θ ,··· ,∂g(xn |θ)\\n∂θ\\ni⊤\\n5 return g(τ|θ) and G'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 354, 'page_label': '337'}, page_content='Deep Learning 337\\nThe Levenberg–Marquardt algorithm is not suitable for networks with a large number\\nof parameters, because the cost of the matrix computations becomes prohibitive. For in-\\nstance, obtaining the Levenberg–Marquardt search direction in (B.28) usually incurs an\\nO(d3) cost. In addition, the Levenberg–Marquardt algorithm is applicable only when we\\nwish to train the network using the squared-error loss. Both of these shortcomings are\\nmitigated to an extent with the quasi-Newton or adaptive gradient methods described next.\\n9.4.3 Limited-Memory BFGS Method\\nAll the methods discussed so far have beenfirst-order optimization methods, that is, meth-\\nods that only use the gradient vectorut := ∂ℓτ\\n∂θ (θt) at the current (and/or immediate past) can-\\ndidate solution θt. In trying to design a more e fficient second-order optimization method,\\nwe may be tempted to use Newton’s method with a search direction: ☞ 410\\n−H−1\\nt ut,\\nwhere Ht is the d ×d matrix of second-order partial derivatives of ℓτ(g(·|θ)) at θt.\\nThere are two problems with this approach. First, while the computation of ut via Al-\\ngorithm 9.3.2 typically costs O(d), the computation of Ht costs O(d2). Second, even if we\\nhave somehow computedHt very fast, computing the search directionH−1\\nt ut still incurs an\\nO(d3) cost. Both of these considerations make Newton’s method impractical for large d.\\nInstead, a practical alternative is to use a quasi-Newton method, in which we directly quasi-newton\\nmethodaim to approximate H−1\\nt via a matrix Ct that satisfies the secant condition:\\n☞ 411Ct 1t = δt,\\nwhere δt := θt −θt−1 and 1t := ut −ut−1.\\nAn ingenious formula that generates a suitable sequence of approximating matrices\\n{Ct}(each satisfying the secant condition) is the BFGS updating formula (B.23), which\\ncan be written as the recursion (see Exercise 9):\\nCt =\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11⊤\\nCt−1\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11\\n+ υt δtδ⊤\\nt , υ t := (1⊤\\nt δt)−1. (9.7)\\nThis formula allows us to update Ct−1 to Ct and then compute Ct ut in O(d2) time. While\\nthis quasi-Newton approach is better than the O(d3) cost of Newton’s method, it may be\\nstill too costly in large-scale applications.\\nInstead, an approximate or limited memory BFGS limited memory\\nbfgs\\nupdating can be achieved in O(d)\\ntime. The idea is to store a few of the most recent pairs{δt,1t}in order to evaluate its action\\non a vector ut without explicitly constructing and storing Ct in computer memory. This is\\npossible, because updating C0 to C1 in (9.7) requires only the pair δ1,11, and similarly\\ncomputing Ct from C0 only requires the history of the updates δ1,11 ..., δt,1t, which can\\nbe shown as follows.\\nDefine the matrices At,..., A0 via the backward recursion ( j = 1,..., t):\\nAt := I, Aj−1 :=\\n\\x10\\nI −υj 1jδ⊤\\nj\\n\\x11\\nAj,\\nand observe that all matrix vector products: Aj u =: qj,for j = 0,..., t can be computed\\nefficiently via the backward recursion starting with qt = u:\\nτj := δ⊤\\nj qj, qj−1 = qj −υjτj 1j, j = t,t −1,..., 1. (9.8)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 355, 'page_label': '338'}, page_content='338 Methods for Training\\nIn addition to {qj}, we will make use of the vectors {rj}defined via the recursion:\\nr0 := C0 q0, rj = rj−1 + υj\\n\\x10\\nτj −1⊤\\nj rj−1\\n\\x11\\nδj, j = 1,..., t. (9.9)\\nAt the final iteration t, the BFGS updating formula (9.7) can be rewritten in the form:\\nCt = A⊤\\nt−1Ct−1At−1 + υt δtδ⊤\\nt .\\nBy iterating the recursion (9.7) backwards to C0, we can write:\\nCt = A⊤\\n0 C0A0 +\\ntX\\nj=1\\nυj A⊤\\nj δjδ⊤\\nj Aj,\\nthat is, we can expressCt in terms of the initialC0 and the entire history of all BFGS values\\n{δj,1j}, as claimed. Further, with the {qj,rj}computed via (9.8) and (9.9), we can write:\\nCt u = A⊤\\n0 C0 q0 +\\ntX\\nj=1\\nυj\\n\\x10\\nδ⊤\\nj qj\\n\\x11\\nA⊤\\nj δj\\n= A⊤\\n0 r0 + υ1τ1A⊤\\n1 δ1 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj\\n= A⊤\\n1\\n\\x02\\x00I −υ1δ11⊤\\n1\\n\\x01 r0 + υ1τ1δ1\\n\\x03 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj.\\nHence, from the definition of the {rj}in (9.9), we obtain\\nCtu = A⊤\\n1 r1 +\\ntX\\nj=2\\nυjτjA⊤\\nj δj\\n= A⊤\\n2 r2 +\\ntX\\nj=3\\nυjτjA⊤\\nj δj\\n= ··· = A⊤\\nt rt + 0 = rt.\\nGiven C0 and the history of all recent BFGS values{δj,1j}h\\nj=1, the computation of the quasi-\\nNewton search direction d = −Ch u can be accomplished via the recursions (9.8) and (9.9)\\nas summarized in Algorithm 9.4.3.\\nNote that if C0 is a diagonal matrix, say the identity matrix, then C0 q is cheap to\\ncompute and the cost of running Algorithm 9.4.3 is O(h d). Thus, for a fixed length of the\\nBFGS history, the cost of the limited-memory BFGS updating grows linearly ind, making\\nit a viable optimization algorithm in large-scale applications.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 356, 'page_label': '339'}, page_content='Deep Learning 339\\nAlgorithm 9.4.3: Limited-Memory BFGS Update\\ninput: BFGS history list {δj,1j}h\\nj=1, initial C0, and input u.\\noutput: d = −Ch u, where Ct = \\x00I −υt δt1⊤\\nt\\n\\x01Ct−1\\n\\x10\\nI −υt 1tδ⊤\\nt\\n\\x11\\n+ υt δtδ⊤\\nt .\\n1 q ←u\\n2 for i = h,h −1,..., 1 do // backward recursion to compute A0 u\\n3 υi ←\\n\\x10\\nδ⊤\\ni 1i\\n\\x11−1\\n4 τi ←δ⊤\\ni q\\n5 q ←q −υiτi 1i\\n6 q ←C0 q // compute C0(A0 u)\\n7 for i = 1,..., h do // compute recursion (9.9)\\n8 q ←q + υi(τi −1⊤\\ni q) δi\\n9 return d ←−q, the value of −Ch u\\nIn summary, a quasi-Newton algorithm with limited-memory BFGS updating reads as\\nfollows.\\nAlgorithm 9.4.4: Quasi-Newton Minimization with Limited-Memory BFGS\\ninput: Training set τ= {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 =: θ1, activation functions {Sl}L\\nl=1, and history parameter h.\\noutput: The parameters of the trained learner.\\n1 t ←1, δ←0.1 ×1, ut−1 ←0 // initialization\\n2 while stopping condition is not met do\\n3 Compute ℓvalue = ℓτ(g(·|θt)) and ut = ∂ℓτ\\n∂θ (θt) via Algorithm 9.3.2.\\n4 1←ut −ut−1\\n5 Add (δ,1) to the BFGS history as the newest BFGS pair.\\n6 if the number of pairs in the BFGS history is greater than h then\\n7 remove the oldest pair from the BFGS history\\n8 Compute d via Algorithm 9.4.3 using the BFGS history, C0 = I, and ut.\\n9 α←1\\n10 while ℓτ(g(·|θt + αd)) ⩾ℓvalue + 10−4αd⊤ut do\\n11 α←α/1.5 // line-search along quasi-Newton direction\\n12 δ←αd\\n13 θt+1 ←θt + δ\\n14 t ←t + 1\\n15 return θt as the minimizer of the training loss\\n9.4.4 Adaptive Gradient Methods\\nRecall that the limited-memory BFGS method in the previous section determines a search\\ndirection using the recent history of previously computed gradients{ut}and input paramet-\\ners {θt}. This is because the BFGS pairs{δt,1t}can be easily constructed from the identities:\\nδt = θt −θt−1 and 1t = ut −ut−1. In other words, using only past gradient computations and\\nwith little extra computation, it is possible to infer some of the second-order information'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 357, 'page_label': '340'}, page_content='340 Methods for Training\\ncontained in the Hessian matrix of ℓτ(θ). In addition to the BFGS method, there are other\\nways in which we can exploit the history of past gradient computations.\\nOne approach is to use the normal approximation method, in which the Hessian of ℓτ☞414\\nat θt is approximated via\\nbHt = γI + 1\\nh\\ntX\\ni=t−h+1\\nuiu⊤\\ni , (9.10)\\nwhere ut−h+1,..., ut are the h most recently computed gradients andγis a tuning parameter\\n(for example, γ= 1/h). The search direction is then given by\\n−bH−1\\nt ut,\\nwhich can be computed quickly in O(h2 d) time either using the QR decomposition (Exer-\\ncises 5 and 6), or the Sherman–Morrison Algorithm A.6.1. This approach requires that we☞373\\nstore the last h gradient vectors in memory.\\nAnother approach that completely bypasses the need to invert a Hessian approximation\\nis the Adaptive Gradient or AdaGradAdaGrad method, in which we only store the diagonal of bHt\\nand use the search direction:\\n−diag(bHt)−1/2ut.\\nWe can avoid storing any of the gradient history by instead using the slightly di fferent\\nsearch direction2\\n−ut\\n.p\\nvt + γ×1,\\nwhere the vector vt is updated recursively via\\nvt =\\n \\n1 −1\\nh\\n!\\nvt−1 + 1\\nh ut ⊙ut.\\nWith this updating of vt, the difference between the vector vt + γ×1 and the diagonal of\\nthe Hessian bHt will be negligible.\\nA more sophisticated version of AdaGrad is the adaptive moment estimation or AdamAdam\\nmethod, in which we not only average the vectors{vt}, but also average the gradient vectors\\n{ut}, as follows.\\nAlgorithm 9.4.5: Updating of Search Direction at Iteration t via Adam\\ninput: ut, but−1, vt−1, θt, and parameters (α,hv,hu), equal to, e.g., (10−3,103,10).\\noutput: but, vt, θt+1.\\n1 but ←\\n\\x10\\n1 −1\\nhu\\n\\x11\\nbut−1 + 1\\nhu\\nut\\n2 vt ←\\n\\x10\\n1 −1\\nhv\\n\\x11\\nvt−1 + 1\\nhv\\nut ⊙ut\\n3 u∗\\nt ←but\\n.\\x10\\n1 −(1 −h−1\\nu )t\\n\\x11\\n4 v∗\\nt ←vt\\n.\\x10\\n1 −(1 −h−1\\nv )t\\n\\x11\\n5 θt+1 ←θt −αu∗\\nt\\n.\\x10p\\nv∗\\nt + 10−8 ×1\\n\\x11\\n6 return but, vt, θt+1\\n2Here we divide two vectors componentwise.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 358, 'page_label': '341'}, page_content=\"Deep Learning 341\\nYet another computationally cheap approach is the momentum method, in which the momentum\\nmethodsteepest descent iteration (9.6) is modified to\\nθt+1 = θt −αt ut + γδt,\\nwhere δt = θt −θt−1 and γis a tuning parameter. This strategy frequently performs better\\nthan the “vanilla” steepest descent method, because the search direction is less likely to\\nchange abruptly.\\nNumerical experience suggests that the vanilla steepest-descent Algorithm 9.4.1 and\\nthe Levenberg–Marquardt Algorithm B.3.3 are e ffective for networks with shallow archi-\\ntectures, but not for networks with deep architectures. In comparison, the stochastic gradi-\\nent descent method, the limited-memory BFGS Algorithm 9.4.4, or any of the adaptive\\ngradient methods in this section, can frequently handle networks with many hidden lay-\\ners (provided that any tuning parameters and initialization values are carefully chosen via\\nexperimentation).\\n9.5 Examples in Python\\nIn this section we provide two numerical examples in Python. In the first example, we\\ntrain a neural network with the stochastic gradient descent method using the polynomial\\nregression data from Example 2.1, and without using any specialized Python packages. ☞ 26\\nIn the second example, we consider a realistic application of a neural network to image\\nrecognition and classification. Here we use the specialized open-source Python package\\nPytorch.\\n9.5.1 Simple Polynomial Regression\\nConsider again the polynomial regression data set depicted in Figure 2.4. We use a network\\nwith architecture\\n[p0,p1,p2,p3] = [1,20,20,1].\\nIn other words, we have two hidden layers with 20 neurons, resulting in a learner with a\\ntotal of dim(θ) = 481 parameters. To implement such a neural network, we first import the\\nnumpy and the matplotlib packages, then read the regression problem data and define\\nthe feed-forward neural network layers.\\nNeuralNetPurePython.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n#%%\\n# import data\\ndata = np.genfromtxt( 'polyreg.csv ',delimiter= ',')\\nX = data[:,0].reshape(-1,1)\\ny = data[:,1].reshape(-1,1)\\n# Network setup\\np = [X.shape[1],20,20,1] # size of layers\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 359, 'page_label': '342'}, page_content='342 Examples in Python\\nL = len (p)-1 # number of layers\\nNext, the initialize method generates random initial weight matrices and bias vec-\\ntors {Wl,bl}L\\nl=1. Specifically, all parameters are initialized with values distributed according\\nto the standard normal distribution.\\ndef initialize(p, w_sig = 1):\\nW, b = [[]]* len (p), [[]]* len (p)\\nfor l in range (1,len (p)):\\nW[l]= w_sig * np.random.randn(p[l], p[l-1])\\nb[l]= w_sig * np.random.randn(p[l], 1)\\nreturn W,b\\nW,b = initialize(p) # initialize weight matrices and bias vectors\\nThe following code implements the ReLU activation function from Figure 9.2 and the\\nsquared error loss. Note that these functions return both the function values and the corres-\\nponding gradients.\\ndef RELU(z,l): # RELU activation function: value and derivative\\nif l == L: return z, np.ones_like(z)\\nelse :\\nval = np.maximum(0,z) # RELU function element -wise\\nJ = np.array(z>0, dtype = float ) # derivative of RELU\\nelement -wise\\nreturn val, J\\ndef loss_fn(y,g):\\nreturn (g - y)**2, 2 * (g - y)\\nS = RELU\\nNext, we implement the feed-forward and backward-propagation Algorithm 9.3.1.\\nHere, we have implemented Algorithm 9.3.2 inside the backward-propagation loop.\\ndef feedforward(x,W,b):\\na, z, gr_S = [0]*(L+1), [0]*(L+1), [0]*(L+1)\\na[0] = x.reshape(-1,1)\\nfor l in range (1,L+1):\\nz[l] = W[l] @ a[l-1] + b[l] # affine transformation\\na[l], gr_S[l] = S(z[l],l) # activation function\\nreturn a, z, gr_S\\ndef backward(W,b,X,y):\\nn =len (y)\\ndelta = [0]*(L+1)\\ndC_db, dC_dW = [0]*(L+1), [0]*(L+1)\\nloss=0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 360, 'page_label': '343'}, page_content='Deep Learning 343\\nfor i in range (n): # loop over training examples\\na, z, gr_S = feedforward(X[i,:].T, W, b)\\ncost, gr_C = loss_fn(y[i], a[L]) # cost i and gradient wrt g\\nloss += cost/n\\ndelta[L] = gr_S[L] @ gr_C\\nfor l in range (L,0,-1): # l = L,...,1\\ndCi_dbl = delta[l]\\ndCi_dWl = delta[l] @ a[l-1].T\\n# ---- sum up over samples ----\\ndC_db[l] = dC_db[l] + dCi_dbl/n\\ndC_dW[l] = dC_dW[l] + dCi_dWl/n\\n# -----------------------------\\ndelta[l-1] = gr_S[l-1] * W[l].T @ delta[l]\\nreturn dC_dW, dC_db, loss\\nAs explained in Section 9.4, it is sometimes more convenient to collect all the weight\\nmatrices and bias vectors {Wl,bl}L\\nl=1 into a single vector θ. Consequently, we code two\\nfunctions that map the weight matrices and the bias vectors into a single parameter vector,\\nand vice versa.\\ndef list2vec(W,b):\\n# converts list of weight matrices and bias vectors into\\n# one column vector\\nb_stack = np.vstack([b[i] for i in range (1,len (b))] )\\nW_stack = np.vstack(W[i].flatten().reshape(-1,1) for i in range\\n(1,len (W)))\\nvec = np.vstack([b_stack, W_stack])\\nreturn vec\\n#%%\\ndef vec2list(vec, p):\\n# converts vector to weight matrices and bias vectors\\nW, b = [[]]* len (p),[[]]* len (p)\\np_count = 0\\nfor l in range (1,len (p)): # construct bias vectors\\nb[l] = vec[p_count:(p_count+p[l])].reshape(-1,1)\\np_count = p_count + p[l]\\nfor l in range (1,len (p)): # construct weight matrices\\nW[l] = vec[p_count:(p_count + p[l]*p[l-1])].reshape(p[l], p[\\nl-1])\\np_count = p_count + (p[l]*p[l-1])\\nreturn W, b\\nFinally, we run the stochastic gradient descent for 104 iterations using a minibatch of\\nsize 20 and a constant learning rate of αt = 0.005.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 361, 'page_label': '344'}, page_content='344 Examples in Python\\nbatch_size = 20\\nlr = 0.005\\nbeta = list2vec(W,b)\\nloss_arr = []\\nn = len (X)\\nnum_epochs = 10000\\nprint (\"epoch | batch loss\")\\nprint (\"----------------------------\")\\nfor epoch in range (1,num_epochs+1):\\nbatch_idx = np.random.choice(n,batch_size)\\nbatch_X = X[batch_idx].reshape(-1,1)\\nbatch_y=y[batch_idx].reshape(-1,1)\\ndC_dW, dC_db, loss = backward(W,b,batch_X,batch_y)\\nd_beta = list2vec(dC_dW,dC_db)\\nloss_arr.append(loss.flatten()[0])\\nif (epoch==1 or np.mod(epoch,1000)==0):\\nprint (epoch,\": \",loss.flatten()[0])\\nbeta = beta - lr*d_beta\\nW,b = vec2list(beta,p)\\n# calculate the loss of the entire training set\\ndC_dW, dC_db, loss = backward(W,b,X,y)\\nprint (\"entire training set loss = \",loss.flatten()[0])\\nxx = np.arange(0,1,0.01)\\ny_preds = np.zeros_like(xx)\\nfor i in range (len (xx)):\\na, _, _ = feedforward(xx[i],W,b)\\ny_preds[i], = a[L]\\nplt.plot(X,y, \\'r.\\', markersize = 4,label = \\'y\\')\\nplt.plot(np.array(xx), y_preds, \\'b\\',label = \\'fit\\')\\nplt.legend()\\nplt.xlabel( \\'x\\')\\nplt.ylabel( \\'y\\')\\nplt.show()\\nplt.plot(np.array(loss_arr), \\'b\\')\\nplt.xlabel( \\'iteration \\')\\nplt.ylabel( \\'Training Loss \\')\\nplt.show()\\nepoch | batch loss\\n----------------------------\\n1 : 158.6779278688539\\n1000 : 54.52430507401445\\n2000 : 38.346572088604965\\n3000 : 31.02036319180713\\n4000 : 22.91114276931535\\n5000 : 27.75810262906341\\n6000 : 22.296907007032928\\n7000 : 17.337367420038046\\n8000 : 19.233689945334195\\n9000 : 39.54261478969857\\n10000 : 14.754724387604416\\nentire training set loss = 28.904957963612727'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 362, 'page_label': '345'}, page_content='Deep Learning 345\\nThe left panel of Figure 9.5 shows a trained neural network with a training loss of\\napproximately 28 .9. As seen from the right panel of Figure 9.5, the algorithm initially\\nmakes rapid progress until it settles down into a stationary regime after 400 iterations.\\n0.00\\n 0.25\\n 0.50\\n 0.75\\n 1.00\\ninput u\\n0\\n20\\n40output y\\nﬁt\\ny\\n0\\n 500\\n 1000\\n 1500\\n 2000\\niteration\\n0\\n100\\n200\\n300Batch Loss\\nFigure 9.5: Left panel: The fitted neural network with training loss of ℓτ(gτ) ≈28.9. Right\\npanel: The evolution of the estimated loss,bℓτ(gτ(·|θ)), over the steepest-descent iterations.\\n9.5.2 Image Classification\\nIn this section, we will use the package Pytorch, which is an open-source machine learn-\\ning library for Python. Pytorch can easily exploit any graphics processing unit (GPU)\\nfor accelerated computation. As an example, we consider the Fashion-MNIST data set\\nfrom https://www.kaggle.com/zalando-research/fashionmnist. The Fashion-\\nMNIST data set contains 28 ×28 gray-scale images of clothing. Our task is to classify\\neach image according to its label. Specifically, the labels are: T-Shirt, Trouser, Pullover,\\nDress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. Figure 9.6 depicts a typical\\nankle boot in the left panel and a typical dress in the right panel. To start with, we import\\nthe required libraries and load the Fashion-MNIST data set.\\nFigure 9.6: Left: an ankle boot. Right: a dress.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 363, 'page_label': '346'}, page_content=\"346 Examples in Python\\nImageClassificationPytorch.py\\nimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom PIL import Image\\nimport torch.nn.functional as F\\n#################################################################\\n# data loader class\\n#################################################################\\nclass LoadData(Dataset):\\ndef __init__(self, fName, transform=None):\\ndata = pd.read_csv(fName)\\nself.X = np.array(data.iloc[:, 1:], dtype=np.uint8).reshape\\n(-1, 1, 28, 28)\\nself.y = np.array(data.iloc[:, 0])\\ndef __len__(self):\\nreturn len (self.X)\\ndef __getitem__(self, idx):\\nimg = self.X[idx]\\nlbl = self.y[idx]\\nreturn (img, lbl)\\n# load the image data\\ntrain_ds = LoadData( 'fashionmnist/fashion-mnist_train.csv ')\\ntest_ds = LoadData( 'fashionmnist/fashion-mnist_test.csv ')\\n# set labels dictionary\\nlabels = {0 : 'T-Shirt ', 1 : 'Trouser ', 2 : 'Pullover ',\\n3 : 'Dress ', 4 : 'Coat ', 5 : 'Sandal ', 6 : 'Shirt ',\\n7 : 'Sneaker ', 8 : 'Bag', 9 : 'Ankle Boot '}\\nSince an image input data is generally memory intensive, it is important to partition\\nthe data set into (mini-)batches. The code below defines a batch size of 100 images and\\ninitializes the Pytorch data loader objects. These objects will be used for efficient iteration\\nover the data set.\\n# load the data in batches\\nbatch_size = 100\\ntrain_loader = torch.utils.data.DataLoader(dataset=train_ds,\\nbatch_size=batch_size,\\nshuffle=True)\\ntest_loader = torch.utils.data.DataLoader(dataset=test_ds,\\nbatch_size=batch_size,\\nshuffle=True)\\nNext, to define the network architecture in Pytorch all we need to do is define an\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 364, 'page_label': '347'}, page_content='Deep Learning 347\\ninstance of the torch.nn.Moduleclass. Choosing a network architecture with good gen-\\neralization properties can be a difficult task. Here, we use a network with two convolution\\nlayers (defined in the cnn_layerblock), a 3×3 kernel, and three hidden layers (defined in\\nthe flat_layerblock). Since there are ten possible output labels, the output layer has ten\\nnodes. More specifically, the first and the second convolution layers have 16 and 32 output\\nchannels. Combining this with the definition of the 3 ×3 kernel, we conclude that the size\\nof the first flat hidden layer should be:\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nsecond convolution layer\\nz                      }|                      {\\n(28 −3 + 1)|        {z        }\\nfirst convolution layer\\n−3 + 1\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n2\\n×32 = 18432,\\nwhere the multiplication by 32 follows from the fact that the second convolution layer has\\n32 output channels. Having said that, the flat_fts variable determines the number of\\noutput layers of the convolution block. This number is used to define the size of the first\\nhidden layer of the flat_layerblock. The rest of the hidden layers have 100 neurons and\\nwe use the ReLU activation function for all layers. Finally, note that the forwardmethod\\nin the CNNclass implements the forward pass.\\n# define the network\\nclass CNN(nn.Module):\\ndef __init__(self):\\nsuper (CNN, self).__init__()\\nself.cnn_layer = nn.Sequential(\\nnn.Conv2d(1, 16, kernel_size=3, stride=(1,1)),\\nnn.ReLU(),\\nnn.Conv2d(16, 32, kernel_size=3, stride=(1,1)),\\nnn.ReLU(),\\n)\\nself.flat_fts = (((28-3+1)-3+1)**2)*32\\nself.flat_layer = nn.Sequential(\\nnn.Linear(self.flat_fts, 100),\\nnn.ReLU(),\\nnn.Linear(100, 100),\\nnn.ReLU(),\\nnn.Linear(100, 100),\\nnn.ReLU(),\\nnn.Linear(100, 10))\\ndef forward(self, x):\\nout = self.cnn_layer(x)\\nout = out.view(-1, self.flat_fts)\\nout = self.flat_layer(out)\\nreturn out\\nNext, we specify how the network will be trained. We choose the device type, namely,\\nthe central processing unit (CPU) or the GPU (if available), the number of training itera-\\ntions (epochs), and the learning rate. Then, we create an instance of the proposed convolu-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 365, 'page_label': '348'}, page_content='348 Examples in Python\\ntion network and send it to the predefined device (CPU or GPU). Note how easily one can\\nswitch between the CPU or the GPU without major changes to the code.\\nIn addition to the specifications above, we need to choose an appropriate loss function\\nand training algorithm. Here, we use the cross-entropy loss and the Adam adaptive gradi-☞267\\nent Algorithm 9.4.5. Once these parameters are set, the learning proceeds to evaluate the\\ngradient of the loss function via the back-propagation algorithm.\\n# learning parameters\\nnum_epochs = 50\\nlearning_rate = 0.001\\n#device = torch.device ( \\'cpu \\') # use this to run on CPU\\ndevice = torch.device ( \\'cuda \\') # use this to run on GPU\\n#instance of the Conv Net\\ncnn = CNN()\\ncnn.to(device=device)\\n#loss function and optimizer\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\\n# the learning loop\\nlosses = []\\nfor epoch in range (1,num_epochs+1):\\nfor i, (images, labels) in enumerate (train_loader):\\nimages = Variable(images. float ()).to(device=device)\\nlabels = Variable(labels).to(device=device)\\noptimizer.zero_grad()\\noutputs = cnn(images)\\nloss = criterion(outputs, labels)\\nloss.backward()\\noptimizer.step()\\nlosses.append(loss.item())\\nif (epoch==1 or epoch % 10 == 0):\\nprint (\"Epoch : \", epoch, \", Training Loss: \", loss.item())\\n# evaluate on the test set\\ncnn. eval ()\\ncorrect = 0\\ntotal = 0\\nfor images, labels in test_loader:\\nimages = Variable(images. float ()).to(device=device)\\noutputs = cnn(images)\\n_, predicted = torch. max (outputs.data, 1)\\ntotal += labels.size(0)\\ncorrect += (predicted.cpu() == labels). sum ()\\nprint (\"Test Accuracy of the model on the 10,000 training test images\\n: \", (100 * correct.item() / total),\"%\")\\n# plot'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 366, 'page_label': '349'}, page_content=\"Deep Learning 349\\nplt.rc( 'text ', usetex=True)\\nplt.rc( 'font ', family= 'serif ',size=20)\\nplt.tight_layout()\\nplt.plot(np.array(losses)[10: len (losses)])\\nplt.xlabel(r '{iteration} ',fontsize=20)\\nplt.ylabel(r '{Batch Loss} ',fontsize=20)\\nplt.subplots_adjust(top=0.8)\\nplt.show()\\nEpoch : 1 , Training Loss: 0.412550151348114\\nEpoch : 10 , Training Loss: 0.05452106520533562\\nEpoch : 20 , Training Loss: 0.07233225554227829\\nEpoch : 30 , Training Loss: 0.01696968264877796\\nEpoch : 40 , Training Loss: 0.0008199119474738836\\nEpoch : 50 , Training Loss: 0.006860652007162571\\nTest Accuracy of the model on the 10,000 training test images: 91.02 %\\nFinally, we evaluate the network performance using the test data set. A typical mini-\\nbatch loss as a function of iteration is shown in Figure 9.7 and the proposed neural network\\nachieves about 91% accuracy on the test set.\\n0\\n 200\\n 400\\n 600\\n 800\\n 1000\\niteration\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nBatch Loss\\nFigure 9.7: The batch loss history.\\nFurther Reading\\nA popular book written by some of the pioneers of deep learning is [53]. For an excellent\\nand gentle introduction to the intuition behind neural networks, we recommend [94]. A\\nsummary of many effective gradient descent methods for training of deep networks is given\\nin [105]. An early resource on the limited-memory BFGS method is [81], and a more recent\\nresource includes [13], which makes recommendations on the best choice for the length of\\nthe BFGS history (that is, the value of the parameter h).\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 367, 'page_label': '350'}, page_content='350 Exercises\\nExercises\\n1. Show that the softmax function\\nsoftmax : z 7→ exp(z)P\\nk exp(zk)\\nsatisfies the invariance property:\\nsoftmax(z) = softmax(z + c ×1), for any constant c.\\n2. Projection pursuitprojection\\npursuit\\nis a network with one hidden layer that can be written as:\\ng(x) = S (ω⊤x),\\nwhere S is a univariate smoothing cubic spline . If we use squared-error loss with τn =☞235\\n{yi,xi}n\\ni=1, we need to minimize the training loss:\\n1\\nn\\nnX\\ni=1\\n\\x00yi −S (ω⊤xi)\\x012\\nwith respect to ωand all cubic smoothing splines. This training of the network is typically\\ntackled iteratively in a manner similar to the EM algorithm. In particular, we iterate ( t =☞139\\n1,2,... ) the following steps until convergence.\\n(a) Given the missing data ωt, compute the splineS t by training a cubic smoothing spline\\non {yi,ω⊤\\nt xi}. The smoothing coe fficient of the spline may be determined as part of\\nthis step.\\n(b) Given the spline function S t, compute the next projection vector ωt+1 via iterative\\nreweighted least squares:iterative\\nreweighted\\nleast squares ωt+1 = argmin\\nβ\\n(et −Xβ)⊤Σt (et −Xβ), (9.11)\\nwhere\\net,i := ω⊤\\nt xi + yi −S t(ω⊤\\nt xi)\\nS ′\\nt (ω⊤\\nt xi) , i = 1,..., n\\nis the adjusted response, and Σ1/2\\nt = diag(S ′\\nt (ω⊤\\nt x1),..., S ′\\nt (ω⊤\\nt xn)) is a diagonal mat-\\nrix.\\nApply Taylor’s Theorem B.1 to the function S t and derive the iterative reweighted☞400\\nleast-squares optimization program (9.11).\\n3. Suppose that in the stochastic gradient descent method we wish to repeatedly draw☞336\\nminibatches of size N from τn, where we assume that N ×m = n for some large integer m.\\nInstead of repeatedly resampling from τn, an alternative is to reshuffle τn via a random per-\\nmutation Π and then advance sequentially through the reshu ffled training set to construct☞115\\nm non-overlapping minibatches. A single traversal of such a reshuffled training set is called\\nan epochepoch . The following pseudo-code describes the procedure.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 368, 'page_label': '351'}, page_content='Deep Learning 351\\nAlgorithm 9.5.1: Stochastic Gradient Descent with Reshuffling\\ninput: Training set τn = {(xi,yi)}n\\ni=1, initial weight matrices and bias vectors\\n{Wl,bl}L\\nl=1 →θ1, activation functions {Sl}L\\nl=1, learning rates {α1,α2,... }.\\noutput: The parameters of the trained learner.\\n1 t ←1 and epoch ←0\\n2 while stopping condition is not met do\\n3 Draw U1,..., Un\\niid\\n∼U(0,1).\\n4 Let Π be the permutation of {1,..., n}that satisfies UΠ1 <··· <UΠn .\\n5 (xi,yi) ←(xΠi ,yΠi ) for i = 1,..., n // reshuffle τn\\n6 for j = 1,..., m do\\n7 bℓτ ←1\\nN\\nPjN\\ni=( j−1)N+1 Loss(yi,g(xi |θ))\\n8 θt+1 ←θt −αt\\n∂bℓτ\\n∂θ (θt)\\n9 t ←t + 1\\n10 epoch ←epoch + 1 // number of reshuffles or epochs\\n11 return θt as the minimizer of the training loss\\nWrite Python code that implements the stochastic gradient descent with data reshu ffling,\\nand use it to train the neural net in Section 9.5.1. ☞ 341\\n4. Denote the pdf of the N(0,Σ) distribution by φΣ(·), and let\\nD(µ0,Σ0 |µ1,Σ1) =\\nZ\\nRd\\nφΣ0 (x −µ0) ln φΣ0 (x −µ0)\\nφΣ1 (x −µ1) dx\\nbe the Kullback–Leibler divergence between the densities of the N(µ0,Σ0) and N(µ1,Σ1) ☞ 42\\ndistributions on Rd. Show that\\n2D(µ0,Σ0 |µ1,Σ1) = tr(Σ−1\\n1 Σ0) −ln |Σ−1\\n1 Σ0|+ (µ1 −µ0)⊤Σ−1\\n1 (µ1 −µ0) −d.\\nHence, deduce the formula in (B.22).\\n5. Suppose that we wish to compute the inverse and log-determinant of the matrix\\nIn + UU⊤,\\nwhere U is an n ×h matrix with h ≪n. Show that\\n(In + UU⊤)−1 = In −QnQ⊤\\nn ,\\nwhere Qn contains the first n rows of the (n + h) ×h matrix Q in the QR factorization of ☞ 375\\nthe (n + h) ×h matrix: \"U\\nIh\\n#\\n= QR.\\nIn addition, show that ln |In + UU⊤|= Ph\\ni=1 ln r2\\nii, where {rii}are the diagonal elements of\\nthe h ×h matrix R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 369, 'page_label': '352'}, page_content='352 Exercises\\n6. Suppose that\\nU = [u0,u1,..., uh−1],\\nwhere all u ∈ Rn are column vectors and we have computed ( In + UU⊤)−1 via the QR\\nfactorization method in Exercise 5. If the columns of matrix U are updated to\\n[u1,..., uh−1,uh],\\nshow that the inverse ( In + UU⊤)−1 can be updated in O(h n) time (rather than computed\\nfrom scratch in O(h2 n) time). Deduce that the computing cost of updating the Hessian\\napproximation (9.10) is the same as that for the limited-memory BFGS Algorithm 9.4.3.\\nIn your solution you may use the following facts from [29]. Suppose we are given the\\nQ and R factors in the QR factorization of a matrix A ∈Rn×h. If a row/column is added to\\nmatrix A, then the Q and R factors need not be recomputed from scratch (in O(h2 n) time),\\nbut can be updated e fficiently in O(h n) time. Similarly, if a row/column is removed from\\nmatrix A, then the Q and R factors can be updated in O(h2) time.\\n7. Suppose that U ∈Rn×h has its k-th column v replaced with w, giving the updated eU.\\n(a) If e ∈Rh denotes the unit-length vector such that ek = ∥e∥= 1 and\\nr±:=\\n√\\n2\\n2 U⊤(w −v) +\\n√\\n2 ∥w −v∥2\\n4 e ±\\n√\\n2\\n2 e,\\nshow that\\neU⊤eU = U⊤U + r+r⊤\\n+ −r−r⊤\\n−.\\n[Hint: You may find Exercise 16 in Chapter 6 useful.]☞247\\n(b) Let B := (Ih + U⊤U)−1. Use the Woodbury identity (A.15) to show that☞371\\n(In + eUeU⊤)−1 = In −eU\\n\\x10\\nB−1 + r+r⊤\\n+ −r−r⊤\\n−\\n\\x11−1 eU⊤.\\n(c) Suppose that we have stored B in computer memory. Use Algorithm 6.8.1 and parts\\n(a) and (b) to write pseudo-code that updates (In+UU⊤)−1 to (In+eUeU⊤)−1 in O((n+h)h)\\ncomputing time.\\n8. Equation (9.7) gives the rank-two BFGS update of the inverse Hessian Ct−1 to Ct. In-\\nstead of using a two-rank update, we can consider a one-rank update, in which Ct−1 is\\nupdated to Ct by the general rank-one formula:\\nCt = Ct−1 + υt rt r⊤\\nt .\\nFind values for the scalar υt and vector rt, such that Ct satisfies the secant condition Ct1t =\\nδt.\\n9. Show that the BFGS formula (B.23) can be written as:\\nC ←\\n\\x10\\nI −υ1δ⊤\\x11⊤\\nC\\n\\x10\\nI −υ1δ⊤\\x11\\n+ υδδ⊤,\\nwhere υ:= (1⊤δ)−1.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 370, 'page_label': '353'}, page_content='Deep Learning 353\\n10. Show that the BFGS formula (B.23) is the solution to the constrained optimization\\nproblem:\\nCBFGS = argmin\\nA subject to A1= δ,A = A⊤\\nD(0,C |0,A),\\nwhere Dis the Kullback–Leibler discrepancy defined in (B.22). On the other hand, show\\nthat the DFP formula (B.24) is the solution to the constrained optimization problem:\\nCDFP = argmin\\nA subject to A1= δ,A = A⊤\\nD(0,A |0,C).\\n11. Consider again the logistic regression model in Exercise 5.18, which used iterative ☞ 213\\nreweighted least squares for training the learner. Repeat all the computations, but this\\ntime using thelimited-memory BFGS Algorithm 9.4.4. Which training algorithm converges\\nfaster to the optimal solution?\\n12. Download the seeds_dataset.txtdata set from the book’s GitHub site, which con-\\ntains 210 independent examples. The categorical output (response) here is the type of wheat\\ngrain: Kama, Rosa, and Canadian (encoded as 1, 2, and 3), so that c = 3. The seven con-\\ntinuous features (explanatory variables) are measurements of the geometrical properties of\\nthe grain (area, perimeter, compactness, length, width, asymmetry coe fficient, and length\\nof kernel groove). Thus, x ∈R7 (which does not include the constant feature 1) and the\\nmulti-logit pre-classifier in Example 9.2 can be written as g(x) = softmax(Wx + b), where ☞ 329\\nW ∈R3×7 and b ∈R3. Implement and train this pre-classifier on the firstn = 105 examples\\nof the seeds data set using, for example, Algorithm 9.4.1. Use the remaining n′= 105\\nexamples in the data set to estimate the generalization risk of the learner using the cross-\\nentropy loss. [Hint: Use the cross-entropy loss formulas from Example 9.4.]\\n13. In Exercise 12 above, we train the multi-logit classifier using a weight matrixW ∈R3×7\\nand bias vectorb ∈R3. Repeat the training of the multi-logit model, but this time keepingz1\\nas an arbitrary constant (say z1 = 0), and thus setting c = 0 to be a “reference” class. This\\nhas the e ffect of removing a node from the output layer of the network, giving a weight\\nmatrix W ∈R2×7 and bias vector b ∈R2 of smaller dimensions than in (7.16). ☞ 267\\n14. Consider again Example 9.4, where we used a softmax output function SL in con- ☞ 334\\njunction with the cross-entropy loss: C(θ) = −ln gy+1(x |θ).Find formulas for ∂C\\n∂g and ∂SL\\n∂zL\\n.\\nHence, verify that:\\n∂SL\\n∂zL\\n∂C\\n∂g = g(x |θ) −ey+1,\\nwhere ei is the unit length vector with an entry of 1 in the i-th position.\\n15. Derive the formula (B.25) for a diagonal Hessian update in a quasi-Newton method ☞ 412\\nfor minimization. In other words, given a current minimizer xt of f (x), a diagonal matrix\\nC of approximating the Hessian of f , and a gradient vector u = ∇f (xt), find the solution\\nto the constrained optimization program:\\nmin\\nA\\nD(xt,C |xt −Au,A)\\nsubject to: A1⩾δ, A is diagonal,\\nwhere Dis the Kullback–Leibler distance defined in (B.22) (see Exercise 4).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 371, 'page_label': '354'}, page_content='354 Exercises\\n16. Consider again the Python implementation of the polynomial regression in Sec-\\ntion 9.5.1, where the stochastic gradient descent was used for training.\\nUsing the polynomial regression data set, implement and run the following four altern-\\native training methods:\\n(a) the steepest-descent Algorithm 9.4.1;\\n(b) the Levenberg–Marquardt Algorithm B.3.3, in conjunction with Algorithm 9.4.2 for☞415\\ncomputing the matrix of Jacobi;\\n(c) the limited-memory BFGS Algorithm 9.4.4;\\n(d) the Adam Algorithm 9.4.5, which uses past gradient values to determine the next\\nsearch direction.\\nFor each training algorithm, using trial and error, tune any algorithmic parameters so that\\nthe network training is as fast as possible. Comment on the relative advantages and disad-\\nvantages of each training/optimization method. For example, comment on which optimiz-\\nation method makes rapid initial progress, but gets trapped in a suboptimal solution, and\\nwhich method is slower, but more consistent in finding good optima.\\n17. Consider again the Pytorch code in Section 9.5.2. Repeat all the computations, but\\nthis time using the momentum method for training of the network. Comment on which\\nmethod is preferable: the momentum or the Adam method?'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 372, 'page_label': '355'}, page_content='APPENDIX A\\nLINEAR ALGEBRA AND FUNCTIONAL\\nANALYSIS\\nThe purpose of this appendix is to review some important topics in linear algebra\\nand functional analysis. We assume that the reader has some familiarity with matrix\\nand vector operations, including matrix multiplication and the computation of determ-\\ninants.\\nA.1 Vector Spaces, Bases, and Matrices\\nLinear algebra is the study of vector spaces and linear mappings. Vectors are, by defini-\\ntion, elements of some vector space vector spaceVand satisfy the usual rules of addition and scalar\\nmultiplication, e.g.,\\nif x ∈V and y ∈V, then αx + βy ∈V for all α,β ∈R (or C).\\nWe will be dealing mostly with vectors in the Euclidean vector space Rn for some n. That\\nis, we view the points of Rn as objects that can be added up and multiplied with a scalar,\\ne.g., (x1,x2) + (y1,y2) = (x1 + y1,x2 + y2) for points in R2. Sometimes it is convenient to\\nwork with the complex vector space Cn instead of Rn; see also Section A.3.\\nVectors v1,..., vk are called linearly independent linearly\\nindependent\\nif none of them can be expressed as\\na linear combination of the others; that is, if α1v1 + ··· + αnvn = 0, then it must hold that\\nαi = 0 for all i = 1,..., n.\\nDefinition A.1: Basis of a Vector Space\\nA set of vectors B= {v1,..., vn}is called a basis basisof the vector space Vif every\\nvector x ∈V can be written as a unique linear combination of the vectors in B:\\nx = α1 v1 + ··· + αn vn.\\nThe (possibly infinite) number n is called the dimension dimensionof V.\\n355'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 373, 'page_label': '356'}, page_content='356 Vector Spaces, Bases, and Matrices\\nUsing a basis Bof V, we can thus represent each vector x ∈V as a row or column of\\nnumbers\\n[α1,...,α n] or\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nα1\\n...\\nαn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (A.1)\\nTypically, vectors in Rn are represented via the standard basisstandard basis , consisting of unit\\nvectors (points) e1 = (1,0,..., 0),..., en = (0,0,..., 0,1). As a consequence, any point\\n(x1,..., xn) ∈Rn can be represented, using the standard basis, as a row or column vec-\\ntor of the form (A.1) above, with αi = xi,i = 1,..., n. We will also write [x1,x2,..., xn]⊤,\\nfor the corresponding column vector, where ⊤denotes the transposetranspose .\\nTo avoid confusion, we will use the convention from now on that a generic vectorx\\nis always represented via the standard basis as a column vector. The corresponding\\nrow vector is denoted by x⊤.\\nA matrix can be viewed as an array of m rows and n columns that defines a linearmatrix\\ntransformation from Rn to Rm (or for complex matrices, from Cn to Cm). The matrix is saidlinear\\ntransformation to be square if m = n. If a1,a2,..., an are the columns of A, that is, A = [a1,a2,..., an],\\nand if x = [x1,..., xn]⊤, then Ax = x1 a1 + ··· + xn an. In particular, the standard basis\\nvector ek is mapped to the vectorak, k = 1,..., n. We sometimes use the notationA = [ai j],\\nto denote a matrix whose (i, j)-th element is ai j. When we wish to emphasize that a matrix\\nA is real-valued withm rows and n columns, we write A ∈Rm×n. The rankrank of a matrix is the\\nnumber of linearly independent rows or, equivalently, the number of linearly independent\\ncolumns.\\nExample A.1 (Linear Transformation) Take the matrix\\nA =\\n\" 1 1\\n−0.5 −2\\n#\\n.\\nIt transforms the two basis vectors [1,0]⊤and [0,1]⊤, shown in red and blue in the left panel\\nof Figure A.1, to the vectors [1 ,−0.5]⊤ and [1,−2]⊤, shown on the right panel. Similarly,\\nthe points on the unit circle are transformed to an ellipse.\\n-1 -0.5 0 0.5 1\\nx\\n-1\\n-0.5\\n0\\n0.5\\n1\\ny\\n-2 0 2\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure A.1: A linear transformation of the unit circle.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 374, 'page_label': '357'}, page_content='Linear Algebra and Functional Analysis 357\\nSuppose A = [a1,..., an], where the A= {ai}form a basis of Rn. Take any vector x =\\n[x1,..., xn]⊤\\nE with respect to the standard basis E(we write subscript Eto stress this). Then\\nthe representation of this vector with respect to Ais simply\\ny = A−1 x,\\nwhere A−1 is the inverse inverseof A; that is, the matrix such that AA−1 = A−1A = In, where\\nIn is the n-dimensional identity matrix. To see this, note that A−1 ai gives the i-th unit\\nvector representation, for i = 1,..., n, and recall that each vector in Rn is a unique linear\\ncombination of these basis vectors.\\nExample A.2 (Basis Representation) Consider the matrix\\nA =\\n\"1 2\\n3 4\\n#\\nwith inverse A−1 =\\n\"−2 1\\n3/2 −1/2\\n#\\n. (A.2)\\nThe vector x = [1,1]⊤\\nE in the standard basis has representation y = A−1 x = [−1,1]⊤\\nAin the\\nbasis consisting of the columns of A. Namely,\\nAy = −\\n\"1\\n3\\n#\\n+\\n\"2\\n4\\n#\\n=\\n\"1\\n1\\n#\\n.\\nThe transpose transposeof a matrix A = [ai j] is the matrixA⊤= [aji]; that is, the (i, j)-th element\\nof A⊤ is the ( j,i)-th element of A. The trace traceof a square matrix is the sum of its diagonal\\nelements. A useful result is the following cyclic property.\\nTheorem A.1: Cyclic Property\\nThe trace is invariant under cyclic permutations: tr(ABC) = tr(BCA) = tr(CAB).\\nProof: It suffices to show that tr( DE) is equal to tr( ED) for any m ×n matrix D = [di j]\\nand n ×m matrix E = [ei j]. The diagonal elements of DE are Pn\\nj=1 di j eji,i = 1,..., m and\\nthe diagonal elements of ED are Pm\\ni=1 eji di j, j = 1,..., n. They sum up to the same numberPm\\ni=1\\nPn\\nj=1 di j eji. □\\nA square matrix has an inverse if and only if its columns (or rows) are linearly in-\\ndependent. This is the same as the matrix being of full rank; that is, its rank is equal to\\nthe number of columns. An equivalent statement is that its determinant is not zero. The\\ndeterminant determinantof an n ×n matrix A = [ai,j] is defined as\\ndet(A) :=\\nX\\nπ\\n(−1)ζ(π)\\nnY\\ni=1\\naπi,i, (A.3)\\nwhere the sum is over all permutations π= (π1,...,π n) of (1,..., n), and ζ(π) is the num-\\nber of pairs ( i, j) for which i < j and πi > πj. For example, ζ(2,3,4,1) = 3 for the pairs\\n(1,4),(2,4),(3,4). The determinant of a diagonal matrix diagonal matrix— a matrix with only zero ele-\\nments off the diagonal — is simply the product of its diagonal elements.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 375, 'page_label': '358'}, page_content='358 Vector Spaces, Bases, and Matrices\\nGeometrically, the determinant of a square matrix A = [a1,..., an] is the (signed)\\nvolume of the parallelepiped ( n-dimensional parallelogram) defined by the columns\\na1,..., an; that is, the set of points x = Pn\\ni=1 αi ai, where 0 ⩽αi ⩽1,i = 1,..., n.\\nThe easiest way to compute a determinant of a general matrix is to apply simple op-\\nerations to the matrix that potentially reduce its complexity (as in the number of non-zero\\nelements, for example), while retaining its determinant:\\n• Adding a multiple of one column (or row) to another, does not change the determin-\\nant.\\n• Multiplying a column (or row) with a number multiplies the determinant by the same\\nnumber.\\n• Swapping two rows changes the sign of the determinant.\\nBy applying these rules repeatedly one can reduce any matrix to a diagonal matrix.\\nIt follows then that the determinant of the original matrix is equal to the product of the\\ndiagonal elements of the resulting diagonal matrix multiplied by a known constant.\\nExample A.3 (Determinant and Volume) Figure A.2 illustrates how the determinant\\nof a matrix can be viewed as a signed volume, which can be computed by repeatedly apply-\\ning the first rule above. Here, we wish to compute the area of red parallelogram determined\\nby the matrix A given in (A.2). In particular, the corner points of the parallelogram corres-\\npond to the vectors [0,0]⊤,[1,3]⊤,[2,4]⊤, and [3,7]⊤.\\n0 0.5 1 1.5 2 2.5 3\\n-2\\n0\\n2\\n4\\n6\\n8\\nFigure A.2: The volume of the red parallelogram can be obtained by a number of shear\\noperations that do not change the volume.\\nAdding −2 times the first column of A to the second column gives the matrix\\nB =\\n\"1 0\\n3 −2\\n#\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 376, 'page_label': '359'}, page_content='Linear Algebra and Functional Analysis 359\\ncorresponding to the blue parallelogram. The linear operation that transforms the red to the\\nblue parallelogram can be thought of as a succession of two linear transformations. The\\nfirst is to transform the coordinates of points on the red parallelogram (in standard basis)\\nto the basis formed by the columns of A. Second, relative to this new basis, we apply the\\nmatrix B above. Note that the input of this matrix is with respect to the new basis, whereas\\nthe output is with respect to the standard basis. The matrix for the combined operation is\\nnow\\nBA−1 =\\n\"1 0\\n3 −2\\n# \"−2 1\\n3/2 −1/2\\n#\\n=\\n\"−2 1\\n−9 4\\n#\\n,\\nwhich maps [1 ,3]⊤ to [1,3]⊤ (does not change) and [2 ,4]⊤ to [0,−2]⊤. We say that we\\napply a shear shearin the direction [1,3]⊤. The significance of such an operation is that a shear\\ndoes not alter the volume of the parallelogram . The second (blue) parallelogram has an\\neasier form, because one of the sides is parallel to the y-axis. By applying another shear,\\nin the direction [0,−2]⊤, we can obtain a simple (green) rectangle, whose volume is 2. In\\nmatrix terms, we add 3/2 times the second column of B to the first column of B, to obtain\\nthe matrix\\nC =\\n\"1 0\\n0 −2\\n#\\n,\\nwhich is a diagonal matrix, whose determinant is −2, corresponding to the volume 2 of all\\nthe parallelograms.\\nTheorem A.2 summarizes a number of useful matrix rules for the concepts that we have\\ndiscussed so far. We leave the proofs, which typically involves “writing out” the equations,\\nas an exercise for the reader; see also [116].\\nTheorem A.2: Useful Matrix Rules\\n1. ( AB)⊤= B⊤A⊤\\n2. ( AB)−1 = B−1A−1\\n3. ( A−1)⊤= (A⊤)−1 =: A−⊤\\n4. det( AB) = det(A) det(B)\\n5. x⊤Ax = tr \\x00Axx⊤\\x01\\n6. det( A) = Q\\ni aii if A = [ai j] is triangular\\nNext, consider an n ×p matrix A for which the matrix inverse fails to exist. That is, A\\nis either non-square ( n , p) or its determinant is 0. Instead of the inverse, we can use its\\nso-called pseudo-inverse, which always exists.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 377, 'page_label': '360'}, page_content='360 Inner Product\\nDefinition A.2: Moore–Penrose Pseudo-Inverse\\nThe Moore–Penrose pseudo-inverseMoore–Penrose\\npseudo-inverse\\nof a real matrix A ∈Rn×p is defined as the\\nunique matrix A+ ∈Rp×n that satisfies the conditions:\\n1. AA+A = A\\n2. A+AA+ = A+\\n3. ( AA+)⊤= AA+\\n4. ( A+A)⊤= A+A\\nWe can write A+ explicitly in terms of A when A has a full column or row rank. For\\nexample, we always have\\nA⊤AA+ = A⊤(AA+)⊤= ((AA+)A)⊤= (A)⊤= A⊤. (A.4)\\nIf A has a full column rank p, then ( A⊤A)−1 exists, so that from (A.4) it follows that\\nA+ = (A⊤A)−1A⊤. This is referred to as the left pseudo-inverseleft\\npseudo-inverse\\n, as A+A = Ip. Similarly, if\\nA has a full row rank n, that is, (AA⊤)−1 exists, then it follows from\\nA+AA⊤= (A+A)⊤A⊤= (A(A+A))⊤= A⊤\\nthat A+ = A⊤(AA⊤)−1. This is the right pseudo-inverseright\\npseudo-inverse\\n, as AA+ = In. Finally, ifA is of full\\nrank and square, then A+ = A−1.\\nA.2 Inner Product\\nThe (Euclidean) inner productinner product of two real vectors x = [x1,..., xn]⊤ and y = [y1,..., yn]⊤\\nis defined as the number\\n⟨x,y⟩=\\nnX\\ni=1\\nxi yi = x⊤y.\\nHere x⊤y is the matrix multiplication of the (1 ×n) matrix x⊤ and the ( n ×1) matrix y.\\nThe inner product induces a geometry on the linear spaceRn, allowing for the definition of\\nlength, angle, and so on. The inner product satisfies the following properties:\\n1. ⟨αx + βy,z⟩= α⟨x,z⟩+ β⟨y,z⟩;\\n2. ⟨x,y⟩= ⟨y,x⟩;\\n3. ⟨x,x⟩⩾0;\\n4. ⟨x,x⟩= 0 if and only if x = 0.\\nVectors x and y are called perpendicular (or orthogonalorthogonal ) if ⟨x,y⟩= 0. The Euclidean\\nnormEuclidean norm (or length) of a vector x is defined as\\n||x||=\\nq\\nx2\\n1 + ··· + x2\\nn =\\np\\n⟨x,x⟩.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 378, 'page_label': '361'}, page_content='Linear Algebra and Functional Analysis 361\\nIf x and y are perpendicular, then Pythagoras’ theorem Pythagoras’\\ntheorem\\nholds:\\n||x + y||2 = ⟨x + y, x + y⟩= ⟨x,x⟩+ 2 ⟨x,y⟩+ ⟨y,y⟩= ||x||2 + ||y||2. (A.5)\\nA basis {v1,..., vn}of Rn in which all the vectors are pairwise perpendicular and have\\nnorm 1 is called an orthonormal orthonormal(short for orthogonal and normalized) basis. For example,\\nthe standard basis is orthonormal.\\nTheorem A.3: Orthonormal Basis Representation\\nIf {v1,..., vn}is an orthonormal basis ofRn, then any vector x ∈Rn can be expressed\\nas\\nx = ⟨x,v1⟩v1 + ··· + ⟨x,vn⟩vn. (A.6)\\nProof: Observe that, because the {vi}form a basis, there exist unique α1,...,α n such that\\nx = α1v1 + ··· + αnvn. By the linearity of the inner product and the orthonormality of the\\n{vi}it follows that ⟨x,vj⟩= ⟨P\\ni αivi,vj⟩= αj. □\\nAn n ×n matrix V whose columns form an orthonormal basis is called an orthogonal\\nmatrix orthogonal\\nmatrix\\n.1 Note that for an orthogonal matrix V = [v1,..., vn], we have\\nV⊤V =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nv⊤\\n1\\nv⊤\\n2\\n...\\nv⊤\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n[v1,v2,..., vn] =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nv⊤\\n1 v1 v⊤\\n1 v2 ... v⊤\\n1 vn\\n... ... ... ...\\nv⊤\\nn v1 v⊤\\nn v2 ... v⊤\\nn vn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = In.\\nHence, V−1 = V⊤. Note also that an orthogonal transformation is length preserving length\\npreserving\\n; that\\nis, Vx has the same length as x. This follows from\\n||Vx||2 = ⟨Vx,Vx⟩= x⊤V⊤Vx = x⊤x = ||x||2.\\nA.3 Complex Vectors and Matrices\\nInstead of the vector space Rn of n-dimensional real vectors, it is sometimes useful to\\nconsider the vector space Cn of n-dimensional complex vectors. In this case the adjoint adjoint\\nor conjugate transpose operation (∗) replaces the transpose operation ( ⊤). This involves\\nthe usual transposition of the matrix or vector with the additional step that any complex\\nnumber z = x + i y is replaced by its complex conjugate z = x −i y. For example, if\\nx =\\n\"a1 + i b1\\na2 + i b2\\n#\\nand A =\\n\"a11 + i b11 a12 + i b12\\na21 + i b21 a22 + i b22\\n#\\n,\\nthen\\nx∗= [a1 −i b1, a2 −i b2] and A∗=\\n\"a11 −i b11 a21 −i b21\\na12 −i b12 a22 −i b22\\n#\\n.\\n1The qualifier “orthogonal” for such matrices has been fixed by history. A better term would have been\\n“orthonormal”.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 379, 'page_label': '362'}, page_content='362 Orthogonal Projections\\nThe (Euclidean) inner product of x and y (viewed as column vectors) is now defined as\\n⟨x,y⟩= y∗x =\\nnX\\ni=1\\nxi yi,\\nwhich is no longer symmetric: ⟨x,y⟩= ⟨y,x⟩. Note that this generalizes the real-valued\\ninner product. The determinant of a complex matrix A is defined exactly as in (A.3). As a\\nconsequence, det(A∗) = det(A).\\nA complex matrix is said to be Hermitian or self-adjoint if A∗= A, and unitary ifHermitian\\nunitary A∗A = I (that is, if A∗ = A−1). For real matrices “Hermitian” is the same as “symmetric”,\\nand “unitary” is the same as “orthogonal”.\\nA.4 Orthogonal Projections\\nLet {u1,..., uk}be a set of linearly independent vectors in Rn. The set\\nV= Span {u1,..., uk}= {α1u1 + ··· + αkuk, α1,...,α k ∈R},\\nis called the linear subspace spanned by {u1,..., uk}. The orthogonal complement of V,linear subspace\\northogonal\\ncomplement\\ndenoted by V⊥, is the set of all vectors w that are orthogonal to V, in the sense that\\n⟨w,v⟩= 0 for all v ∈V. The matrix P such that Px = x, for all x ∈V, and Px = 0, for all\\nx ∈V⊥is called the orthogonal projection matrixorthogonal\\nprojection\\nmatrix\\nonto V. Suppose that U = [u1,..., uk]\\nhas full rank, in which case U⊤U is an invertible matrix. The orthogonal projection matrix\\nP onto V= Span {u1,..., uk}is then given by\\nP = U(U⊤U)−1U⊤.\\nNamely, since PU = U, the matrix P projects any vector in Vonto itself. Moreover, P\\nprojects any vector in V⊥onto the zero vector. Using the pseudo-inverse, it is possible to\\nspecify the projection matrix also for the case where U is not of full rank, leading to the\\nfollowing theorem.\\nTheorem A.4: Orthogonal Projection\\nLet U = [u1,..., uk]. Then, the orthogonal projection matrix P onto V= Span{u1,\\n..., uk}is given by\\nP = U U+, (A.7)\\nwhere U+ is the (right) pseudo-inverse of U.\\nProof: By Property 1 of Definition A.2 we have PU = UU+U = U, so that P projects any\\nvector in Vonto itself. Moreover, P projects any vector in V⊥onto the zero vector. □\\nNote that in the special case where u1,..., uk above form an orthonormal basis of V,\\nthen the projection onto Vis very simple to describe, namely we have\\nPx = UU⊤x =\\nkX\\ni=1\\n⟨x,ui⟩ui. (A.8)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 380, 'page_label': '363'}, page_content='Linear Algebra and Functional Analysis 363\\nFor any point x ∈Rn, the point in Vthat is closest to x is its orthogonal projection Px,\\nas the following theorem shows.\\nTheorem A.5: Orthogonal Projection and Minimal Distance\\nLet {u1,..., uk}be an orthonormal basis of subspace Vand let P be the orthogonal\\nprojection matrix onto V. The solution to the minimization program\\nmin\\ny∈V\\n∥x −y∥2\\nis y = Px. That is, Px ∈V is closest to x.\\nProof: We can write each point y ∈V as y = Pk\\ni=1 αi ui. Consequently,\\n∥x −y∥2 =\\n\\x1c\\nx −\\nkX\\ni=1\\nαi ui, x −\\nkX\\ni=1\\nαi ui\\n\\x1d\\n= ∥x∥2 −2\\nkX\\ni=1\\nαi ⟨x,ui⟩+\\nkX\\ni=1\\nα2\\ni .\\nMinimizing this with respect to the {αi}gives αi = ⟨x,ui⟩,i = 1,..., k. In view of (A.8),\\nthe optimal y is thus Px. □\\nA.5 Eigenvalues and Eigenvectors\\nLet A be an n ×n matrix. If Av = λv for some number λand non-zero vector v, then λis\\ncalled an eigenvalue of A with eigenvector v. eigenvalue\\neigenvectorIf (λ,v) is an (eigenvalue, eigenvector) pair, the matrix λI −A maps any multiple of v\\nto the zero vector. Consequently, the columns of λI −A are linearly dependent, and hence\\nits determinant is 0. This provides a way to identify the eigenvalues, namely as the r ⩽n\\ndifferent roots λ1,...,λ r of the characteristic polynomial characteristic\\npolynomial\\ndet(λI −A) = (λ−λ1)α1 ··· (λ−λr)αr ,\\nwhere α1 + ··· + αr = n. The integer αi is called the algebraic multiplicity of λi. The algebraic\\nmultiplicityeigenvectors that correspond to an eigenvalueλi lie in the kernel or null space of the matrix\\nnull spaceλiI −A; that is, the linear space of vectors v such that (λiI −A)v = 0. This space is called\\nthe eigenspace of λi. Its dimension, di ∈{1,..., n}, is called the geometric multiplicity of geometric\\nmultiplicityλi. It always holds that di ⩽αi. If P\\ni di = n, then we can construct a basis for Rn consisting\\nof eigenvectors, as illustrated next.\\nExample A.4 (Linear Transformation (cont.)) We revisit the linear transformation in\\nFigure A.1, where\\nA =\\n\" 1 1\\n−1/2 −2\\n#\\n.\\nThe characteristic polynomial is ( λ−1)(λ+ 2) + 1/2, with roots λ1 = −1/2 −\\n√\\n7/2 ≈\\n−1.8229 and λ2 = −1/2 +\\n√\\n7/2 ≈0.8229. The corresponding unit eigenvectors are v1 ≈\\n[0.3339,−0.9426]⊤ and v2 ≈[0.9847,−0.1744]⊤. The eigenspace corresponding to λ1 is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 381, 'page_label': '364'}, page_content='364 Eigenvalues and Eigenvectors\\nV1 = Span {v1}= {βv1 : β∈R}and the eigenspace corresponding to λ2 is V2 = Span {v2}.\\nThe algebraic and geometric multiplicities are 1 in this case. Any pair of vectors taken\\nfrom V1 and V2 forms a basis for R2. Figure A.3 shows how v1 and v2 are transformed to\\nAv1 ∈V1 and Av2 ∈V2, respectively.\\n-2 0 2\\nx\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\ny\\nFigure A.3: The dashed arrows are the unit eigenvectorsv1 (blue) and v2 (red) of matrix A.\\nTheir transformed values Av1 and Av2 are indicated by solid arrows.\\nA matrix for which the algebraic and geometric multiplicities of all its eigenvalues\\nare the same is called semi-simple. This is equivalent to the matrix being diagonalizable,semi-simple\\ndiagonalizable meaning that there is a matrix V and a diagonal matrix D such that\\nA = VDV−1.\\nTo see that this so-called eigen-decompositioneigen-\\ndecomposition\\nholds, suppose A is a semi-simple matrix\\nwith eigenvalues\\nλ1,...,λ 1|     {z     }\\nd1\\n,··· ,λr,...,λ r|     {z     }\\ndr\\n.\\nLet D be the diagonal matrix whose diagonal elements are the eigenvalues of A, and let V\\nbe a matrix whose columns are linearly independent eigenvectors corresponding to these\\neigenvalues. Then, for each (eigenvalue, eigenvector) pair (λ,v), we have Av = λv. Hence,\\nin matrix notation, we have A V= VD, and so A = VDV−1.\\nA.5.1 Left- and Right-Eigenvectors\\nThe eigenvector as defined in the previous section is called aright-eigenvector, as it lies on\\nthe right of A in the equation Av = λv.\\nIf A is a complex matrix with an eigenvalueλ, then the eigenvalue’s complex conjugate\\nλ is an eigenvalue of A∗. To see this, define B := λI −A and B∗ := λI −A∗. Since λ is\\nan eigenvalue, we have det( B) = 0. Applying the identity det( B) = det(B∗), we see that'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 382, 'page_label': '365'}, page_content='Linear Algebra and Functional Analysis 365\\ntherefore det(B∗) = 0, and hence that λ is an eigenvalue of A∗. Let w be an eigenvector\\ncorresponding to λ. Then, A∗w = λw or, equivalently,\\nw∗A = λw∗.\\nFor this reason, we call w∗ the left-eigenvector left-\\neigenvector\\nof A for eigenvalue λ. If v is a (right-) ei-\\ngenvector of A, then its adjoint v∗ is usually not a left-eigenvector, unless A∗A = AA∗\\n(such matrices are called normal; normal matrixa real symmetric matrix is normal). However, the im-\\nportant property holds that left- and right-eigenvectors belonging to different eigenvalues\\nare orthogonal. Namely, ifw∗is a left-eigenvalue ofλ1 and v a right-eigenvalue ofλ2 , λ1,\\nthen\\nλ1w∗v = w∗Av = λ2w∗v,\\nwhich can only be true if w∗v = 0.\\nTheorem A.6: Schur Triangulation\\nFor any complex matrix A, there exists a unitary matrix U such that T = U−1AU is\\nupper triangular.\\nProof: The proof is by induction on the dimension n of the matrix. Clearly, the statement\\nis true for n = 1, as A is simply a complex number and we can take U equal to 1. Suppose\\nthat the result is true for dimension n. We wish to show that it also holds for dimension\\nn + 1. Any matrix A always has at least one eigenvalue λwith eigenvector v, normalized\\nto have length 1. Let U be any unitary matrix whose first column is v. Such a matrix can\\nalways be constructed2. As U is unitary, the first row ofU−1 is v∗, and U−1AU is of the form\\n\" v∗\\n∗\\n#\\nA\\nh\\nv ∗\\ni\\n|   {z   }\\nU\\n=\\n\" λ ∗\\n0 B\\n#\\n,\\nfor some matrix B. By the induction hypothesis, there exists a unitary matrix W and an\\nupper triangular matrix T such that W−1BW = T. Now, define\\nV :=\\n\" 1 0⊤\\n0 W\\n#\\n.\\nThen,\\nV−1 \\x10\\nU−1AU\\n\\x11\\nV =\\n\" 1 0⊤\\n0 W−1\\n#\" λ ∗\\n0 B\\n#\" 1 0⊤\\n0 W\\n#\\n=\\n\" λ ∗\\n0 W−1BW\\n#\\n=\\n\" λ ∗\\n0 T\\n#\\n,\\nwhich is upper triangular of dimension n + 1. Since UV is unitary, this completes the\\ninduction, and hence the result is true for all n. □\\nThe theorem above can be used to prove an important property of Hermitian matrices,\\ni.e., matrices for which A∗= A.\\n2After specifying v we can complete the rest of the unitary matrix via the Gram–Schmidt procedure, for\\nexample; see Section A.6.4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 383, 'page_label': '366'}, page_content='366 Eigenvalues and Eigenvectors\\nTheorem A.7: Eigenvalues of a Hermitian Matrix\\nAny n ×n Hermitian matrix has real eigenvalues. The corresponding matrix of nor-\\nmalized eigenvectors is a unitary matrix.\\nProof: Let A be a Hermitian matrix. By Theorem A.6 there exists a unitary matrix U such\\nthat U−1AU = T, where T is upper triangular. It follows that the adjoint ( U−1AU)∗ = T∗\\nis lower triangular. However, (U−1AU)∗ = U−1AU, since A∗ = A and U∗ = U−1. Hence,\\nT and T∗ must be the same, which can only be the case if T is a real diagonal matrix D.\\nSince AU = DU, the diagonal elements are exactly the eigenvalues and the corresponding\\neigenvectors are the columns of U. □\\nIn particular, the eigenvalues of a real symmetric matrix are real. We can now repeat\\nthe proof of Theorem A.6 with real eigenvalues and eigenvectors, so that there exists an\\northogonal matrix Q such that Q−1AQ = Q⊤AQ = D. The eigenvectors can be chosen as\\nthe columns of Q, which form an orthonormal basis. This proves the following theorem.\\nTheorem A.8: Real Symmetric Matrices are Orthogonally Diagonizable\\nAny real symmetric matrix A can be written as\\nA = QDQ⊤,\\nwhere D is the diagonal matrix of (real) eigenvalues and Q is an orthogonal matrix\\nwhose columns are eigenvectors of A.\\nExample A.5 (Real Symmetric Matrices and Ellipses) As we have seen, linear trans-\\nformations map circles into ellipses. We can use the above theory for real symmetric\\nmatrices to identify the principal axes. Consider, for example, the transformation with mat-\\nrix A = [1,1; −1/2,−2] in (A.1). A point x on the unit circle is mapped to a point y = Ax.\\nSince for such points ∥x∥2 = x⊤x = 1, we have that y satisfies y⊤(A−1)⊤A−1 y = 1, which\\ngives the equation for the ellipse\\n17 y2\\n1\\n9 + 20 y1y2\\n9 + 8 y2\\n2\\n9 = 1.\\nLet Q be the orthogonal matrix of eigenvectors of the symmetric matrix ( A−1)⊤A−1 =\\n(AA⊤)−1, so Q⊤(AA⊤)−1Q = D for some diagonal matrix D. Taking the inverse on both\\nsides of the previous equation, we have Q⊤AA⊤Q = D−1, which shows that Q is also the\\nmatrix of eigenvectors of AA⊤. These eigenvectors point precisely in the direction of the\\nprincipal axes, as shown in Figure A.4. It turns out, see Section A.6.5, that the square roots\\nof the eigenvalues of AA⊤, here approximately 2.4221 and 0.6193, correspond to the sizes\\nof the principal axes of the ellipse, as illustrated in Figure A.4.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 384, 'page_label': '367'}, page_content='Linear Algebra and Functional Analysis 367\\n-1 0 1\\n-2\\n-1\\n0\\n1\\n2\\nFigure A.4: The eigenvectors and eigenvalues of AA⊤determine the principal axes of the\\nellipse.\\nThe following definition generalizes the notion of positivity of a real variable to that of a\\n(Hermitian) matrix, providing a crucial concept for multivariate differentiation and optim-\\nization; see Appendix B. ☞ 397\\nDefinition A.3: Positive (Semi)Definite Matrix\\nA Hermitian matrix A is called positive semidefinite positive\\nsemidefinite\\n(we write A ⪰0) if ⟨Ax,x⟩⩾0\\nfor all x. It is called positive definite (we write A ≻0) if ⟨Ax,x⟩>0 for all x , 0.\\nThe positive (semi)definiteness of a matrix can be directly related to the positivity of\\nits eigenvalues, as follows:\\nTheorem A.9: Eigenvalues of a Positive Semidefinite Matrix\\nAll eigenvalues of a positive semidefinite matrix are non-negative and all eigenval-\\nues of a positive definite matrix are strictly positive.\\nProof: Let A be a positive semidefinite matrix. By Theorem A.7, the eigenvalues ofA are\\nall real. Suppose λis an eigenvalue with eigenvector v. As A is positive semidefinite, we\\nhave\\n0 ⩽⟨Av,v⟩= λ⟨v,v⟩= λ∥v∥2,\\nwhich can only be true if λ⩾0. Similarly, for a positive definite matrix, λmust be strictly\\ngreater than 0. □\\nCorollary A.1 Any real positive semidefinite matrix A can be written as\\nA = BB⊤\\nfor some real matrix B. Conversely, for any real matrix B, the matrix BB⊤ is positive\\nsemidefinite.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 385, 'page_label': '368'}, page_content='368 Matrix Decompositions\\nProof: The matrix A is both Hermitian (by definition) and real (by assumption) and hence\\nit is symmetric. By Theorem A.8, we can write A = QDQ⊤, where D is the diagonal\\nmatrix of (real) eigenvalues of A. By Theorem A.9 all eigenvalues are non-negative, and\\nthus their square root is real-valued. Now, define B = Q\\n√\\nD, where\\n√\\nD is defined as the\\ndiagonal matrix whose diagonal elements are the square roots of the eigenvalues of A.\\nThen, BB⊤ = Q\\n√\\nD (\\n√\\nD)⊤Q⊤ = QDQ⊤ = A. The converse statement follows from the\\nfact that x⊤BB⊤x = ∥B⊤x∥2 ⩾0 for all x. □\\nA.6 Matrix Decompositions\\nMatrix decompositions are frequently used in linear algebra to simplify proofs, avoid nu-\\nmerical instability, and to speed up computations. We mention three important matrix de-\\ncompositions: (P)LU, QR, and SVD.\\nA.6.1 (P)LU Decomposition\\nEvery invertible matrix A can be written as the product of three matrices:\\nA = PLU, (A.9)\\nwhere L is a lower triangular matrix, U an upper triangular matrix, and P a permutation\\nmatrixpermutation\\nmatrix\\n. A permutation matrix is a square matrix with a single 1 in each row and column,\\nand zeros otherwise. The matrix product PB simply permutes the rows of a matrix B and,\\nlikewise, BP permutes its columns. A decomposition of the form (A.9) is called a PLU\\ndecompositionPLU\\ndecomposition\\n. As a permutation matrix is orthogonal, its transpose is equal to its inverse,\\nand so we can write (A.9) as\\nP⊤A = LU.\\nThe decomposition is not unique, and in many cases P can be taken to be the identity\\nmatrix, in which case we speak of the LU decomposition of A, also called the LR for\\nleft–right (triangular) decomposition.\\nA PLU decomposition of an invertible n ×n matrix A0 can be obtained recursively as\\nfollows. The first step is to swap the rows ofA0 such that the element in the first column and\\nfirst row of the pivoted matrix is as large as possible in absolute value. Write the resulting\\nmatrix as\\neP0A0 =\\n\"a1 b⊤\\n1\\nc1 D1\\n#\\n,\\nwhere eP0 is the permutation matrix that swaps the first and k-th row, where k is the row\\nthat contains the largest element in the first column. Next, add the matrix −c1[1,b⊤\\n1 /a1] to\\nthe last n −1 rows of eP0A0, to obtain the matrix\\n\"a1 b⊤\\n1\\n0 D 1 −c1 b⊤\\n1 /a1\\n#\\n=:\\n\"a1 b⊤\\n1\\n0 A 1\\n#\\n.\\nIn effect, we add some multiple of the first row to each of the remaining rows in order to\\nobtain zeros in the first column, except for the first element.\\nWe now apply the same procedure toA1 as we did toA0 and then to subsequent smaller\\nmatrices A2,..., An−1:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 386, 'page_label': '369'}, page_content='Linear Algebra and Functional Analysis 369\\n1. Swap the first row with the row having the maximal absolute value element in the\\nfirst column.\\n2. Make every other element in the first column equal to 0 by adding appropriate mul-\\ntiples of the first row to the other rows.\\nSuppose that At has a PLU decomposition PtLtUt. Then it is easy to check that\\neP⊤\\nt−1\\n\"1 0⊤\\n0 P t\\n#\\n|        {z        }\\nPt−1\\n\" 1 0⊤\\nP⊤\\nt ct/at Lt\\n#\\n|           {z           }\\nLt−1\\n\"at b⊤\\nt\\n0 U t\\n#\\n|   {z   }\\nUt−1\\n(A.10)\\nis a PLU decomposition ofAt−1. Since the PLU decomposition for the scalarAn−1 is trivial,\\nby working backwards we obtain a PLU decomposition P0L0U0 of A.\\nExample A.6 (PLU Decomposition) Take\\nA =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 −1 7\\n3 2 0\\n1 1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nOur goal is to modify A via Steps 1 and 2 above so as to obtain an upper triangular matrix\\nwith maximal elements on the diagonal. We first swap the first and second row. Next, we\\nadd −1/3 times the first row to the third row and 1/3 times the second row to the third row:\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 −1 7\\n3 2 0\\n1 1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb−→\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3 2 0\\n0 −1 7\\n1 1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb−→\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3 2 0\\n0 −1 7\\n0 1 /3 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb−→\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3 2 0\\n0 −1 7\\n0 0 10 /3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nThe final matrix is U0, and in the process we have applied the permutation matrices\\neP0 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 1 0\\n1 0 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, eP1 =\\n\"1 0\\n0 1\\n#\\n.\\nUsing the recursion (A.10) we can now recover P0 and L0. Namely, at the final iteration\\nwe have P2 = 1,L2 = 1, and U2 = 10/3. And subsequently,\\nP1 =\\n\"1 0\\n0 1\\n#\\n, L1 =\\n\" 1 0\\n−1/3 1\\n#\\n, P0 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 1 0\\n1 0 0\\n0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, L0 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0\\n0 1 0\\n1/3 −1/3 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nobserving that a1 = 3,c1 = [0,1]⊤, a2 = −1, and c2 = 1/3.\\nPLU decompositions can be used to solve large systems of linear equations of the form\\nAx = b efficiently, especially when such an equation has to be solved for many different b.\\nThis is done by first decomposing A into PLU, and then solving two triangular systems:\\n1. Ly = P⊤b.\\n2. Ux = y.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 387, 'page_label': '370'}, page_content='370 Matrix Decompositions\\nThe first equation can be solved efficiently via forward substitution, and the second viaforward\\nsubstitution backward substitution, as illustrated in the following example.\\nbackward\\nsubstitution Example A.7 (Solving Linear Equations with an LU Decomposition) Let A = PLU\\nbe the same as in Example A.6. We wish to solve Ax = [1,2,3]⊤. First, solving\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0\\n0 1 0\\n1/3 −1/3 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ny1\\ny2\\ny3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n1\\n3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ngives, y1 = 2, y2 = 1 and y3 = 3 −2/3 + 1/3 = 8/3, by forward substitution. Next,\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3 2 0\\n0 −1 7\\n0 0 10 /3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\nx3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n1\\n8/3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ngives x3 = 4/5,x2 = −1 + 28/5 = 23/5, and x1 = 2(1 −23/5)/3 = −12/5, so x =\\n[−12,23,4]⊤/5.\\nA.6.2 Woodbury Identity\\nLU (or more generally PLU) decompositions can also be applied to block matrices. A\\nstarting point is the following LU decomposition for a general 2 ×2 matrix:\\n\"a b\\nc d\\n#\\n=\\n\"a 0\\nc d −bc/a\\n#\"1 b/a\\n0 1\\n#\\n,\\nwhich holds as long as a , 0; this can be seen by simply writing out the matrix product.\\nThe block matrix generalization for matrices A ∈Rn×n,B ∈Rn×k,C ∈Rk×n,D ∈Rk×k is\\nΣ :=\\n\"A B\\nC D\\n#\\n=\\n\"A O n×k\\nC D −CA−1B\\n#\" In A−1B\\nOk×n Ik\\n#\\n, (A.11)\\nprovided that A is invertible (again, write out the block matrix product). Here, we use the\\nnotation Op×q to denote the p ×q matrix of zeros. We can further rewrite this as:\\nΣ =\\n\" In On×k\\nCA−1 Ik\\n#\" A O n×k\\nOk×n D −CA−1B\\n#\" In A−1B\\nOk×n Ik\\n#\\n.\\nThus, inverting both sides, we obtain\\nΣ−1 =\\n\" In A−1B\\nOk×n Ik\\n#−1 \" A O n×k\\nOk×n D −CA−1B\\n#−1 \" In On×k\\nCA−1 Ik\\n#−1\\n.\\nInversion of the above block matrices gives (again write out)\\nΣ−1 =\\n\" In −A−1B\\nOk×n Ik\\n#\" A−1 On×k\\nOk×n (D −CA−1B)−1\\n#\" In On×k\\n−CA−1 Ik\\n#\\n. (A.12)\\nAssuming that D is invertible, we could also perform a block UL (as opposed to LU)\\ndecomposition:\\nΣ =\\n\"A −BD−1C B\\nOk×n D\\n#\" In On×k\\nD−1C I k\\n#\\n, (A.13)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 388, 'page_label': '371'}, page_content='Linear Algebra and Functional Analysis 371\\nwhich, after a similar calculation as the one above, yields\\nΣ−1 =\\n\" In On×k\\n−D−1C I k\\n#\"(A −BD−1C)−1 On×k\\nOk×n D−1\\n#\" In −BD−1\\nOk×n Ik\\n#\\n. (A.14)\\nThe upper-left block of Σ−1 from (A.14) must be the same as the upper-left block of Σ−1\\nfrom (A.12), leading to the Woodbury identity Woodbury\\nidentity\\n:\\n(A −BD−1C)−1 = A−1 + A−1B(D −CA−1B)−1CA−1. (A.15)\\nFrom (A.11) and the fact that the determinant of a product is the product of the determ-\\ninants, we see that det( Σ) = det(A) det(D −CA−1B). Similarly, from (A.13) we have\\ndet(Σ) = det(A −BD−1C) det(D), leading to the identity\\ndet(A −BD−1C) det(D) = det(A) det(D −CA−1B). (A.16)\\nThe following special cases of (A.16) and (A.15) are of particular importance.\\nTheorem A.10: Sherman–Morrison Formula\\nSuppose that A ∈Rn×n is invertible and x,y ∈Rn. Then,\\ndet(A + xy⊤) = det(A)(1 + y⊤A−1 x).\\nIf in addition y⊤A−1 x , −1, then the Sherman–Morrison formula Sherman–\\nMorrison\\nformula\\nholds:\\n(A + xy⊤)−1 = A−1 −A−1 xy⊤A−1\\n1 + y⊤A−1 x.\\nProof: Take B = x, C = −y⊤, and D = 1 in (A.16) and (A.15). □\\nOne important application of the Sherman–Morrison formula is in the efficient solution\\nof the linear system Ax = b, where A is an n ×n matrix of the form:\\nA = A0 +\\npX\\nj=1\\naj a⊤\\nj\\nfor some column vectors a1,..., ap ∈Rn and n×n diagonal (or otherwise easily invertible)\\nmatrix A0. Such linear systems arise, for example, in the context of ridge regression and ☞ 217\\noptimization. ☞ 414\\nTo see how the Sherman–Morrison formula can be exploited, define the matrices\\nA0,..., Ap via the recursion:\\nAk = Ak−1 + ak a⊤\\nk , k = 1,..., p.\\nApplication of Theorem A.10 for k = 1,..., p yields the identities:3\\nA−1\\nk = A−1\\nk−1 −A−1\\nk−1 ak a⊤\\nk A−1\\nk−1\\n1 + a⊤\\nk A−1\\nk−1 ak\\n|Ak|= |Ak−1|×\\n\\x10\\n1 + a⊤\\nk A−1\\nk−1 ak\\n\\x11\\n.\\n3Here |A|is a shorthand notation for det(A).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 389, 'page_label': '372'}, page_content='372 Matrix Decompositions\\nTherefore, by evolving the recursive relationships up until k = p, we obtain:\\nA−1\\np = A−1\\n0 −\\npX\\nj=1\\nA−1\\nj−1 aj a⊤\\nj A−1\\nj−1\\n1 + a⊤\\nj A−1\\nj−1 aj\\n|Ap|= |A0|×\\npY\\nj=1\\n\\x10\\n1 + a⊤\\nj A−1\\nj−1 aj\\n\\x11\\n.\\nThese expressions will allow us to easily compute A−1 = A−1\\np and |A|= |Ap|provided the\\nfollowing quantities are available:\\nck,j := A−1\\nk−1 aj, k = 1,..., p −1, j = k + 1,..., p.\\nSince, by Theorem A.10, we can write:\\nA−1\\nk−1 aj = A−1\\nk−2 aj −A−1\\nk−2 ak−1 a⊤\\nk−1A−1\\nk−2\\n1 + a⊤\\nk−1A−1\\nk−2 ak−1\\naj,\\nthe quantities {ck,j}can be computed from the recursion:\\nc1,j = A−1\\n0 aj, j = 1,..., p\\nck,j = ck−1,j − a⊤\\nk−1 ck−1,j\\n1 + a⊤\\nk−1 ck−1,k−1\\nck−1,k−1, k = 2,..., p, j = k,..., p. (A.17)\\nObserve that this recursive computation takesO(p2n) time and that once{ck,j}are available,\\nwe can express A−1 and |A|as:\\nA−1 = A−1\\n0 −\\npX\\nj=1\\ncj,j c⊤\\nj,j\\n1 + a⊤\\nj cj,j\\n|A|= |A0|×\\npY\\nj=1\\n\\x10\\n1 + a⊤\\nj cj,j\\n\\x11\\n.\\nIn summary, we have proved the following.\\nTheorem A.11: Sherman–Morrison Recursion\\nThe inverse and determinant of the n ×n matrix A = A0 + Pp\\nk=1 ak a⊤\\nk are given\\nrespectively by:\\nA−1 = A−1\\n0 −CD−1C⊤\\ndet(A) = det(A0) det(D),\\nwhere C ∈Rn×p and D ∈Rp×p are the matrices\\nC :=\\nh\\nc1,1,..., cp,p\\ni\\n, D := diag\\n\\x10\\n1 + a⊤\\n1 c1,1 ,··· , 1 + a⊤\\np cp,p\\n\\x11\\n,\\nand all the {cj,k}are computed from the recursion (A.17) in O(p2n) time.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 390, 'page_label': '373'}, page_content='Linear Algebra and Functional Analysis 373\\nAs a consequence of Theorem A.11, the solution to the linear system Ax = b can be\\ncomputed in O(p2n) time via:\\nx = A−1\\n0 b −CD−1[C⊤b].\\nIf n > p, the Sherman–Morrison recursion can frequently be much faster than the O(n3)\\ndirect solution via the LU decomposition method in Section A.6.1. ☞ 368\\nIn summary, the following algorithm computes the matricesC and D in Theorem A.11\\nvia the recursion (A.17).\\nAlgorithm A.6.1: Sherman–Morrison Recursion\\ninput: Easily invertible matrix A0 and column vectors a1,..., ap.\\noutput: Matrices C and D such that CD−1C⊤= A−1\\n0 −\\n\\x10\\nA0 + P\\nj aj a⊤\\nj\\n\\x11−1\\n.\\n1 ck ←A−1\\n0 ak for k = 1,..., p (assuming A0 is diagonal or easily invertible matrix)\\n2 for k = 1,..., p −1 do\\n3 dk ←1 + a⊤\\nk ck\\n4 for j = k + 1,..., p do\\n5 cj ←cj −a⊤\\nk cj\\ndk\\nck\\n6 dp ←1 + a⊤\\np cp\\n7 C ←[c1,..., cp]\\n8 D ←diag(d1,..., dp)\\n9 return C and D\\nFinally, note that if A0 is a diagonal matrix and we only store the diagonal elements of\\nD and A0 (as opposed to storing the full matrices D and A0), then the storage or memory\\nrequirements of Algorithm A.6.1 are only O(p n).\\nA.6.3 Cholesky Decomposition\\nIf A is a real-valued positive definite matrix (and therefore symmetric), e.g., a covariance\\nmatrix, then an LU decomposition can be achieved with matrices L and U = L⊤.\\nTheorem A.12: Cholesky Decomposition\\nA real-valued positive definite matrix A = [ai j] ∈Rn×n can be decomposed as\\nA = LL⊤,\\nwhere the real n ×n lower triangular matrix L = [lk j] satisfies the recursive formula\\nlk j = ak j −Pj−1\\ni=1 lji lki\\nq\\naj j −Pj−1\\ni=1 l2\\nji\\n, where\\n0X\\ni=1\\nlji lki := 0 (A.18)\\nfor k = 1,..., n and j = 1,..., k.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 391, 'page_label': '374'}, page_content='374 Matrix Decompositions\\nProof: The proof is by inductive construction. For k = 1,..., n, let Ak be the left-upper\\nk ×k submatrix of A = An. With e1 := [1,0,..., 0]⊤, we have A1 = a11 = e⊤\\n1 Ae1 >0 by the\\npositive-definiteness ofA. It follows thatl11 = √a11. Suppose thatAk−1 has a Cholesky fac-\\ntorization Lk−1L⊤\\nk−1 with Lk−1 having strictly positive diagonal elements, we can construct\\na Cholesky factorization of Ak as follows. First write\\nAk =\\n\"Lk−1L⊤\\nk−1 ak−1\\na⊤\\nk−1 akk\\n#\\nand propose Lk to be of the form\\nLk =\\n\"Lk−1 0\\nl⊤\\nk−1 lkk\\n#\\nfor some vector lk−1 ∈Rk−1 and scalar lkk, for which it must hold that\"Lk−1L⊤\\nk−1 ak−1\\na⊤\\nk−1 akk\\n#\\n=\\n\"Lk−1 0\\nl⊤\\nk−1 lkk\\n#\" L⊤\\nk−1 lk−1\\n0⊤ lkk\\n#\\n.\\nTo establish that such an lk−1 and lkk exist, we must verify that the set of equations\\nLk−1 lk−1 = ak−1\\nl⊤\\nk−1 lk−1 + l2\\nkk = akk\\n(A.19)\\nhas a solution. The system Lk−1 lk−1 = ak−1 has a unique solution, because (by assump-\\ntion) Lk−1 is lower diagonal with strictly positive entries down the main diagonal and we\\ncan solve for lk−1 using forward substitution: lk−1 = L−1\\nk−1 ak−1. We can solve the second\\nequation as lkk =\\np\\nakk −∥lk−1∥2, provided that the term within the square root is positive.\\nWe demonstrate this using the fact that A is a positive definite matrix. In particular, for\\nx ∈Rn of the form [x⊤\\n1 ,x2,0⊤]⊤, where x1 is a non-zero (k −1)-dimensional vector and x2\\na non-zero number, we have\\n0 < x⊤Ax = \\x02x⊤\\n1 ,x2\\n\\x03\"Lk−1L⊤\\nk−1 ak−1\\na⊤\\nk−1 akk\\n#\" x1\\nx2\\n#\\n= ∥L⊤\\nk−1 x1∥2 + 2x⊤\\n1 ak−1 x2 + akk x2\\n2.\\nNow take x1 = −x2 L−⊤\\nk−1 lk−1 to obtain 0 < x⊤Ax = x2\\n2 (akk −∥lk−1∥2). Therefore, (A.19)\\ncan be uniquely solved. As we have already solved it for k = 1, we can solve it for any\\nk = 1,..., n, leading to the recursive formula (A.18) and Algorithm A.6.2 below. □\\nAn implementation of Cholesky’s decomposition that uses the notation in the proof of\\nTheorem A.6.3 is the following algorithm, whose running cost is O(n3).\\nAlgorithm A.6.2: Cholesky Decomposition\\ninput: Positive-definite n ×n matrix An with entries {ai j}.\\noutput: Lower triangular Ln such that LnL⊤\\nn = An.\\n1 L1 ←√a11\\n2 for k = 2,..., n do\\n3 ak−1 ←[a1k,..., ak−1,k]⊤\\n4 lk−1 ←L−1\\nk−1 ak−1 (computed in O(k2) time via forward substitution)\\n5 lkk ←\\np\\nakk −l⊤\\nk−1 lk−1\\n6 Lk ←\\n\"Lk−1 0\\nl⊤\\nk−1 lkk\\n#\\n7 return Ln'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 392, 'page_label': '375'}, page_content='Linear Algebra and Functional Analysis 375\\nA.6.4 QR Decomposition and the Gram–Schmidt Procedure\\nLet A be an n ×p matrix, where p ⩽n. Then, there exists a matrix Q ∈Rn×p satisfying\\nQ⊤Q = Ip, and an upper triangular matrix R ∈Rp×p, such that\\nA = QR.\\nThis is the QR decomposition for real-valued matrices. When A has full column rank, such\\na decomposition can be obtained via the Gram–Schmidt procedure, which constructs an Gram–Schmidtorthonormal basis {u1,..., up}of the column space of A spanned by {a1,..., ap}, in the\\nfollowing way (see also Figure A.5):\\n1. Take u1 = a1/∥a1∥.\\n2. Let p1 be the projection of a2 onto Span {u1}. That is, p1 = ⟨u1,a2⟩u1. Now take\\nu2 = (a2 −p1)/∥a2 −p1∥. This vector is perpendicular to u1 and has unit length.\\n3. Let p2 be the projection ofa3 onto Span{u1,u2}. That is, p2 = ⟨u1,a3⟩u1+⟨u2,a3⟩u2.\\nNow take u3 = (a3 −p2)/∥a3 −p2∥. This vector is perpendicular to both u1 and u2\\nand has unit length.\\n4. Continue this process to obtain u4,..., up.\\n0 1 2\\n-1\\n0\\n1\\n2\\n3\\np1\\na1\\na2\\na2 ! p1\\nu1\\nu2\\nFigure A.5: Illustration of the Gram–Schmidt procedure.\\nAt the end of the procedure, a set {u1,..., up}of p orthonormal vectors are obtained.\\nConsequently, as a result of Theorem A.3,\\naj =\\njX\\ni=1\\n⟨aj,ui⟩| {z }\\nri j\\nui, j = 1,..., p,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 393, 'page_label': '376'}, page_content='376 Matrix Decompositions\\nfor some numbers ri j, j = 1,..., i, i = 1,..., p. Denoting the corresponding upper triangu-\\nlar matrix [ri j] by R, we have in matrix notation:\\nQR = [u1,..., up]\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nr11 r12 r13 ... r1p\\n0 r22 r23 ... r2p\\n... 0 ... ... ...\\n0 0 0 ... rpp\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= [a1,..., ap] = A,\\nwhich yields a QR decomposition. The QR decomposition can be used to efficiently solve\\nleast-squares problems; this will be shown shortly. It can also be used to calculate the\\ndeterminant of the matrix A, whenever A is square. Namely, det( A) = det(Q) det(R) =\\ndet(R); and since R is triangular, its determinant is the product of its diagonal elements.\\nThere exist various improvements of the Gram–Schmidt process (for example, the House-\\nholder transformation [52]) that not only improve the numerical stability of the QR de-\\ncomposition, but also can be applied even when A is not full rank.\\nAn important application of the QR decomposition is found in solving the least-squares\\nproblem in O(p2 n) time:\\nmin\\nβ∈Rp\\n∥Xβ−y∥2\\nfor some X ∈Rn×p (model) matrix. Using the defining properties of the pseudo-inverse in\\nDefinition A.2, one can show that∥XX+y −y∥2 ⩽∥Xβ−y∥2 for any β. In other words,bβ:=☞360\\nX+y minimizes ∥Xβ−y∥. If we have the QR decomposition X = QR, then a numerically\\nstable way to calculate bβwith an O(p2 n) cost is via\\nbβ= (QR)+y = R+Q+y = R+Q⊤y.\\nIf X has full column rank, then R+ = R−1.\\nNote that while the QR decomposition is the method of choice for solving the ordinary\\nleast-squares regression problem, theSherman–Morrison recursionis the method of choice☞372\\nfor solving the regularized least-squares (or ridge) regression problem.☞217\\nA.6.5 Singular Value Decomposition\\nOne of the most useful matrix decompositions is the singular value decompositionsingular value\\ndecomposition\\n(SVD).\\nTheorem A.13: Singular Value Decomposition\\nAny (complex) matrix m ×n matrix A admits a unique decomposition\\nA = UΣV∗,\\nwhere U and V are unitary matrices of dimension m and n, respectively, and Σ is a\\nreal m ×n diagonal matrix. If A is real, then U and V are both orthogonal matrices.\\nProof: Without loss of generality we can assume thatm ⩾n (otherwise consider the trans-\\npose of A). Then A∗A is a positive semidefinite Hermitian matrix, because ⟨A∗Av,v⟩=\\nv∗A∗Av = ∥Av∥2 ⩾0 for all v. Hence, A∗A has non-negative real eigenvalues, λ1 ⩾λ2 ⩾'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 394, 'page_label': '377'}, page_content='Linear Algebra and Functional Analysis 377\\n··· ⩾λn ⩾0. By Theorem A.7 the matrix V = [v1,..., vn] of right-eigenvectors is a unit-\\nary matrix. Define the i-th singular value singular valueas σi = √λi, i = 1,..., n and suppose λ1,...,λ r\\nare all greater than 0, and λr+1,...,λ n = 0. In particular, Avi = 0 for i = r + 1,..., n. Let\\nui = Avi/σi, i = 1,..., r. Then, for i, j ⩽r,\\n⟨ui,uj⟩= u∗\\njui =\\nv∗\\njA∗Avi\\nσi σj\\n= λi 1{i = j}\\nσiσj\\n= 1{i = j}.\\nWe can extendu1,..., ur to an orthonormal basis {u1,..., um}of Cm (e.g., using the Gram–\\nSchmidt procedure). Let U = [u1,..., un] be the corresponding unitary matrix. Defining Σ\\nto be the m ×n diagonal matrix with diagonal (σ1,...,σ r,0,..., 0), we have,\\nUΣ = [Av1,..., Avr,0,..., 0] = A V,\\nand hence A = UΣV∗. □\\nNote that\\nAA∗= UΣV∗VΣ∗U∗= UΣΣ⊤U∗ and A∗A = VΣ∗U∗UΣV∗= VΣ⊤ΣV∗.\\nSo, U is a unitary matrix whose columns are eigenvectors ofAA∗and V is a unitary matrix\\nwhose columns are eigenvectors of A∗A.\\nThe SVD makes it possible to write the matrixA as a sum of rank-1 matrices, weighted\\nby the singular values {σi}:\\nA =\\nh\\nu1,u2,..., um\\ni\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nσ1 0 ... ... 0\\n0 ... 0 ... 0\\n0 ... σ r ... 0\\n0 ... ... 0 ... 0\\n0 ... ... ... ... 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nv∗\\n1\\nv∗\\n2\\n...\\nv∗\\nn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nrX\\ni=1\\nσi ui v∗\\ni , (A.20)\\nwhich is called the dyade or spectral representation spectral\\nrepresentation\\nof A.\\nFor real-valued matrices, the SVD has a nice geometric interpretation, illustrated in\\nFigure A.6. The linear mapping defined by matrix A can be thought of as a succession of\\nthree linear operations: (1) an orthogonal transformation (i.e., a rotation with a possible\\nflipping of some axes), corresponding to matrix V⊤, followed by (2) a simple scaling of\\nthe unit vectors, corresponding to Σ, followed by (3) another orthogonal transformation,\\ncorresponding to U.\\n-4 -2 0 2 4\\n-2\\n0\\n2\\n-4 -2 0 2 4\\n-2\\n0\\n2\\n-4 -2 0 2 4\\n-2\\n0\\n2\\n-4 -2 0 2 4\\n-2\\n0\\n2\\nFigure A.6: The figure shows how the unit circle and unit vectors (first panel) are first\\nrotated (second panel), then scaled (third panel), and finally rotated and flipped.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 395, 'page_label': '378'}, page_content='378 Matrix Decompositions\\nExample A.8 (Ellipses) We continue Example A.5. Using thesvd method of the mod-\\nule numpy.linalg, we obtain the following SVD matrices for matrix A:\\nU =\\n\"−0.5430 0 .8398\\n0.8398 0 .5430\\n#\\n, Σ =\\n\"2.4221 0\\n0 0 .6193\\n#\\n, and V =\\n\"−0.3975 0 .9176\\n−0.9176 −0.3975\\n#\\n.\\nFigure A.4 shows the columns of the matrixUΣ as the two principal axes of the ellipse that☞367\\nis obtained by applying matrix A to the points of the unit circle.\\nA practical method to compute the pseudo-inverse of a real-valued matrix A is via the\\nsingular value decomposition A = UΣV⊤, where Σ is the diagonal matrix collecting all the\\npositive singular values, say σ1,...,σ r, as in Theorem A.13. In this case, A+ = VΣ+U⊤,\\nwhere Σ+ is the n ×m diagonal (pseudo-inverse) matrix:\\nΣ+ =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nσ−1\\n1 0 ... ... 0\\n0 ... 0 ... 0\\n0 ... σ −1\\nr ... 0\\n0 ... ... 0 ... 0\\n0 ... ... ... ... 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nWe conclude with a typical application of the pseudo-inverse for a least-squares optim-\\nization problem from data science.\\nExample A.9 (Rank-Deficient Least Squares) Given is an n ×p data matrix\\nX =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11 x12 ··· x1p\\nx21 x22 ··· x2p\\n... ... ... ...\\nxn1 xn2 ··· xnp\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nIt is assumed that the matrix is of full row rank (all rows ofX are linearly independent) and\\nthat the number of rows is less than the number of columns: n < p. Under this setting, any\\nsolution to the equation Xβ= y provides a perfect fit to the data and minimizes (to 0) the\\nleast-squares problem\\nbβ= argmin\\nβ∈Rp\\n∥Xβ−y∥2. (A.21)\\nIn particular, if β∗ minimizes ∥Xβ−y∥2 then so does β∗ + u for all u in the null space\\nNX := {u : Xu = 0}, which has dimension p −n. To cope with the non-uniqueness of\\nsolutions, a possible approach is to solve instead the following optimization problem:\\nminimize\\nβ∈Rp\\nβ⊤β\\nsubject to Xβ−y = 0.\\nThat is, we are interested in a solution β with the smallest squared norm (or, equival-\\nently, the smallest norm). The solution can be obtained via Lagrange’s method (see Sec-\\ntion B.2.2). Specifically, set L(β,λ) = β⊤β−λ⊤(Xβ−y), and solve☞406\\n∇βL(β,λ) = 2β−X⊤λ= 0, (A.22)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 396, 'page_label': '379'}, page_content='Linear Algebra and Functional Analysis 379\\nand\\n∇λL(β,λ) = Xβ−y = 0. (A.23)\\nFrom (A.22) we get β= X⊤λ/2. By substituting it in (A.23), we arrive at λ= 2(XX⊤)−1 y,\\nand hence βis given by\\nβ= X⊤λ\\n2 = X⊤2(XX⊤)−1 y\\n2 = X⊤(XX⊤)−1 y = X+y.\\nAn example Python code is given below.\\nsvdexample.py\\nfrom numpy import diag, zeros,vstack\\nfrom numpy.random import rand, seed\\nfrom numpy.linalg import svd, pinv\\nseed(12345)\\nn = 5\\np = 8\\nX = rand(n,p)\\ny = rand(n,1)\\nU,S,VT = svd(X)\\nSI = diag(1/S)\\n# compute pseudo inverse\\npseudo_inv = VT.T @ vstack((SI, zeros((p-n,n)))) @ U.T\\nb = pseudo_inv @ y\\n#b = pinv(X) @ y #remove comment for the built -in pseudo inverse\\nprint (X @ b - y)\\n[[5.55111512e-16]\\n[1.11022302e-16]\\n[5.55111512e-16]\\n[8.60422844e-16]\\n[2.22044605e-16]]\\nA.6.6 Solving Structured Matrix Equations\\nFor a general matrix A ∈Cn×n, performing matrix–vector multiplications takes O(n2) op-\\nerations; and solving linear systems Ax = b, and carrying out LU decompositions takes\\nO(n3) operations. However, when A is sparse (i.e., has relatively few non-zero elements)\\nor has a special structure, the computational complexity for these operations can often be\\nreduced. Matrices A that are “structured” in this way often satisfy a Sylvester equation Sylvester\\nequation\\n, of\\nthe form\\nM1A −AM∗\\n2 = G1G∗\\n2, (A.24)\\nwhere Mi ∈Cn×n,i = 1,2 are sparse matrices and Gi ∈Cn×r, i = 1,2 are matrices of rank\\nr ≪n. The elements of A must be easy to recover from these matrices, e.g., with O(1) op-\\nerations. A typical example is a (square)Toeplitz matrix Toeplitz matrix, which has the following structure:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 397, 'page_label': '380'}, page_content='380 Matrix Decompositions\\nA =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na0 a−1 ··· a−(n−2) a−(n−1)\\na1 a0 a−1 a−(n−2)\\n... a1 a0\\n... ...\\nan−2\\n... ... a−1\\nan−1 an−2 ··· a1 a0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nA general square Toeplitz matrixA is completely determined by the 2n −1 elements along\\nits first row and column. If A is also Hermitian (i.e., A∗= A), then clearly it is determined\\nby only n elements. If we define the matrices:\\nM1 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 ··· 0 1\\n1 0 0 0\\n... 1 0 ... ...\\n0 ... ... 0\\n0 0 ··· 1 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand M2 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 1 ··· 0 0\\n0 0 1 0\\n... 0 0 ... ...\\n0 ... ... 1\\n−1 0 ··· 0 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\nthen (A.24) is satisfied with\\nG1G∗\\n2 :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0\\n0 a1 + a−(n−1)\\n0 a2 + a−(n−2)\\n... ...\\n0 an−1 + a−1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\"an−1 −a−1 an−2 −a−2 ... a1 −a−(n−1) 2a0\\n0 0 ... 0 1\\n#\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nan−1 −a−1 an−2 −a−2 ... a1 −a−(n−1) 2a0\\n0 0 ... 0 a1 + a−(n−1)\\n... ... ... ... a2 + a−(n−2)\\n... ... ... ... ...\\n0 0 ... 0 an−1 + a−1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\nwhich has rank r ⩽2.\\nExample A.10 (Discrete Convolution of Vectors) The convolution of two vectors\\ncan be represented as multiplication of one of the vectors by a Toeplitz matrix. Sup-\\npose a = [a1,..., an]⊤ and b = [b1,..., bn]⊤ are two complex-valued vectors. Then, their\\nconvolutionconvolution is defined as the vector a ∗b with i-th element\\n[a ∗b]i =\\nnX\\nk=1\\nak bi−k+1, i = 1,..., n,\\nwhere bj := 0 for j ⩽0. It is easy to verify that the convolution can be written as\\na ∗b = Ab,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 398, 'page_label': '381'}, page_content='Linear Algebra and Functional Analysis 381\\nwhere, denoting the d-dimensional column vector of zeros by 0d, we have that\\nA =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na 0\\n0n−1 a ...\\n0n−2\\n... 0n−2\\n... a 0n−1\\n0 a\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nClearly, the matrix A is a (sparse) Toeplitz matrix.\\nA circulant matrix circulant\\nmatrix\\nis a special Toeplitz matrix which is obtained from a vector c by\\ncircularly permuting its indices as follows:\\nC =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nc0 cn−1 ... c2 c1\\nc1 c0 cn−1 c2\\n... c1 c0\\n... ...\\ncn−2\\n... ... cn−1\\ncn−1 cn−2 ... c1 c0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (A.25)\\nNote that C is completely determined by the n elements of its first column, c.\\nTo illustrate how structured matrices allow for faster matrix computations, consider\\nsolving the n ×n linear system:\\nAn xn = an\\nfor xn = [x1,..., xn]⊤, where an = [a1,..., an]⊤, and\\nAn :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 a1 ... an−2 an−1\\na1 1 ... an−2\\n... ... ... ... ...\\nan−2\\n... ... a1\\nan−1 an−2 ··· a1 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(A.26)\\nis a real-valued symmetric positive-definite Toeplitz matrix (so that it is invertible). Note\\nthat the entries of An are completely determined by the right-hand side of the linear equa-\\ntion: vector an. As we shall see shortly in Example A.11, the solution to the more general\\nlinear equation An xn = bn, where bn is arbitrary, can be e fficiently computed using the\\nsolution to this specific system An xn = an, obtained via a special recursive algorithm\\n(Algorithm A.6.3 below).\\nFor every k = 1,..., n the k ×k Toeplitz matrix Ak satisfies\\nAk = Pk Ak Pk,\\nwhere Pk is a permutation matrix that “flips” the order of elements — rows when pre-\\nmultiplying and columns when post-multiplying. For example,\\n\"1 2 3 4 5\\n6 7 8 9 10\\n#\\nP5 =\\n\" 5 4 3 2 1\\n10 9 8 7 6\\n#\\n, where P5 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 0 0 1\\n0 0 0 1 0\\n0 0 1 0 0\\n0 1 0 0 0\\n1 0 0 0 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 399, 'page_label': '382'}, page_content='382 Matrix Decompositions\\nClearly, Pk = P⊤\\nk and PkPk = Ik hold, so that in fact Pk is an orthogonal matrix.\\nWe can solve the n ×n linear system An xn = an in O(n2) time recursively, as follows.\\nAssume that we have somehow solved for the upper k ×k block Ak xk = ak and now we\\nwish to solve for the (k + 1) ×(k + 1) block:\\nAk+1 xk+1 = ak+1 ⇐⇒\\n\" Ak Pk ak\\na⊤\\nk Pk 1\\n#\" z\\nα\\n#\\n=\\n\" ak\\nak+1\\n#\\n.\\nTherefore,\\nα= ak+1 −a⊤\\nk Pk z\\nAk z = ak −αPk ak.\\nSince A−1\\nk Pk = Pk A−1\\nk , the second equation above simplifies to\\nz = A−1\\nk ak −αA−1\\nk Pk ak\\n= xk −αPk xk.\\nSubstituting z = xk −αPk xk into α= ak+1 −a⊤\\nk Pk z and solving for αyields:\\nα= ak+1 −a⊤\\nk Pk xk\\n1 −a⊤\\nk xk\\n.\\nFinally, with the value of αcomputed above, we have\\nxk+1 =\\n\"xk −αPk xk\\nα\\n#\\n.\\nThis gives the following Levinson–DurbinLevinson–\\nDurbin\\nrecursive algorithm for solving An xn = an.\\nAlgorithm A.6.3: Levinson–Durbin Recursion for Solving An xn = an\\ninput: First row [1,a1,..., an−1] = [1,a⊤\\nn−1] of matrix An.\\noutput: Solution xn = A−1\\nn an.\\n1 x1 ←a1\\n2 for k = 1,..., n −1 do\\n3 βk ←1 −a⊤\\nk xk\\n4 ˘x ←[xk,k,xk,k−1,..., xk,1]⊤\\n5 α←(ak+1 −a⊤\\nk ˘x)/βk\\n6 xk+1 ←\\n\"xk −α˘x\\nα\\n#\\n7 return xn\\nIn the algorithm above, we have identified xk = [xk,1,xk,2,..., xk,k]⊤. The advantage of\\nthe Levinson–Durbin algorithm is that its running cost is O(n2), instead of the usual O(n3).\\nUsing the {xk,βk}computed in Algorithm A.6.3, we construct the following lower tri-\\nangular matrix recursively, setting L1 = 1 and\\nLk+1 =\\n\" Lk 0k\\n−(Pk xk)⊤ 1\\n#\\n, k = 1,..., n −1. (A.27)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 400, 'page_label': '383'}, page_content='Linear Algebra and Functional Analysis 383\\nThen, we have the following factorization of An.\\nTheorem A.14: Diagonalization of Toeplitz Correlation Matrix An\\nFor a real-valued symmetric positive-definite Toeplitz matrixAn of the form (A.26),\\nwe have\\nLn An L⊤\\nn = Dn,\\nwhere Ln is a the lower diagonal matrix (A.27) and Dn := diag(1,β1,...,β n−1) is a\\ndiagonal matrix.\\nProof: We give a proof by induction. Obviously,L1A1L⊤\\n1 = 1 ·1 ·1 = 1 = D1 is true. Next,\\nassume that the factorization LkAkL⊤\\nk = Dk holds for a given k. Observe that\\nLk+1Ak+1 =\\n\" Lk 0k\\n−(Pk xk)⊤ 1\\n#\" Ak Pk ak\\na⊤\\nk Pk 1\\n#\\n=\\n\" LkAk , LkPk ak\\n−(Pk xk)⊤Ak + a⊤\\nk Pk , −(Pk xk)⊤Pk ak + 1\\n#\\n.\\nIt is straightforward to verify that [ −(Pk xk)⊤Ak + a⊤\\nk Pk,−(Pk xk)⊤Pk ak + 1] = [0⊤\\nk ,βk],\\nyielding the recursion\\nLk+1Ak+1 =\\n\"LkAk LkPk ak\\n0⊤\\nk βk\\n#\\n.\\nSecondly, observe that\\nLk+1Ak+1L⊤\\nk+1 =\\n\"LkAk LkPk ak\\n0⊤\\nk βk\\n#\"L⊤\\nk −Pk xk\\n0⊤\\nk 1\\n#\\n=\\n\"LkAkL⊤\\nk , −LkAkPk xk + LkPk ak\\n0⊤\\nk , β k\\n#\\n.\\nBy noting that AkPk xk = PkPkAkPk xk = PkAk xk = Pk ak, we obtain:\\nLk+1Ak+1L⊤\\nk+1 =\\n\"LkAkL⊤\\nk 0k\\n0⊤\\nk βk\\n#\\n.\\nHence, the result follows by induction. □\\nExample A.11 (Solving An xn = bn in O(n2) Time) One application of the factoriza-\\ntion in Theorem A.14 is in the fast solution of a linear system An xn = bn, where the right-\\nhand side is an arbitrary vector bn. Since the solution xn can be written as\\nxn = A−1\\nn bn = L⊤\\nn D−1\\nn Ln bn,\\nwe can compute xn in O(n2) time, as follows.\\nAlgorithm A.6.4: Solving An xn = bn for a General Right-Hand Side\\ninput: First row [1,a⊤\\nn−1] of matrix An and right-hand side bn.\\noutput: Solution xn = A−1\\nn bn.\\n1 Compute Ln in (A.27) and the numbers β1,...,β n−1 via Algorithm A.6.3.\\n2 [x1,..., xn]⊤←Ln bn (computed in O(n2) time)\\n3 xi ←xi/βi−1 for i = 2,..., n (computed in O(n) time)\\n4 [x1,..., xn] ←[x1,..., xn] Ln (computed in O(n2) time)\\n5 return xn ←[x1,..., xn]⊤'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 401, 'page_label': '384'}, page_content='384 Functional Analysis\\nNote that it is possible to avoid the explicit construction of the lower triangular matrix\\nin (A.27) via the following modification of Algorithm A.6.3, which only stores an extra\\nvector y at each recursive step of the Levinson–Durbin algorithm.\\nAlgorithm A.6.5: Solving An xn = bn with O(n) Memory Cost\\ninput: First row [1,a⊤\\nn−1] of matrix An and right-hand side bn.\\noutput: Solution xn = A−1\\nn bn.\\n1 x ←b1\\n2 y ←a1\\n3 for k = 1,..., n −1 do\\n4 ˘x ←[xk,xk−1,..., x1]\\n5 ˘y ←[yk,yk−1,..., y1]\\n6 β←1 −a⊤\\nk y\\n7 αx ←(bk+1 −b⊤\\nk ˘x)/β\\n8 αy ←(ak+1 −a⊤\\nk ˘y)/β\\n9 x ←[x −αx ˘x,αx]⊤\\n10 y ←[y −αy ˘y,αy]⊤\\n11 return x\\nA.7 Functional Analysis\\nMuch of the previous theory on Euclidean vector spaces can be generalized to vector spaces\\nof functions. Every element of a (real-valued) function space His a function from somefunction space\\nset Xto R, and elements can be added and scalar multiplied as if they were vectors. In\\nother words, if f ∈H and g ∈H, then αf + βg ∈H for all α,β ∈R. On Hwe can impose\\nan inner product as a mapping ⟨·,·⟩from H×H to Rthat satisfies\\n1. ⟨αf1 + βf2,g⟩= α⟨f1,g⟩+ β⟨f2,g⟩;\\n2. ⟨f,g⟩= ⟨g, f ⟩;\\n3. ⟨f, f ⟩⩾0;\\n4. ⟨f, f ⟩= 0 if and only if f = 0 (the zero function).\\nWe focus on real-valued function spaces, although the theory for complex-valued\\nfunction spaces is similar (and sometimes easier), under suitable modifications (e.g.,\\n⟨f,g⟩= ⟨g, f ⟩).\\nSimilar to the linear algebra setting in Section A.2, we say that two elements f and g\\nin Hare orthogonal to each other with respect to this inner product if ⟨f,g⟩= 0. Given an\\ninner product, we can measure distances between elements of the function space Husing\\nthe normnorm\\n∥f ∥:=\\np\\n⟨f, f ⟩.\\nFor example, the distance between two functions fm and fn is given by ∥fm −fn∥. The space\\nHis said to be completecomplete if every sequence of functions f1, f2,... ∈H for which\\n∥fm −fn∥→ 0 as m,n →∞, (A.28)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 402, 'page_label': '385'}, page_content='Linear Algebra and Functional Analysis 385\\nconverges to some f ∈H; that is, ∥f −fn∥→ 0 as n →∞. A sequence that satisfies (A.28)\\nis called a Cauchy sequence. Cauchy\\nsequenceA complete inner product space is called aHilbert space. The most fundamental Hilbert\\nHilbert spacespace of functions is the space L2. An in-depth introduction to L2 requires some measure\\ntheory [6]. For our purposes, it suffices to assume that X⊆ Rd and that on Xa measure measureµis\\ndefined which assigns to each suitable4 set A a positive number µ(A) ⩾0 (e.g., its volume).\\nIn many cases of interest µis of the form\\nµ(A) =\\nZ\\nA\\nw(x) dx (A.29)\\nwhere w ⩾0 is a positive function on Xwhich is called the density densityof µ with respect to\\nthe Lebesgue measure (the natural volume measure on Rd). We write µ(dx) = w(x) dx to\\nindicate that µhas density w. Another important case is where\\nµ(A) =\\nX\\nx ∈A∩Zd\\nw(x), (A.30)\\nwhere w ⩾0 is again called the density of µ, but now with respect to the counting measure\\non Zd (which counts the points of Zd). Integrals with respect to measures µin (A.29) and\\n(A.30) can now be defined as\\nZ\\nf (x) µ(dx) =\\nZ\\nf (x) w(x) dx,\\nand Z\\nf (x) µ(dx) =\\nX\\nx\\nf (x) w(x),\\nrespectively. We assume for simplicity that µ has the form (A.29). For measures of the\\nform (A.30) (so-called discrete measures), replace integrals by sums in what follows.\\nDefinition A.4: L2 Space\\nLet Xbe a subset of Rd with measure µ(dx) = w(x) dx. The Hilbert space L2(X,µ)\\nis the linear space of functions from Xto Rthat satisfy\\nZ\\nX\\nf (x)2 w(x) dx <∞, (A.31)\\nand with inner product\\n⟨f,g⟩=\\nZ\\nX\\nf (x) g(x) w(x) dx. (A.32)\\nLet Hbe a Hilbert space. A set of functions {fi,i ∈I} is called an orthonormal system orthonormal\\nsystemif\\n4Not all sets have a measure. Suitable sets are Borel sets, which can be thought of as countable unions\\nof rectangles.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 403, 'page_label': '386'}, page_content='386 Functional Analysis\\n1. the norm of every fi is 1; that is, ⟨fi, fi⟩= 1 for all i ∈I,\\n2. the {fi}are orthogonal; that is, ⟨fi, fj⟩= 0 for i , j.\\nIt follows then that the {fi}are linearly independent; that is, the only linear combinationP\\nj αj fj(x) that is equal to fi(x) for all x is the one where αi = 1 and αj = 0 for j , i. An\\northonormal system {fi}is called an orthonormal basis if there is no other f ∈H that isorthonormal\\nbasis orthogonal to all the {fi,i ∈I} (other than the zero function). Although the general theory\\nallows for uncountable bases, in practice5 the set Iis taken to be countable.\\nExample A.12 (Trigonometric Orthonormal Basis) Let H be the Hilbert space\\nL2((0,2π),µ), where µ(dx) = w(x) dx and w is the constant function w(x) = 1, 0 < x <2π.\\nAlternatively, take X= Rand w the indicator function on (0,2π). The trigonometric func-\\ntions\\ng0(x) = 1√\\n2π\\n, gk(x) = 1√πcos(kx), hk(x) = 1√πsin(kx), k = 1,2,...\\nform a countable infinite-dimensional orthonormal basis of H.\\nA Hilbert space Hwith an orthonormal basis {f1, f2,... }behaves very similarly to the\\nfamiliar Euclidean vector space. In particular, every element (i.e., function) f ∈H can be\\nwritten as a unique linear combination of the basis vectors:\\nf =\\nX\\ni\\n⟨f, fi⟩fi, (A.33)\\nexactly as in Theorem A.3. The right-hand side of (A.33) is called a (generalized) Fourier\\nexpansion of f . Note that such a Fourier expansion does not require a trigonometric basis;Fourier\\nexpansion any orthonormal basis will do.\\nExample A.13 (Example A.12 (cont.)) Consider the indicator function f (x) = 1{0 <\\nx < π}. As the trigonometric functions {gk}and {hk}form a basis for L2((0,2π),1dx), we\\ncan write\\nf (x) = a0\\n1√\\n2π\\n+\\n∞X\\nk=1\\nak\\n1√πcos(kx) +\\n∞X\\nk=1\\nbk\\n1√πsin(kx), (A.34)\\nwhere a0 =\\nR π\\n0 1/\\n√\\n2πdx = √π/2, ak =\\nR π\\n0 cos(kx)/√πdx and bk =\\nR π\\n0 sin(kx)/√πdx, k =\\n1,2,... . This means that ak = 0 for all k, bk = 0 for even k, and bk = 2/(k √π) for odd k.\\nConsequently,\\nf (x) = 1\\n2 + 2\\nπ\\n∞X\\nk=1\\nsin(kx)\\nk . (A.35)\\nFigure A.7 shows several Fourier approximations obtained by truncating the infinite sum\\nin (A.35).\\n5The function spaces typically encountered in machine learning and data science are usually separable\\nspaces, which allows for the set Ito be considered countable; see, e.g., [106].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 404, 'page_label': '387'}, page_content='Linear Algebra and Functional Analysis 387\\n0 0.5 1 1.5 2 2.5 3 3.5\\n0\\n0.5\\n1\\nFigure A.7: Fourier approximations of the unit step function f on the interval (0,π), trun-\\ncating the infinite sum in (A.35) to i = 2, 4, and 14 terms, giving the dotted blue, dashed\\nred, and solid green curves, respectively.\\nStarting from any countable basis, we can use the Gram–Schmidt procedure to obtain ☞ 375\\nan orthonormal basis, as illustrated in the following example.\\nExample A.14 (Legendre Polynomials) Take the function spaceL2(R,w(x) dx), where\\nw(x) = 1{−1 < x <1}. We wish to construct an orthonormal basis of polynomial functions\\ng0,g1,g2,... , starting from the collection of monomials:ι0,ι1,ι2,... , where ιk : x 7→xk. Us-\\ning Gram–Schmidt, the first normalized zero-degree polynomial is g0 = ι0/∥ι0∥= √1/2.\\nTo find g1 (a polynomial of degree 1), project ι1 (the identity function) onto the space\\nspanned by g0. The resulting projection is p1 := ⟨g0,ι1⟩g0, written out as\\np1(x) =\\n Z 1\\n−1\\nx g0(x) dx\\n!\\ng0(x) = 1\\n2\\nZ 1\\n−1\\nx dx = 0.\\nHence, g1 = (ι1 −p1)/∥ι1 −p1∥is a linear function; that is, of the form g1(x) = ax. The\\nconstant a is found by normalization:\\n1 = ∥g1∥2 =\\nZ 1\\n−1\\ng2\\n1(x) dx = a2\\nZ 1\\n−1\\nx2 dx = a2 2\\n3,\\nso that g1(x) = √3/2x. Continuing the Gram–Schmidt procedure, we find g2(x) =√5/8(3x2 −1),g3(x) = √7/8(5x3 −3x) and, in general,\\ngk(x) =\\n√\\n2k + 1\\n2k+ 1\\n2 k!\\ndk\\ndxk (x2 −1)k, k = 0,1,2,....\\nThese are the (normalized)Legendre polynomials. The graphs ofg0,g1,g2,and g3 are given Legendre\\npolynomialsin Figure A.8.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 405, 'page_label': '388'}, page_content='388 Functional Analysis\\n-1 -0.5 0 0.5 1\\n-1\\n1\\nFigure A.8: The first 4 normalized Legendre polynomials.\\nAs the Legendre polynomials form an orthonormal basis of L2(R,1{−1 < x < 1}dx),\\nthey can be used to approximate arbitrary functions in this space. For example, Figure A.9\\nshows an approximation using the first 51 Legendre polynomials ( k = 0,1,..., 50) of the\\nFourier expansion of the indicator function on the interval ( −1/2,1/2). These Legendre\\npolynomials form the basis of a 51-dimensional linear subspace onto which the indicator\\nfunction is orthogonally projected.\\n-1 -0.5 0 0.5 1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure A.9: Approximation of the indicator function on the interval (−1/2,1/2), using the\\nLegendre polynomials g0,g1,..., g50.\\nThe Legendre polynomials were produced in the following way: We started with an\\nunnormalized probability density onR— in this case the probability density of the uniform☞424\\ndistribution on ( −1,1). We then constructed a sequence of polynomials by applying the\\nGram–Schmidt procedure to the monomials 1,x,x2,... .\\nBy using exactly the same procedure, but with a di fferent probability density, we can\\nproduce other such orthogonal polynomials. For example, the density of the standard expo-orthogonal\\npolynomials nential6 distribution, w(x) = e−x,x ⩾0, gives the Laguerre polynomials, which are defined\\nLaguerre\\npolynomials\\n6This can be further generalized to the density of a gamma distribution.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 406, 'page_label': '389'}, page_content='Linear Algebra and Functional Analysis 389\\nby the recurrence\\n(n + 1)gn+1(x) = (2n + 1 −x)gn(x) −ngn−1(x), n = 1,2,...,\\nwith g0(x) = 1 and g1(x) = 1 −x, for x ⩾0. The Hermite polynomials Hermite\\npolynomials\\nare obtained when\\nusing instead the density of the standard normal distribution: w(x) = e−x2/2/\\n√\\n2π,x ∈R.\\nThese polynomials satisfy the recursion\\ngn+1(x) = xgn(x) −dgn(x)\\ndx , n = 0,1,...,\\nwith g0(x) = 1, x ∈R. Note that the Hermite polynomials as defined above have not been\\nnormalized to have norm 1. To normalize, use the fact that∥gn∥2 = n!.\\nWe conclude with a number of key results in functional analysis. The first one is the\\ncelebrated Cauchy–Schwarz Cauchy–\\nSchwarz\\ninequality.\\nTheorem A.15: Cauchy–Schwarz\\nLet Hbe a Hilbert space. For every f,g ∈H it holds that\\n|⟨f,g⟩|⩽∥f ∥∥g∥.\\nProof: The inequality is trivially true for g = 0 (zero function). For g , 0, we can write\\nf = αg + h, where h ⊥g and α = ⟨f,g⟩/∥g∥2. Consequently, ∥f ∥2 = |α|2 ∥g∥2 + ∥h∥2 ⩾\\n|α|2 ∥g∥2. The result follows after rearranging this last inequality. □\\nLet Vand Wbe two linear vector spaces (for example, Hilbert spaces) on which norms\\n∥·∥ V and ∥·∥ W are defined. Suppose A : V→W is a mapping from Vto W. When\\nW= V, such a mapping is often called anoperator; when W= Rit is called afunctional. operator\\nfunctionalMapping A is said to be linear if A(αf + βg) = αA( f ) + βA(g). In this case we write A f\\nlinear mappinginstead of A( f ). If there exists γ< ∞such that\\n∥A f∥W ⩽γ∥f ∥V, f ∈V, (A.36)\\nthen A is said to be abounded mapping. The smallest γfor which (A.36) holds is called the bounded\\nmappingnorm of A; denoted by ∥A∥. A (not necessarily linear) mapping A : V→W is said to be\\nnormcontinuous at f if for any sequence f1, f2,... converging to f the sequence A( f1),A( f2),...\\ncontinuous\\nmappingconverges to A( f ). That is, if\\n∀ε> 0,∃δ> 0 : ∀g ∈V,∥f −g∥V <δ ⇒∥A( f ) −A(g)∥W <ε. (A.37)\\nIf the above property holds for everyf ∈V, then the mapping A itself is called continuous.\\nTheorem A.16: Continuity and Boundedness for Linear Mappings\\nFor a linear mapping, continuity and boundedness are equivalent.\\nProof: Let A be linear and bounded. We may assume that A is non-zero (otherwise the\\nstatement holds trivially), and that therefore 0<∥A∥<∞. Takingδ<ε/ ∥A∥in (A.37) now\\nensures that ∥A f −Ag∥W ⩽∥A∥∥f −g∥V <∥A∥δ<ε . This shows that A is continuous.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 407, 'page_label': '390'}, page_content='390 Fourier Transforms\\nConversely, suppose A is continuous. In particular, it is continuous at f = 0 (the zero-\\nelement of V). Thus, take f = 0 and let ε and δ be as in (A.37). For any g , 0, let h =\\nδ/(2∥g∥V) g. As ∥h∥V = δ/2 <δ, it follows from (A.37) that\\n∥Ah∥W = δ\\n2∥g∥V\\n∥Ag∥W <ε.\\nRearranging the last inequality gives ∥Ag∥W <2ε/δ∥g∥V, showing that A is bounded. □\\nTheorem A.17: Riesz Representation Theorem\\nAny bounded linear functional ϕon a Hilbert space Hcan be represented as ϕ(h) =\\n⟨h,g⟩, for some g ∈H (depending on ϕ).\\nProof: Let P be the projection of Honto the nullspace Nof ϕ; that is, N = {g ∈H :\\nϕ(g) = 0}. If ϕ is not the 0-functional, then there exists a g0 , 0 with ϕ(g0) , 0. Let\\ng1 = g0 −Pg0. Then g1 ⊥N and ϕ(g1) = ϕ(g0). Take g2 = g1/ϕ(g1). For any h ∈H ,\\nf := h −ϕ(h)g2 lies in N. As g2 ⊥N it holds that ⟨f,g2⟩= 0, which is equivalent to\\n⟨h,g2⟩= ϕ(h) ∥g2∥2. By defining g = g2/∥g2∥2 we have found our representation. □\\nA.8 Fourier Transforms\\nWe will now briefly introduce the Fourier transform. Before doing so, we will extend the\\nconcept of L2 space of real-valued functions as follows.☞385\\nDefinition A.5: Lp Space\\nLet Xbe a subset ofRd with measure µ(dx) = w(x) dx and p ∈[1,∞). Then Lp(X,µ)\\nis the linear space of functions from Xto Cthat satisfy\\nZ\\nX\\n|f (x)|p w(x) dx <∞. (A.38)\\nWhen p = 2, L2(X,µ) is in fact a Hilbert space equipped with inner product\\n⟨f,g⟩=\\nZ\\nX\\nf (x) g(x) w(x) dx. (A.39)\\nWe are now in a position to define the Fourier transform (with respect to the Lebesgue\\nmeasure). Note that in the following Definitions A.6 and A.7 we have chosen a particular\\nconvention. Equivalent (but not identical) definitions exist that include scaling constants\\n(2π)d or (2π)−d and where −2πt is replaced with 2πt, t, or −t.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 408, 'page_label': '391'}, page_content='Linear Algebra and Functional Analysis 391\\nDefinition A.6: (Multivariate) Fourier Transform\\nThe Fourier transform Fourier\\ntransform\\nF[ f ] of a (real- or complex-valued) function f ∈L1(Rd) is\\nthe function ef defined as\\nef (t) :=\\nZ\\nRd\\ne−i 2πt⊤x f (x) dx , t ∈Rd.\\nThe Fourier transform ef is continuous, uniformly bounded (since f ∈L1(Rd) im-\\nplies that |ef (t)|⩽\\nR\\nRd |f (x)|dx <∞), and satisfies lim ∥t∥→∞ ef (t) = 0 (a result known as\\nthe Riemann–Lebesgue lemma ). However, |ef |does not necessarily have a finite integ-\\nral. A simple example in R1 is the Fourier transform of f (x) = 1{−1/2 < x <1/2}. Then\\nef (t) = sin(πt)/(πt) = sinc(πt), which is not absolutely integrable.\\nDefinition A.7: (Multivariate) Inverse Fourier Transform\\nThe inverse Fourier transform inverse Fourier\\ntransform\\nF−1[ ef ] of a (real- or complex-valued) function\\nef ∈L1(Rd) is the function ˘f defined as\\n˘f (x) :=\\nZ\\nRd\\nei 2πt⊤x ef (t) dt, x ∈Rd.\\nAs one would hope, it holds that if f and F[ f ] are both in L1(Rd), then f = F−1[F[ f ]]\\nalmost everywhere.\\nThe Fourier transform enjoys many interesting and useful properties, some of which\\nwe list below.\\n1. Linearity: For f,g ∈L1(Rd) and constants a,b ∈R,\\nF[a f + bg] = a F[ f ] + b F[g].\\n2. Space Shifting and Scaling: Let A ∈Rd×d be an invertible matrix and b ∈Rd a con-\\nstant vector. Let f ∈L1(Rd) and define h(x) := f (Ax + b). Then\\nF[h](t) = ei 2π(A−⊤t)⊤b ef (A−⊤t)/|det(A)|,\\nwhere A−⊤:= (A⊤)−1 = (A−1)⊤.\\n3. Frequency Shifting and Scaling: Let A ∈Rd×d be an invertible matrix and b ∈Rd a\\nconstant vector. Let f ∈L1(Rd) and define\\nh(x) := e−i 2πb⊤A−⊤x f (A−⊤x)/|det(A)|.\\nThen F[h](t) = ef (At + b).\\n4. Differentiation: Let f ∈L1(Rd) ∩C1(Rd) and let fk := ∂f /∂xk be the partial derivat-\\nive of f with respect to xk. If fk ∈L1(Rd) for k = 1,..., d, then\\nF[ fk](t) = (i 2πtk) ef (t).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 409, 'page_label': '392'}, page_content='392 Fourier Transforms\\n5. Convolution: Let f,g ∈L1(Rd) be real or complex valued functions. Their convolu-\\ntion, f ∗g, is defined as\\n( f ∗g)(x) =\\nZ\\nRd\\nf (y) g(x −y) dy,\\nand is also in L1(Rd). Moreover, the Fourier transform satisfies\\nF[ f ∗g] = F[ f ] F[g].\\n6. Duality: Let f and F[ f ] both be in L1(Rd). Then F[F[ f ]](t) = f (−t).\\n7. Product Formula: Let f,g ∈L1(Rd) and denote by ef ,eg their respective Fourier trans-\\nforms. Then ef g, f eg ∈L1(Rd), and\\nZ\\nRd\\nef (z) g(z) dz =\\nZ\\nRd\\nf (z)eg(z) dz.\\nThere are many additional properties which hold if f ∈L1(Rd) ∩L2(Rd). In particular,\\nif f,g ∈L1(Rd) ∩L2(Rd), then ef ,eg ∈L2(Rd) and ⟨ef ,eg⟩= ⟨f,g⟩, a result often known as\\nParseval’s formula. Putting g = f gives the result often referred to asPlancherel’s theorem.\\nThe Fourier transform can be extended in several ways, in the first instance to functions\\nin L2(Rd) by continuity. A substantial extension of the theory is realized by replacing integ-\\nration with respect to the Lebesgue measure (i.e.,\\nR\\nRd ··· dx) with integration with respect\\nto a (finite Borel) measure µ (i.e.,\\nR\\nRd ··· µ(dx)). Moreover, there is a close connection\\nbetween the Fourier transform and characteristic functions arising in probability theory.\\nIndeed, if X is a random vector with pdf f , then its characteristic function ψsatisfies☞441\\nψ(t) := Eei t⊤X = F[ f ](−t/(2π)).\\nA.8.1 Discrete Fourier Transform\\nHere, we introduce the (univariate) discrete Fourier transform, which can be viewed as a\\nspecial case of the Fourier transform introduced in Definition A.6, whered = 1, integration\\nis with respect to the counting measure, and f (x) = 0 for x <0 and x >(n −1).\\nDefinition A.8: Discrete Fourier Transform\\nThe discrete Fourier transformdiscrete\\nFourier\\ntransform\\n(DFT) of a vector x = [x0,..., xn−1]⊤∈Cn is the\\nvector ex = [ex0,..., exn−1]⊤whose elements are given by\\next =\\nn−1X\\ns=0\\nωst xs, t = 0,..., n −1, (A.40)\\nwhere ω= exp(−i 2π/n).\\nIn other words, ex is obtained from x via the linear transformation\\nex = Fx,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 410, 'page_label': '393'}, page_content='Linear Algebra and Functional Analysis 393\\nwhere\\nF =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 1 1 ... 1\\n1 ω ω 2 ... ω n−1\\n1 ω2 ω4 ... ω 2(n−1)\\n... ... ... ... ...\\n1 ωn−1 ω2(n−1) ... ω (n−1)2\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nThe matrix F is a so-called Vandermonde matrix, and is clearly symmetric (i.e., F = F⊤).\\nMoreover, F/√n is in fact a unitary matrix and hence its inverse is simply its complex\\nconjugate F/√n. Thus, F−1 = F/n and we have that the inverse discrete Fourier transform inverse discrete\\nFourier\\ntransform(IDFT) is given by\\nxt = 1\\nn\\nn−1X\\ns=0\\nω−st exs, t = 0,..., n −1, (A.41)\\nor in terms of matrices and vectors,\\nx = Fex/n.\\nObserve that the IDFT of a vector y is related to the DFT of its complex conjugate y, since\\nF y/n = F y/n.\\nConsequently, an IDFT can be computed via a DFT.\\nThere is a close connection between circulant matrices C and the DFT. To make this\\nconnection concrete, let C be the circulant matrix corresponding to the vector c ∈Cn and\\ndenote by ft the t-th column of the discrete Fourier matrix F, t = 0,1,..., n −1. Then, the\\ns-th element of C ft is\\nn−1X\\nk=0\\nc(s−k) mod n ωtk =\\nn−1X\\ny=0\\ncy ωt(s−y) = ωts\\n|{z}\\ns-th element of ft\\nn−1X\\ny=0\\ncy ω−ty\\n|      {z      }\\nλs\\n.\\nHence, the eigenvalues of C are\\nλt = c⊤ ft, t = 0,1,..., n −1,\\nwith corresponding eigenvectors ft. Collecting the eigenvalues into the vector λ =\\n[λ0,...,λ n−1]⊤= Fc, we therefore have the eigen-decomposition\\nC = F diag(λ) F/n.\\nConsequently, one can compute the circular convolution of a vector a = [a1,..., an]⊤\\nand c = [c0,..., cn−1]⊤ by a series of DFTs as follows. Construct the circulant matrix C\\ncorresponding to c. Then, the circular convolution of a and c is given by y = Ca. Proceed\\nin four steps:\\n1. Compute z = Fa/n.\\n2. Compute λ= Fc.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 411, 'page_label': '394'}, page_content='394 Fourier Transforms\\n3. Compute p = z ⊙λ= [z1 λ0,..., zn λn−1]⊤.\\n4. Compute y = Fp.\\nSteps 1 and 2 are (up to constants) in the form of an IDFT, and step 4 is in the form of a\\nDFT. These are computable via the FFT (Section A.8.2) in O(n ln n) time. Step 3 is a dot☞394\\nproduct computable in O(n) time. Thus, the circular convolution can be computed with the\\naid of the FFT in O(n ln n) time.\\nOne can also efficiently compute the product of an n ×n Toeplitz matrix T and an n ×1\\nvector a in O(n ln n) time by embeddingT into a circulant matrixC of size 2n×2n. Namely,\\ndefine\\nC =\\n\"T B\\nB T\\n#\\n,\\nwhere\\nB =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 tn−1 ··· t2 t1\\nt−(n−1) 0 tn−1 t2\\n... t−(n−1) 0 ... ...\\nt−2\\n... ... tn−1\\nt−1 t−2 ··· t−(n−1) 0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nThen a product of the form y = Ta can be computed in O(n ln n) time, since we may write\\nC\\n\"a\\n0\\n#\\n=\\n\"T B\\nB T\\n#\" a\\n0\\n#\\n=\\n\"Ta\\nBa\\n#\\n.\\nThe left-hand side is a product of a 2 n ×2n circulant matrix with vector of length 2n, and\\nso can be computed in O(n ln n) time via the FFT, as previously discussed.\\nConceptually, one can also solve equations of the formCx = b for a given vectorb ∈Cn\\nand circulant matrix C (corresponding to c ∈Cn, assuming all its eigenvalues are non-zero)\\nvia the following four steps:\\n1. Compute z = Fb/n.\\n2. Compute λ= Fc.\\n3. Compute p = z/λ= [z1/λ0,..., zn/λn−1]⊤.\\n4. Compute x = Fp.\\nOnce again, Steps 1 and 2 are (up to constants) in the form of an IDFT, and Step 4 is in\\nthe form of a DFT, all of which are computable via the FFT in O(n ln n) time, and Step 3\\nis computable in O(n) time, meaning the solution x can be computed using the FFT in\\nO(n ln n) time.\\nA.8.2 Fast Fourier Transform\\nThe fast Fourier transformfast Fourier\\ntransform\\n(FFT) is a numerical algorithm for the fast evaluation of (A.40)\\nand (A.41). By using a divide-and-conquer strategy, the algorithm reduces the compu-\\ntational complexity from O(n2) (for the naïve evaluation of the linear transformation) to\\nO(n ln n) [60].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 412, 'page_label': '395'}, page_content='Linear Algebra and Functional Analysis 395\\nThe essence of the algorithm lies in the following observation. Suppose n = r1r2. Then\\none can express any index t appearing in (A.40) via a pair ( t0,t1), with t = t1r1 + t0,\\nwhere t0 ∈{0,1,..., r1 −1}and t1 ∈{0,1,..., r2 −1}. Similarly, one can express any index\\ns appearing in (A.40) via a pair (s0,s1), with s = s1r2 + s0, where s0 ∈{0,1,..., r2 −1}and\\ns1 ∈{0,1,..., r1 −1}.\\nIdentifying ext ≡ext1,t0 and xs ≡xs1,s0 , we may re-express (A.40) as\\next1,t0 =\\nr2−1X\\ns0=0\\nωs0t\\nr1−1X\\ns1=0\\nωs1r2t xs1,s0 , t0 = 0,1,..., r1 −1 ,t1 = 0,1,..., r2 −1. (A.42)\\nObserve that ωs1r2t = ωs1r2t0 (because ωr1r2 = 1), so that the inner sum over s1 depends only\\non s0 and t0. Define\\nyt0,s0 :=\\nr1−1X\\ns1=0\\nωs1r2t0 xs1,s0 , t0 = 0,1,..., r1 −1 ,s0 = 0,1,..., r2 −1.\\nComputing each yt0,s0 requires O(n r1) operations. In terms of the {yt0,s0 }, (A.42) can be\\nwritten as\\next1,t0 =\\nr2−1X\\ns0=0\\nωs0t yt0,s0 , t1 = 0,1,..., r2 −1 ,t0 = 0,1,..., r1 −1,\\nrequiring O(n r2) operations to compute. Thus, calculating the DFT using this two-step\\nprocedure requires O(n (r1 + r2)) operations, rather than O(n2).\\nNow supposing n = r1r2 ··· rm, repeated application the above divide-and-conquer idea\\nyields an m-step procedure requiring O(n (r1 + r2 + ··· + rm)) operations. In particular, if\\nrk = r for all k = 1,2,..., m, we have that n = rm and m = logr n, so that the total number\\nof operations is O(r n m) ≡O(r n logr(n)). Typically, theradix r is a small (not necessarily\\nprime) number, for instance r = 2.\\nFurther Reading\\nA good reference book on matrix computations is Golub and Van Loan [52]. A useful list\\nof many common vector and matrix calculus identities can be found in [95]. Strang’s in-\\ntroduction to linear algebra [116] is a classic textbook, and his recent book [117] combines\\nlinear algebra with the foundations of deep learning. Fast reliable algorithms for matrices\\nwith structure can be found in [64]. Kolmogorov and Fomin’s masterpiece on the theory\\nof functions and functional analysis [67] still provides one of the best introductions to the\\ntopic. A popular choice for an advanced course in functional analysis is Rudin [106].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 413, 'page_label': '396'}, page_content='396'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 414, 'page_label': '397'}, page_content='APPENDIX B\\nMULTIVARIATE DIFFERENTIATION AND\\nOPTIMIZATION\\nThe purpose of this appendix is to review various aspects of multivariate di fferen-\\ntiation and optimization. We assume the reader is familiar with di fferentiating a real-\\nvalued function.\\nB.1 Multivariate Differentiation\\nFor a multivariate function f that maps a vector x = [x1,..., xn]⊤ to a real number f (x),\\nthe partial derivative with respect to xi, denoted ∂f\\n∂xi\\n, is the derivative taken with respect partial\\nderivativeto xi while all other variables are held constant. We can write all the n partial derivatives\\nneatly using the “scalar/vector” derivative notation:\\nscalar/vector: ∂f\\n∂x :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f\\n∂x1\\n...\\n∂f\\n∂xn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (B.1)\\nThis vector of partial derivatives is known as thegradient of f at x and is sometimes written gradient\\nas ∇f (x).\\nNext, suppose that f is a multivalued (vector-valued) function taking values in Rm,\\ndefined by\\nx =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\n...\\nxn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n7→\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nf1(x)\\nf2(x)\\n...\\nfm(x)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=: f(x).\\nWe can compute each of the partial derivatives∂fi/∂xj and organize them neatly in a “vec-\\ntor/vector” derivative notation:\\nvector/vector: ∂f\\n∂x :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f1\\n∂x1\\n∂f2\\n∂x1\\n··· ∂fm\\n∂x1\\n∂f1\\n∂x2\\n∂f2\\n∂x2\\n··· ∂fm\\n∂x2\\n... ... ··· ...\\n∂f1\\n∂xn\\n∂f2\\n∂xn\\n··· ∂fm\\n∂xn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (B.2)\\n397'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 415, 'page_label': '398'}, page_content='398 Multivariate Differentiation\\nThe transpose of this matrix is known as the matrix of Jacobimatrix ofJacobi of f at x (sometimes\\ncalled the Fréchet derivative of f at x); that is,\\nJf (x) :=\\n\"∂f\\n∂x\\n#⊤\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f1\\n∂x1\\n∂f1\\n∂x2\\n··· ∂f1\\n∂xn\\n∂f2\\n∂x1\\n∂f2\\n∂x2\\n··· ∂f2\\n∂xn\\n... ... ··· ...\\n∂fm\\n∂x1\\n∂fm\\n∂x2\\n··· ∂fm\\n∂xn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (B.3)\\nIf we define g(x) := ∇f (x) and take the “vector/vector” derivative of g with respect to\\nx, we obtain the matrix of second-order partial derivatives of f :\\nHf (x) := ∂g\\n∂x =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2 f\\n∂2 x1\\n∂2 f\\n∂x1∂x2\\n··· ∂2 f\\n∂x1∂xm\\n∂2 f\\n∂x2∂x1\\n∂2 f\\n∂2 x2\\n··· ∂2 f\\n∂x2∂xm\\n... ... ··· ...\\n∂2 f\\n∂xm∂x1\\n∂2 f\\n∂xm∂x2\\n··· ∂2 f\\n∂2 xm\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n, (B.4)\\nwhich is known as the Hessian matrixHessian matrix of f at x, also denoted as ∇2 f (x). If these second-\\norder partial derivatives arecontinuous in a region around x, then ∂f\\n∂xi∂xj\\n= ∂f\\n∂xj∂xi\\nand, hence,\\nthe Hessian matrix Hf (x) is symmetric.\\nFinally, note that we can also define a “scalar /matrix” derivative of y with respect to\\nX ∈Rm×n with (i, j)-th entry xi j:\\n∂y\\n∂X :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂y\\n∂x11\\n∂y\\n∂x12\\n··· ∂y\\n∂x1n\\n∂y\\n∂x21\\n∂y\\n∂x22\\n··· ∂y\\n∂x2n\\n... ... ··· ...\\n∂y\\n∂xm1\\n∂y\\n∂xm2\\n··· ∂y\\n∂xmn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand a “matrix/scalar” derivative:\\n∂X\\n∂y :=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂x11\\n∂y\\n∂x12\\n∂y ··· ∂x1n\\n∂y\\n∂x21\\n∂y\\n∂x22\\n∂y ··· ∂x2n\\n∂y\\n... ... ··· ...\\n∂xm1\\n∂y\\n∂xm2\\n∂y ··· ∂xmn\\n∂y\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nExample B.1 (Scalar/Matrix Derivative) Let y = a⊤Xb, where X ∈Rm×n, a ∈Rm,\\nand b ∈Rn. Since y is a scalar, we can writey = tr(y) = tr(Xba⊤), using the cyclic property\\nof the trace (see Theorem A.1). Defining C := ba⊤, we have☞357\\ny =\\nmX\\ni=1\\n[XC]ii =\\nmX\\ni=1\\nnX\\nj=1\\nxi jcji,\\nso that ∂y/∂xi j = cji or, in matrix form,\\n∂y\\n∂X = C⊤= ab⊤.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 416, 'page_label': '399'}, page_content='Multivariate Differentiation and Optimization 399\\nExample B.2 (Scalar/Matrix Derivative via the Woodbury Identity) Let y = tr\\n\\x10\\nX−1A\\n\\x11\\n,\\nwhere X,A ∈Rn×n. We now prove that\\n∂y\\n∂X = −X−⊤A⊤X−⊤.\\nTo show this, apply the Woodbury matrix identity to an infinitesimal perturbation,X + εU,\\nof X, and take ε↓0 to obtain the following: ☞ 371\\n(X + εU)−1 −X−1\\nε = −X−1 U (I + εX−1 U)−1 X−1 −→−X−1 U X−1.\\nTherefore, as ε↓0\\ntr\\n\\x10\\n(X + εU)−1A\\n\\x11\\n−tr\\n\\x10\\nX−1A\\n\\x11\\nε −→−tr\\n\\x10\\nX−1U X−1A\\n\\x11\\n= −tr\\n\\x10\\nU X−1AX−1\\x11\\n.\\nNow, suppose that U is an all zero matrix with a one in the (i, j)-th position. We can write,\\n∂y\\n∂xi j\\n= lim\\nε↓0\\ntr\\n\\x10\\n(X + εU)−1A\\n\\x11\\n−tr\\n\\x10\\nX−1A\\n\\x11\\nε = −tr\\n\\x10\\nUX−1AX−1\\x11\\n= −\\nh\\nX−1AX−1i\\nji .\\nTherefore, ∂y\\n∂X = −\\n\\x10\\nX−1AX−1\\n\\x11⊤\\n.\\nThe following two examples specify multivariate derivatives for the important special\\ncases of linear and quadratic functions.\\nExample B.3 (Gradient of a Linear Function) Let f(x) = Ax for some m×n constant\\nmatrix A. Then, its vector/vector derivative (B.2) is the matrix\\n∂f\\n∂x = A⊤. (B.5)\\nTo see this, let ai j denote the (i, j)-th element of A, so that\\nf(x) = Ax =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPn\\nk=1 a1k xk\\n...Pn\\nk=1 amk xk\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nTo find the (j,i)-th element of ∂f\\n∂x , we differentiate the i-th element of f with respect to xj:\\n∂fi\\n∂xj\\n= ∂\\n∂xj\\nnX\\nk=1\\naik xk = ai j.\\nIn other words, the (i, j)-th element of ∂f\\n∂x is aji, the (i, j)-th element of A⊤.\\nExample B.4 (Gradient and Hessian of a Quadratic Function) Let f (x) = x⊤Ax for\\nsome n ×n constant matrix A. Then,\\n∇f (x) = (A + A⊤)x. (B.6)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 417, 'page_label': '400'}, page_content='400 Multivariate Differentiation\\nIt follows immediately that if A is symmetric, that is, A = A⊤, then ∇(x⊤Ax) = 2Ax and\\n∇2 (x⊤Ax) = 2A.\\nTo prove (B.6), first observe thatf (x) = x⊤Ax = Pn\\ni=1\\nPn\\nj=1 ai j xi xj, which is a quadratic\\nform in x, is real-valued, with\\n∂f\\n∂xk\\n= ∂\\n∂xk\\nnX\\ni=1\\nnX\\nj=1\\nai jxi xj =\\nnX\\nj=1\\nak jxj +\\nnX\\ni=1\\naik xi.\\nThe first term on the right-hand side is equal to thek-th element of Ax, whereas the second\\nterm equals the k-th element of x⊤A,or equivalently the k-th element of A⊤x.\\nB.1.1 Taylor Expansion\\nThe matrix of Jacobi and the Hessian matrix feature prominently in multidimensional\\nTaylor expansions.\\nTheorem B.1: Multidimensional Taylor Expansions\\nLet Xbe an open subset of Rn and let a ∈X. If f : X→ Ris a continuously twice\\ndifferentiable function with Jacobian matrix Jf (x) and Hessian matrix Hf (x), then\\nfor every x ∈X we have the following first- and second-order Taylor expansions:\\nf (x) = f (a) + Jf (a) (x −a) + O(∥x −a∥2) (B.7)\\nand\\nf (x) = f (a) + Jf (a) (x −a) + 1\\n2(x −a)⊤Hf (a) (x −a) + O(∥x −a∥3) (B.8)\\nas ∥x −a∥→ 0. By dropping the Oremainder terms, one obtains the corresponding\\nTaylor approximations.\\nThe result is essentially saying that a smooth enough function behaves locally (in the\\nneighborhood of a point x) like a linear and quadratic function. Thus, the gradient or Hes-\\nsian of an approximating linear or quadratic function is a basic building block of many\\napproximation and optimization algorithms.\\nRemark B.1 (Version Without Remainder Terms) An alternative version of Taylor’s\\ntheorem states that there exists an a′ that lies on the line segment between x and a such\\nthat (B.7) and (B.8) hold without remainder terms, with Jf (a) in (B.7) replaced by Jf (a′)\\nand Hf (a) in (B.8) replaced by Hf (a′).\\nB.1.2 Chain Rule\\nConsider the functions f : Rk →Rm and g : Rm →Rn. The function x 7→g( f(x)) is called\\nthe compositioncomposition of g and f, written as g ◦f, and is a function from Rk to Rn. Suppose\\ny = f(x) and z = g(y), as in Figure B.1. Let Jf (x) and Jg(y) be the (Fréchet) derivatives\\nof f (at x) and g (at y), respectively. We may think of Jf (x) as the matrix that describes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 418, 'page_label': '401'}, page_content='Multivariate Differentiation and Optimization 401\\nhow, in a neighborhood of x, the function f can be approximated by a linear function:\\nf(x + h) ≈f(x) + Jf (x)h, and similarly for Jg(y). The well-known chain rule chain ruleof calculus\\nsimply states that the derivative of the composition g ◦f is the matrix product of the\\nderivatives of g and f; that is,\\nJg◦f (x) = Jg(y) Jf (x).\\ng ◦ f\\nx y z\\nfg\\nRnRmRk\\nFigure B.1: Function composition. The blue arrows symbolize the linear mappings.\\nIn terms of our vector/vector derivative notation, we have\\n\"∂z\\n∂x\\n#⊤\\n=\\n\"∂z\\n∂y\\n#⊤\"∂y\\n∂x\\n#⊤\\nor, more simply,\\n∂z\\n∂x = ∂y\\n∂x\\n∂z\\n∂y. (B.9)\\nIn a similar way we can establish a scalar/matrix chain rule. In particular, suppose X is\\nan n ×p matrix, which is mapped to y := Xαfor a fixed p-dimensional vector α. In turn, y\\nis mapped to a scalarz := g(y) for some functiong. Denote the columns ofX by x1,..., xp.\\nThen,\\ny = Xα=\\npX\\nj=1\\nαj xj,\\nand, therefore, ∂y/∂xj = αjIn. It follows by the chain rule (B.9) that\\n∂z\\n∂xi\\n= ∂y\\n∂xi\\n∂z\\n∂y = αiIn\\n∂z\\n∂y = αi\\n∂z\\n∂y.\\nTherefore,\\n∂z\\n∂X =\\nh ∂z\\n∂x1\\n,..., ∂z\\n∂xp\\ni\\n=\\nh\\nα1\\n∂z\\n∂y ,...,α p\\n∂z\\n∂y\\ni\\n= ∂z\\n∂y α⊤. (B.10)\\nExample B.5 (Derivative of the Log-Determinant) Suppose we are given a positive\\ndefinite matrix A ∈Rp×p and wish to compute the scalar/matrix derivative ∂ln |A|\\n∂A . The result\\nis ∂ln |A|\\n∂A = A−1.\\nTo see this, we can reason as follows. By Theorem A.8, we can write A = Q D Q⊤, where ☞ 366'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 419, 'page_label': '402'}, page_content='402 Optimization Theory\\nQ is an orthogonal matrix andD = diag(λ1,...,λ p) is the diagonal matrix of eigenvalues of\\nA. The eigenvalues are strictly positive, since A is positive definite. Denoting the columns\\nof Q by (qi), we have\\nλi = q⊤\\ni Aqi = tr \\x00qiAq⊤\\ni\\n\\x01, i = 1,..., p. (B.11)\\nFrom the properties of determinants, we havey := ln |A|= ln |Q D Q⊤|= ln(|Q||D||Q⊤|) =\\nln |D|= Pp\\ni=1 ln λi. We can thus write\\n∂ln |A|\\n∂A =\\npX\\ni=1\\n∂ln λi\\n∂A =\\npX\\ni=1\\n∂λi\\n∂A\\n∂ln λi\\n∂λi\\n=\\npX\\ni=1\\n∂λi\\n∂A\\n1\\nλi\\n,\\nwhere the second equation follows from the chain rule applied to the function composition\\nA 7→λi 7→y. From (B.11) and Example B.1 we have ∂λi/∂A = qi q⊤\\ni . It follows that\\n∂y\\n∂A =\\npX\\ni=1\\nqi q⊤\\ni\\n1\\nλi\\n= Q D−1 Q⊤= A−1.\\nB.2 Optimization Theory\\nOptimization is concerned with finding minimal or maximal solutions of a real-valued\\nobjective functionobjective\\nfunction\\nf in some set X:\\nmin\\nx∈X\\nf (x) or max\\nx∈X\\nf (x). (B.12)\\nSince any maximization problem can easily be converted into a minimization problem via\\nthe equivalence maxx f (x) ≡−minx −f (x), we focus only on minimization problems. We\\nuse the following terminology. A local minimizerlocal minimizer of f (x) is an element x∗∈X such that\\nf (x∗) ⩽f (x) for all x in some neighborhood of x∗. If f (x∗) ⩽f (x) for all x ∈X, then x∗is\\ncalled a global minimizerglobal\\nminimizer\\nor global solution. The set of global minimizers is denoted by\\nargmin\\nx∈X\\nf (x).\\nThe function value f (x∗) corresponding to a local/global minimizer x∗is referred to as the\\nlocal/global minimumlocal/global\\nminimum\\nof f (x).\\nOptimization problems may be classified by the set Xand the objective function f .\\nIf Xis countable, the optimization problem is called discrete or combinatorial. If instead\\nXis a nondenumerable set such as Rn and f takes values in a nondenumerable set, then\\nthe problem is said to be continuous. Optimization problems that are neither discrete nor\\ncontinuous are said to be mixed.\\nThe search set Xis often defined by means of constraints. A standard setting for con-\\nstrained optimization (minimization) is the following:\\nmin\\nx∈Y\\nf (x)\\nsubject to: hi(x) = 0, i = 1,..., m,\\ngi(x) ⩽0, i = 1,..., k.\\n(B.13)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 420, 'page_label': '403'}, page_content='Multivariate Differentiation and Optimization 403\\nHere, f is the objective function, and {gi}and {hi}are given functions so that hi(x) = 0\\nand gi(x) ⩽0 represent the equality and inequality constraints, respectively. The region\\nX⊆Y where the objective function is defined and where all the constraints are satisfied\\nis called the feasible region feasible region. An optimization problem without constraints is said to be an\\nunconstrained problem.\\nFor an unconstrained continuous optimization problem, the search space Xis often\\ntaken to be (a subset of) Rn, and f is assumed to be a Ck function for su fficiently high\\nk (typically k = 2 or 3 su ffices); that is, its k-th order derivative is continuous. For a C1\\nfunction the standard approach to minimizing f (x) is to solve the equation\\n∇f (x) = 0, (B.14)\\nwhere ∇f (x) is the gradient of f at x. The solutions x∗ to (B.14) are called station- ☞ 397\\nary points stationary\\npoints\\n. Stationary points can be local /global minimizers, local /global maximizers, or\\nsaddle points (which are neither). If, in addition, the function is C2, the condition saddle points\\ny⊤(∇2 f (x∗)) y >0 for all y , 0 (B.15)\\nensures that the stationary point x∗ is a local minimizer of f . The condition (B.15) states\\nthat the Hessian matrix of f at x∗is positive definite. Recall that we writeH ≻0 to indicate ☞ 398\\nthat a matrix H is positive definite.\\nIn Figure B.2 we have a multiextremal objective function on X= R. There are four\\nstationary points: two are local minimizers, one is a local maximizer, and one is neither a\\nminimizer nor a maximizer, but a saddle point.\\nx\\nLocal minimum\\nLocal maximum\\nGlobal minimum\\nSaddle point\\nf(x)\\nFigure B.2: A multiextremal objective function in one dimension.\\nB.2.1 Convexity and Optimization\\nAn important class of optimization problems is related to the notion of convexity. A set X\\nis said to be convex if for all x1,x2 ∈X it holds that αx1 + (1 −α) x2 ∈X for all 0 ⩽α⩽1.\\nIn addition, the objective function f is a convex function convex\\nfunction\\nprovided that for each x in the\\ninterior of Xthere exists a vector v such that\\nf (y) ⩾f (x) + (y −x)⊤v, y ∈X. (B.16)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 421, 'page_label': '404'}, page_content='404 Optimization Theory\\nThe vector v in (B.16) may not be unique and is referred to as a subgradient of f .subgradient\\nOne of the crucial properties of a convex function f is that Jensen’s inequality holds\\n(see Exercise 14 in Chapter 2):☞63\\nEf (X) ⩾f (EX),\\nfor any random vector X.\\nExample B.6 (Convexity and Directional Derivative) The directional derivativedirectional\\nderivative\\nof a\\nmultivariate function f at x in the direction d is defined as the right derivative of g(t) : =\\nf (x + t d) at t = 0:\\nlim\\nt↓0\\nf (x + t d) −f (x)\\nt = lim\\nt↑∞\\nt ( f (x + d/t) −f (x)).\\nThis right derivative may not always exist. However, if f is a convex function, then the\\ndirectional derivative of f at x in the interior of its domain always exists (in any direction\\nd).\\nTo see this, lett1 ⩾t2 >0. By Jensen’s inequality we have for anyx and y in the interior\\nof the domain:\\nt2\\nt1\\nf (y) +\\n \\n1 −t2\\nt1\\n!\\nf (x) ⩾f\\n t2\\nt1\\ny +\\n \\n1 −t2\\nt1\\n!\\nx\\n!\\n.\\nMaking the substitution y = x + t1 d and rearranging the last equation yields:\\nf (x + t1 d) −f (x)\\nt1\\n⩾f (x + t2 d) −f (x)\\nt2\\n.\\nIn other words, the function t 7→( f (x + t d) −f (x))/t is increasing for t >0 and therefore\\nthe directional derivative satisfies:\\nlim\\nt↓0\\nf (x + t d) −f (x)\\nt = inf\\nt>0\\nf (x + t d) −f (x)\\nt .\\nHence, to show existence it is enough to show that ( f (x + t d) −f (x))/t is bounded from\\nbelow.\\nSince x lies in the interior of the domain of f , we can choose t small enough so that\\nx + t d also lies in the interior. Therefore, the convexity of f implies that there exists a\\nsubgradient vector v such that f (x + t d) ⩾f (x) + v⊤(t d). In other words,\\nf (x + t d) −f (x)\\nt ⩾v⊤d\\nprovides a lower bound for all t >0, and the directional derivative of f at an interior x\\nalways exists (in any direction).\\nA function f satisfying (B.16) with strict inequality is said to be strictly convex. It is\\nsaid to be a (strictly) concave functionconcave\\nfunction\\nif −f is (strictly) convex. Assuming that Xis an\\nopen set, convexity for f ∈C1 is equivalent to\\nf (y) ⩾f (x) + (y −x)⊤∇f (x) for all x,y ∈X.\\nMoreover, for f ∈C2 strict convexity is equivalent to the Hessian matrix being positive\\ndefinite for all x ∈X, and convexity is equivalent to the Hessian matrix being positive\\nsemidefinite for all x; that is, y⊤\\n\\x10\\n∇2 f (x)\\n\\x11\\ny ⩾0 for all y and x. Recall that we write H ⪰0\\nto indicate that a matrix H is positive semidefinite.☞367'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 422, 'page_label': '405'}, page_content='Multivariate Differentiation and Optimization 405\\nExample B.7 (Convexity and Differentiability) If f is a continuously di fferentiable\\nmultivariate function, then f is convex if and only if the univariate function\\ng(t) := f (x + t d), t ∈[0,1]\\nis a convex function for any x and x + d in the interior of the domain of f . This property\\nprovides an alternative definition for convexity of a multivariate and differentiable function.\\nTo see why it is true, first assume that f is convex and t1,t2 ∈[0,1]. Then, using the\\nsubgradient definition of convexity in (B.16), we have f (a) ⩾f (b) + (a −b)⊤v for some\\nsubgradient v. Substituting with a = x + t1 d and b = x + t2 d, we obtain\\ng(t1) ⩾g(t2) + (t1 −t2) v⊤d\\nfor any two points t1,t2 ∈[0,1]. Therefore, g is convex, because we have identified the\\nexistence of a subgradient v⊤d for each t2.\\nConversely, assume thatg is convex fort ∈[0,1]. Since f is differentiable, then so is g.\\nThen, the convexity ofg implies that there is a subgradientv at 0 such that: g(t) ⩾g(0) +t v\\nfor all t ∈[0,1]. Rearranging,\\nv ⩾g(t) −g(0)\\nt ,\\nand taking the right limit as t ↓0 we obtain v ⩾g′(0) = d⊤∇f (x).Therefore,\\ng(t) ⩾g(0) + t v ⩾g(0) + t d⊤∇f (x)\\nand substituting t = 1 yields:\\nf (x + d) ⩾f (x) + d⊤∇f (x),\\nso that there exists a subgradient vector, namely ∇f (x), for each x. Hence, f is convex by\\nthe definition in (B.16).\\nAn optimization program of the form (B.13) is said to be a convex programming prob-\\nlem if: convex\\nprogramming\\nproblem1. The objective f is a convex function.\\n2. The inequality constraint functions {gi}are convex.\\n3. The equality constraint functions {hi}are affine, that is, of the form a⊤\\ni x −bi. This is\\nequivalent to both hi and −hi being convex for all i.\\nTable B.1 summarizes some commonly encountered problems, all of which are convex,\\nwith the exception of the quadratic programs with A ⪰̸0.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 423, 'page_label': '406'}, page_content='406 Optimization Theory\\nTable B.1: Some common classes of optimization problems.\\nName f (x) Constraints\\nLinear Program (LP) c⊤x Ax = b and x ⩾0\\nInequality Form LP c⊤x Ax ⩽b\\nQuadratic Program (QP) 1\\n2 x⊤Ax + b⊤x Dx ⩽d, Ex = e\\nConvex QP 1\\n2 x⊤Ax + b⊤x Dx ⩽d, Ex = e (A ⪰0)\\nConvex Program f (x) convex {gi(x)}convex, {hi(x)}of the form a⊤\\ni x −bi\\nRecognizing convex optimization problems or those that can be transformed to convex\\noptimization problems can be challenging. However, once formulated as convex optimiz-\\nation problems, these can be e fficiently solved using subgradient [112], bundle [57], and\\ncutting-plane methods [59].\\nB.2.2 Lagrangian Method\\nThe main components of the Lagrangian method are the Lagrange multipliers and the\\nLagrange function. The method was developed by Lagrange in 1797 for the optimization\\nproblem (B.13) with only equality constraints. In 1951 Kuhn and Tucker extended Lag-\\nrange’s method to inequality constraints. Given an optimization problem (B.13) containing\\nonly equality constraints hi(x) = 0, i = 1,..., m, the Lagrange functionLagrange\\nfunction\\nis defined as\\nL(x,β) = f (x) +\\nmX\\ni=1\\nβi hi(x),\\nwhere the coefficients {βi}are called the Lagrange multipliersLagrange\\nmultipliers\\n. A necessary condition for a\\npoint x∗ to be a local minimizer of f (x) subject to the equality constraints hi(x) = 0, i =\\n1,..., m, is\\n∇x L(x∗,β∗) = 0,\\n∇βL(x∗,β∗) = 0,\\nfor some value β∗. The above conditions are also sufficient if L(x,β∗) is a convex function\\nof x.\\nGiven the original optimization problem (B.13), containing both the equality and in-\\nequality constraints, the generalized Lagrange function, or LagrangianLagrangian , is defined as\\nL(x,α,β) = f (x) +\\nkX\\ni=1\\nαi gi(x) +\\nmX\\ni=1\\nβi hi(x).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 424, 'page_label': '407'}, page_content='Multivariate Differentiation and Optimization 407\\nTheorem B.2: Karush–Kuhn–Tucker (KKT) Conditions\\nA necessary condition for a point x∗to be a local minimizer of f (x) in the optimiz-\\nation problem (B.13) is the existence of an α∗and β∗such that\\n∇x L(x∗,α∗,β∗) = 0,\\n∇βL(x∗,α∗,β∗) = 0,\\ngi(x∗) ⩽0, i = 1,..., k,\\nα∗\\ni ⩾0, i = 1,..., k,\\nα∗\\ni gi(x∗) = 0, i = 1,..., k.\\nFor convex programs we have the following important results [18, 43]:\\n1. Every local solution x∗ to a convex programming problem is a global solution and\\nthe set of global solutions is convex. If, in addition, the objective function is strictly\\nconvex, then any global solution is unique.\\n2. For a strictly convex programming problem with C1 objective and constraint func-\\ntions, the KKT conditions are necessary and sufficient for a unique global solution.\\nB.2.3 Duality\\nThe aim of duality is to provide an alternative formulation of an optimization problem\\nwhich is often more computationally efficient or has some theoretical significance (see [43,\\nPage 219]). The original problem (B.13) is referred to as the primal primalproblem whereas the\\nreformulated problem, based on Lagrange multipliers, is called the dual dualproblem. Duality\\ntheory is most relevant to convex optimization problems. It is well known that if the primal\\noptimization problem is (strictly) convex then the dual problem is (strictly) concave and\\nhas a (unique) solution from which the (unique) optimal primal solution can be deduced.\\nThe Lagrange dual program Lagrange dual\\nprogram\\n(also called the Wolfe dual) of the primal program (B.13),\\nis:\\nmax\\nα,β\\nL∗(α,β)\\nsubject to: α⩾0,\\nwhere L∗is the Lagrange dual function:\\nL∗(α,β) = inf\\nx∈X\\nL(x,α,β), (B.17)\\ngiving the greatest lower bound (infimum) of L(x,α,β) over all possible x ∈X.\\nIt is not di fficult to see that if f ∗ is the minimal value of the primal problem, then\\nL∗(α,β) ⩽ f ∗ for any α ⩾0 and any β. This property is called weak duality. The Lag-\\nrangian dual program thus determines the best lower bound on f ∗. If d∗ is the optimal\\nvalue for the dual problem then d∗⩽f ∗. The difference f ∗−d∗is called the duality gap.\\nThe duality gap is extremely useful for providing lower bounds for the solutions of\\nprimal problems that may be impossible to solve directly. It is important to note that for'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 425, 'page_label': '408'}, page_content='408 Numerical Root-Finding and Minimization\\nlinearly constrained problems, if the primal is infeasible (does not have a solution satisfying\\nthe constraints), then the dual is either infeasible or unbounded. Conversely, if the dual\\nis infeasible then the primal has no solution. Of crucial importance is the strong dualitystrong duality\\ntheorem, which states that for convex programs (B.13) with linear constrained functionshi\\nand gi the duality gap is zero, and any x∗ and (α∗,β∗) satisfying the KKT conditions are\\n(global) solutions to the primal and dual programs, respectively. In particular, this holds for\\nlinear and convex quadratic programs (note that not all quadratic programs are convex).\\nFor a convex primal program withC1 objective and constraint functions, the Lagrangian\\ndual function (B.17) can be obtained by simply setting the gradient (with respect to x) of\\nthe Lagrangian L(x,α,β) to zero. One can further simplify the dual program by substitut-\\ning into the Lagrangian the relations between the variables thus obtained.\\nFurther, for a convex primal problem, if there is a strictly feasible point ex (that is, a\\nfeasible point satisfying all of the inequality constraints with strict inequality), then the\\nduality gap is zero, and strong duality holds. This is known as Slater’s condition [18, Page\\n226].\\nThe Lagrange dual problem is an important example of asaddle-point problem or min-\\nimax problem. In such problems the aim is to locate a point (x∗,y∗) ∈X×Y that satisfies\\nsup\\ny∈Y\\ninf\\nx∈X\\nf (x,y) = inf\\nx∈X\\nf (x,y∗) = f (x∗,y∗) = sup\\ny∈Y\\nf (x∗,y) = inf\\nx∈X\\nsup\\ny∈Y\\nf (x,y).\\nThe equation\\nsup\\ny∈Y\\ninf\\nx∈X\\nf (x,y) = inf\\nx∈X\\nsup\\ny∈Y\\nf (x,y)\\nis known as the minimaxminimax equality. Other problems that fall into this framework are zero-\\nsum games in game theory; see also [24] for a number of combinatorial optimization prob-\\nlems that can be viewed as minimax problems.\\nB.3 Numerical Root-Finding and Minimization\\nIn order to minimize a C1 function f : Rn →Rone may solve\\n∇f (x) = 0,\\nwhich gives a stationary point of f . As a consequence, any technique for root-finding can\\nbe transformed into an unconstrained optimization method by attempting to locate roots\\nof the gradient. However, as noted in Section B.2, not all stationary points are minima,\\nand so additional information (such as is contained in the Hessian, if f is C2) needs to be\\nconsidered in order to establish the type of stationary point.\\nAlternatively, a root of a continuous functiong : Rn →Rn may be found by minimizing\\nthe norm of g(x) over all x; that is, by solving minx f (x), with f (x) := ∥g(x)∥p, where for\\np ⩾1 the p-normp-norm of y = [y1,..., yn]⊤is defined as\\n∥y∥p :=\\n\\x12 nX\\ni=1\\n|yi|p\\n\\x131/p\\n.\\nHence, any (un)constrained optimization method can be transformed into a technique for\\nlocating the roots of a function.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 426, 'page_label': '409'}, page_content='Multivariate Differentiation and Optimization 409\\nStarting with an initial guess x0, most minimization and root-finding algorithms create\\na sequence x0,x1,... using the iterative updating rule:\\nxt+1 = xt + αt dt, t = 0,1,2,..., (B.18)\\nwhere αt > 0 is a (typically small) step size, called the learning rate learning rate, and the vector dt\\nis the search direction at step t. The iteration (B.18) continues until the sequence {xt}is\\ndeemed to have converged to a solution, or a computational budget has been exhausted.\\nThe performance of all such iterative methods depends crucially on the quality of the initial\\nguess x0.\\nThere are two broad categories of iterative optimization algorithms of the form (B.18):\\n• Those of line search line searchtype, where at iteration t we first compute a direction dt and\\nthen determine a reasonable step size αt along this direction. For example, in the\\ncase of minimization, αt >0 may be chosen to approximately minimize f (xt + αdt)\\nfor fixed xt and dt.\\n• Those of trust region trust regiontype, where at each iteration t we first determine a suitable step\\nsize αt and then compute an approximately optimal direction dt.\\nIn the following sections, we review several widely-used root-finding and optimization\\nalgorithms of the line search type.\\nB.3.1 Newton-Like Methods\\nSuppose we wish to find roots of a function f : Rn →Rn. If f is in C1, we can approximate\\nf around a point xt as\\nf(x) ≈f(xt) + Jf (xt)(x −xt),\\nwhere Jf is the matrix of Jacobi — the matrix of partial derivatives of f; see (B.3). When ☞ 398\\nJf (xt) is invertible, this linear approximation has root xt −J−1\\nf (xt) f(xt). This gives the\\niterative updating formula (B.18) for finding roots of f with direction dt = −J−1\\nf (xt) f(xt)\\nand learning rate αt = 1. This is known as Newton’s method Newton’s\\nmethod\\n(or the Newton–Raphson\\nmethod) for root-finding.\\nInstead of a unit learning rate, sometimes it is more e ffective to use an αt that satisfies\\nthe Armijo inexact line search Armijo inexact\\nline search\\ncondition:\\n∥f(xt + αt dt)∥<(1 −ε1 αt) ∥f(xt)∥,\\nwhere ε1 is a small heuristically chosen constant, say ε1 = 10−4. For C1 functions, such an\\nαt always exists by continuity and can be computed as in the following algorithm.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 427, 'page_label': '410'}, page_content='410 Numerical Root-Finding and Minimization\\nAlgorithm B.3.1: Newton–Raphson for Finding Roots of f(x) = 0\\ninput: An initial guess x and stopping error ε> 0.\\noutput: The approximate root of f(x) = 0.\\n1 while ∥f(x)∥>ε and budget is not exhausted do\\n2 Solve the linear system Jf (x) d = −f(x).\\n3 α←1\\n4 while ∥f(x + αd)∥>(1 −10−4α) ∥f(x)∥do\\n5 α←α/2\\n6 x ←x + αd\\n7 return x\\nWe can adapt a root-finding Newton-like method in order to minimize a di fferentiable\\nfunction f : Rn →R. We simply try to locate a zero of the gradient of f . When f is a\\nC2 function, the function ∇f : Rn →Rn is continuous, and so the root of ∇f leads to the\\nsearch direction\\ndt = −H−1\\nt ∇f (xt), (B.19)\\nwhere Ht is the Hessian matrix at xt (the matrix of Jacobi of the gradient is the Hessian).\\nWhen the learning rate αt is equal to 1, the update xt −H−1\\nt ∇f (xt) can alternatively be\\nderived by assuming that f (x) is approximately quadratic and convex in the neighborhood\\nof xt, that is,\\nf (x) ≈f (xt) + (x −xt)⊤∇f (xt) + 1\\n2(x −xt)⊤Ht(x −xt), (B.20)\\nand then minimizing the right-hand side of (B.20) with respect to x.\\nThe following algorithm uses an Armijo inexact line search for minimization and\\nguards against the possibility that the Hessian may not be positive definite (that is, its\\nCholesky decomposition does not exist).☞373\\nAlgorithm B.3.2: Newton–Raphson for Minimizing f (x)\\ninput: An initial guess x; stopping error ε> 0; line search parameter ξ∈(0,1).\\noutput: An approximate minimizer of f (x).\\n1 L ←In (the identity matrix)\\n2 while ∥∇f (x)∥>ε and budget is not exhausted do\\n3 Compute the Hessian H at x.\\n4 if H ≻0 then // Cholesky is successful\\n5 Update L to be the Cholesky factor satisfying LL⊤= H.\\n6 else\\n7 Do not update the lower triangular L.\\n8 d ←−L−1∇f (x) (computed by forward substitution)\\n9 d ←L−⊤d (computed by backward substitution)\\n10 α←1\\n11 while f (x + αd) > f (x) + α10−4∇f (x)⊤d do\\n12 α←α×ξ\\n13 x ←x + αd\\n14 return x'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 428, 'page_label': '411'}, page_content='Multivariate Differentiation and Optimization 411\\nA downside with all Newton-like methods is that at each step they require the calcu-\\nlation and inversion of an n ×n Hessian matrix, which has computing time of O(n3), and\\nis thus infeasible for large n. One way to avoid this cost is to use quasi-Newton methods,\\ndescribed next.\\nB.3.2 Quasi-Newton Methods\\nThe idea behind quasi-Newton methods is to replace the inverse Hessian in (B.19) at iter-\\nation t by an n ×n matrix C satisfying the secant condition secant\\ncondition\\n:\\nC 1= δ, (B.21)\\nwhere δ←xt −xt−1 and 1←∇f (xt) −∇f (xt−1) are vectors stored in memory at each iter-\\nation t. The secant condition is satisfied, for example, by the Broyden’s family of matrices:\\nA + 1\\nu⊤1(δ−A 1) u⊤\\nfor some u , 0 and A. Since there is an infinite number of matrices that satisfy the condi-\\ntion (B.21), we need a way to determine a uniqueC at each iteration t such that computing\\nand storing C from one step to the next is fast and avoids any costly matrix inversion. The\\nfollowing examples illustrate how, starting with an initial guess C = I at t = 0, such a\\nmatrix C can be efficiently updated from one iteration to the next.\\nExample B.8 (Low-Rank Hessian Update) The quadratic model (B.20) can be\\nstrengthened by further assuming that exp( −f (x)) is proportional to a probability density\\nthat can be approximated in the neighborhood of xt by the pdf of the N(xt+1,H−1\\nt ) dis-\\ntribution. This normal approximation allows us to measure the discrepancy between two\\npairs (x1,H0) and (x2,H1) using the Kullback–Leibler divergence between the pdfs of the ☞ 42\\nN(x1,H−1\\n0 ) and N(x2,H−1\\n1 ) distributions (see Exercise 4 on page 351):\\nD(x1,H−1\\n0 |x2,H−1\\n1 ) := 1\\n2\\n\\x10\\ntr(H1H−1\\n0 ) −ln |H1H−1\\n0 |+ (x2 −x1)⊤H1(x2 −x1) −n\\n\\x11\\n. (B.22)\\nSuppose that the latest approximation to the inverse Hessian is C and we wish to com-\\npute an updated approximation for step t. One approach is to find the symmetric matrix\\nthat minimizes its Kullback–Leibler discrepancy from C, as defined above, subject to the\\nconstraint (B.21). In other words,\\nmin\\nA\\nD(0,C |0,A)\\nsubject to: A1= δ, A = A⊤.\\nThe solution to this constrained optimization (see Exercise 10 on page 353) yields the\\nBroyden–Fletcher–Goldfarb–Shanno or BFGS formula bfgs formulafor updating the matrixC from one\\niteration to the next:\\nCBFGS = C + 1⊤δ+ 1⊤C 1\\n(1⊤δ)2 δδ⊤− 1\\n1⊤δ\\n\\x00δ1⊤C + (δ1⊤C)⊤\\x01\\n|                                                     {z                                                     }\\nBFGS update\\n. (B.23)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 429, 'page_label': '412'}, page_content='412 Numerical Root-Finding and Minimization\\nIn a practical implementation, we keep a single copy of C in memory and apply the BFGS\\nupdate to it at every iteration. Note that if the currentC is symmetric, then so is the updated\\nmatrix. Moreover, the BFGS update is a matrix of rank two.\\nSince the Kullback–Leibler divergence is not symmetric, it is possible to flip the roles\\nof H0 and H1 in (B.22) and instead solve\\nmin\\nA\\nD(0,A |0,C)\\nsubject to: A1= δ, A = A⊤.\\nThe solution (see Exercise 10 on page 353) gives the Davidon–Fletcher–Powell or DFP\\nformuladfp formula for updating the matrix C from one iteration to the next:\\nCDFP = C + δδ⊤\\n1⊤δ −C 11⊤C\\n1⊤C1|             {z             }\\nDFP update\\n. (B.24)\\nNote that if the curvature condition 1⊤δ>0 holds and the current C is symmetric positive\\ndefinite, then so is its update.\\nExample B.9 (Diagonal Hessian Update) The original BFGS formula requires O(n2)\\nstorage and computation, which may be unmanageable for largen. One way to circumvent\\nthe prohibitive quadratic cost is to only store and update a diagonal Hessian matrix from\\none iteration to the next. If C is diagonal, then we may not be able to satisfy the secant\\ncondition (B.21) and maintain positive definiteness. Instead the secant condition (B.21)\\ncan be relaxed to the set of inequalities 1⩾C−1δ, which are related to the definition of a\\nsubgradient for convex functions. We can then find a unique diagonal matrix by minimizing☞403\\nD(xt,C |xt+1,A) with respect to A and subject to the constraints that A1⩾δ and A is\\ndiagonal. The solution (Exercise 15 on page 353) yields the updating formula for a diagonal\\nelement ci of C:\\nci ←\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n2ci\\n1 +\\nq\\n1 + 4ciu2\\ni\\n, if 2ci\\n1 +\\nq\\n1 + 4ciu2\\ni\\n⩾δi/1i\\nδi/1i, otherwise,\\n(B.25)\\nwhere u := ∇f (xt) and we assume a unit learning rate: xt+1 = xt −Au.\\nExample B.10 (Scalar Hessian Update) If the identity matrix is used in place of the\\nHessian in (B.19), one obtains steepest descentsteepest\\ndescent\\nor gradient descent methods, in which the\\niteration (B.18) reduces to xt+1 = xt −αt ∇f (xt).\\nThe rationale for the name steepest descent is as follows. If we start from any point\\nx and make an infinitesimal move in some direction, then the function value is reduced\\nby the largest magnitude in the (unit norm) direction: u∗:= −∇f (x)/∥∇f (x)∥. This is seen\\nfrom the following inequality for all unit vectors u (that is, ∥u∥= 1):\\nd\\ndt f (x + t u∗)\\n\\x0c\\x0c\\x0c\\x0ct=0\\n⩽d\\ndt f (x + t u)\\n\\x0c\\x0c\\x0c\\x0ct=0\\n.\\nObserve that equality is achieved if and only if u = u∗. This inequality is an easy con-\\nsequence of the Cauchy–Schwarz inequality:☞389'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 430, 'page_label': '413'}, page_content='Multivariate Differentiation and Optimization 413\\n−∇f ⊤u ⩽|∇f ⊤u| ⩽|{z}\\nCauchy–Schwartz\\n∥u∥∥∇f ∥= ∥∇f ∥= −∇f ⊤u∗.\\nThe steepest descent iteration, xt+1 = xt −αt∇f (xt), still requires a suitable choice of the\\nlearning rate αt. An alternative way to think about the iteration is to assume that the learning\\nrate is always unity, and that at each iteration we use an inverse Hessian matrix of the form\\nαtI for some positive constantαt. Satisfying the secant condition (B.21) with a matrix of the\\nform C = αI is not possible. However, it is possible to chooseαso that the secant condition\\n(B.21) is satisfied in the direction of1(or alternatively δ). This gives theBarzilai–Borwein\\nformulas Barzilai–\\nBorwein\\nformulas\\nfor the learning rate at iteration t:\\nαt = 1⊤δ\\n∥1∥2\\n \\nor alternatively αt = ∥δ∥2\\nδ⊤1\\n!\\n. (B.26)\\nB.3.3 Normal Approximation Method\\nLet φH−1\\nt\\n(x−xt+1) denote the pdf of theN(xt+1,H−1\\nt ) distribution. As we already saw in Ex-\\nample B.8, the quadratic approximation (B.20) of f in the neighborhood of xt is equivalent\\n(up to a constant) to the minus of the logarithm of the pdf φH−1\\nt\\n(x −xt+1). In other words,\\nwe use φH−1\\nt\\n(x −xt+1) as a simple model for the density\\nexp(−f (x))\\n.Z\\nexp(−f (y)) dy.\\nOne consequence of the normal approximation is that for x in the neighborhood of xt+1,\\nwe can write:\\n−∇f (x) ≈ ∂\\n∂x ln φH−1\\nt\\n(x −xt+1) = −Ht(x −xt+1).\\nIn other words, using the fact that H⊤\\nt = Ht,\\n∇f (x)[∇f (x)]⊤≈Ht(x −xt+1)(x −xt+1)⊤Ht,\\nand taking expectations on both sides with respect to X ∼N(xt+1,H−1\\nt ) gives:\\nE∇f (X) [∇f (X)]⊤≈Ht.\\nThis suggests that, given the gradient vectors computed in the past h (where h stands for\\nhistory) of Newton iterations:\\nui := ∇f (xi), i = t −(h −1),..., t,\\nthe Hessian matrix Ht can be approximated via the average\\n1\\nh\\ntX\\ni=t−h+1\\nuiu⊤\\ni .\\nA shortcoming of this approximation is that, unless h is large enough, the Hessian approx-\\nimation Pt\\ni=t−h+1 uiu⊤\\ni may not be full rank and hence not invertible. To ensure that the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 431, 'page_label': '414'}, page_content='414 Numerical Root-Finding and Minimization\\nHessian approximation is invertible, we add a suitable diagonal matrix A0 to obtain the\\nregularized version of the approximation:☞217\\nHt ≈A0 + 1\\nh\\ntX\\ni=t−h+1\\nuiu⊤\\ni .\\nWith this full-rank approximation of the Hessian, the Newton search direction in (B.19)\\nbecomes:\\ndt = −\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edA0 + 1\\nh\\ntX\\ni=t−h+1\\nuiu⊤\\ni\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n−1\\nut. (B.27)\\nThus, dt can be computed in O(h2 n) time via the Sherman–Morrison Algorithm A.6.1.☞373\\nFurther to this, the search direction (B.27) can be efficiently updated to the next one:\\ndt+1 = −\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edA0 + 1\\nh\\nt+1X\\ni=t−h+2\\nuiu⊤\\ni\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n−1\\nut+1\\nin O(h n) time, thus avoiding the usual O(h2 n) cost (see Exercise 6 on page 352).\\nB.3.4 Nonlinear Least Squares\\nConsider the squared-error training loss in nonlinear regression:☞188\\nℓτ(g(·|β)) = 1\\nn\\nnX\\ni=1\\n(g(xi |β) −yi)2,\\nwhere g(·|β) is a nonlinear prediction function that depends on the parameter β(for ex-\\nample, (5.29) shows the nonlinear logistic prediction function). The training loss can be\\nwritten as 1\\nn ∥g(τ|β) −y∥2, where g(τ|β) := [g(x1 |β),..., g(xn |β)]⊤ is the vector of out-\\nputs.\\nWe wish to minimize the training loss in terms of β. In the Newton-like methods in\\nSection B.3.1, one derives an iterative minimization algorithm that is inspired by a Taylor\\nexpansion of ℓτ(g(·|β)). Instead, given a current guess βt, we can consider the Taylor ex-\\npansion of the nonlinear prediction function g:\\ng(τ|β) ≈g(τ|βt) + Gt(β−βt),\\nwhere Gt := Jg(βt) is the matrix of Jacobi of g(τ|β) at βt. Denoting the residual et :=☞398\\ng(τ|βt) −y and replacing g(τ|β) with its Taylor approximation in ℓτ(g(·|β)), we obtain\\nthe approximation to the training loss in the neighborhood of βt:\\nℓτ(g(·|β)) ≈1\\nn\\n\\r\\r\\rGt(β−βt) + et\\n\\r\\r\\r\\n2\\n.\\nThe minimization of the right-hand side is a linear least-squares problem and therefore\\ndt := β−βt satisfies the normal equations: G⊤\\nt Gt dt = G⊤\\nt (−et). Assuming that G⊤\\nt Gt is☞28\\ninvertible, the normal equations yield the Gauss–NewtonGauss–Newton search direction:\\ndt = −(G⊤\\nt Gt)−1G⊤\\nt et.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 432, 'page_label': '415'}, page_content='Multivariate Differentiation and Optimization 415\\nUnlike the search direction (B.19) for Newton-like algorithms, the search direction of a\\nGauss–Newton algorithm does not require the computation of a Hessian matrix.\\nObserve that in the Gauss–Newton approach we determine dt by viewing the search\\ndirection as coefficients in a linear regression with feature matrixGt and response −et. This\\nsuggests that instead of using a linear regression, we can compute dt via a ridge regression\\nwith a suitable choice for the regularization parameter γ: ☞ 217\\ndt = −(G⊤\\nt Gt + nγIp)−1G⊤\\nt et.\\nIf we replace nIp with the diagonal matrix diag( G⊤\\nt Gt), we then obtain the Levenberg–\\nMarquardt Levenberg–\\nMarquardt\\nsearch direction:\\ndt = −(G⊤\\nt Gt + γdiag(G⊤\\nt Gt))−1G⊤\\nt et. (B.28)\\nRecall that the ridge regularization parameterγhas the following effect on the least-squares\\nsolution: When it is zero, then the solution dt coincides with the search direction of the\\nGauss–Newton method, and when γ tends to infinity, then ∥dt∥tends to zero. Thus, γ\\ncontrols both the magnitude and direction of vectordt. A simple version of the Levenberg–\\nMarquardt algorithm is the following.\\nAlgorithm B.3.3: Levenberg–Marquardt for Minimizing 1\\nn ∥g(τ|β) −y∥2\\ninput: An initial guess β0; stopping error ε> 0; training set τ.\\noutput: An approximate minimizer of 1\\nn ∥g(τ|β) −y∥2.\\n1 t ←0 and γ←0.01 (or another default value)\\n2 while stopping condition is not met do\\n3 Compute the search direction dt via (B.28).\\n4 et+1 ←g(τ|βt + dt) −y\\n5 if ∥et+1∥<∥et∥then\\n6 γ←γ/10, et+1 ←et, βt+1 ←βt + dt\\n7 else\\n8 γ←γ×10\\n9 t ←t + 1\\n10 return βt\\nB.4 Constrained Minimization via Penalty Functions\\nA constrained optimization problem of the form (B.13) can sometimes be reformulated as a\\nsimpler unconstrained problem — for example, the unconstrained setYcan be transformed\\nto the feasible region Xof the constrained problem via a function ϕ: Rn →Rn such that\\nX= ϕ(Y). Then, (B.13) is equivalent to the minimization problem\\nmin\\ny∈Y\\nf (ϕ(y)),\\nin the sense that a solution x∗ of the original problem is obtained from a transformed\\nsolution y∗via x∗= ϕ(y∗). Table B.2 lists some examples of possible transformations.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 433, 'page_label': '416'}, page_content='416 Constrained Minimization via Penalty Functions\\nTable B.2: Some transformations to eliminate constraints.\\nConstrained Unconstrained\\nx >0 exp( y)\\nx ⩾0 y2\\na ⩽x ⩽b a + (b −a) sin2(y)\\nUnfortunately, an unconstrained minimization method used in combination with these\\ntransformations is rarely effective. Instead, it is more common to use penalty functions.\\nThe overarching idea of penalty functionspenalty\\nfunctions\\nis to transform a constrained problem into\\nan unconstrained problem by adding weighted constraint-violation terms to the original\\nobjective function, with the premise that the new problem has a solution that is identical or\\nclose to the original one.\\nFor example, if there are only equality constraints, then\\nef (x) := f (x) +\\nmX\\ni=1\\nai |hi(x)|p\\nfor some constants a1,..., am > 0 and integer p ∈{1,2}, gives an exact penalty function,\\nin the sense that the minimizer of the penalized function ef is equal to the minimizer of f\\nsubject to the m equality constraints h1,..., hm. With the addition of inequality constraints,\\none could use\\nef (x) = f (x) +\\nmX\\ni=1\\nai |hi(x)|p +\\nkX\\nj=1\\nbj max{gj(x),0}\\nfor some constants a1,..., am,b1,..., bk >0.\\nExample B.11 (Alternating Direction Method of Multipliers) The Lagrange method\\nis designed to handle convex minimization subject to equality constraints. Nevertheless,☞406\\nsome practical algorithms may still use the penalty function approach in combination with\\nthe Lagrangian method. An example is the alternating direction method of multipliersalternating\\ndirection\\nmethod of\\nmultipliers\\n(ADMM) [17]. The ADMM solves problems of the form:\\nmin\\nx∈Rn ,z∈Rm\\nf (x) + g(z)\\nsubject to: Ax + Bz = c,\\n(B.29)\\nwhere A ∈Rp×n, B ∈Rp×m, and c ∈Rp, and f : Rn →Rand g : Rm →Rare convex func-\\ntions. The approach is to form an augmented Lagrangian\\nLϱ(x,z,β) := f (x) + g(z) + β⊤(Ax + Bz −c) + ϱ\\n2 ∥Ax + Bz −c∥2,\\nwhere ϱ> 0 is a penalty parameter, andβ∈Rp are dual variables. The ADMM then iterates\\nthrough updates of the following form:\\nx(t+1) = argmin\\nx∈Rn\\nLϱ(x,z(t),β(t))\\nz(t+1) = argmin\\nz∈Rm\\nLϱ(x(t+1),z,β(t))\\nβ(t+1) = β(t) + ϱ\\n\\x10\\nAx(t+1) + Bz(t+1) −c\\n\\x11\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 434, 'page_label': '417'}, page_content='Multivariate Differentiation and Optimization 417\\nSuppose that (B.13) has inequality constraints only. Barrier functions Barrier\\nfunctions\\nare an important ex-\\nample of penalty functions that can handle inequality constraints. The prototypical example\\nis a logarithmic barrier function which gives the unconstrained optimization:\\nef (x) = f (x) −ν\\nkX\\nj=1\\nln(−gj(x)), ν> 0,\\nsuch that the minimizer of ef tends to the minimizer of f as ν→0. Direct minimization\\nof ef via an unconstrained minimization algorithm is frequently too di fficult. Instead, it\\nis common to combine the logarithmic barrier function with the Lagrangian method as\\nfollows.\\nThe idea is to introducek nonnegative auxiliary orslack variables slack variabless1,..., sk that satisfy\\nthe equalities gj(x) + sj = 0 for all j. These equalities ensure that the inequality constraints\\nare maintained: gj(x) = −sj ⩽0 for all j. Then, instead of the unconstrained optimization\\nof ef , we consider the unconstrained optimization of the Lagrangian:\\nL(x,s,β) = f (x) −ν\\nkX\\nj=1\\nln sj +\\nkX\\nj=1\\nβj (gj(x) + sj), (B.30)\\nwhere ν> 0 and βare the Lagrange multipliers for the equalitiesgj(x)+sj = 0, j = 1,..., k.\\nObserve how the logarithmic barrier function keeps the slack variables positive. In\\naddition, while the optimization of ef is over n dimensions (recall that x ∈Rn), the optimiz-\\nation of the Lagrangian function Lis over n + 2k dimensions. Despite this enlargement of\\nthe search space with the variables s and β, the optimization of the Lagrangian Lis easier\\nin practice than the direct optimization of ef .\\nExample B.12 (Interior-Point Method for Nonnegativity) One of the simplest and\\nmost common constrained optimization problems can be formulated as the minimization\\nof f (x) subject to nonnegative x, that is: min x⩾0 f (x). In this case, the Lagrangian with\\nlogarithmic barrier (B.30) is:\\nL(x,s,β) = f (x) −ν\\nX\\nk\\nln sk + β⊤(s −x).\\nThe KKT conditions in Theorem B.2 are a necessary condition for a minimizer, and yield\\nthe nonlinear system for [x⊤,s⊤,β⊤]⊤∈R3n:\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∇f (x) −β\\n−ν/s + β\\ns −x\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = 0,\\nwhere ν/s is a shorthand notation for a column vector with components{ν/sj}. To solve this\\nsystem, we can use Newton’s method for root finding (see, for example, Algorithm B.3.1),\\nwhich requires a formula for the matrix of Jacobi of L. Here, this (3n) ×(3n) matrix is:\\nJL(x,s,β) =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nH O −I\\nO D I\\n−I I O\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\" H B\\nB⊤ E\\n#\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 435, 'page_label': '418'}, page_content='418 Constrained Minimization via Penalty Functions\\nwhere H is the n ×n Hessian of f at x; D := diag (ν/(s ⊙s)) is an n ×n diagonal matrix;\\nB := [O,−I] is an n ×(2n) matrix, and1\\nE :=\\n\"D I\\nI O\\n#\\n=\\n\"O I\\nI −D\\n#−1\\n.\\nFurther, we define\\nHν := (H −BE−1B⊤)−1 = (H + D)−1.\\nUsing this notation and applying the matrix blockwise inversion formula (A.14), we obtain☞371\\nthe inverse of the matrix of Jacobi:\\n\" H B\\nB⊤ E\\n#−1\\n=\\n\" Hν −HνBE−1\\n−E−1B⊤Hν E−1 + E−1B⊤HνBE−1\\n#\\n=\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nHν Hν −HνD\\nHν Hν I −HνD\\n−DHν I −DHν DHνD −D\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nTherefore, the search direction in Newton’s root-finding method is given by:\\n−J−1\\nL\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∇f (x) −β\\n−ν/s + β\\ns −x\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ndx\\ndx + x −s\\nν/s −β−D(dx + x −s)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\nwhere\\ndx := −(H + D)−1 h\\n∇f (x) −2ν/s + Dx\\ni\\n,\\nand we have assumed that H + D is a positive-definite matrix. If at any step of the iteration\\nthe matrix H + D fails to be positive-definite, then Newton’s root-finding algorithm may\\nfail to converge. Thus, any practical implementation will have to include a fail-safe feature\\nto guard against this possibility.\\nIn summary, for a given penalty parameter ν >0, we can locate the approximate non-\\nnegative minimizer of f using, for example, the version of the Newton–Raphson root-\\nfinding method given in Algorithm B.4.1.\\nIn practice, one needs to choose a su fficiently small value for ν, so that the output xν\\nof Algorithm B.4.1 is a good approximation to x∗ = argminx⩾0 f (x). Alternatively, one\\ncan create a decreasing sequence of penalty parameters ν1 > ν2 > ··· and compute the\\ncorresponding solutions xν1 ,xν2 ,... of the penalized problems. In the so-called interior-\\npoint method, a given xνi is used as an initial guess for computing xνi+1 and so on until theinterior-point\\nmethod approximation to the minimizer x∗= argminx⩾0 f (x) is deemed accurate.\\n1Here O is an n ×n matrix of zeros and I is the n ×n identity matrix.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 436, 'page_label': '419'}, page_content='Multivariate Differentiation and Optimization 419\\nAlgorithm B.4.1: Approximating x∗= argminx⩾0 f (x) with Logarithmic Barrier\\ninput: An initial guess x and stopping error ε> 0.\\noutput: The approximate nonnegative minimizer xν of f .\\n1 s ←x, β←ν/s, d x ←β\\n2 while ∥dx∥>ε and budget is not exhausted do\\n3 Compute the gradient u and the Hessian H of f at x.\\n4 s1 ←ν/s, s2 ←s1/s, w ←2s1 −u −s2 ⊙x\\n5 if (H + diag(s2)) ≻0 then // if Cholesky successful\\n6 Compute the Cholesky factor L satisfying LL⊤= H + diag(s2).\\n7 dx ←L−1w (computed by forward substitution)\\n8 dx ←L−⊤dx (computed by backward substitution)\\n9 else\\n10 dx ←w/s2 // if Cholesky fails, do steepest descent\\n11 ds ←dx + x −s, d β←s1 −β−s2 ⊙ds, α←1\\n12 while minj{sj + αdsj}<0 do\\n13 α←α/2 // ensure nonnegative slack variables\\n14 x ←x + αdx, s ←s + αds, β←β+ αdβ\\n15 return xν ←x\\nFurther Reading\\nFor an excellent introduction to convex optimization and Lagrangian duality see [18]. A\\nclassical text on optimization algorithms and, in particular, on quasi-Newton methods is\\n[43]. For more details on the alternating direction method of multipliers see [17].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 437, 'page_label': '420'}, page_content='420'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 438, 'page_label': '421'}, page_content='APPENDIX C\\nPROBABILITY AND STATISTICS\\nThe purpose of this chapter is to establish the baseline probability and statistics\\nbackground for this book. We review basic concepts such as the sum and product rules\\nof probability, random variables and their probability distributions, expectations, in-\\ndependence, conditional probability, transformation rules, limit theorems, and Markov\\nchains. The properties of the multivariate normal distribution are discussed in more de-\\ntail. The main ideas from statistics are also reviewed, including estimation techniques\\n(such as maximum likelihood estimation), confidence intervals, and hypothesis testing.\\nC.1 Random Experiments and Probability Spaces\\nThe basic notion in probability theory is that of a random experiment random\\nexperiment\\n: an experiment\\nwhose outcome cannot be determined in advance. Mathematically, a random experiment is\\nmodeled via a triplet (Ω,H,P), where:\\n• Ω is the set of all possible outcomes of the experiment, called the sample space sample space.\\n• His the collection of all subsets of Ω to which a probability can be assigned; such\\nsubsets are called events events.\\n• Pis a probability measure probability\\nmeasure\\n, which assigns to each event A a number P[A] between 0\\nand 1, indicating the likelihood that the outcome of the random experiment lies inA.\\nAny probability measure Pmust satisfy the following Kolmogorov axioms Kolmogorov\\naxioms\\n:\\n1. P[A] ⩾0 for every event A.\\n2. P[Ω] = 1.\\n3. For any sequence A1,A2,... of events,\\nP\\nh[\\ni\\nAi\\ni\\n⩽\\nX\\ni\\nP[Ai], (C.1)\\nwith strict equality whenever the events are disjoint (that is, non-overlapping).\\n421'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 439, 'page_label': '422'}, page_content='422 Random Variables and Probability Distributions\\nWhen (C.1) holds as an equality, it is often referred to as the sum rulesum rule of probability. It\\nsimply states that if an event can happen in a number of di fferent but not simultaneous\\nways, the probability of that event is the sum of the probabilities of the comprising events.\\nIf the events are allowed to overlap, then the inequality (C.1) is called the union boundunion bound .\\nIn many applications the sample space is countable; that is, Ω = {a1,a2,... }. In this\\ncase the easiest way to specify a probability measure P is to first assign a number pi to\\neach elementary eventelementary\\nevent\\n{ai}, with P\\ni pi = 1, and then to define\\nP[A] =\\nX\\ni:ai∈A\\npi for all A ⊆Ω.\\nHere the collection of events Hcan be taken to be equal to the collection of all subsets\\nof Ω. The triple ( Ω,H,P) is called a discrete probability spacediscrete\\nprobability\\nspace\\n. This idea is graphically\\nrepresented in Figure C.1. Each element ai, represented by a dot, is assigned a weight (that\\nis, probability) pi, indicated by the size of the dot. The probability of the event A is simply\\nthe sum of the weights of all the outcomes in A.\\nΩ\\nA\\nFigure C.1: A discrete probability space.\\nRemark C.1 (Equilikely Principle) A special case of a discrete probability space oc-\\ncurs when a random experiment has finitely many outcomes that are all equally likely. In\\nthis case the probability measure is given by\\nP[A] = |A|\\n|Ω|, (C.2)\\nwhere |A|denotes the number of outcomes in A and |Ω|is the total number of outcomes.\\nThus, the calculation of probabilities reduces to counting numbers of outcomes in events.\\nThis is called the equilikely principleequilikely\\nprinciple\\n.\\nC.2 Random Variables and Probability Distributions\\nIt is often convenient to describe a random experiment via “random variables”, repres-\\nenting numerical measurements of the experiment. Random variables are usually denoted\\nby capital letters from the last part of the alphabet. From a mathematical point of view, a\\nrandom variablerandom\\nvariable\\nX is a function from Ω to R such that sets of the form {a <X ⩽b}:=\\n{ω∈Ω : a <X(ω) ⩽b}are events (and so can be assigned a probability).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 440, 'page_label': '423'}, page_content='Probability and Statistics 423\\nAll probabilities involving a random variableX can be computed, in principle, from its\\ncumulative distribution function cumulative\\ndistribution\\nfunction\\n(cdf), defined by\\nF(x) = P[X ⩽x], x ∈R.\\nFor example P[a < X ⩽b] = P[X ⩽b] −P[X ⩽a] = F(b) −F(a). Figure C.2 shows a\\ngeneric cdf. Note that any cdf is right-continuous, increasing, and lies between 0 and 1.\\n0\\n1\\nFigure C.2: A cumulative distribution function (cdf).\\nA cdf Fd is called discrete discrete cdfif there exist numbersx1,x2,... and probabilities 0 < f (xi) ⩽\\n1 summing up to 1, such that for all x\\nFd(x) =\\nX\\nxi⩽x\\nf (xi). (C.3)\\nSuch a cdf is piecewise constant and has jumps of sizesf (x1), f (x2),... at points x1,x2,... ,\\nrespectively. The function f (x) is called a probability mass function or discrete probability\\ndensity function (pdf). discrete pdfIt is often easier to use the pdf rather than the cdf, since probabilities\\ncan simply be calculated from it via summation:\\nP[X ∈B] =\\nX\\nx∈B\\nf (x),\\nas illustrated in Figure C.3.\\nFigure C.3: Discrete probability density function (pdf). The darker area corresponds to the\\nprobability P[X ∈B].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 441, 'page_label': '424'}, page_content='424 Random Variables and Probability Distributions\\nA cdf Fc is called continuous1,continuous cdf if there exists a positive function f such that for all x\\nFc(x) =\\nZ x\\n−∞\\nf (u) du. (C.4)\\nNote that such an Fc is differentiable (and hence continuous) with derivative f . The func-\\ntion f is called the probability density function (continuous pdf).pdf By the fundamental the-\\norem of integration, we have\\nP[a <X ⩽b] = F(b) −F(a) =\\nZ b\\na\\nf (x) dx.\\nThus, calculating probabilities reduces to integration, as illustrated in Figure C.4.\\nFigure C.4: Continuous probability density function (pdf). The shaded area corresponds to\\nthe probability P[X ∈B], with B being here the interval (a,b].\\nRemark C.2 (Probability Density and Probability Mass) It is important to note that\\nwe deliberately use the same name, “pdf”, and symbol, f , in both the discrete and the\\ncontinuous case, rather than distinguish between a probability mass function (pmf) and\\nprobability density function (pdf). From a theoretical point of view the pdf plays exactly\\nthe same role in the discrete and continuous cases. We use the notation X ∼Dist, X ∼ f ,\\nand X ∼F to indicate that X has distribution Dist, pdf f , and cdf F.\\nTables C.1 and C.2 list a number of important continuous and discrete distributions.\\nNote that in Table C.1, Γ is the gamma function: Γ(α) =\\nR ∞\\n0 e−x xα−1 dx, α> 0.\\n1In advanced probability, we would say “absolutely continuous with respect to the Lebesgue measure”.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 442, 'page_label': '425'}, page_content='Probability and Statistics 425\\nTable C.1: Commonly used continuous distributions.\\nName Notation f (x) x ∈ Parameters\\nUniform U[α,β] 1\\nβ−α [α,β] α<β\\nNormal N(µ,σ2) 1\\nσ\\n√\\n2π\\ne−1\\n2 ( x−µ\\nσ )\\n2\\nR σ> 0, µ∈R\\nGamma Gamma(α,λ) λαxα−1e−λx\\nΓ(α) R+ α,λ> 0\\nInverse Gamma InvGamma(α,λ) λαx−α−1e−λx−1\\nΓ(α) R+ α,λ> 0\\nExponential Exp(λ) λe−λx R+ λ> 0\\nBeta Beta(α,β) Γ(α+ β)\\nΓ(α)Γ(β) xα−1(1 −x)β−1 [0,1] α,β> 0\\nWeibull Weib(α,λ) αλ(λx)α−1e−(λx)α\\nR+ α,λ> 0\\nPareto Pareto(α,λ) αλ(1 + λx)−(α+1) R+ α,λ> 0\\nStudent tν\\nΓ(ν+1\\n2 )\\n√νπΓ(ν\\n2 )\\n \\n1 + x2\\nν\\n!−(ν+1)/2\\nR ν> 0\\nF F(m,n) Γ(m+n\\n2 ) (m/n)m/2 x(m−2)/2\\nΓ(m\\n2 ) Γ(n\\n2 ) [1 + (m/n)x](m+n)/2 R+ m,n ∈N+\\nThe Gamma(n/2,1/2) distribution is called thechi-squared distribution χ2\\nn distributionwith n degrees\\nof freedom, denoted χ2\\nn. The t1 distribution is also called the Cauchy distribution.\\nTable C.2: Commonly used discrete distributions.\\nName Notation f (x) x ∈ Parameters\\nBernoulli Ber(p) px(1 −p)1−x {0,1} 0 ⩽p ⩽1\\nBinomial Bin(n,p)\\n n\\nx\\n!\\npx(1 −p)n−x {0,1,..., n} 0 ⩽p ⩽1,\\nn ∈N\\nDiscrete\\nuniform U{1,..., n} 1\\nn {1,..., n} n ∈{1,2,... }\\nGeometric Geom(p) p(1 −p)x−1 {1,2,... } 0 ⩽p ⩽1\\nPoisson Poi(λ) e −λλx\\nx! N λ> 0'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 443, 'page_label': '426'}, page_content='426 Expectation\\nC.3 Expectation\\nIt is often useful to consider different kinds of numerical characteristics of a random vari-\\nable. One such quantity is the expectation, which measures the “average” value of the\\ndistribution.\\nThe expectationexpectation (or expected value or mean) of a random variableX with pdf f , denoted\\nby EX or2 E[X] (and sometimes µ), is defined by\\nEX =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nP\\nx x f (x) discrete case,R ∞\\n−∞x f (x) dx continuous case.\\nIf X is a random variable, then a function of X, such as X2 or sin(X), is again a random\\nvariable. Moreover, the expected value of a function of X is simply a weighted average of\\nthe possible values that this function can take. That is, for any real function h\\nEh(X) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nP\\nx h(x) f (x) discrete case ,R ∞\\n−∞h(x) f (x) dx continuous case,\\nprovided that the sum or integral are well-defined.\\nThe variancevariance of a random variable X, denoted by Var X (and sometimes σ2), is defined\\nby\\nVar X = E(X −E[X])2 = EX2 −(EX)2.\\nThe square root of the variance is called the standard deviationstandard\\ndeviation\\n. Table C.3 lists the expect-\\nations and variances for some well-known distributions. Both variance and standard devi-\\nation measure the spread or dispersion of the distribution. Note, however, that the standard\\ndeviation measures the dispersion in the same units as the random variable, unlike the\\nvariance, which uses squared units.\\nTable C.3: Expectations and variances for some well-known distributions.\\nDist. EX Var X\\nBin(n,p) np np (1 −p)\\nGeom(p) 1\\np\\n1 −p\\np2\\nPoi(λ) λ λ\\nU[α,β] α+ β\\n2\\n(β−α)2\\n12\\nExp(λ) 1\\nλ\\n1\\nλ2\\ntν 0 ( ν> 1) ν\\nν−2 (ν> 2)\\nDist. EX Var X\\nGamma(α,λ) α\\nλ\\nα\\nλ2\\nN(µ,σ2) µ σ 2\\nBeta(α,β) α\\nα+β\\nαβ\\n(α+β)2(1+α+β)\\nWeib(α,λ) Γ(1/α)\\nαλ\\n2Γ(2/α)\\nα −\\n\\x10Γ(1/α)\\nαλ\\n\\x112\\nF(m,n) n\\nn−2 (n >2) 2n2(m+n−2)\\nm(n−2)2(n−4) (n >4)\\n2We only use brackets in an expectation if it is unclear with respect to which random variable the ex-\\npectation is taken.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 444, 'page_label': '427'}, page_content='Probability and Statistics 427\\nIt is sometimes useful to consider themoment generating function moment\\ngenerating\\nfunction\\nof a random variable\\nX. This is the function M defined by\\nM(s) = EesX , s ∈R. (C.5)\\nThe moment generating functions of two random variables coincide if and only if the ran-\\ndom variables have the same distribution; see also Theorem C.12.\\nExample C.1 (Moment Generation Function of the Gamma(α,λ) Distribution) Let\\nX ∼Gamma(α,λ). For s <λ, the moment generating function of X at s is given by\\nM(s) = EesX =\\nZ ∞\\n0\\nesx e−λx λα xα−1\\nΓ(α) dx\\n=\\n\\x12 λ\\nλ−s\\n\\x13α Z ∞\\n0\\ne−(λ−s)x (λ−s)α xα−1\\nΓ(α)|                    {z                    }\\npdf of Gamma(α,λ−s)\\ndx =\\n\\x12 λ\\nλ−s\\n\\x13α\\n.\\nFor s ⩾λ, M(s) = ∞. Interestingly, the moment generating function has a much simpler\\nformula than the pdf.\\nC.4 Joint Distributions\\nDistributions for random vectors and stochastic processes can be specified in much the\\nsame way as for random variables. In particular, the distribution of a random vector X =\\n[X1,..., Xn]⊤is completely determined by specifying the joint cdf joint cdfF, defined by\\nF(x1,..., xn) = P[X1 ⩽x1,..., Xn ⩽xn], xi ∈R, i = 1,..., n.\\nSimilarly, the distribution of a stochastic process stochastic\\nprocess\\n, that is, a collection of random vari-\\nables {Xt,t ∈T }, for some index setT , is completely determined by its finite-dimensional\\ndistributions; specifically, the distributions of the random vectors [ Xt1 ,..., Xtn ]⊤ for every\\nchoice of n and t1,..., tn.\\nBy analogy to the one-dimensional case, a random vector X = [X1,..., Xn]⊤ taking\\nvalues in Rn is said to have a pdf f if, in the continuous case,\\nP[X ∈B] =\\nZ\\nB\\nf (x) dx, (C.6)\\nfor all n-dimensional rectangles B. Replace the integral with a sum for the discrete case.\\nThe pdf is also called the joint pdf joint pdfof X1,..., Xn. The pdfs of the individual components —\\ncalled marginal pdfs — marginal pdfcan be recovered from the joint pdf by “integrating out the other\\nvariables”. For example, for a continuous random vector [ X,Y]⊤with pdf f , the pdf fX of\\nX is given by\\nfX(x) =\\nZ\\nf (x,y) dy.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 445, 'page_label': '428'}, page_content='428 Conditioning and Independence\\nC.5 Conditioning and Independence\\nConditional probabilities and conditional distributions are used to model additional inform-\\nation on a random experiment. Independence is used to model lack of such information.\\nC.5.1 Conditional Probability\\nSuppose some event B ⊆ Ω occurs. Given this fact, event A will occur if and only if\\nA ∩B occurs, and the relative chance of A occurring is therefore P[A ∩B]/P[B], provided\\nP[B] >0. This leads to the definition of the conditional probabilityconditional\\nprobability\\nof A given B:\\nP[A |B] = P[A ∩B]\\nP[B] , if P[B] >0. (C.7)\\nThe above definition breaks down if P[B] = 0. Such conditional probabilities must be\\ntreated with more care [11].\\nThree important consequences of the definition of conditional probability are:\\n1. Product ruleProduct rule : For any sequence of events A1,A2,..., An,\\nP[A1 ··· An] = P[A1] P[A2 |A1] P[A3 |A1A2] ··· P[An |A1 ··· An−1], (C.8)\\nusing the abbreviation A1A2 ··· Ak := A1 ∩A2 ∩···∩ Ak.\\n2. Law of total probabilityLaw of total\\nprobability\\n: If {Bi}forms a partition of Ω (that is, Bi ∩Bj = ∅,i , j and\\n∪iBi = Ω), then for any event A\\nP[A] =\\nX\\ni\\nP[A |Bi] P[Bi]. (C.9)\\n3. Bayes’ ruleBayes’ rule : Let {Bi}form a partition of Ω. Then, for any event A with P[A] >0,\\nP[Bj |A] = P[A |Bj] P[Bj]P\\ni P[A |Bi] P[Bi]. (C.10)\\nC.5.2 Independence\\nTwo events A and B are said to be independentindependent\\nevents\\nif the knowledge that B has occurred does\\nnot change the probability that A occurs. That is, A, B independent ⇔P[A |B] = P[A].\\nSince P[A |B] P[B] = P[A ∩B], an alternative definition of independence is\\nA, B independent ⇔P[A ∩B] = P[A] P[B].\\nThis definition covers the case where P[B] = 0 and can be extended to arbitrarily many\\nevents: events A1,A2,... are said to be (mutually) independent if for any k and any choice\\nof distinct indices i1,..., ik,\\nP[Ai1 ∩Ai2 ∩···∩ Aik ] = P[Ai1 ] P[Ai2 ] ··· P[Aik ].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 446, 'page_label': '429'}, page_content='Probability and Statistics 429\\nThe concept of independence can also be formulated for random variables. Random\\nvariables X1,X2,... are said to be independent independent\\nrandom\\nvariables\\nif the events {Xi1 ⩽xi1 },..., {Xin ⩽xin }are\\nindependent for all finite choices of n distinct indices i1,..., in and values xi1 ,..., xin .\\nAn important characterization of independent random variables is the following (for a\\nproof, see [101], for example).\\nTheorem C.1: Independence Characterization\\nRandom variables X1,..., Xn with marginal pdfs fX1 ,..., fXn and joint pdf f are in-\\ndependent if and only if\\nf (x1,..., xn) = fX1 (x1) ··· fXn (xn) for all x1,..., xn. (C.11)\\nMany probabilistic models involve random variables X1,X2,... that are independent\\nand identically distributed , abbreviated as iid iid. We use this abbreviation throughout this\\nbook.\\nC.5.3 Expectation and Covariance\\nSimilar to the univariate case, the expected value of a real-valued function h of a random\\nvector X ∼ f is a weighted average of all values that h(X) can take. Specifically, in the\\ncontinuous case, Eh(X) =\\nR\\nh(x) f (x) dx. In the discrete case replace this multidimensional\\nintegral with a sum. Using this result, it is not di fficult to show that for any collection of\\ndependent or independent random variables X1,..., Xn,\\nE[a + b1X1 + b2X2 + ··· + bnXn] = a + b1EX1 + ··· + bnEXn (C.12)\\nfor all constants a, b1,..., bn. Moreover, for independent random variables,\\nE[X1X2 ··· Xn] = EX1 EX2 ··· EXn. (C.13)\\nWe leave the proofs as an exercise.\\nThe covariance covarianceof two random variables X and Y with expectations µX and µY , respect-\\nively, is defined as\\nCov(X,Y) = E[(X −µX)(Y −µY )].\\nThis is a measure of the amount of linear dependency between the variables. Let σ2\\nX =\\nVar X and σ2\\nY = Var Y. A scaled version of the covariance is given by the correlation\\ncoefficient correlation\\ncoefficient\\n,\\nϱ(X,Y) = Cov(X,Y)\\nσX σY\\n.\\nThe following properties follow directly from the definitions of variance and covariance.\\n1. Var X = EX2 −µ2\\nX.\\n2. Var[aX + b] = a2 σ2\\nX.\\n3. Cov(X,Y) = E[XY] −µX µY .\\n4. Cov(X,Y) = Cov(Y,X).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 447, 'page_label': '430'}, page_content='430 Conditioning and Independence\\n5. −σXσY ⩽Cov(X,Y) ⩽σXσY .\\n6. Cov(aX + bY,Z) = a Cov(X,Z) + b Cov(Y,Z).\\n7. Cov(X,X) = σ2\\nX.\\n8. Var[X + Y] = σ2\\nX + σ2\\nY + 2 Cov(X,Y).\\n9. If X and Y are independent, then Cov(X,Y) = 0.\\nAs a consequence of Properties 2 and 8 we have that for any sequence of independent\\nrandom variables X1, . . . ,Xn with variances σ2\\n1,...,σ 2\\nn,\\nVar[a1X1 + a2X2 + ··· + anXn] = a2\\n1 σ2\\n1 + a2\\n2 σ2\\n2 + ··· + a2\\nn σ2\\nn, (C.14)\\nfor any choice of constants a1,..., an.\\nFor random column vectors, such as X = [X1,..., Xn]⊤, it is convenient to write the\\nexpectations and covariances in vector and matrix notation. For a random vector X we\\ndefine its expectation vectorexpectation\\nvector\\nas the vector of expectations\\nµ= [µ1,...,µ n]⊤= [EX1,..., EXn]⊤.\\nSimilarly, if the expectation of a matrix is the matrix of expectations, then given two ran-\\ndom vectors X ∈Rn and Y ∈Rm, the n ×m matrix\\nCov(X,Y) = E[(X −EX)(Y −EY)⊤] (C.15)\\nhas (i, j)-th element Cov(Xi,Yj) = E[(Xi −EXi)(Yj −EYj)].A consequence of this definition\\nis that\\nCov(AX,BY) = ACov(X,Y)B⊤,\\nwhere A and B are two matrices with n and m columns, respectively.\\nThe covariance matrixcovariance\\nmatrix\\nof the vector X is defined as then×n matrix Cov(X,X). The co-\\nvariance matrix is also denoted as Var(X) = Cov(X,X), in analogy with the scalar identity\\nVar(X) = Cov(X,X).\\nA useful application of the cyclic property of the trace of a matrix (see Theorem A.1)\\nis the following.☞357\\nTheorem C.2: Expectation of a Quadratic Form\\nLet A be an n ×n matrix and X an n-dimensional random vector with expectation\\nvector µand covariance matrix Σ. The random variable Y := X⊤AX has expectation\\ntr(AΣ) + µ⊤Aµ.\\nProof: Since Y is a scalar, it is equal to its trace. Now, using the cyclic property: EY =\\nEtr(Y) = Etr(X⊤AX) = Etr(AXX⊤) = tr(A E[XX⊤]) = tr(A(Σ + µµ⊤)) = tr(AΣ) +\\ntr(Aµµ⊤) = tr(AΣ) + µ⊤Aµ. □'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 448, 'page_label': '431'}, page_content='Probability and Statistics 431\\nC.5.4 Conditional Density and Conditional Expectation\\nSuppose X and Y are both discrete or both continuous, with joint pdff , and suppose fX(x) >\\n0. Then, the conditional pdf conditional pdfof Y given X = x is given by\\nfY|X(y |x) = f (x,y)\\nfX(x) for all y. (C.16)\\nIn the discrete case, the formula is a direct translation of (C.7), with fY|X(y |x) = P[Y =\\ny |X = x]. In the continuous case, a similar interpretation in terms of densities can be used;\\nsee, for example, [101, Page 221]. The corresponding distribution is called theconditional\\ndistribution of Y given X = x. Note that (C.16) implies that conditional\\ndistribution\\nf (x,y) = fX(x) fY |X(y |x).\\nThis is useful when the marginal and conditional pdfs are given, rather than the joint one.\\nMore generally, for the n-dimensional case we have\\nf (x1,..., xn) = fX1 (x1) fX2 |X1 (x2 |x1) ··· fXn |X1,...,Xn−1 (xn |x1,..., xn−1), (C.17)\\nwhich is in essence a rephrasing of the product rule (C.8) in terms of probability densities. ☞ 428\\nAs a conditional pdf has all the properties of an ordinary pdf, we may define expecta-\\ntions with respect to it. The conditional expectation of a random variable Y given X = x is conditional\\nexpectationdefined as\\nE[Y |X = x] =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nP\\ny y fY|X(y |x) discrete case,R\\ny fY|X(y |x) dy continuous case.\\n(C.18)\\nNote that E[Y |X = x] is a function of x. The corresponding random variable is written\\nas E[Y |X]. A similar formalism can be used when conditioning on a sequence of random\\nvariables X1,..., Xn. The conditional expectation has similar properties to the ordinary\\nexpectation. Other useful properties (see, for example, [127]) are:\\n1. Tower property: If EY exists, then\\nEE[Y |X] = EY. (C.19)\\n2. Taking out what is known: If EY exists, then\\nE[XY |X] = XE[Y |X].\\nC.6 Functions of Random Variables\\nLet x = [x1,..., xn]⊤be a column vector inRn and A an m×n matrix. The mapping x 7→z,\\nwith z = Ax, is a linear transformation, as discussed in Section A.1. Now consider a ☞ 355\\nrandom vector X = [X1,..., Xn]⊤and let Z := AX. Then Z is a random vector in Rm. The\\nfollowing theorem details how the distribution of Z is related to that of X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 449, 'page_label': '432'}, page_content='432 Functions of Random Variables\\nTheorem C.3: Linear Transformation\\nIf X has an expectation vector µX and covariance matrix ΣX, then the expectation\\nvector of Z is\\nµZ = A µX (C.20)\\nand the covariance matrix of Z is\\nΣZ = A ΣX A⊤. (C.21)\\nIf, in addition, A is an invertible n ×n matrix and X is a continuous random vector\\nwith pdf fX, then the pdf of the continuous random vector Z = AX is given by\\nfZ(z) = fX(A−1 z)\\n|det(A)|, z ∈Rn, (C.22)\\nwhere |det(A)|denotes the absolute value of the determinant of A.\\nProof: We have µZ = EZ = E[AX] = A EX = AµX and\\nΣZ = E[(Z −µZ)(Z −µZ)⊤] = E[A(X −µX)(A(X −µX))⊤]\\n= A E[(X −µX)(X −µX)⊤]A⊤\\n= A ΣX A⊤.\\nFor A invertible and X continuous (as opposed to discrete), let z = Ax and x = A−1 z.\\nConsider the n-dimensional cube C = [z1,z1 + h] ×···× [zn,zn + h]. Then,\\nP[Z ∈C] ≈hn fZ(z),\\nby definition of the joint density of Z. Let D be the image of C under A−1 — that is, all\\npoints x such that Ax ∈C. Recall from Section A.1 that any matrixB linearly transforms an☞355\\nn-dimensional rectangle with volume V into an n-dimensional parallelepiped with volume\\nV |det(B)|. Thus, in addition to the above expression for P[Z ∈C], we also have\\nP[Z ∈C] = P[X ∈D] ≈hn|det(A−1)|fX(x) = hn|det(A)|−1 fX(x).\\nEquating these two expressions for P[Z ∈C], dividing both sides by hn, and letting h go to\\n0, we obtain (C.22). □\\nFor a generalization of the linear transformation rule (C.22), consider an arbitrary map-\\nping x 7→g(x), written out:\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\n...\\nxn\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n7→\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ng1(x)\\ng2(x)\\n...\\ngn(x)\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 450, 'page_label': '433'}, page_content='Probability and Statistics 433\\nTheorem C.4: Transformation Rule\\nLet X be an n-dimensional vector of continuous random variables with pdf fX. Let\\nZ = g(X), where g is an invertible mapping with inverse g−1 and matrix of Jacobi\\nJg; that is, the matrix of partial derivatives ofg. Then, at z = g(x) the random vector\\nZ has pdf\\nfZ(z) = fX(x)\\n|det(Jg(x))|= fX(g−1(z)) |det(Jg−1 (z))|, z ∈Rn. (C.23)\\nProof: For a fixed x, let z = g(x); and thus x = g−1(z). In the neighborhood of x, the\\nfunction g behaves like a linear function, in the sense that g(x + δ) ≈g(x) + Jg(x) δfor\\nsmall vectors δ; see also Section B.1. Consequently, an infinitesimally smalln-dimensional ☞ 397\\nrectangle at x with volume V is transformed into an infinitesimally small n-dimensional\\nparallelepiped at z with volume V |det(Jg(x))|. Now, as in the proof of the linear case, let\\nC be a small cube around z = g(x) with volume hn. Let D be the image of C under g−1.\\nThen,\\nhn fZ(z) ≈P[Z ∈C] ≈hn|det(Jg−1 (z))|fX(x),\\nand since |det(Jg−1 (z))|= 1/|det(Jg(x))|, (C.23) follows as h goes to 0. □\\nTypically, in coordinate transformations it is g−1 that is given — that is, an expres-\\nsion for x as a function of z.\\nExample C.2 (Polar Transform) Suppose X,Y are independent and have standard nor-\\nmal distribution. The joint pdf is\\nfX,Y (x,y) = 1\\n2πe−1\\n2 (x2+y2), (x,y) ∈R2.\\nIn polar coordinates we have\\nX = R cos Θ and Y = R sin Θ, (C.24)\\nwhere R ⩾0 is the radius andΘ ∈[0,2π) the angle of the point (X,Y). What is the joint pdf\\nof R and Θ? By the radial symmetry of the bivariate normal distribution, we would expect\\nΘ to be uniform on (0 ,2π). But what is the pdf of R? To work out the joint pdf, consider\\nthe inverse transformation g−1, defined by\\n\"r\\nθ\\n#\\ng−1\\n7−→\\n\"r cos θ\\nr sin θ\\n#\\n=\\n\"x\\ny\\n#\\n.\\nThe corresponding matrix of Jacobi is\\nJg−1 (r,θ) =\\n\"cos θ −r sin θ\\nsin θ r cos θ\\n#\\n,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 451, 'page_label': '434'}, page_content='434 Multivariate Normal Distribution\\nwhich has determinant r. Since x2 + y2 = r2(cos2 θ+ sin2 θ) = r2, it follows by the trans-\\nformation rule (C.23) that the joint pdf of R and Θ is given by\\nfR,Θ(r,θ) = fX,Y (x,y) r = 1\\n2πe−1\\n2 r2\\nr, θ ∈(0,2π), r ⩾0.\\nBy integrating out θand r, respectively, we find fR(r) = r e−r2/2 and fΘ(θ) = 1/(2π). Since\\nfR,Θ is the product of fR and fΘ, the random variables R and Θ are independent.\\nC.7 Multivariate Normal Distribution\\nThe normal (or Gaussian) distribution — especially its multidimensional version — plays\\na central role in data science and machine learning. Recall from Table C.1 that a random\\nvariable X is said to have a normalnormal\\ndistribution\\ndistribution with parameters µand σ2 if its pdf is given\\nby\\nf (x) = 1\\nσ\\n√\\n2π\\ne−1\\n2 (\\nx−µ\\nσ )\\n2\\n, x ∈R. (C.25)\\nWe write X ∼N(µ,σ2). The parameters µand σ2 are the expectation and variance of the\\ndistribution, respectively. Ifµ= 0 and σ= 1 then\\nf (x) = 1√\\n2π\\ne−x2/2,\\nand the distribution is known as the standard normalstandard\\nnormal\\ndistribution. The cdf of the standard\\nnormal distribution is often denoted by Φ and its pdf by φ. In Figure C.5 the pdf of the\\nN(µ,σ2) distribution for various µand σ2 is plotted.\\n-4 -2 0 2 4 6\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nN(0,1)\\nN(0,1/4)\\nN(2,1)\\nFigure C.5: The pdf of the N(µ,σ2) distribution for various µand σ2.\\nWe next consider some important properties of the normal distribution.\\nTheorem C.5: Standardization\\nLet X ∼N(µ,σ2) and define Z = (X −µ)/σ. Then Z has a standard normal distribu-\\ntion.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 452, 'page_label': '435'}, page_content='Probability and Statistics 435\\nProof: The cdf of Z is given by\\nP[Z ⩽z] = P[(X −µ)/σ⩽z] = P[X ⩽µ+ σz]\\n=\\nZ µ+σz\\n−∞\\n1\\nσ\\n√\\n2π\\ne−1\\n2 (\\nx−µ\\nσ )\\n2\\ndx =\\nZ z\\n−∞\\n1√\\n2π\\ne−y2/2dy = Φ(z),\\nwhere we make a change of variable y = (x −µ)/σ in the fourth equation. Hence, Z ∼\\nN(0,1). □\\nThe rescaling procedure in Theorem C.5 is calledstandardization standardization. It follows from The-\\norem C.5 that any X ∼N(µ,σ2) can be written as\\nX = µ+ σZ, where Z ∼N(0,1).\\nIn other words, any normal random variable can be viewed as an affine transformation affine\\ntransformation\\n—\\nthat is, a linear transformation plus a constant — of a standard normal random variable.\\nWe now generalize this to n dimensions. Let Z1,..., Zn be independent and standard\\nnormal random variables. The joint pdf of Z = [Z1,..., Zn]⊤is given by\\nfZ(z) =\\nnY\\ni=1\\n1√\\n2π\\ne−1\\n2 z2\\ni = (2π)−n\\n2 e−1\\n2 z⊤z, z ∈Rn. (C.26)\\nWe write Z ∼N(0,I), where I is the identity matrix. Consider the affine transformation\\nX = µ+ B Z (C.27)\\nfor some m ×n matrix B and m-dimensional vector µ. Note that, by (C.20) and (C.21), X ☞ 432\\nhas expectation vector µand covariance matrix Σ = BB⊤.We say that X has a multivariate\\nnormal multivariate\\nnormal\\nor multivariate Gaussian distribution with mean vector µand covariance matrix Σ.\\nWe write X ∼N(µ,Σ).\\nThe following theorem states that any a ffine combination of independent multivariate\\nnormal random variables is again multivariate normal.\\nTheorem C.6: Affine Transformation of Normal Random Vectors\\nLet X1,X2,..., Xr be independent mi-dimensional normal random vectors, with\\nXi ∼ N(µi,Σi), i = 1,..., r. Then, for any n ×1 vector a and n ×mi matrices\\nB1,..., Br,\\na +\\nrX\\ni=1\\nBi Xi ∼N\\n\\x12\\na +\\nrX\\ni=1\\nBi µi,\\nrX\\ni=1\\nBi Σi B⊤\\ni\\n\\x13\\n. (C.28)\\nProof: Denote the n-dimensional random vector in the left-hand side of (C.28) by Y. By\\ndefinition, each Xi can be written as µi +Ai Zi, where the {Zi}are independent (because the\\n{Xi}are independent), so that\\nY = a +\\nrX\\ni=1\\nBi (µi + Ai Zi) = a +\\nrX\\ni=1\\nBi µi +\\nrX\\ni=1\\nBiAi Zi,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 453, 'page_label': '436'}, page_content='436 Multivariate Normal Distribution\\nwhich is an affine combination of independent standard normal random vectors. Hence, Y\\nis multivariate normal. Its expectation vector and covariance matrix can be found easily\\nfrom Theorem C.3. □☞432\\nThe next theorem shows that the distribution of a subvector of a multivariate normal\\nrandom vector is again normal.\\nTheorem C.7: Marginal Distributions of Normal Random Vectors\\nLet X ∼N(µ,Σ) be an n-dimensional normal random vector. Decompose X, µ, and\\nΣ as\\nX =\\n\"Xp\\nXq\\n#\\n, µ=\\n\"µp\\nµq\\n#\\n, Σ =\\n\"Σp Σr\\nΣ⊤\\nr Σq\\n#\\n, (C.29)\\nwhere Σp is the upper left p ×p corner of Σ and Σq is the lower right q ×q corner of\\nΣ. Then, Xp ∼N(µp,Σp).\\nProof: We give a proof assuming that Σ is positive definite. Let BB⊤ be the (lower)\\nCholesky decomposition of Σ. We can write☞373\\n\"Xp\\nXq\\n#\\n=\\n\"µp\\nµq\\n#\\n+\\n\"Bp O\\nCr Cq\\n#\\n|     {z     }\\nB\\n\"Zp\\nZq\\n#\\n, (C.30)\\nwhere Zp and Zq are independent p- and q-dimensional standard normal random vectors.\\nIn particular, Xp = µp + Bp Zp, which means that Xp ∼N(µp,Σp), since BpB⊤\\np = Σp. □\\nBy relabeling the elements of X we see that Theorem C.7 implies thatany subvector of\\nX has a multivariate normal distribution. For example, Xq ∼N(µq,Σq).\\nThe following theorem shows that not only the marginal distributions of a normal ran-\\ndom vector are normal, but also its conditional distributions.\\nTheorem C.8: Conditional Distributions of Normal Random Vectors\\nLet X ∼N(µ,Σ) be an n-dimensional normal random vector with det( Σ) > 0. If X\\nis decomposed as in (C.29), then\\n\\x10\\nXq |Xp = xp\\n\\x11\\n∼N(µq + Σ⊤\\nr Σ−1\\np (xp −µp), Σq −Σ⊤\\nr Σ−1\\np Σr). (C.31)\\nAs a consequence, Xp and Xq are independent if and only if they are uncorrelated;\\nthat is, if Σr = O (zero matrix).\\nProof: From (C.30) we see thatXp = µp+Bp Zp and Xq = µq+Cr Zp+Cq Zq. Consequently,\\n(Xq |Xp = xp) = µq + Cr B−1\\np (xp −µp) + Cq Zq,\\nwhere Zq is a q-dimensional multivariate standard normal random vector. It follows that\\nXq conditional on Xp = xp has a N(µq + Cr B−1\\np (xp −µp),CqC⊤\\nq ) distribution. The proof of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 454, 'page_label': '437'}, page_content='Probability and Statistics 437\\n(C.31) is completed by observing that Σ⊤\\nr Σ−1\\np = CrB⊤\\np (B⊤\\np )−1B−1\\np = Cr B−1\\np , and\\nΣq −Σ⊤\\nr Σ−1\\np Σr = CrC⊤\\nr + CqC⊤\\nq −CrB−1\\np Σr|{z}\\nBpC⊤r\\n= CqC⊤\\nq .\\nIf Xp and Xq are independent, then they are obviously uncorrelated, as Σr = E[(Xp −\\nµp)(Xq −µq)⊤] = E(Xp −µp) E(Xq −µq)⊤ = O. Conversely, if Σr = O, then by (C.31) the\\nconditional distribution of Xq given Xp is the same as the unconditional distribution of Xq;\\nthat is, N(µq,Σq). In other words, Xq is independent of Xp. □\\nThe next few results are about the relationships between the normal, chi-squared,\\nχ2 distributionStudent, and F distributions, defined in Table C.1. Recall that the chi-squared family of\\ndistributions, denoted by χ2\\nn, are simply Gamma(n/2,1/2) distributions, where the para-\\nmeter n ∈{1,2,3,... }is called the degrees of freedom.\\nTheorem C.9: Relationship Between Normal and χ2 Distributions\\nIf X ∼N(µ,Σ) is an n-dimensional normal random vector with det(Σ) >0, then\\n(X −µ)⊤Σ−1(X −µ) ∼χ2\\nn. (C.32)\\nProof: Let BB⊤ be the Cholesky decomposition of Σ, where B is invertible. Since X can\\nbe written as µ+ BZ, where Z = [Z1,..., Zn]⊤is a vector of independent standard normal\\nrandom variables, we have\\n(X −µ)⊤Σ−1(X −µ) = (X −µ)⊤(BB⊤)−1(X −µ) = Z⊤Z =\\nnX\\ni=1\\nZ2\\ni .\\nUsing the independence of Z1,..., Zn, the moment generating function of Y = Pn\\ni=1 Z2\\ni is ☞ 427\\ngiven by\\nEesY = Ees(Z2\\n1 +···+Z2\\nn ) = E[esZ2\\n1 ··· esZ2\\nn ] =\\n\\x10\\nEesZ2 \\x11n\\n,\\nwhere Z ∼N(0,1). The moment generating function of Z2 is\\nEesZ2\\n=\\nZ ∞\\n−∞\\nesz2 1√\\n2π\\ne−z2/2dz = 1√\\n2π\\nZ ∞\\n−∞\\ne−1\\n2 (1−2s)z2\\ndz = 1√\\n1 −2s\\n,\\nso that EesY =\\n\\x10\\n1\\n2 /(1\\n2 −s)\\n\\x11n\\n2\\n, s < 1\\n2 , which is the moment generating function of the\\nGamma(n/2,1/2) distribution; that is, the χ2\\nn distribution — see Example C.1. The res- ☞ 427\\nult now follows from the uniqueness of the moment generating function. □\\nA consequence of Theorem C.9 is that if X = [X1,..., Xn]⊤is n-dimensional standard\\nnormal, then the squared length ∥X∥2 = X2\\n1 + ··· + X2\\nn has a χ2\\nn distribution. If instead Xi ∼\\nN(µi,1), i = 1,... , then ∥X∥2 is said to have a noncentral χ2\\nn distribution noncentral χ2\\nn\\ndistribution\\n. This distribution\\ndepends on the {µi}only through the norm ∥µ∥. We write ∥X∥2 ∼χ2\\nn(θ), where θ = ∥µ∥is\\nthe noncentrality parameter noncentrality\\nparameter\\n.\\nSuch distributions frequently occur when considering projections of multivariate nor-\\nmal random variables, as summarized in the following theorem.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 455, 'page_label': '438'}, page_content='438 Multivariate Normal Distribution\\nTheorem C.10: Relationship Between Normal and Noncentral χ2 Distributions\\nLet X ∼N(µ,In) be an n-dimensional normal random vector and let Vk ⊂Vm be\\nlinear subspaces of dimensions k and m, respectively, with k <m ⩽n. Let Xk and\\nXm be orthogonal projections of X onto Vk and Vm, and let µk and µm be the cor-\\nresponding projections of µ. Then, the following holds.\\n1. The random vectors Xk, Xm −Xk, and X −Xm are independent.\\n2. ∥Xk∥2 ∼χ2\\nk(∥µk∥), ∥Xm −Xk∥2 ∼χ2\\nm−k(∥µm −µk∥), and ∥X −Xm∥2 ∼χ2\\nn−m(∥µ−\\nµm∥).\\nProof: Let v1,..., vn be an orthonormal basis of Rn such that v1,..., vk spans Vk and\\nv1,..., vm spans Vm. By (A.8) we can write the orthogonal projection matrices onto Vj,☞362\\nas Pj = Pj\\ni=1 viv⊤\\ni , j = k,m,n, where Vn is defined as Rn. Note that Pn is simply the iden-\\ntity matrix. Let V := [v1,..., vn] and define Z := [Z1,..., Zn]⊤= V⊤X. Recall from Sec-\\ntion A.2 that any orthogonal transformation such as z = V⊤x is length preserving; that is,☞361\\n∥z∥= ∥x∥.\\nTo prove the first statement of the theorem, note that V⊤Xj = V⊤Pj X = [Z1,..., Zj,\\n0,..., 0]⊤, j = k,m. It follows that V⊤(Xm −Xk) = [0,..., 0,Zk+1,..., Zm,0,..., 0]⊤ and\\nV⊤(X −Xm) = [0,..., 0,Zm+1,..., Zn]⊤. Moreover, being a linear transformation of a nor-\\nmal random vector, Z is also normal, with covariance matrix V⊤V = In. In particular, the\\n{Zi}are independent. This shows that Xk, Xm −Xk and X −Xm are independent as well.\\nNext, observe that ∥Xk∥= ∥V⊤Xk∥= ∥Zk∥, where Zk := [Z1,..., Zk]⊤. The latter vector\\nhas independent components with variances 1, and its squared norm has therefore (by\\ndefinition) a χ2\\nk(θ) distribution. The noncentrality parameter is θ= ∥EZk∥= ∥EXk∥= ∥µk∥,\\nagain by the length-preserving property of orthogonal transformations. This shows that\\n∥Xk∥2 ∼χ2\\nk(∥µk∥). The distributions of ∥Xm −Xk∥2 and ∥X −Xm∥2 follow by analogy. □\\nTheorem C.10 is frequently used in the statistical analysis ofnormal linear models; see\\nSection 5.4. In typical situations µlies in the subspace Vm or even Vk — in which case☞182\\n∥Xm −Xk∥2 ∼χ2\\nm−k and ∥X −Xm∥2 ∼χ2\\nn−m, independently. The (scaled) quotient then turns\\nout to have an F distribution — a consequence of the following theorem.\\nTheorem C.11: Relationship Between χ2 and F Distributions\\nLet U ∼χ2\\nm and V ∼χ2\\nn be independent. Then,\\nU/m\\nV/n ∼F(m,n).\\nProof: For notational simplicity, let c = m/2 and d = n/2. The pdf of W = U/V is\\ngiven by fW (w) =\\nR ∞\\n0 fU (wv) v fV (v) dv. Substituting the pdfs of the correspondingGamma'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 456, 'page_label': '439'}, page_content='Probability and Statistics 439\\ndistributions, we have\\nfW (w) =\\nZ ∞\\n0\\n(wv)c−1 e−wv/2\\nΓ(c) 2c v vd−1e−v/2\\nΓ(d) 2d dv = wc−1\\nΓ(c) Γ(d) 2c+d\\nZ ∞\\n0\\nvc+d−1 e−(1+w)v/2 dv\\n= Γ(c + d)\\nΓ(c) Γ(d)\\nwc−1\\n(1 + w)c+d ,\\nwhere the last equality follows from the fact that the integrand is equal to Γ(α)λ−α times\\nthe density of the Gamma(α,λ) distribution with α= c + d and λ= (1 + w)/2. The density\\nof Z = n\\nm\\nU\\nV is given by\\nfZ(z) = fW (z m/n) m/n.\\nThe proof is completed by comparing the resulting expression with the pdf of the F distri-\\nbution given in Table C.1. □ ☞ 425\\nCorollary C.1 (Relationship Between Normal, χ2, and t Distributions) Let Z ∼N(0,1)\\nand V ∼χ2\\nn be independent. Then,\\nZ√V/n\\n∼tn.\\nProof: Let T = Z/√V/n. Because Z2 ∼χ2\\n1, we have by Theorem C.11 that T2 ∼F(1,n).\\nThe result follows now from the symmetry around 0 of the pdf of T and the fact that the\\nsquare of a tn random variable has an F(1,n) distribution. □\\nC.8 Convergence of Random Variables\\nRecall that a random variableX is a function fromΩ to R. If we have a sequence of random\\nvariables X1,X2,... (for instance, Xn(ω) = X(ω)+ 1\\nn for each ω∈Ω), then one can consider\\nthe pointwise convergence:\\nlim\\nn→∞\\nXn(ω) = X(ω), for all ω∈Ω,\\nin which case we say that X1,X2,... converges surely sure\\nconvergence\\nto X. A more interesting type of\\nconvergence uses the probability measure Passociated with X.\\nDefinition C.1: Convergence in Probability\\nThe sequence of random variables X1,X2,... converges in probability to a random\\nvariable X if, for all ε> 0,\\nlim\\nn→∞\\nP[|Xn −X|>ε] = 0.\\nWe denote the convergence in probability convergence in\\nprobability\\nas Xn\\nP\\n−→X.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 457, 'page_label': '440'}, page_content='440 Convergence of Random Variables\\nConvergence in probability refers only to the distribution ofXn. Instead, if the sequence\\nX1,X2,... is defined on a common probability space, then we can consider the following\\nmode of convergence that uses the joint distribution of the sequence of random variables.\\nDefinition C.2: Almost Sure Convergence\\nThe sequence of random variables X1,X2,... converges almost surely to a random\\nvariable X if for every ε> 0\\nlim\\nn→∞\\nP\\n\"\\nsup\\nk⩾n\\n|Xk −X|>ε\\n#\\n= 0.\\nWe denote the almost sure convergencealmost sure\\nconvergence\\nas Xn\\na.s.\\n−→X.\\nNote that in accordance with these definitionsXn\\na.s.\\n−→0 is equivalent to supk⩾n |Xk|\\nP\\n−→0.\\nExample C.3 (Convergence in Probability Versus Almost Sure Convergence) Since\\nthe event {|Xn −X|> ε}is contained in {supk⩾n |Xk −X|> ε}, we can conclude that almost\\nsure convergence implies convergence in probability. However, the converse is not true in\\ngeneral. For instance, consider the iid sequence X1,X2,... with marginal distribution\\nP[Xn = 1] = 1 −P[Xn = 0] = 1/n.\\nClearly, Xn\\nP\\n−→0. However, for ε< 1 and any n = 1,2,... we have,\\nP\\n\"\\nsup\\nk⩾n\\n|Xk|⩽ε\\n#\\n= P[Xn ⩽ε,Xn+1 ⩽ε,... ]\\n= P[Xn ⩽ε] ×P[Xn+1 ⩽ε] ×··· (using independence)\\n= lim\\nm→∞\\nmY\\nk=n\\nP[Xk ⩽ε] = lim\\nm→∞\\nmY\\nk=n\\n \\n1 −1\\nk\\n!\\n= lim\\nm→∞\\nn −1\\nn × n\\nn + 1 ×···× m −1\\nm = 0.\\nIt follows that P[supk⩾n |Xk −0|>ε] = 1 for any 0 <ε< 1 and all n ⩾1. In other words, it\\nis not true that Xn\\na.s.\\n−→0.\\nAnother important type of convergence is useful when we are interested in estimating\\nexpectations or multidimensional integrals via Monte Carlo methodology.☞67\\nDefinition C.3: Convergence in Distribution\\nThe sequence of random variables X1,X2,... is said to converge in distribution to a\\nrandom variable X with distribution function FX(x) = P[X ⩽x] provided that:\\nlim\\nn→∞\\nP[Xn ⩽x] = FX(x) for all x such that lim\\na→x\\nFX(a) = FX(x). (C.33)\\nWe denote the convergence in distributionconvergence in\\ndistribution\\nas either Xn\\nd\\n−→X, or Xn\\nd\\n−→FX.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 458, 'page_label': '441'}, page_content='Probability and Statistics 441\\nThe generalization to random vectors replaces (C.33) with\\nlim\\nn→∞\\nP[Xn ∈A] = P[X ∈A] for all A ⊂Rn such that P[X ∈∂A] = 0, (C.34)\\nwhere ∂A denotes the boundary of the set A.\\nA useful tool for demonstrating convergence in distribution is the characteristic func-\\ntion ψX of a random vector X, defined as the expectation: characteristic\\nfunction\\n☞ 225ψX(t) := Eeit⊤X, t ∈Rn. (C.35)\\nThe moment generating function in (C.5) is a special case of the characteristic function\\nevaluated at t = −is. Note that while the moment generating function of a random variable\\nmay not exist, its characteristic function always exists. The characteristic function of a\\nrandom vector X ∼f is closely related to the Fourier transform of its pdf f . ☞ 390\\nExample C.4 (Characteristic Function of a Multivariate Gaussian Random Vector)\\nThe density of the multivariate standard normal distribution is given in (C.26) and thus the\\ncharacteristic function of Z ∼N(0,In) is\\nψZ(t) = Eeit⊤Z = (2π)−n/2\\nZ\\nRn\\neit⊤z−1\\n2 ∥z∥2\\ndz\\n= e−∥t∥2/2(2π)−n/2\\nZ\\nRn\\ne−1\\n2 ∥z−it⊤∥2\\ndz = e−∥t∥2/2, t ∈Rn.\\nHence, the characteristic function of the random vector X = µ+ BZ in (C.27) with mul-\\ntivariate normal distribution N(µ,Σ) is given by ☞ 435\\nψX(t) = Eeit⊤X = Eeit⊤(µ+BZ)\\n= eit⊤µEei(B⊤t)⊤Z = eit⊤µψZ(B⊤t)\\n= eit⊤µ−∥B⊤t∥2/2 = eit⊤µ−t⊤Σt/2.\\nThe importance of the characteristic function is mainly derived from the following\\nresult, for which a proof can be found, for example, in [11].\\nTheorem C.12: Characteristic Function\\nSuppose that ψX1 (t),ψX2 (t),... are the characteristic functions of the sequence of\\nrandom vectors X1,X2,... and ψX(t) is the characteristic function of X. Then, the\\nfollowing three statements are equivalent:\\n1. lim n→∞ψXn (t) = ψX(t) for all t ∈Rn.\\n2. Xn\\nd\\n−→X.\\n3. lim n→∞Eh(Xn) = Eh(X) for all bounded continuous functions h : Rd 7→R.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 459, 'page_label': '442'}, page_content='442 Convergence of Random Variables\\nExample C.5 (Convergence in Distribution) Define the random variables Y1,Y2,...\\nas\\nYn :=\\nnX\\nk=1\\nXk\\n 1\\n2\\n!k\\n, n = 1,2,...,\\nwhere X1,X2,...\\niid\\n∼Ber(1/2). We now show thatYn\\nd\\n−→U(0,1). First, note that\\nEexp(itYn) =\\nnY\\nk=1\\nEexp(itXk/2k) = 2−n\\nnY\\nk=1\\n(1 + exp(it/2k)).\\nSecond, from the collapsing product, (1 −exp(it/2n)) Qn\\nk=1(1 + exp(it/2k)) = 1 −exp(it),\\nwe have\\nEexp(itYn) = (1 −exp(it)) 1/2n\\n1 −exp(it/2n).\\nIt follows that limn→∞Eexp(itYn) = (exp(it) −1)/(it),which we recognize as the charac-\\nteristic function of the U(0,1) distribution.☞441\\nYet another mode of convergence is the following.\\nDefinition C.4: Convergence in Lp-norm\\nThe sequence of random variables X1,X2,... converges in L p-norm to a random\\nvariable X if\\nlim\\nn→∞\\nE|Xn −X|p = 0, p ⩾1.\\nWe denote the convergence in Lp-normconvergence in\\nLp-norm\\nas Xn\\nLp\\n−→X.\\nThe case for p = 2 corresponds to convergence in mean squared error. The following\\nexample illustrates that convergence inLp-norm is qualitatively different from convergence\\nin distribution.\\nExample C.6 (Comparison of Modes of Convergence) Define Xn := 1 −X, where X\\nhas a uniform distribution on the interval (0,1). Clearly, Xn\\nd\\n−→U(0,1). However, E|Xn −\\nX|−→ E|1 −2X|= 1/2 and so the sequence does not converge in L1-norm. In addition,\\nP[|Xn −X|>ε] −→1 −ε, 0 and so Xn does not converge in probability as well.\\nThus, in general Xn\\nd\\n−→X implies neither Xn\\nP\\n−→X, nor Xn\\nL1\\n−→X.\\nWe mention, however, that if Xn\\nd\\n−→c for some constant c, then Xn\\nP\\n−→c as well. To\\nsee this, note that Xn\\nd\\n−→c stands for\\nlim\\nn→∞\\nP[Xn ⩽x] =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\n1, x >c\\n0, x <c .\\nIn other words, we can write:\\nP[|Xn −c|>ε] ⩽1 −P[Xn ⩽c + ε] + P[Xn ⩽c −ε] −→1 −1 + 0 = 0, n →∞,\\nwhich shows that Xn\\nP\\n−→c by definition.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 460, 'page_label': '443'}, page_content='Probability and Statistics 443\\nDefinition C.5: Complete Convergence\\nThe sequence of random variables X1,X2,... is said to converge completely to X if\\nfor all ε> 0 X\\nn\\nP[|Xn −X|>ε] <∞.\\nWe denote the complete convergence complete\\nconvergence\\nas Xn\\ncpl.\\n−→X.\\nExample C.7 (Complete and Almost Sure Convergence) We show that complete\\nconvergence implies almost sure convergence. We can bound the criterion for almost sure\\nconvergence as follows:\\nP[sup\\nk⩾n\\n|Xk −X|>ε] = P[∪k⩾n{|Xk −X|>ε}]\\n⩽\\nX\\nk⩾n\\nP[|Xk −X|>ε] by union bound in (C.1)\\n⩽\\n∞X\\nk=1\\nP[|Xk −X|>ε]\\n|                  {z                  }\\n= c<∞from Xn\\ncpl.\\n−→X\\n−\\nn−1X\\nk=1\\nP[|Xk −X|>ε]\\n⩽c −\\nn−1X\\nk=1\\nP[|Xk −X|>ε] −→c −c = 0, n →∞.\\nHence, by definition Xn\\na.s.\\n−→X.\\nThe next theorem shows how the di fferent types of convergence are related to each\\nother. For example, in the diagram below, the notation\\np⩾q\\n⇒means that Lp-norm convergence\\nimplies Lq-norm convergence under the assumption that p ⩾q ⩾1.\\nTheorem C.13: Modes of Convergence\\nThe most general relationships among the various modes of convergence for numer-\\nical random variables are shown on the following hierarchical diagram:\\nXn\\ncpl.\\n−→X ⇒ Xn\\na.s.\\n−→X\\n⇓\\nXn\\nP\\n−→X ⇒ Xn\\nd\\n−→X\\n⇑\\nXn\\nLp\\n−→X\\np⩾q\\n⇒ Xn\\nLq\\n−→X\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 461, 'page_label': '444'}, page_content='444 Convergence of Random Variables\\nProof: 1. First, we show that Xn\\nP\\n−→X ⇒Xn\\nd\\n−→X using the inequality P[A ∩B] ⩽P[A]\\nfor any event B. To this end, consider the distribution function FX of X:\\nFXn (x) = P[Xn ⩽x] = P[Xn ⩽x,|Xn −X|>ε] + P[Xn ⩽x,|Xn −X|⩽ε]\\n⩽P[|Xn −X|>ε] + P[Xn ⩽x,X ⩽Xn + ε]\\n⩽P[|Xn −X|>ε] + P[X ⩽x + ε].\\nNow, in the arguments above we can switch the roles of Xn and X (there is a symmetry) to\\ndeduce the analogous result: FX(x) ⩽P[|X −Xn|> ε] + P[Xn ⩽x + ε]. Therefore, making\\nthe switch x →x −εgives FX(x −ε) ⩽P[|X −Xn|> ε] + FXn (x). Putting it all together\\ngives:\\nFX(x −ε) −P[|X −Xn|>ε] ⩽FXn (x) ⩽P[|Xn −X|>ε] + FX(x + ε).\\nTaking n →∞ on both sides yields for any ε> 0:\\nFX(x −ε) ⩽lim\\nn→∞\\nFXn (x) ⩽FX(x + ε).\\nSince FX is continuous at x by assumption we can take ε↓0 to conclude that\\nlimn→∞FXn (x) = FX(x).\\n2. Second, we show that Xn\\nLp\\n−→X ⇒Xn\\nLq\\n−→X for p ⩾q ⩾1. Since the function\\nf (x) = xq/p is concave for q/p ⩽1, Jensen’s inequality yields:☞63\\n(E|X|p)q/p = f (E|X|p) ⩾Ef (|X|p) = E|X|q.\\nIn other words, (E|Xn−X|q)1/q ⩽(E|Xn−X|p)1/p −→0, proving the statement of the theorem.\\n3. Third, we show that Xn\\nL1\\n−→X ⇒Xn\\nP\\n−→X. First note that for any random variable\\nY, we can write: E|Y|⩾E[|Y|1{|Y|>ε}] ⩾E[|ε|1{|Y|>ε}] = εP[|Y|> ε].Therefore, we obtain\\nChebyshev’s inequalityChebyshev’s\\ninequality\\n:\\nP[|Y|>ε] ⩽E|Y|\\nε . (C.36)\\nUsing Chebyshev’s inequality and Xn\\nL1\\n−→X, we can write\\nP[|Xn −X|>ε] ⩽E|Xn −X|\\nε −→0, n →∞.\\nHence, by definition Xn\\nP\\n−→X.\\n4. Finally, Xn\\ncpl.\\n−→X ⇒Xn\\na.s.\\n−→X ⇒Xn\\nP\\n−→X is proved in Examples C.7 and C.3. □\\nFinally, we will make use of the following theorem.\\nTheorem C.14: Slutsky\\nLet g(x,y) be a continuous scalar function of vectorsx and y. Suppose that Xn\\nd\\n−→X\\nand Yn\\nP\\n−→c for some finite constant c. Then,\\ng(Xn,Yn)\\nd\\n−→g(X,c).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 462, 'page_label': '445'}, page_content='Probability and Statistics 445\\nProof: We prove the theorem for scalar X and Y. The proof for random vectors is analog-\\nous. First, we show that Zn :=\\n\"Xn\\nYn\\n#\\nd\\n−→\\n\"X\\nc\\n#\\n=: Z using, for example, Theorem C.12. In ☞ 441\\nother words, we wish to show that the characteristic function of the joint distribution ofXn\\nand Yn converges pointwise as n →∞:\\nψXn,Yn (t) = Eei(t1Xn+t2Yn) −→eit2cEeit1X = ψX,c(t), ∀t ∈R2.\\nTo show the limit above, consider\\n|ψXn,Yn (t) −ψX,c(t)|⩽|ψXn,c(t) −ψX,c(t)|+ |ψXn,Yn (t) −ψXn,c(t)|\\n= |eit2c E(eit1Xn −eit1X)|+ |Eei(t1Xn+t2c)(eit2(Yn−c) −1)|\\n⩽|eit2c|×|E(eit1Xn −eit1X)|+ E|ei(t1Xn+t2c)|×|eit2(Yn−c) −1|\\n⩽|ψXn (t1) −ψX(t1)|+ E|eit2(Yn−c) −1|.\\nSince Xn\\nd\\n−→X, Theorem C.12 implies that ψXn (t1) −→ψX(t1), and the first term |ψXn (t1) −\\nψX(t1)|goes to zero. For the second term we use the fact that\\n|eix −1|=\\n\\x0c\\x0c\\x0c\\nR x\\n0 i eiθ dθ\\n\\x0c\\x0c\\x0c ⩽\\n\\x0c\\x0c\\x0c\\nR x\\n0 |i eiθ|dθ\\n\\x0c\\x0c\\x0c = |x|, x ∈R\\nto obtain the bound:\\nE|eit2(Yn−c) −1|= E|eit2(Yn−c) −1|1{|Yn−c|>ε}+ E|eit2(Yn−c) −1|1{|Yn−c|⩽ε}\\n⩽2E1{|Yn−c|>ε}+ E|t2(Yn −c)|1{|Yn−c|⩽ε}\\n⩽2P[|Yn −c|>ε] + |t2|ε−→|t2|ε, n →∞.\\nSince εis arbitrary, we can letε↓0 to conclude that limn→∞|ψXn,Yn (t)−ψX,c(t)|= 0. In other\\nwords, Zn\\nd\\n−→Z, and by the continuity of g, we have g(Zn)\\nd\\n−→g(Z) or g(Xn,Yn)\\nd\\n−→\\ng(X,c). □\\nExample C.8 (Necessity of Slutsky’s Condition) The condition that Yn converges in\\nprobability to a constant cannot be relaxed. For example, suppose that g(x,y) = x + y,\\nXn\\nd\\n−→X ∼N(0,1) and Yn\\nd\\n−→Y ∼N(0,1). Then, our intuition tempts us to incorrectly\\nconclude that Xn + Yn\\nd\\n−→N(0,2). This intuition is false, because we can have Yn = −Xn\\nfor all n so that Xn + Yn = 0, while both X and Y have the same marginal distribution (in\\nthis case standard normal).\\nC.9 Law of Large Numbers and Central Limit Theorem\\nTwo main results in probability are thelaw of large numbers and the central limit theorem.\\nBoth are limit theorems involving sums of independent random variables. In particular,\\nconsider a sequence X1,X2,... of iid random variables with finite expectation µand finite\\nvariance σ2. For each n define Xn := (X1 +···+Xn)/n. What can we say about the (random)\\nsequence of averagesX1,X2,X3,... ? By (C.12) and (C.14) we haveEXn = µand Var Xn = ☞ 429\\nσ2/n.Hence, as n increases, the variance of the (random) averageXn goes to 0. This means'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 463, 'page_label': '446'}, page_content='446 Law of Large Numbers and Central Limit Theorem\\nthat by Definition C.8, the average Xn converges to µin L2-norm as n →∞, that is, Xn\\nL2\\n−→\\nµ.\\nIn fact, to obtain convergence in probability the variance need not be finite — it is\\nsufficient to assume that µ= EX <∞.\\nTheorem C.15: Weak Law of Large Numbers\\nlaw of large\\nnumbers\\nIf X1,..., Xn are iid with finite expectation µ, then for all ε> 0\\nlim\\nn→∞\\nP\\nh\\n|Xn −µ|>ε\\ni\\n= 0.\\nIn other words, Xn\\nP\\n−→µ.\\nThe theorem has a natural generalization for random vectors. Namely, if µ= EX <∞,\\nthen P\\nh\\n∥Xn −µ∥>ε\\ni\\n→0, where ∥·∥ is the Euclidean norm. We give a proof in the scalar☞355\\ncase.\\nProof: Let Zk := Xk −µ for all k, so that EZ = 0. We thus need to show that Zn\\nP\\n−→0.\\nWe use the properties of the characteristic function of Z denoted as ψZ. Due to the iid☞441\\nassumption, we have\\nψZn (t) = EeitZn = E\\nnY\\ni=1\\neitZi/n =\\nnY\\ni=1\\nEeiZit/n =\\nnY\\ni=1\\nψZ(t/n) = [ψZ(t/n)]n. (C.37)\\nAn application of Taylor’s Theorem B.1 in the neighborhood oft = 0 yields\\nψZ(t/n) = ψZ(0) + o(t/n).\\nSince ψZ(0) = 1, we have:\\nψZn (t) = [ψZ(t/n)]n = [1 + o(1/n)]n −→1, n →∞.\\nThe characteristic function of a random variable that always equals zero is 1. Therefore,\\nTheorem C.12 implies that Zn\\nd\\n−→0. However, according to Example C.6, convergence in\\ndistribution to a constant implies convergence in probability. Hence,Zn\\nP\\n−→0. □\\nThere is also a stronger version of this theorem, as follows.\\nTheorem C.16: Strong Law of Large Numbers\\nstrong law of\\nlarge numbers\\nIf X1,..., Xn are iid with expectation µand EX2 <∞, then for all ε> 0\\nlim\\nn→∞\\nP\\n\"\\nsup\\nk⩾n\\n|Xk −µ|>ε\\n#\\n= 0.\\nIn other words, Xn\\na.s.\\n−→µ.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 464, 'page_label': '447'}, page_content='Probability and Statistics 447\\nProof: First, note that any random variable X can be written as the di fference of two non-\\nnegative random variables: X = X+ −X−, where X+ := max{X,0}and X−:= −min{X,0}.\\nThus, without loss of generality, we assume that the random variables in the theorem above\\nare nonnegative.\\nSecond, from the sequence{X1,X2,X3,... }we can pick up the subsequence{X1,X4,X9,\\nX16,... }=: {X j2 }. Then, from Chebyshev’s inequality (C.36) and the iid condition, we have\\n∞X\\nj=1\\nP\\nh\\x0c\\x0c\\x0cX j2 −µ\\n\\x0c\\x0c\\x0c >ε\\ni\\n⩽Var X\\nε2\\n∞X\\nj=1\\n1\\nj2 <∞.\\nTherefore, by definition Xn2\\ncpl.\\n−→µand from Theorem C.13 we conclude that Xn2\\na.s.\\n−→µ.\\nThird, for any arbitrary n, we can find a k, say k = ⌊√n⌋, so that k2 ⩽n ⩽(k + 1)2.For\\nsuch a k and nonnegative X1,X2,... , it holds that\\nk2\\n(k + 1)2 Xk2 ⩽Xn ⩽X(k+1)2\\n(k + 1)2\\nk2 .\\nSince Xk2 and X(k+1)2 converge almost surely to µ as k (and hence n) goes to infinity, we\\nconclude that Xn\\na.s.\\n−→µ. □\\nNote that the condition EX2 < ∞in Theorem C.16 can be weakened to E|X|< ∞and\\nthe iid condition on the variablesX1,..., Xn can be relaxed to mere pairwise independence.\\nThe corresponding proof, however, is significantly more difficult.\\nThe Central Limit Theorem Central Limit\\nTheorem\\ndescribes the approximate distribution of Xn, and it applies\\nto both continuous and discrete random variables. Loosely, it states that\\nthe average of a large number of iid random variables\\napproximately has a normal distribution.\\nSpecifically, the random variable Xn has a distribution that is approximately normal, with\\nexpectation µand variance σ2/n.\\nTheorem C.17: Central Limit Theorem\\nIf X1,..., Xn are iid with finite expectation µ and finite variance σ2, then for all\\nx ∈R,\\nlim\\nn→∞\\nP\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nXn −µ\\nσ/√n ⩽x\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = Φ(x),\\nwhere Φ is the cdf of the standard normal distribution.\\nProof: Let Zk := (Xk −µ)/σfor all k, so that EZ = 0 and EZ2 = 1. We thus need to show\\nthat √n Zn\\nd\\n−→N(0,1). We again use the properties of the characteristic function. Let ψZ ☞ 441\\nbe the characteristic function of an iid copy of Z, then due to the iid assumption a similar\\ncalculation to the one in (C.37) yields:\\nψ√n Zn (t) = Eeit √n Zn = [ψZ(t/√n)]n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 465, 'page_label': '448'}, page_content='448 Law of Large Numbers and Central Limit Theorem\\nAn application of Taylor’s Theorem B.1 in the neighborhood oft = 0 yields\\nψZ(t/√n) = 1 + t√nψ′\\nZ(0) + t2\\n2nψ′′\\nZ (0) + o(t2/n).\\nSince ψ′\\nZ(0) = E d\\ndt eitZ\\n\\x0c\\x0c\\x0ct=0 = i EZ = 0 and ψ′′\\nZ (0) = i2 EZ2 = −1, we have:\\nψ√n Zn (t) =\\nh\\nψZ(t/√n)\\nin\\n=\\n\"\\n1 − t2\\n2n + o(1/n)\\n#n\\n−→e−t2/2, n →∞.\\nFrom Example C.4, we recognize e−t2/2 as the characteristic function of the standard normal\\ndistribution. Thus, from Theorem C.12 we conclude that √n Zn\\nd\\n−→N(0,1). □\\nFigure C.6 shows the central limit theorem in action. The left part shows the pdfs of\\nX1,2X2,..., 4X4 for the case where the {Xi}have a U[0,1] distribution. The right part\\nshows the same for the Exp(1) distribution. In both cases, we clearly see convergence to a\\nbell-shaped curve, characteristic of the normal distribution.\\n0 1 2 3 4\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0 2 4 6 8\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure C.6: Illustration of the central limit theorem for (left) the uniform distribution and\\n(right) the exponential distribution.\\nThe multivariate version of the central limit theorem is the basis for many asymptotic\\n(in the size of the training set) results in machine learning and data science.\\nTheorem C.18: Multivariate Central Limit Theorem\\nLet X1,..., Xn be iid random vectors with expectation vectorµand finite covariance\\nmatrix Σ. Define Xn := (X1 + ··· + Xn)/n. Then,\\n√n (Xn −µ)\\nd\\n−→N(0,Σ) as n →∞.\\nOne application is as follows. Suppose that a parameter of interest, θ∗, is the unique\\nsolution of the system of equations Eψ(X |θ∗) = 0, where ψis a vector-valued (or multi-\\nvalued) function and the distribution of X does not depend on θ. An M-estimatorM-estimator of θ∗,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 466, 'page_label': '449'}, page_content='Probability and Statistics 449\\ndenoted bθn, is the solution to the system of equations that results from approximating the\\nexpectation with respect to X using an average of n iid copies of X:\\nψn(θ) := 1\\nn\\nnX\\ni=1\\nψ(Xi |θ).\\nThus, ψn(bθn) = 0.\\nTheorem C.19: M-estimator\\nThe M-estimator is asymptotically normal as n →∞:\\n√n (bθn −θ∗)\\nd\\n−→N(0,A−1BA−⊤), (C.38)\\nwhere A := −E∂ψ\\n∂θ(X |θ∗) ☞ 398 andB := E\\x02ψ(X |θ∗) ψ(X |θ∗)⊤\\x03 is the covariance matrix\\nof ψ(X |θ∗).\\nProof: We give a proof under the simplifying assumption3 that bθn is a unique root, that is,\\nfor any θand ε, there exists a δ> 0 such that ∥bθn −θ∥>ε implies that ∥ψn(θ)∥>δ.\\nFirst, we argue that bθn\\nP\\n−→θ∗; that is, P[∥bθn −θ∗∥> ε] →0. From the multivariate\\nextension of Theorem C.15, we have that\\nψn(θ∗)\\nP\\n−→Eψn(θ∗) = Eψ(X |θ∗) = 0.\\nTherefore, using the uniqueness of bθn, we can show thatbθn\\nP\\n−→θ∗via the bound:\\nP\\nh\\n∥bθn −θ∗∥>ε\\ni\\n⩽P\\nh\\n∥ψn(θ∗)∥>δ\\ni\\n= P\\nh\\n∥ψn(θ∗) −Eψn(θ∗)∥>δ\\ni\\n→0, n →∞.\\nSecond, we take a Taylor expansion of each component of the vector ψn(bθn) around θ∗ to\\nobtain:\\nψn(bθn) = ψn(θ∗) + Jn(θ′)(bθn −θ∗),\\nwhere Jn(θ) is the Jacobian of ψn at θ, and θ′ lies on the line segment joining bθn and θ∗.\\nRearrange the last equation and multiply both sides by √n A−1 to obtain:\\n−A−1Jn(θ′)√n (bθn −θ∗) = A−1√n ψn(θ∗).\\nBy the central limit theorem, √n ψ(θ∗) converges in distribution to N(0,B). Therefore,\\n−A−1Jn(θ′)√n (bθn −θ∗)\\nd\\n−→N(0,A−1BA−⊤).\\nTheorem C.15 (the weak law of large numbers) applied to the iid random matrices\\n{∂\\n∂θ ψ(Xi |θ)}shows that\\nJn(θ)\\nP\\n−→E ∂\\n∂θψ(X |θ).\\nMoreover, sincebθn\\nP\\n−→θ∗and Jn is continuous inθ, we have thatJn(θ′)\\nP\\n−→−A. Therefore,\\nby Slutsky’s theorem, −A−1Jn(θ′)√n (bθn −θ∗) −√n (bθn −θ∗)\\nP\\n−→0. □ ☞ 444\\n3The result holds under far less stringent assumptions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 467, 'page_label': '450'}, page_content='450 Law of Large Numbers and Central Limit Theorem\\nFinally, we mention Laplace’s approximationLaplace’s\\napproximation\\n, which shows how integrals or expecta-\\ntions behave under the normal distribution with a vanishingly small variance.\\nTheorem C.20: Laplace’s Approximation\\nSuppose that θn →θ∗, where θ∗lies in the interior of the open set Θ ⊆Rp and that\\nΣn is a p ×p covariance matrix such that Σn →Σ∗. Let g : Θ 7→Rbe a continuous\\nfunction with g(θ∗) , 0. Then, as n →∞,\\nnp/2\\nZ\\nΘ\\ng(θ) e−n\\n2 (θ−θn)⊤Σ−1\\nn (θ−θn) dθ →g(θ∗)\\np\\n|2πΣ∗|. (C.39)\\nProof: (Sketch for a bounded domain Θ.) The left-hand side of (C.39) can be written as\\nthe expectation with respect to the N(θn,Σn/n) distribution:\\np\\n|2πΣn|\\nZ\\nΘ\\ng(θ)\\nexp\\n\\x10\\n−n\\n2 (θ−θn)⊤Σ−1\\nn (θ−θn)\\n\\x11\\n|2πΣn/n|1/2 dθ=\\np\\n|2πΣn|E[g(Xn)1{Xn ∈Θ}],\\nwhere Xn ∼N(θn,Σn/n). Let Z ∼N(0,I). Then, θn + Σ1/2\\nn Z/√n has the same distribution\\nas Xn and\\n\\x10\\nθn + Σ1/2\\nn Z/√n\\n\\x11\\n→θ∗as n →∞. By continuity of g(θ)1{θ∈Θ}in the interior\\nof Θ, as n →∞:4\\nE[g(Xn)1{Xn ∈Θ}] = E\\n\\x14\\ng\\n\\x12\\nθn + Σ1/2\\nn Z√n\\n\\x13\\n1\\n\\x1a\\x12\\nθn + Σ1/2\\nn Z√n\\n\\x13\\n∈Θ\\n\\x1b\\x15\\n−→g(θ∗)1{θ∗∈Θ}.\\nSince θ∗lies in the interior of Θ, we have 1{θ∗∈Θ}= 1, completing the proof. □\\nAs an application of Theorem C.20 we can show the following.\\nTheorem C.21: Approximation of Integrals\\nSuppose that r : θ7→R is twice continuously di fferentiable with a unique global\\nminimum at θ∗and g : θ7→Ris continuous with g(θ∗) >0. Then, as n →∞,\\nln\\nZ\\nRp\\ng(θ) e−n r(θ) dθ≃−n r(θ∗) −p\\n2 ln n. (C.40)\\nMore generally, if rn has a unique global minimum θn and rn →r ⇒θn →θ∗, then\\nln\\nZ\\nRp\\ng(θ) e−n rn(θ) dθ≃−n r(θ∗) −p\\n2 ln n.\\nProof: We only sketch the proof of (C.40). Let H(θ) be the Hessian matrix of r at θ. By☞400\\nTaylor’s theorem we can write\\nr(θ) −r(θ∗) = (θ−θ∗)⊤∂r(θ∗)\\n∂θ| {z }\\n= 0\\n+1\\n2(θ−θ∗)⊤H(θ)(θ−θ∗),\\n4We can exchange the limit and expectation, as g(θ)1{θ∈Θ}⩽maxθ∈Θ g(θ) and\\nR\\nΘ maxθ∈Θ g(θ) dθ =\\n|Θ|maxθ∈Θ g(θ) <∞.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 468, 'page_label': '451'}, page_content='Probability and Statistics 451\\nwhere θ is a point that lies on the line segment joining θ∗ and θ. Since θ∗ is a unique\\nglobal minimum, there must be a small enough neighborhood of θ∗, say Θ, such that r is a\\nstrictly (also known as strongly) convex function on Θ. In other words, H(θ) is a positive ☞ 403\\ndefinite matrix for all θ ∈Θ and there exists a smallest positive eigenvalue λ1 >0 such\\nthat x⊤H(θ)x ⩾λ1∥x∥2 for all x. In addition, since the maximum eigenvalue of H(θ) is a\\ncontinuous function of θ ∈Θ and Θ is bounded, there must exist a constant λ2 >λ1 such\\nthat x⊤H(θ)x ⩽λ2∥x∥2 for all x. In other words, denoting r∗:= r(θ∗), we have the bounds:\\n−λ2\\n2 ∥θ−θ∗∥2 ⩽−(r(θ) −r∗) ⩽−λ1\\n2 ∥θ−θ∗∥2, θ∈Θ.\\nTherefore,\\ne−n r∗\\nZ\\nΘ\\ng(θ) e−nλ2\\n2 ∥θ−θ∗∥2\\ndθ⩽\\nZ\\nΘ\\ng(θ) e−n r(θ)dθ⩽e−n r∗\\nZ\\nΘ\\ng(θ) e−nλ1\\n2 ∥θ−θ∗∥2\\ndθ.\\nAn application of Theorem C.20 yields\\nR\\nΘ g(θ) e−n r(θ) dθ= O(e−n r∗\\n/np/2) and, more import-\\nantly,\\nln\\nZ\\nΘ\\ng(θ) e−n r(θ) dθ≃−n r∗−p\\n2 ln n.\\nThus, the proof will be complete once we show that\\nR\\nΘ g(θ) e−n r(θ) dθ,with Θ := Rp \\\\Θ, is\\nasymptotically negligible compared to\\nR\\nΘ g(θ) e−n r(θ) dθ. Since θ∗is a global minimum that\\nlies outside any neighborhood ofΘ, there must exists a constantc >0 such thatr(θ)−r∗>c\\nfor all θ∈Θ. Therefore,\\nZ\\nΘ\\ng(θ) e−n r(θ) dθ= e−(n−1) r∗\\nZ\\nΘ\\ng(θ) e−r(θ) e−(n−1)(r(θ)−r∗) dθ\\n⩽e−(n−1) r∗\\nZ\\nΘ\\ng(θ) e−r(θ) e−(n−1)c dθ\\n⩽e−(n−1)(r∗+c)\\nZ\\nRp\\ng(θ) e−r(θ) dθ= O(e−n(r∗+c)).\\nThe last expression is of order o(e−n r∗\\n/np/2), concluding the proof. □\\nC.10 Markov Chains\\nDefinition C.6: Markov Chain\\nA Markov chain Markov chainis a collection {Xt,t = 0,1,2,... }of random variables (or ran-\\ndom vectors) whose futures are conditionally independent of their pasts given their\\npresent values. That is,\\nP[Xt+1 ∈A |Xs,s ⩽t] = P[Xt+1 ∈A |Xt] for all t. (C.41)\\nIn other words, the conditional distribution of the future variable Xt+1, given the entire\\npast {Xs,s ⩽t}, is the same as the conditional distribution ofXt+1 given only the present Xt.\\nProperty (C.41) is called the Markov property Markov\\nproperty\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 469, 'page_label': '452'}, page_content='452 Markov Chains\\nThe index t in Xt is usually seen as a “time” or “step” parameter. The index set\\n{0,1,2,... }in the definition above was chosen out of convenience. It can be replaced by any\\ncountable index set. We restrict ourselves totime-homogeneoustime-\\nhomogeneous\\nMarkov chains — Markov\\nchains for which the conditional pdfs fXt+1 |Xt (y |x) do not depend on t; we abbreviate these\\nas q(y |x). The {q(y |x)}are called the (one-step) transition densitiestransition\\ndensity\\nof the Markov chain.\\nNote that the random variables or vectors {Xt}may be discrete (e.g., taking values in some\\nset {1,..., r}) or continuous (e.g., taking values in an interval [0,1] or Rd). In particular, in\\nthe discrete case, each q(y |x) is a probability: q(y |x) = P[Xt+1 = y |Xt = x].\\nThe distribution of X0 is called the initial distributioninitial\\ndistribution\\nof the Markov chain. The one-\\nstep transition densities and the initial distribution completely specify the distribution of\\nthe random vector [ X0,X1,..., Xt]⊤. Namely, we have by the product rule (C.17) and the☞431\\nMarkov property that the joint pdf is given by\\nfX0,...,Xt (x0,..., xt) = fX0 (x0) fX1 |X0 (x1 |x0) ··· fXt |Xt−1,...,X0 (xt |xt−1,..., x0)\\n= fX0 (x0) fX1 |X0 (x1 |x0) ··· fXt |Xt−1 (xt |xt−1)\\n= fX0 (x0) q(x1 |x0) q(x2 |x1) ··· q(xt |xt−1).\\nA Markov chain is said to be ergodicergodic if the probability distribution of Xt converges to\\na fixed distribution as t →∞. Ergodicity is a property of many Markov chains. Intuitively,\\nthe probability of encountering the Markov chain in a state x at a time t far into the future\\nshould not depend on the t, provided that the Markov chain can reach every state from any\\nother state — such Markov chains are said to be irreducible — and does not “escape” to\\ninfinity. Thus, for an ergodic Markov chain the pdf fXt (x) converges to a fixed limiting pdflimiting pdf\\nf (x) as t →∞, irrespective of the starting state. For the discrete case, f (x) corresponds to\\nthe long-run fraction of times that the Markov process visits x.\\nUnder mild conditions (such as irreducibility) the limiting pdf f (x) can be found by\\nsolving the global balance equationsglobal balance\\nequations\\n:\\nf (x) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nP\\ny f (y) q(x |y) (discrete case) ,R\\nf (y) q(x |y) dy (continuous case). (C.42)\\nFor the discrete case the rationale behind this is as follows. Since f (x) is the long-run\\nproportion of time that the Markov chain spends in x, the proportion of transitions out of\\nx is f (x). This should be balanced with the proportion of transitions into state x, which isP\\ny f (y) q(x |y).\\nOne is often interested in a stronger type of balance equations. Imagine that we have\\ntaken a video of the evolution of the Markov chain, which we may run in forward and\\nreverse time. If we cannot determine whether the video is running forward or backward\\n(we cannot determine any systematic “looping”, which would indicate in which direction\\ntime is flowing), the chain is said to be time-reversible or simply reversiblereversible .\\nAlthough not every Markov chain is reversible, each ergodic Markov chain, when run\\nbackwards, gives another Markov chain — the reverse Markov chainreverse\\nMarkov chain\\n— with transition\\ndensities eq(y |x) = f (y) q(x |y)/f (x). To see this, first observe that f (x) is the long-run\\nproportion of time spent in x for both the original and reverse Markov chain. Secondly,\\nthe “probability flux” from x to y in the reversed chain must be equal to the probability\\nflux from y to x in the original chain, meaning f (x)eq(y |x) = f (y) q(x |y), which yields the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 470, 'page_label': '453'}, page_content='Probability and Statistics 453\\nstated transition probabilities for the reversed chain. In particular, for a reversible Markov\\nchain we have\\nf (x) q(y |x) = f (y) q(x |y) for all x,y. (C.43)\\nThese are the detailed (or local) balance equations . Note that the detailed balance equa- local balance\\nequationstions imply the global balance equations. Hence, if a Markov chain is irreducible and there\\nexists a pdf such that (C.43) holds, then f (x) must be the limiting pdf. In the discrete state\\nspace case an additional condition is that the chain must be aperiodic aperiodic, meaning that the\\nreturn times to the same state cannot always be a multiple of some integer ⩾2.\\nExample C.9 (Random Walk on a Graph) Consider a Markov chain that performs a\\n“random walk” on the graph in Figure C.7, at each step jumping from the current vertex\\n(node) to one of the adjacent vertices, with equal probability. Clearly this Markov chain is\\nreversible. It is also irreducible and aperiodic. Let f (x) denote the limiting probability that\\nthe chain is in vertex x. By symmetry, f (1) = f (2) = f (7) = f (8), f (4) = f (5) and f (3) =\\nf (6). Moreover, by the detailed balance equations, f (4)/5 = f (1)/3, and f (3)/4 = f (1)/3.\\nIt follows that f (1) + ··· + f (8) = 4 f (1) + 2 ×5/3 f (1) + 2 ×4/3 f (1) = 10 f (1) = 1, so\\nthat f (1) = 1/10, f (3) = 2/15, and f (4) = 1/6.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nFigure C.7: The random walk on this graph is reversible.\\nC.11 Statistics\\nStatistics deals with the gathering, summarization, analysis, and interpretation of data. The\\ntwo main branches of statistics are:\\n1. Classical or frequentist statistics frequentist\\nstatistics\\n: Here the observed data τ is viewed as the out-\\ncome of random data Tdescribed by a probabilistic model — usually the model is\\nspecified up to a (multidimensional) parameter; that is, T∼ g(·|θ) for some θ. The\\nstatistical inference is then purely concerned with the model and in particular with\\nthe parameter θ. For example, on the basis of the data one may wish to\\n(a) estimate the parameter,\\n(b) perform statistical tests on the parameter, or'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 471, 'page_label': '454'}, page_content='454 Estimation\\n(c) validate the model.\\n2. Bayesian statisticsBayesian\\nstatistics\\n: In this approach we average over all possible values of the\\nparameter θ using a user-specified weight function g(θ) and obtain the model\\nT∼\\nR\\ng(·|θ) g(θ) dθ. For practical computations, this means that we can treat θas a\\nrandom variable with pdf g(θ). Bayes’ formula g(θ|τ) ∝g(τ|θ) g(θ) is used to learn☞48\\nθbased on the observed data τ.\\nExample C.10 (Iid Sample) The most fundamental statistical model is where the data\\nT= X1,..., Xn is such that the random variablesX1,..., Xn are assumed to be independent\\nand identically distributed:\\nX1,..., Xn\\niid\\n∼Dist,\\naccording to some known or unknown distribution Dist. An iid sample is often called a\\nrandom samplerandom sample in the statistics literature. Note that the word “sample” can refer to both a\\ncollection of random variables and to a single random variable. It should be clear from the\\ncontext which meaning is being used.\\nOften our guess or model for the true distribution is specified up to an unknown para-\\nmeter θ, with θ∈Θ. The most common model is:\\nX1,..., Xn\\niid\\n∼N(µ,σ2),\\nin which case θ= (µ,σ2) and Θ = R×R+.\\nC.12 Estimation\\nSuppose the model g(·|θ) for the data Tis completely specified up to an unknown para-\\nmeter vector θ. The aim is to estimate θon the basis of the observed data τonly (an altern-\\native goal could be to estimate η= ψ(θ) for some vector-valued function ψ). Specifically,\\nthe goal is to find an estimator T = T(T) that is close to the unknown θ. The correspond-estimator\\ning outcome t = T(τ) is the estimate of θ. The bias of an estimator T of θ is defined asestimate\\nbias ET −θ. An estimator T of θis said to be unbiased if EθT = θ. We often write bθfor both\\nan estimator and estimate of θ. The mean squared error (MSE) of a real-valued estimatormean squared\\nerror T is defined as\\nMSE = Eθ(T −θ)2.\\nAn estimator T1 is said to be moreefficientefficient than an estimator T2 if the MSE of T1 is smaller\\nthan the MSE of T2. The MSE can be written as the sum\\nMSE = (EθT −θ)2 + VarθT.\\nThe first term measures the unbiasedness and the second is the variance of the estimator.\\nIn particular, for an unbiased estimator the MSE of an estimator is simply equal to its\\nvariance.\\nFor simulation purposes it is often important to include the running time of the estim-\\nator in efficiency comparisons. One way to compare two unbiased estimators T1 and T2 is\\nto compare their relative time variance productsrelative time\\nvariance\\nproducts\\n,\\nri Var Ti\\n(ETi)2 , i = 1,2, (C.44)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 472, 'page_label': '455'}, page_content='Probability and Statistics 455\\nwhere r1 and r2 are the times required to calculate the estimators T1 and T2, respectively.\\nIn this scheme, T1 is considered more efficient than T2 if its relative time variance product\\nis smaller. We discuss next two systematic approaches for constructing sound estimators.\\nC.12.1 Method of Moments\\nSuppose x1,..., xn are outcomes from an iid sample X1,..., Xn ∼iid g(x |θ), where θ =\\n[θ1,...,θ k]⊤ is unknown. The moments of the sampling distribution can be easily estim-\\nated. Namely, if X ∼g(x |θ), then the r-th moment of X, that is µr(θ) = EθXr (assuming\\nit exists), can be estimated through the sample r-th moment: 1\\nn\\nPn\\ni=1 xr\\ni .The method of mo- sample r-th\\nmomentments involves choosing the estimate bθ of θ such that each of the first k sample and true\\nmethod of\\nmomentsmoments are matched:\\n1\\nn\\nnX\\ni=1\\nxr\\ni = µr(bθ), r = 1,2,..., k.\\nIn general, this set of equations is nonlinear and so its solution often has to be found\\nnumerically.\\nExample C.11 (Sample Mean and Sample Variance) Suppose the data is given by\\nT= {X1,..., Xn}, where the {Xi}form an iid sample from a general distribution with mean\\nµand variance σ2 <∞. Matching the first two moments gives the set of equations\\n1\\nn\\nnX\\ni=1\\nxi = µ,\\n1\\nn\\nnX\\ni=1\\nx2\\ni = µ2 + σ2.\\nThe method of moments estimates for µand σ2 are therefore the sample mean sample mean\\nbµ= x = 1\\nn\\nnX\\ni=1\\nxi, (C.45)\\nand\\ncσ2 = 1\\nn\\nnX\\ni=1\\nx2\\ni −(x)2 = 1\\nn\\nnX\\ni=1\\n(xi −x)2. (C.46)\\nThe corresponding estimator for µ, X, is unbiased. However, the estimator forσ2 is biased:\\nEcσ2 = σ2(n −1)/n. An unbiased estimator is the sample variance sample variance\\nS 2 = cσ2 n\\nn −1 = 1\\nn −1\\nnX\\ni=1\\n(Xi −X)2.\\nIts square root, S =\\n√\\nS 2, is called the sample standard deviation sample\\nstandard\\ndeviation\\n.\\nExample C.12 (Sample Covariance Matrix) The method of moments can also be\\nused to estimate the covariance matrix of a random vector. In particular, let the X1,..., Xn'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 473, 'page_label': '456'}, page_content='456 Estimation\\nbe iid copies of a d-dimensional random vector X with mean vector µ and covari-\\nance matrix Σ. We assume n ⩾d. The moment estimator for µ is, as in the d = 1 case,\\nX = (X1 + ··· + Xn)/n. As the covariance matrix can be written (see (C.15)) as☞430\\nΣ = E(X −µ)(X −µ)⊤,\\nthe method of moments yields the estimator\\nbΣ = 1\\nn\\nnX\\ni=1\\n(Xi −X)(Xi −X)⊤. (C.47)\\nSimilar to the one-dimensional case (d = 1), replacing the factor 1/n with 1/(n −1) gives\\nan unbiased estimator, called the sample covariance matrixsample\\ncovariance\\nmatrix\\n.\\nC.12.2 Maximum Likelihood Method\\nThe concept of likelihood is central in statistics. It describes in a precise way the informa-\\ntion about model parameters that is contained in the observed data.\\nLet Tbe a (random) data object that is modeled as a draw from the pdf g(τ|θ) (dis-\\ncrete or continuous) with parameter vector θ∈Θ. Let τbe an outcome of T. The function\\nL(θ|τ) : = g(τ|θ), θ ∈ Θ, is called the likelihood function of θ, based on τ. The (nat-likelihood\\nfunction ural) logarithm of the likelihood function is called the log-likelihood function and is often\\nlog-likelihood\\nfunction denoted by a lower case l.\\nNote that L(θ|τ) and g(τ|θ) have the same formula, but the first is viewed as a\\nfunction of θfor fixed τ, where the second is viewed as a function of τfor fixed θ.\\nThe concept of likelihood is particularly useful when Tis modeled as an iid sample\\n{X1,..., Xn}from some pdf ˚g. In that case, the likelihood of the data τ = {x1,..., xn}, as a\\nfunction of θ, is given by the product\\nL(θ|τ) =\\nnY\\ni=1\\n˚g(xi |θ). (C.48)\\nLet τ be an observation from T∼ g(τ|θ), and suppose that g(τ|θ) takes its largest\\nvalue at θ = bθ. In a way this bθis our best estimate for θ, as it maximizes the probability\\n(density) for the observation τ. It is called the maximum likelihood estimate (MLE) of θ.\\nNote thatbθ= bθ(τ) is a function of τ. The corresponding random variable, also denotedbθis\\nthe maximum likelihood estimatormaximum\\nlikelihood\\nestimator\\n(also abbreviated as MLE).\\nMaximization of L(θ|τ) as a function of θis equivalent (when searching for the max-\\nimizer) to maximizing the log-likelihood l(θ|τ), as the natural logarithm is an increasing\\nfunction. This is often easier, especially when T is an iid sample from some sampling\\ndistribution. For example, for L of the form (C.48), we have\\nl(θ|τ) =\\nnX\\ni=1\\nln ˚g(xi |θ).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 474, 'page_label': '457'}, page_content='Probability and Statistics 457\\nIf l(θ|τ) is a di fferentiable function with respect to θand the maximum is attained in the\\ninterior of Θ, and there exists a unique maximum point, then we can find the MLE of θby\\nsolving the equations\\n∂\\n∂θi\\nl(θ|τ) = 0, i = 1,..., d.\\nExample C.13 (Bernoulli Random Sample) Suppose we have data τn = {x1,..., xn}\\nand assume the model X1,..., Xn ∼iid Ber(θ). Then, the likelihood function is given by\\nL(θ|τ) =\\nnY\\ni=1\\nθxi (1 −θ)1−xi = θs(1 −θ)n−s, 0 <θ< 1, (C.49)\\nwhere s := x1 +···+ xn =: nx. The log-likelihood is l(θ) = s ln θ+(n −s) ln(1−θ). Through\\ndifferentiation with respect to θ, we find the derivative\\ns\\nθ −n −s\\n1 −θ = s\\nθ(1 −θ) − n\\n1 −θ. (C.50)\\nSolving l′(θ) = 0 gives the ML estimatebθ= x and ML estimator bθ= X.\\nC.13 Confidence Intervals\\nAn essential part in any estimation procedure is to provide an assessment of the accuracy\\nof the estimate. Indeed, without information on its accuracy the estimate itself would be\\nmeaningless. Confidence intervals (also calledinterval estimates interval\\nestimates\\n) provide a precise way of\\ndescribing the uncertainty in the estimate.\\nLet X1,..., Xn be random variables with a joint distribution depending on a parameter\\nθ ∈Θ. Let T1 < T2 be statistics; that is, Ti = Ti(X1,..., Xn), i = 1,2 are functions of the\\ndata, but not of θ.\\n1. The random interval ( T1,T2) is called a stochastic confidence interval stochastic\\nconfidence\\ninterval\\nfor θ with\\nconfidence 1 −αif\\nPθ[T1 <θ< T2] ⩾1 −α for all θ∈Θ. (C.51)\\n2. If t1 and t2 are the observed values of T1 and T2, then the interval (t1,t2) is called the\\n(numerical) confidence interval (numerical)\\nconfidence\\ninterval\\nfor θwith confidence 1 −αfor every θ∈Θ.\\n3. If the right-hand side of (C.51) is merely a heuristic estimate or approximation of\\nthe true probability, then the resulting interval is called an approximate confidence\\ninterval.\\n4. The probability Pθ[T1 < θ <T2] is called the coverage probability coverage\\nprobability\\n. For a 1 −α\\nconfidence interval, it must be at least 1 −α.\\nFor multidimensional parameters θ ∈Rd the stochastic confidence interval is replaced\\nwith a stochastic confidence region confidence\\nregion\\nC⊂ Rd such that Pθ[θ∈C] ⩾1 −αfor all θ.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 475, 'page_label': '458'}, page_content='458 Hypothesis Testing\\nExample C.14 (Approximate Confidence Interval for the Mean) Let X1,X2,..., Xn\\nbe an iid sample from a distribution with mean µ and variance σ2 <∞(both assumed\\nto be unknown). By the central limit theorem and the law of large numbers,☞447\\nT = X −µ\\nS/√n\\napprox.\\n∼ N(0,1),\\nfor large n, where S is the sample standard deviation. Rearranging the approximate equality\\nP[|T| ⩽ z1−α/2] ≈ 1 −α, where z1−α/2 is the 1 −α/2 quantile of the standard normal\\ndistribution, yields\\nP\\n\"\\nX −z1−α/2\\nS√n ⩽µ⩽X + z1−α/2\\nS√n\\n#\\n≈1 −α,\\nso that  \\nX −z1−α/2\\nS√n, X + z1−α/2\\nS√n\\n!\\n, abbreviated as X ±z1−α/2\\nS√n, (C.52)\\nis an approximate stochastic (1 −α) confidence interval for µ.\\nSince (C.52) is an asymptotic result only, care should be taken when applying it to\\ncases where the sample size is small or moderate and the sampling distribution is heavily\\nskewed.\\nC.14 Hypothesis Testing\\nSuppose the model for the data Tis described by a family of probability distributions that\\ndepend on a parameter θ ∈Θ. The aim of hypothesis testing is to decide, on the basis ofhypothesis\\ntesting the observed data τ, which of two competing hypotheses holds true; these being the null\\nhypothesis, H0 : θ∈Θ0, and the alternative hypothesis, H1 : θ∈Θ1.null hypothesis\\nalternative\\nhypothesis\\nIn classical statistics the null hypothesis and alternative hypothesis do not play equival-\\nent roles. H0 contains the “status quo” statement and is only rejected if the observed data\\nare very unlikely to have happened under H0.\\nThe decision whether to accept or reject H0 is dependent on the outcome of a test\\nstatistictest statistic T = T(T). For simplicity, we discuss only the one-dimensional case T ≡T. Two\\n(related) types of decision rules are generally used:\\n1. Decision rule 1: Reject H0 if T falls in the critical region.\\nHere the critical regioncritical region is any appropriately chosen region in R. In practice a critical\\nregion is one of the following:\\n• left one-sided: (−∞,c],\\n• right one-sided: [c,∞),\\n• two-sided: (−∞,c1] ∪[c2,∞).\\nFor example, for a right one-sided test, H0 is rejected if the outcome of the test\\nstatistic is too large. The endpoints c, c1, and c2 of the critical regions are called\\ncritical valuescritical values .'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 476, 'page_label': '459'}, page_content='Probability and Statistics 459\\n2. Decision rule 2: Reject H0 if the P-value is smaller than some significance level α.\\nThe P-value P-valueis the probability that, under H0, the (random) test statistic takes a value\\nas extreme as or more extreme than the one observed. In particular, ift is the observed\\noutcome of the test statistic T, then\\n• left one-sided test: P := PH0 [T ⩽t],\\n• right one-sided: P := PH0 [T ⩾t],\\n• two-sided: P := min{2PH0 [T ⩽t], 2PH0 [T ⩾t]}.\\nThe smaller the P-value, the greater the strength of the evidence againstH0 provided\\nby the data. As a rule of thumb:\\nP <0.10 suggestive evidence,\\nP <0.05 reasonable evidence,\\nP <0.01 strong evidence.\\nWhether the first or the second decision rule is used, one can make two types of errors,\\nas depicted in Table C.4.\\nTable C.4: Type I and II errors in hypothesis testing.\\nTrue statement\\nDecision H0 is true H1 is true\\nAccept H0 Correct Type II Error\\nReject H0 Type I Error Correct\\nThe choice of the test statistic and the corresponding critical region involves a multiob-\\njective optimization criterion, whereby both the probabilities of a type I and type II error\\nshould, ideally, be chosen as small as possible. Unfortunately, these probabilities compete\\nwith each other. For example, if the critical region is made larger (smaller), the probability\\nof a type II error is reduced (increased), but at the same time the probability of a type I\\nerror is increased (reduced).\\nSince the type I error is considered more serious, Neyman and Pearson [93] suggested\\nthe following approach: choose the critical region such that the probability of a type II error\\nis as small as possible, while keeping the probability of a type I error below a predetermined\\nsmall significance level significance\\nlevel\\nα.\\nRemark C.3 (Equivalence of Decision Rules) Note that decision rule 1 and 2 are\\nequivalent in the following sense:\\nReject H0 if T falls in the critical region, at significance level α.\\n⇔\\nReject H0 if the P-value is ⩽significance level α.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 477, 'page_label': '460'}, page_content='460 Hypothesis Testing\\nIn other words, the P-value of the test is the smallest level of significance that would lead\\nto the rejection of H0.\\nIn general, a statistical test involves the following steps:\\n1. Formulate an appropriate statistical model for the data.\\n2. Give the null ( H0) and alternative (H1) hypotheses in terms of the parameters\\nof the model.\\n3. Determine the test statistic (a function of the data only).\\n4. Determine the (approximate) distribution of the test statistic under H0.\\n5. Calculate the outcome of the test statistic.\\n6. Calculate the P-value or the critical region, given a preselected significance\\nlevel α.\\n7. Accept or reject H0.\\nThe actual choice of an appropriate test statistic is akin to selecting a good estimator\\nfor the unknown parameter θ. The test statistic should summarize the information about θ\\nand make it possible to distinguish between the alternative hypotheses.\\nExample C.15 (Hypothesis Testing) We are given outcomes x1,..., xm and y1,..., yn\\nof two simulation studies obtained via independent runs, with m = 100 and n = 50. The\\nsample means and standard deviations are x = 1.3, sX = 0.1 and y = 1.5, sY = 0.3. Thus,\\nthe {xi}are outcomes of iid random variables {Xi}, the {yi}are outcomes of iid random\\nvariables {Yi}, and the {Xi}and {Yi}are independent. We wish to assess whether the expect-\\nations µX = EXi and µY = EYi are the same or not. Going through the 7 steps above, we\\nhave:\\n1. The model is already specified above.\\n2. H0 : µX −µY = 0 versus H1 : µX −µY , 0.\\n3. For similar reasons as in Example C.14, take\\nT = X −Yq\\nS 2\\nX/m + S 2\\nY /n\\n.\\n4. By the central limit theorem, the statistic T has, under H0, approximately a standard\\nnormal distribution (assuming the variances are finite).\\n5. The outcome of T is t = (x −y)/\\nq\\ns2\\nX/m + s2\\nY /n ≈−4.59.\\n6. As this is a two-sided test, the P-value is 2 PH0 [T ⩽−4.59] ≈4 ·10−6.\\n7. Because the P-value is extremely small, there is overwhelming evidence that the two\\nexpectations are not the same.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 478, 'page_label': '461'}, page_content='Probability and Statistics 461\\nFurther Reading\\nAccessible treatises on probability and stochastic processes include [27, 26, 39, 54, 101].\\nKallenberg’s book [61] provides a complete graduate-level overview of the foundations of\\nmodern probability. Details on the convergence of probability measures and limit theorems\\ncan be found in [11]. For an accessible introduction to mathematical statistics with simple\\napplications see, for example, [69, 74, 124]. For a more detailed overview of statistical\\ninference, see [10, 25]. A standard reference for classical (frequentist) statistical inference\\nis [78].'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 479, 'page_label': '462'}, page_content='462'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 480, 'page_label': '463'}, page_content='APPENDIX D\\nPYTHON PRIMER\\nPython has become the programming language of choice for many researchers and\\npractitioners in data science and machine learning. This appendix gives a brief intro-\\nduction to the language. As the language is under constant development and each year\\nmany new packages are being released, we do not pretend to be exhaustive in this in-\\ntroduction. Instead, we hope to provide enough information for novices to get started\\nwith this beautiful and carefully thought-out language.\\nD.1 Getting Started\\nThe main website for Python is\\nhttps://www.python.org/,\\nwhere you will find documentation, a tutorial, beginners’ guides, software examples, and\\nso on. It is important to note that there are two incompatible “branches” of Python, called\\nPython 3 and Python 2. Further development of the language will involve only Python 3,\\nand in this appendix (and indeed the rest of the book) we only consider Python 3. As there\\nare many interdependent packages that are frequently used with a Python installation, it\\nis convenient to install a distribution — for instance, the Anaconda AnacondaPython distribution,\\navailable from\\nhttps://www.anaconda.com/.\\nThe Anaconda installer automatically installs the most important packages and also\\nprovides a convenient interactive development environment (IDE), calledSpyder.\\nUse the Anaconda Navigator to launch Spyder, Jupyter notebook, install and update\\npackages, or open a command-line terminal.\\nTo get started1, try out the Python statements in the input boxes that follow. You can\\neither type these statements at the IPython command prompt or run them as (very short)\\n1We assume that you have installed all the necessary files and have launchedSpyder.\\n463'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 481, 'page_label': '464'}, page_content='464 Getting Started\\nPython programs. The output for these two modes of input can di ffer slightly. For ex-\\nample, typing a variable name in the console causes its contents to be automatically printed,\\nwhereas in a Python program this must be done explicitly by calling the print function.\\nSelecting (highlighting) several program lines in Spyder and then pressing function key 2\\nF9is equivalent to executing these lines one by one in the console.\\nIn Python, data is represented as an objectobject or relation between objects (see also Sec-\\ntion D.2). Basic data types are numeric types (including integers, booleans, and floats),\\nsequence types (including strings, tuples, and lists), sets, and mappings (currently, diction-\\naries are the only built-in mapping type).\\nStrings are sequences of characters, enclosed by single or double quotes. We can print\\nstrings via the print function.\\nprint (\"Hello World!\")\\nHello World!\\nFor pretty-printing output, Python strings can be formatted using theformat function. The\\nbracket syntax {i}provides a placeholder for the i-th variable to be printed, with 0 being\\nthe first index. Individual variables can be formatted separately and as desired; formatting\\nsyntax is discussed in more detail in Section D.9.☞476\\nprint (\"Name:{1} (height {2} m, age {0})\". format (111,\"Bilbo\",0.84))\\nName:Bilbo (height 0.84 m, age 111)\\nLists can contain different types of objects, and are created using square brackets as in the\\nfollowing example:\\nx = [1, \\'string \\',\"another string\"] # Quote type is not important\\n[1, \\'string \\', \\'another string \\']\\nElements in lists are indexed starting from 0, and are mutablemutable (can be changed):\\nx = [1,2]\\nx[0] = 2 # Note that the first index is 0\\nx\\n[2,2]\\nIn contrast, tuples (with round brackets) are immutableimmutable (cannot be changed). Strings are\\nimmutable as well.\\nx = (1,2)\\nx[0] = 2\\nTypeError: \\'tuple \\' object does not support item assignment\\n2This may depend on the keyboard and operating system.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 482, 'page_label': '465'}, page_content=\"Python Primer 465\\nLists can be accessed via the slice slicenotation [start:end]. It is important to note that end\\nis the index of the first element that willnot be selected, and that the first element has index\\n0. To gain familiarity with the slice notation, execute each of the following lines.\\na = [2, 3, 5, 7, 11, 13, 17, 19, 23]\\na[1:4] # Elements with index from 1 to 3\\na[:4] # All elements with index less than 4\\na[3:] # All elements with index 3 or more\\na[-2:] # The last two elements\\n[3, 5, 7]\\n[2, 3, 5, 7]\\n[7, 11, 13, 17, 19, 23]\\n[19, 23]\\nAn operator operatoris a programming language construct that performs an action on one or more\\noperands. The action of an operator in Python depends on the type of the operand(s). For\\nexample, operators such as +, ∗, −, and % that are arithmetic operators when the operands\\nare of a numeric type, can have di fferent meanings for objects of non-numeric type (such\\nas strings).\\n'hello ' + 'world ' # String concatenation\\n'helloworld '\\n'hello ' * 2 # String repetition\\n'hellohello '\\n[1,2] * 2 # List repetition\\n[1, 2, 1, 2]\\n15 % 4 # Remainder of 15/4\\n3\\nSome common Python operators are given in Table D.1. ☞ 468\\nD.2 Python Objects\\nAs mentioned in the previous section, data in Python is represented by objects or relations\\nbetween objects. We recall that basic data types included strings and numeric types (such\\nas integers, booleans, and floats).\\nAs Python is an object-oriented programming language, functions are objects too\\n(everything is an object!). Each object has an identity (unique to each object and immutable\\n— that is, cannot be changed — once created), a type (which determines which operations\\ncan be applied to the object, and is considered immutable), and a value (which is either\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 483, 'page_label': '466'}, page_content='466 Types and Operators\\nmutable or immutable). The unique identity assigned to an object obj can be found by\\ncalling id, as in id(obj).\\nEach object has a list of attributesattributes , and each attribute is a reference to another object.\\nThe function dir applied to an object returns the list of attributes. For example, a string\\nobject has many useful attributes, as we shall shortly see. Functions are objects with the\\n__call__attribute.\\nA class (see Section D.8) can be thought of as a template for creating a custom type of\\nobject.\\ns = \"hello\"\\nd = dir (s)\\nprint (d,flush=True) # Print the list in \"flushed\" format\\n[\\'__add__ \\', \\'__class__ \\', \\'__contains__ \\', \\'__delattr__ \\', \\'__dir__ \\',\\n... (many left out) ... \\'replace \\', \\'rfind \\',\\n\\'rindex \\', \\'rjust \\', \\'rpartition \\', \\'rsplit \\', \\'rstrip \\', \\'split \\',\\n\\'splitlines \\', \\'startswith \\', \\'strip \\', \\'swapcase \\', \\'title \\',\\n\\'translate \\', \\'upper \\', \\'zfill \\']\\nAny attribute attr of an object obj can be accessed via the dot notationdot notation : obj.attr. To\\nfind more information about any object use the help function.\\ns = \"hello\"\\nhelp (s.replace)\\nreplace(...) method of builtins.str instance\\nS.replace(old, new[, count]) -> str\\nReturn a copy of S with all occurrences of substring\\nold replaced by new. If the optional argument count is\\ngiven, only the first count occurrences are replaced.\\nThis shows that the attributereplace is in fact a function. An attribute that is a function is\\ncalled a methodmethod . We can use the replace method to create a new string from the old one\\nby changing certain characters.\\ns = \\'hello \\'\\ns1 = s.replace( \\'e\\',\\'a\\')\\nprint (s1)\\nhallo\\nIn many Python editors, pressing the TAB key, as inobjectname.<TAB>, will bring\\nup a list of possible attributes via the editor’s autocompletion feature.\\nD.3 Types and Operators\\nEach object has a typetype . Three basic data types in Python are str (for string), int (for\\nintegers), and float (for floating point numbers). The function type returns the type of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 484, 'page_label': '467'}, page_content=\"Python Primer 467\\nan object.\\nt1 = type ([1,2,3])\\nt2 = type ((1,2,3))\\nt3 = type ({1,2,3})\\nprint (t1,t2,t3)\\n<class 'list '> <class 'tuple '> <class 'set'>\\nThe assignment assignmentoperator, =, assigns an object to a variable; e.g.,x = 12. An expression\\nis a combination of values, operators, and variables that yields another value or variable.\\nVariable names are case sensitive and can only contain letters, numbers, and under-\\nscores. They must start with either a letter or underscore. Note that reserved words\\nsuch as Trueand Falseare case sensitive as well.\\nPython is a dynamically typed language, and the type of a variable at a particular point\\nduring program execution is determined by its most recent object assignment. That is, the\\ntype of a variable does not need to be explicitly declared from the outset (as is the case in\\nC or Java), but instead the type of the variable is determined by the object that is currently\\nassigned to it.\\nIt is important to understand that a variable in Python is a reference referenceto an object —\\nthink of it as a label on a shoe box. Even though the label is a simple entity, the contents\\nof the shoe box (the object to which the variable refers) can be arbitrarily complex. Instead\\nof moving the contents of one shoe box to another, it is much simpler to merely move the\\nlabel.\\nx = [1,2]\\ny = x # y refers to the same object as x\\nprint (id (x) == id (y)) # check that the object id 's are the same\\ny[0] = 100 # change the contents of the list that y refers to\\nprint (x)\\nTrue\\n[100,2]\\nx = [1,2]\\ny = x # y refers to the same object as x\\ny = [100,2] # now y refers to a different object\\nprint (id (x) == id (y))\\nprint (x)\\nFalse\\n[1,2]\\nTable D.1 shows a selection of Python operators for numerical and logical variables.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 485, 'page_label': '468'}, page_content='468 Functions and Methods\\nTable D.1: Common numerical (left) and logical (right) operators.\\n+ addition ~ binary NOT\\n- subtraction & binary AND\\n* multiplication ^ binary XOR\\n** power | binary OR\\n/ division == equal to\\n// integer division != not equal to\\n% modulus\\nSeveral of the numerical operators can be combined with an assignment operator, as in\\nx += 1to mean x = x + 1. Operators such as +and *can be defined for other data types\\nas well, where they take on a di fferent meaning. This is called operator overloading, an\\nexample of which is the use of <List>␣*␣<Integer>for list repetition as we saw earlier.\\nD.4 Functions and Methods\\nFunctions make it easier to divide a complex program into simpler parts. To create a\\nfunctionfunction , use the following syntax:\\ndef <function name>(<parameter_list>):\\n<statements>\\nA function takes a list of input variables that are references to objects. Inside the func-\\ntion, a number of statements are executed which may modify the objects, but not the ref-\\nerence itself. In addition, the function may return an output object (or will return the value\\nNoneif not explicitly instructed to return output). Think again of the shoe box analogy. The\\ninput variables of a function are labels of shoe boxes, and the objects to which they refer\\nare the contents of the shoe boxes. The following program highlights some of the subtleties\\nof variables and objects in Python.\\nNote that the statements within a function must be indented. This is Python’s way to\\ndefine where a function begins and ends.\\nx = [1,2,3]\\ndef change_list(y):\\ny.append(100) # Append an element to the list referenced by y\\ny[0]=0 # Modify the first element of the same list\\ny = [2,3,4] # The local y now refers to a different list\\n# The list to which y first referred does not change\\nreturn sum (y)\\nprint (change_list(x))\\nprint (x)\\n9\\n[0, 2, 3, 100]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 486, 'page_label': '469'}, page_content='Python Primer 469\\nVariables that are defined inside a function only have local scope ; that is, they are\\nrecognized only within that function. This allows the same variable name to be used in\\ndifferent functions without creating a conflict. If any variable is used within a function,\\nPython first checks if the variable has local scope. If this is not the case (the variable has\\nnot been defined inside the function), then Python searches for that variable outside the\\nfunction (the global scope). The following program illustrates several important points.\\nfrom numpy import array, square, sqrt\\nx = array([1.2,2.3,4.5])\\ndef stat(x):\\nn = len (x) #the length of x\\nmeanx = sum (x)/n\\nstdx = sqrt( sum (square(x - meanx))/n)\\nreturn [meanx,stdx]\\nprint (stat(x))\\n[2.6666666666666665, 1.3719410418171119]\\n1. Basic math functions such as sqrt are unknown to the standard Python interpreter\\nand need to be imported. More on this in Section D.5 below.\\n2. As was already mentioned, indentation is crucial. It shows where the function begins\\nand ends.\\n3. No semicolons 3 are needed to end lines, but the first line of the function definition\\n(here line 5) must end with a colon (:).\\n4. Lists are not arrays (vectors of numbers), and vector operations cannot be performed\\non lists. However, the numpy module is designed specifically with e fficient vec-\\ntor/matrix operations in mind. On the second code line, we define x as a vector\\n(ndarray) object. Functions such as square, sum, and sqrt are then applied to\\nsuch arrays. Note that we used the default Python functions len and sum. More on\\nnumpy in Section D.10.\\n5. Running the program with stat(x)instead of print(stat(x))in line 11 will not\\nshow any output in the console.\\nTo display the complete list of built-in functions, type (using double underscores)\\ndir(__builtin__).\\n3Semicolons can be used to put multiple commands on a single line.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 487, 'page_label': '470'}, page_content='470 Modules\\nD.5 Modules\\nA Python modulemodule is a programming construct that is useful for organizing code into\\nmanageable parts. To each module with name module_name is associated a Python file\\nmodule_name.py containing any number of definitions, e.g., of functions, classes, and\\nvariables, as well as executable statements. Modules can be imported into other programs\\nusing the syntax: import␣<module_name>␣as␣<alias_name>, where <alias_name>\\nis a shorthand name for the module.\\nWhen imported into another Python file, the module name is treated as a namespacenamespace ,\\nproviding a naming system where each object has its unique name. For example, different\\nmodules mod1 and mod2 can have different sum functions, but they can be distinguished by\\nprefixing the function name with the module name via the dot notation, as inmod1.sum and\\nmod2.sum. For example, the following code uses the sqrt function of the numpy module.\\nimport numpy as np\\nnp.sqrt(2)\\n1.4142135623730951\\nA Python package is simply a directory of Python modules; that is, a collection of\\nmodules with additional startup information (some of which may be found in its__path__\\nattribute). Python’s built-in module is called __builtins__. Of the great many useful\\nPython modules, Table D.2 gives a few.\\nTable D.2: A few useful Python modules/packages.\\ndatetime Module for manipulating dates and times.\\nmatplotlib MATLABTM-type plotting package\\nnumpy Fundamental package for scientific computing, including random\\nnumber generation and linear algebra tools. Defines the ubiquitous\\nndarrayclass.\\nos Python interface to the operating system.\\npandas Fundamental module for data analysis. Defines the powerful\\nDataFrameclass.\\npytorch Machine learning library that supports GPU computation.\\nscipy Ecosystem for mathematics, science, and engineering, containing\\nmany tools for numerical computing, including those for integration,\\nsolving differential equations, and optimization.\\nrequests Library for performing HTTP requests and interfacing with the web.\\nseaborn Package for statistical data visualization.\\nsklearn Easy to use machine learning library.\\nstatsmodels Package for the analysis of statistical models.\\nThe numpy package contains various subpackages, such as random, linalg, and fft.\\nMore details are given in Section D.10.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 488, 'page_label': '471'}, page_content='Python Primer 471\\nWhen using Spyder, press Ctrl+Iin front of any object, to display its help file in a\\nseparate window.\\nAs we have already seen, it is also possible to import only specific functions from a\\nmodule using the syntax: from␣<module_name>␣import␣<fnc1,␣fnc2,␣...>.\\nfrom numpy import sqrt, cos\\nsqrt(2)\\ncos(1)\\n1.4142135623730951\\n0.54030230586813965\\nThis avoids the tedious prefixing of functions via the (alias) of the module name. However,\\nfor large programs it is good practice to always use the prefix /alias name construction, to\\nbe able to clearly ascertain precisely which module a function being used belongs to.\\nD.6 Flow Control\\nFlow control in Python is similar to that of many programming languages, with conditional\\nstatements as well as whileand forloops. The syntax for if-then-elseflow control is\\nas follows.\\nif <condition1>:\\n<statements>\\nelif <condition2>:\\n<statements>\\nelse:\\n<statements>\\nHere, <condition1> and <condition2> are logical conditions that are either True or\\nFalse; logical conditions often involve comparison operators (such as ==,␣>,␣<=,␣!=).\\nIn the example above, there is one elif part, which allows for an “else if” conditional\\nstatement. In general, there can be more than oneelifpart, or it can be omitted. Theelse\\npart can also be omitted. The colons are essential, as are the indentations.\\nThe whileand forloops have the following syntax.\\nwhile <condition>:\\n<statements>\\nfor <variable> in <collection>:\\n<statements>\\nAbove, <collection> is an iterable object (see Section D.7 below). For further con-\\ntrol in for and while loops, one can use a break statement to exit the current loop, and\\nthe continue statement to continue with the next iteration of the loop, while abandoning\\nany remaining statements in the current iteration. Here is an example.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 489, 'page_label': '472'}, page_content='472 Iteration\\nimport numpy as np\\nans = \\'y\\'\\nwhile ans != \\'n\\':\\noutcome = np.random.randint(1,6+1)\\nif outcome == 6:\\nprint (\"Hooray a 6!\")\\nbreak\\nelse :\\nprint (\"Bad luck, a\", outcome)\\nans = input (\"Again? (y/n) \")\\nD.7 Iteration\\nIterating over a sequence of objects, such as used in a for loop, is a common operation.\\nTo better understand how iteration works, we consider the following code.\\ns = \"Hello\"\\nfor c in s:\\nprint (c,\\'*\\', end= \\' \\' )\\nH * e * l * l * o *\\nA string is an example of a Python object that can be iterated. One of the methods of a\\nstring object is __iter__. Any object that has such a method is called an iterableiterable . Calling\\nthis method creates an iteratoriterator — an object that returns the next element in the sequence\\nto be iterated. This is done via the method __next__.\\ns = \"Hello\"\\nt = s.__iter__() # t is now an iterator. Same as iter(s)\\nprint (t.__next__() ) # same as next(t)\\nprint (t.__next__() )\\nprint (t.__next__() )\\nH\\ne\\nl\\nThe inbuilt functions next and iter simply call these corresponding double-\\nunderscore functions of an object. When executing a for loop, the sequence /collection\\nover which to iterate must be an iterable. During the execution of the forloop, an iterator\\nis created and the next function is executed until there is no next element. An iterator is\\nalso an iterable, so can be used in aforloop as well. Lists, tuples, and strings are so-called\\nsequencesequence objects and are iterables, where the elements are iterated by their index.\\nThe most common iterator in Python is the rangerange iterator, which allows iteration over\\na range of indices. Note that range returns a rangeobject, not a list.\\nfor i in range (4,20):\\nprint (i, end= \\' \\' )\\nprint (range (4,20))'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 490, 'page_label': '473'}, page_content='Python Primer 473\\n4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\\nrange(4,20)\\nSimilar to Python’s slice operator [i : j], the iterator range(i, j) ranges from i to j,\\nnot including the index j.\\nTwo other common iterables are sets and dictionaries. Python sets setsare, as in mathem-\\natics, unordered collections of unique objects. Sets are defined with curly brackets {}, as\\nopposed to round brackets ( ) for tuples, and square brackets [ ] for lists. Unlike lists, sets do\\nnot have duplicate elements. Many of the usual set operations are implemented in Python,\\nincluding the union A␣|␣Band intersection A␣&␣B.\\nA = {3, 2, 2, 4}\\nB = {4, 3, 1}\\nC = A & B\\nfor i in A:\\nprint (i)\\nprint (C)\\n2\\n3\\n4\\n{3, 4}\\nA useful way to construct lists is by list comprehension list\\ncomprehension\\n; that is, by expressions of the\\nform\\n<expression>␣for␣<element>␣in␣<list>␣if␣<condition>\\nFor sets a similar construction holds. In this way, lists and sets can be defined using very\\nsimilar syntax as in mathematics. Compare, for example, the mathematical definition of\\nthe sets A := {3,2,4,2}= {2,3,4}(no order and no duplication of elements) and B := {x2 :\\nx ∈A}with the Python code below.\\nsetA = {3, 2, 4, 2}\\nsetB = {x**2 for x in setA}\\nprint (setB)\\nlistA = [3, 2, 4, 2]\\nlistB = [x**2 for x in listA]\\nprint (listB)\\n{16, 9, 4}\\n[9, 4, 16, 4]\\nA dictionary dictionaryis a set-like data structure, containing one or more key:value pairs en-\\nclosed in curly brackets. The keys are often of the same type, but do not have to be; the\\nsame holds for the values. Here is a simple example, storing the ages of Lord of the Rings\\ncharacters in a dictionary.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 491, 'page_label': '474'}, page_content=\"474 Classes\\nDICT = { 'Gimly ': 140, 'Frodo ':51, 'Aragorn ': 88}\\nfor key in DICT:\\nprint (key, DICT[key])\\nGimly 140\\nFrodo 51\\nAragorn 88\\nD.8 Classes\\nRecall that objects are of fundamental importance in Python — indeed, data types and\\nfunctions are all objects. A classclass is an object type, and writing a class definition can be\\nthought of as creating a template for a new type of object. Each class contains a number\\nof attributes, including a number of inbuilt methods. The basic syntax for the creation of a\\nclass is:\\nclass <class_name>:\\ndef __init__(self):\\n<statements>\\n<statements>\\nThe main inbuilt method is __init__, which creates an instanceinstance of a class object.\\nFor example, str is a class object (string class), but s␣=␣str('Hello') or simply\\ns␣=␣'Hello', creates an instance, s, of the strclass. Instance attributes are created dur-\\ning initialization and their values may be di fferent for different instances. In contrast, the\\nvalues of class attributes are the same for every instance. The variableselfin the initializ-\\nation method refers to the current instance that is being created. Here is a simple example,\\nexplaining how attributes are assigned.\\nclass shire_person:\\ndef __init__(self,name): # initialization method\\nself.name = name # instance attribute\\nself.age = 0 # instance attribute\\naddress = 'The Shire ' # class attribute\\nprint (dir (shire_person)[1:5], '...',dir (shire_person)[-2:])\\n# list of class attributes\\np1 = shire_person( 'Sam') # create an instance\\np2 = shire_person( 'Frodo ') # create another instance\\nprint (p1.__dict__) # list of instance attributes\\np2.race = 'Hobbit ' # add another attribute to instance p2\\np2.age = 33 # change instance attribute\\nprint (p2.__dict__)\\nprint (getattr (p1, 'address ')) # content of p1 's class attribute\\n['__delattr__ ', '__dict__ ', '__dir__ ', '__doc__ '] ...\\n['__weakref__ ', 'address ']\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 492, 'page_label': '475'}, page_content='Python Primer 475\\n{\\'name \\': \\'Sam\\', \\'age\\': 0}\\n{\\'name \\': \\'Frodo \\', \\'age\\': 33, \\'race \\': \\'Hobbit \\'}\\nThe Shire\\nIt is good practice to create all the attributes of the class object in the__init__method,\\nbut, as seen in the example above, attributes can be created and assigned everywhere, even\\noutside the class definition. More generally, attributes can be added to any object that has\\na __dict__.\\nAn “empty” class can be created via\\nclass␣<class_name>:\\n␣␣␣pass\\nPython classes can be derived from a parent class by inheritance inheritance, via the following\\nsyntax.\\nclass <class_name>(<parent_class_name>):\\n<statements>\\nThe derived class (initially) inherits all of the attributes of the parent class.\\nAs an example, the class shire_person below inherits the attributes name, age, and\\naddress from its parent class person. This is done using the super function, used here\\nto refer to the parent class person without naming it explicitly. When creating a new\\nobject of type shire_person, the __init__ method of the parent class is invoked, and\\nan additional instance attributeShire_addressis created. The dir function confirms that\\nShire_addressis an attribute only of shire_personinstances.\\nclass person:\\ndef __init__(self,name):\\nself.name = name\\nself.age = 0\\nself.address= \\' \\'\\nclass shire_person(person):\\ndef __init__(self,name):\\nsuper ().__init__(name)\\nself.Shire_address = \\'Bag End \\'\\np1 = shire_person(\"Frodo\")\\np2 = person(\"Gandalf\")\\nprint (dir (p1)[:1], dir (p1)[-3:] )\\nprint (dir (p2)[:1], dir (p2)[-3:] )\\n[\\'Shire_address \\'] [\\'address \\', \\'age\\', \\'name \\']\\n[\\'__class__ \\'] [\\'address \\', \\'age\\', \\'name \\']'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 493, 'page_label': '476'}, page_content=\"476 Files\\nD.9 Files\\nTo write to or read from a file, a file first needs to be opened. Theopen function in Python\\ncreates a file object that is iterable, and thus can be processed in a sequential manner in a\\nforor whileloop. Here is a simple example.\\nfout = open ('output.txt ','w')\\nfor i in range (0,41):\\nif i%10 == 0:\\nfout.write( '{:3d}\\\\n '.format (i))\\nfout.close()\\nThe first argument of open is the name of the file. The second argument specifies\\nif the file is opened for reading ( 'r'), writing ( 'w'), appending ( 'a'), and so on. See\\nhelp(open). Files are written in text mode by default, but it is also possible to write in\\nbinary mode. The above program creates a file output.txt with 5 lines, containing the\\nstrings 0, 10, . . . , 40. Note that if we had writtenfout.write(i)in the fourth line of the\\ncode above, an error message would be produced, as the variable iis an integer, and not a\\nstring. Recall that the expressionstring.format()is Python’s way to specify the format\\nof the output string.\\nThe formatting syntax {:3d} indicates that the output should be constrained to a spe-\\ncific width of three characters, each of which is a decimal value. As mentioned in the\\nintroduction, bracket syntax {i}provides a placeholder for the i-th variable to be printed,\\nwith 0 being the first index. The format for the output is further specified by {i:format},\\nwhere formatis typically4 of the form:\\n[width][.precision][type]\\nIn this specification:\\n• widthspecifies the minimum width of output;\\n• precisionspecifies the number of digits to be displayed after the decimal point for\\na floating point values of typef, or the number of digits beforeand after the decimal\\npoint for a floating point values of type g;\\n• type specifies the type of output. The most common types are s for strings, d for\\nintegers, b for binary numbers, f for floating point numbers (floats) in fixed-point\\nnotation, gfor floats in general notation, efor floats in scientific notation.\\nThe following illustrates some behavior of formatting on numbers.\\n'{:5d} '.format (123)\\n'{:.4e} '.format (1234567890)\\n'{:.2f} '.format (1234567890)\\n'{:.2f} '.format (2.718281828)\\n'{:.3f} '.format (2.718281828)\\n'{:.3g} '.format (2.718281828)\\n'{:.3e} '.format (2.718281828)\\n4More formatting options are possible.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 494, 'page_label': '477'}, page_content='Python Primer 477\\n\\'{0:3.3f}; {2:.4e}; \\'.format (123.456789, 0.00123456789)\\n\\' 123\\'\\n\\'1.2346e+09 \\'\\n\\'1234567890.00 \\'\\n\\'2.72\\'\\n\\'2.718 \\'\\n\\'2.72\\'\\n\\'2.718e+00 \\'\\n\\'123.457; 1.2346e-03; \\'\\nThe following code reads the text file output.txt line by line, and prints the output\\non the screen. To remove the newline \\\\n character, we have used the strip method for\\nstrings, which removes any whitespace from the start and end of a string.\\nfin = open (\\'output.txt \\',\\'r\\')\\nfor line in fin:\\nline = line.strip() # strips a newline character\\nprint (line)\\nfin.close()\\n0\\n10\\n20\\n30\\n40\\nWhen dealing with file input and output it is important to always close files. Files that\\nremain open, e.g., when a program finishes unexpectedly due to a programming error, can\\ncause considerable system problems. For this reason it is recommended to open files via\\ncontext management. The syntax is as follows.\\nwith open (\\'output.txt \\', \\'w\\') as f:\\nf.write( \\'Hi there! \\')\\nContext management ensures that a file is correctly closed even when the program is\\nterminated prematurely. An example is given in the next program, which outputs the most-\\nfrequent words in Dicken’sA Tale of Two Cities, which can be downloaded from the book’s\\nGitHub site as ataleof2cities.txt.\\nNote that in the next program, the fileataleof2cities.txtmust be placed in the cur-\\nrent working directory. The current working directory can be determined via import␣os\\nfollowed by cwd␣=␣os.getcwd().\\nnumline = 0\\nDICT = {}\\nwith open (\\'ataleof2cities.txt \\', encoding=\"utf8\") as fin:\\nfor line in fin:\\nwords = line.split()\\nfor w in words:\\nif w not in DICT:\\nDICT[w] = 1'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 495, 'page_label': '478'}, page_content='478 NumPy\\nelse :\\nDICT[w] +=1\\nnumline += 1\\nsd = sorted (DICT,key=DICT.get,reverse=True) #sort the dictionary\\nprint (\"Number of unique words: {}\\\\n\". format (len (DICT)))\\nprint (\"Ten most frequent words:\\\\n\")\\nprint (\"{:8} {}\". format (\"word\", \"count\"))\\nprint (15* \\'-\\')\\nfor i in range (0,10):\\nprint (\"{:8} {}\". format (sd[i], DICT[sd[i]]))\\nNumber of unique words: 19091\\nTen most frequent words:\\nword count\\n---------------\\nthe 7348\\nand 4679\\nof 3949\\nto 3387\\na 2768\\nin 2390\\nhis 1911\\nwas 1672\\nthat 1650\\nI 1444\\nD.10 NumPy\\nThe package NumPy (module name numpy) provides the building blocks for scientific\\ncomputing in Python. It contains all the standard mathematical functions, such as sin,\\ncos, tan, etc., as well as efficient functions for random number generation, linear algebra,\\nand statistical computation.\\nimport numpy as np #import the package\\nx = np.cos(1)\\ndata = [1,2,3,4,5]\\ny = np.mean(data)\\nz = np.std(data)\\nprint (\\'cos(1) = {0:1.8f} mean = {1} std = {2} \\'.format (x,y,z))\\ncos(1) = 0.54030231 mean = 3.0 std = 1.4142135623730951\\nD.10.1 Creating and Shaping Arrays\\nThe fundamental data type in numpy is the ndarray. This data type allows for fast matrix\\noperations via highly optimized numerical libraries such as LAPACK and BLAS; this in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 496, 'page_label': '479'}, page_content=\"Python Primer 479\\ncontrast to (nested) lists. As such,numpy is often essential when dealing with large amounts\\nof quantitative data.\\nndarrayobjects can be created in various ways. The following code creates a 2×3 ×2\\narray of zeros. Think of it as a 3-dimensional matrix or two stacked 3 ×2 matrices.\\nA = np.zeros([2,3,2]) # 2 by 3 by 2 array of zeros\\nprint (A)\\nprint (A.shape) # number of rows and columns\\nprint (type (A)) # A is an ndarray\\n[[[ 0. 0.]\\n[ 0. 0.]\\n[ 0. 0.]]\\n[[ 0. 0.]\\n[ 0. 0.]\\n[ 0. 0.]]]\\n(2, 3, 2)\\n<class 'numpy.ndarray '>\\nWe will be mostly working with 2D arrays; that is, ndarrays that represent ordinary\\nmatrices. We can also use the range method and lists to create ndarrays via the array\\nmethod. Note that arange is numpy’s version of range, with the di fference that arange\\nreturns an ndarrayobject.\\na = np.array( range (4)) # equivalent to np.arange(4)\\nb = np.array([0,1,2,3])\\nC = np.array([[1,2,3],[3,2,1]])\\nprint (a, '\\\\n', b, '\\\\n' , C)\\n[0 1 2 3]\\n[0 1 2 3]\\n[[1 2 3]\\n[3 2 1]]\\nThe dimension of an ndarray can be obtained via its shape method, which returns a\\ntuple. Arrays can be reshaped via the reshape method. This does not change the current\\nndarrayobject. To make the change permanent, a new instance needs to be created.\\na = np.array( range (9)) #a is an ndarray of shape (9,)\\nprint (a.shape)\\nA = a.reshape(3,3) #A is an ndarray of shape (3,3)\\nprint (a)\\nprint (A)\\n[0 1 2 3 4 5 6 7 8]\\n(9,)\\n[[0, 1, 2]\\n[3, 4, 5]\\n[6, 7, 8]]\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 497, 'page_label': '480'}, page_content=\"480 NumPy\\nOne shape dimension for reshape can be specified as −1. The dimension is then\\ninferred from the other dimension(s).\\nThe 'T' attribute of an ndarray gives its transpose. Note that the transpose of a “vector”\\nwith shape (n,) is the same vector. To distinguish between column and row vectors, reshape\\nsuch a vector to an n ×1 and 1 ×n array, respectively.\\na = np.arange(3) #1D array (vector) of shape (3,)\\nprint (a)\\nprint (a.shape)\\nb = a.reshape(-1,1) # 3x1 array (matrix) of shape (3,1)\\nprint (b)\\nprint (b.T)\\nA = np.arange(9).reshape(3,3)\\nprint (A.T)\\n[0 1 2]\\n(3,)\\n[[0]\\n[1]\\n[2]]\\n[[0 1 2]]\\n[[0 3 6]\\n[1 4 7]\\n[2 5 8]]\\nTwo useful methods of joining arrays are hstack and vstack, where the arrays are\\njoined horizontally and vertically, respectively.\\nA = np.ones((3,3))\\nB = np.zeros((3,2))\\nC = np.hstack((A,B))\\nprint (C)\\n[[ 1. 1. 1. 0. 0.]\\n[ 1. 1. 1. 0. 0.]\\n[ 1. 1. 1. 0. 0.]]\\nD.10.2 Slicing\\nArrays can be sliced similarly to Python lists. If an array has several dimensions, a slice for\\neach dimension needs to be specified. Recall that Python indexing starts at '0' and ends\\nat 'len(obj)-1'. The following program illustrates various slicing operations.\\nA = np.array( range (9)).reshape(3,3)\\nprint (A)\\nprint (A[0]) # first row\\nprint (A[:,1]) # second column\\nprint (A[0,1]) # element in first row and second column\\nprint (A[0:1,1:2]) # (1,1) ndarray containing A[0,1] = 1\\nprint (A[1:,-1]) # elements in 2nd and 3rd rows , and last column\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 498, 'page_label': '481'}, page_content='Python Primer 481\\n[[0 1 2]\\n[3 4 5]\\n[6 7 8]]\\n[0 1 2]\\n[1 4 7]\\n1\\n[[1]]\\n[5 8]\\nNote that ndarraysare mutable objects, so that elements can be modified directly, without\\nhaving to create a new object.\\nA[1:,1] = [0,0] # change two elements in the matrix A above\\nprint (A)\\n[[0, 1, 2]\\n[3, 0, 5]\\n[6, 0, 8]]\\nD.10.3 Array Operations\\nBasic mathematical operators and functions act element-wise on ndarrayobjects.\\nx = np.array([[2,4],[6,8]])\\ny = np.array([[1,1],[2,2]])\\nprint (x+y)\\n[[ 3, 5]\\n[ 8, 10]]\\nprint (np.divide(x,y)) # same as x/y\\n[[ 2. 4.]\\n[ 3. 4.]]\\nprint (np.sqrt(x))\\n[[1.41421356 2. ]\\n[2.44948974 2.82842712]]\\nIn order to compute matrix multiplications and compute inner products of vectors,\\nnumpy’s dot function can be used, either as a method of an ndarray instance or as a\\nmethod of np.\\nprint (np.dot(x,y))\\n[[10, 10]\\n[22, 22]]\\nprint (x.dot(x)) # same as np.dot(x,x)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 499, 'page_label': '482'}, page_content='482 NumPy\\n[[28, 40]\\n[60, 88]]\\nSince version 3.5 of Python, it is possible to multiply two ndarrays using the @\\noperator@ operator (which implements the np.matmulmethod). For matrices, this is similar to using\\nthe dot method. For higher-dimensional arrays the two methods behave differently.\\nprint (x @ y)\\n[[10 10]\\n[22 22]]\\nNumPy allows arithmetic operations on arrays of di fferent shapes (dimensions). Spe-\\ncifically, suppose two arrays have dimensions (m1,m2,..., mp) and (n1,n2,..., np), respect-\\nively. The arrays or shapes are said to be alignedaligned if for all i = 1,..., p it holds that\\n• mi = ni, or\\n• min{mi,ni}= 1, or\\n• either mi or ni, or both are missing.\\nFor example, shapes (1 ,2,3) and (4,2,1) are aligned, as are (2 ,, ) and (1,2,3). However,\\n(2,2,2) and (1 ,2,3) are not aligned. NumPy “duplicates” the array elements across the\\nsmaller dimension to match the larger dimension. This process is called broadcastingbroadcasting and\\nis carried out without actually making copies, thus providing efficient memory use. Below\\nare some examples.\\nimport numpy as np\\nA= np.arange(4).reshape(2,2) # (2,2) array\\nx1 = np.array([40,500]) # (2,) array\\nx2 = x1.reshape(2,1) # (2,1) array\\nprint (A + x1) # shapes (2,2) and (2,)\\nprint (A * x2) # shapes (2,2) and (2,1)\\n[[ 40 501]\\n[ 42 503]]\\n[[ 0 40]\\n[1000 1500]]\\nNote that above x1is duplicated row-wise and x2column-wise. Broadcasting also applies\\nto the matrix-wise operator @, as illustrated below. Here, the matrixbis duplicated across\\nthe third dimension resulting in the two matrix multiplications\\n\"0 1\\n2 3\\n#\"0 1\\n2 3\\n#\\nand\\n\"4 5\\n6 7\\n#\"0 1\\n2 3\\n#\\n.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 500, 'page_label': '483'}, page_content=\"Python Primer 483\\nB = np.arange(8).reshape(2,2,2)\\nb = np.arange(4).reshape(2,2)\\nprint (B@b)\\n[[[ 2 3]\\n[ 6 11]]\\n[[10 19]\\n[14 27]]]\\nFunctions such as sum, mean, and std can also be executed as methods of an ndarray\\ninstance. The argument axiscan be passed to specify along which dimension the function\\nis applied. By default axis=None.\\na = np.array( range (4)).reshape(2,2)\\nprint (a.sum (axis=0)) #summing over rows gives column totals\\n[2, 4]\\nD.10.4 Random Numbers\\nOne of the sub-modules in numpy is random. It contains many functions for random vari-\\nable generation.\\nimport numpy as np\\nnp.random.seed(123) # set the seed for the random number generator\\nx = np.random.random() # uniform (0,1)\\ny = np.random.randint(5,9) # discrete uniform 5,...,8\\nz = np.random.randn(4) # array of four standard normals\\nprint (x,y, '\\\\n',z)\\n0.6964691855978616 7\\n[ 1.77399501 -0.66475792 -0.07351368 1.81403277]\\nFor more information on random variable generation in numpy, see\\nhttps://docs.scipy.org/doc/numpy/reference/random/index.html.\\nD.11 Matplotlib\\nThe main Python graphics library for 2D and 3D plotting ismatplotlib, and its subpack-\\nage pyplot contains a collection of functions that make plotting in Python similar to that\\nin MATLAB.\\nD.11.1 Creating a Basic Plot\\nThe code below illustrates various possibilities for creating plots. The style and color of\\nlines and markers can be changed, as well as the font size of the labels. Figure D.1 shows\\nthe result.\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 501, 'page_label': '484'}, page_content=\"484 Matplotlib\\nsqrtplot.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.arange(0, 10, 0.1)\\nu = np.arange(0,10)\\ny = np.sqrt(x)\\nv = u/3\\nplt.figure(figsize = [4,2]) # size of plot in inches\\nplt.plot(x,y, 'g--') # plot green dashed line\\nplt.plot(u,v, 'r.') # plot red dots\\nplt.xlabel( 'x')\\nplt.ylabel( 'y')\\nplt.tight_layout()\\nplt.savefig( 'sqrtplot.pdf ',format ='pdf') # saving as pdf\\nplt.show() # both plots will now be drawn\\n0\\n 2\\n 4\\n 6\\n 8\\n 10\\nx\\n0\\n1\\n2\\n3y\\nFigure D.1: A simple plot created using pyplot.\\nThe library matplotlib also allows the creation of subplots. The scatterplot and histogram\\nin Figure D.2 have been produced using the code below. When creating a histogram there\\nare several optional arguments that a ffect the layout of the graph. The number of bins is\\ndetermined by the parameter bins (the default is 10). Scatterplots also take a number of\\nparameters, such as a string c which determines the color of the dots, and alpha which\\naffects the transparency of the dots.\\nhistscat.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.random.randn(1000)\\nu = np.random.randn(100)\\nv = np.random.randn(100)\\nplt.subplot(121) # first subplot\\nplt.hist(x,bins=25, facecolor= 'b')\\nplt.xlabel( 'X Variable ')\\nplt.ylabel( 'Counts ')\\nplt.subplot(122) # second subplot\\nplt.scatter(u,v,c= 'b', alpha=0.5)\\nplt.show()\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 502, 'page_label': '485'}, page_content=\"Python Primer 485\\n2\\n 0\\n 2\\nX Variable\\n0\\n20\\n40\\n60\\n80\\n100\\n120Counts\\n2\\n 0\\n 2\\n2\\n1\\n0\\n1\\n2\\nFigure D.2: A histogram and scatterplot.\\nOne can also create three-dimensional plots as illustrated below.\\nsurf3dscat.py\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom mpl_toolkits.mplot3d import Axes3D\\ndef npdf(x,y):\\nreturn np.exp(-0.5*( pow (x,2)+ pow (y,2)))/np.sqrt(2*np.pi)\\nx, y = np.random.randn(100), np.random.randn(100)\\nz = npdf(x,y)\\nxgrid, ygrid = np.linspace(-3,3,100), np.linspace(-3,3,100)\\nXarray, Yarray = np.meshgrid(xgrid,ygrid)\\nZarray = npdf(Xarray,Yarray)\\nfig = plt.figure(figsize=plt.figaspect(0.4))\\nax1 = fig.add_subplot(121, projection= '3d')\\nax1.scatter(x,y,z, c= 'g')\\nax1.set_xlabel( '$x$')\\nax1.set_ylabel( '$y$')\\nax1.set_zlabel( '$f(x,y)$ ')\\nax2 = fig.add_subplot(122, projection= '3d')\\nax2.plot_surface(Xarray,Yarray,Zarray,cmap= 'viridis ',\\nedgecolor= 'none ')\\nax2.set_xlabel( '$x$')\\nax2.set_ylabel( '$y$')\\nax2.set_zlabel( '$f(x,y)$ ')\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 503, 'page_label': '486'}, page_content=\"486 Pandas\\nplt.show()\\nx\\n2 1 0 1 2 3\\ny\\n2\\n1\\n0\\n1\\n2\\nf(x,y)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nx\\n3 2 1 0 1 2 3\\ny\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nf(x,y)\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\nFigure D.3: Three-dimensional scatter- and surface plots.\\nD.12 Pandas\\nThe Python package Pandas (module name pandas) provides various tools and data struc-\\ntures for data analytics, including the fundamental DataFrameclass.\\nFor the code in this section we assume that pandas has been imported via\\nimport␣pandas␣as␣pd.\\nD.12.1 Series and DataFrame\\nThe two main data structures inpandas are Seriesand DataFrame. A Seriesobject can\\nbe thought of as a combination of a dictionary and an 1-dimensional ndarray. The syntax\\nfor creating a Seriesobject is\\nseries = pd.Series(<data>, index=['index'])\\nHere, <data>some 1-dimensional data structure, such as a 1-dimensionalndarray, a list,\\nor a dictionary, and indexis a list of names of the same length as <data>. When <data>\\nis a dictionary, the index is created from the keys of the dictionary. When <data> is an\\nndarrayand indexis omitted, the default index will be [0,␣...,␣len(data)-1].\\nDICT = { 'one':1, 'two':2, 'three ':3, 'four ':4}\\nprint (pd.Series(DICT))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 504, 'page_label': '487'}, page_content=\"Python Primer 487\\none 1\\ntwo 2\\nthree 3\\nfour 4\\ndtype: int64\\nyears = [ '2000 ','2001 ','2002 ']\\ncost = [2.34, 2.89, 3.01]\\nprint (pd.Series(cost,index = years, name = 'MySeries ')) #name it\\n2000 2.34\\n2001 2.89\\n2002 3.01\\nName: MySeries, dtype: float64\\nThe most commonly-used data structure inpandas is the two-dimensionalDataFrame,\\nwhich can be thought of as pandas’ implementation of a spreadsheet or as a diction-\\nary in which each “key” of the dictionary corresponds to a column name and the dic-\\ntionary “value” is the data in that column. To create a DataFrame one can use the\\npandas DataFrame method, which has three main arguments: data, index (row labels),\\nand columns (column labels).\\nDataFrame(<data>, index=['<row_name>'], columns=['<column_name>'])\\nIf the index is not specified, the default index is [0,␣...,␣len(data)-1]. Data can\\nalso be read directly from a CSV or Excel file, as is done in Section 1.1. If a dictionary is ☞ 1\\nused to create the data frame (as below), the dictionary keys are used as the column names.\\nDICT = { 'numbers ':[1,2,3,4], 'squared ':[1,4,9,16] }\\ndf = pd.DataFrame(DICT, index = list ('abcd '))\\nprint (df)\\nnumbers squared\\na 1 1\\nb 2 4\\nc 3 9\\nd 4 16\\nD.12.2 Manipulating Data Frames\\nOften data encoded inDataFrameor Seriesobjects need to be extracted, altered, or com-\\nbined. Getting, setting, and deleting columns works in a similar manner as for dictionaries.\\nThe following code illustrates various operations.\\nages = [6,3,5,6,5,8,0,3]\\nd={'Gender ':['M', 'F']*4, 'Age': ages}\\ndf1 = pd.DataFrame(d)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 505, 'page_label': '488'}, page_content=\"488 Pandas\\ndf1.at[0, 'Age']= 60 # change an element\\ndf1.at[1, 'Gender '] = 'Female ' # change another element\\ndf2 = df1.drop( 'Age',1) # drop a column\\ndf3 = df2.copy(); # create a separate copy of df2\\ndf3[ 'Age'] = ages # add the original column\\ndfcomb = pd.concat([df1,df2,df3],axis=1) # combine the three dfs\\nprint (dfcomb)\\nGender Age Gender Gender Age\\n0 M 60 M M 6\\n1 Female 3 Female Female 3\\n2 M 5 M M 5\\n3 F 6 F F 6\\n4 M 5 M M 5\\n5 F 8 F F 8\\n6 M 0 M M 0\\n7 F 3 F F 3\\nNote that the above DataFrame object has two Age columns. The expression\\ndfcomb[’Age’]will return a DataFrame with both these columns.\\nTable D.3: Useful pandas methods for data manipulation.\\nagg Aggregate the data using one or more functions.\\napply Apply a function to a column or row.\\nastype Change the data type of a variable.\\nconcat Concatenate data objects.\\nreplace Find and replace values.\\nread_csv Read a CSV file into a DataFrame.\\nsort_values Sort by values along rows or columns.\\nstack Stack a DataFrame.\\nto_excel Write a DataFrame to an Excel file.\\nIt is important to correctly specify the data type of a variable before embarking on\\ndata summarization and visualization tasks, as Python may treat di fferent types of objects\\nin dissimilar ways. Common data types for entries in a DataFrame object are float,\\ncategory, datetime, bool, and int. A generic object type is object.\\nd={'Gender ':['M', 'F', 'F']*4, 'Age': [6,3,5,6,5,8,0,3,6,6,7,7]}\\ndf=pd.DataFrame(d)\\nprint (df.dtypes)\\ndf['Gender '] = df[ 'Gender '].astype( 'category ') #change the type\\nprint (df.dtypes)\\nGender object\\nAge int64\\ndtype: object\\nGender category\\nAge int64\\ndtype: object\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 506, 'page_label': '489'}, page_content='Python Primer 489\\nD.12.3 Extracting Information\\nExtracting statistical information from a DataFrame object is facilitated by a large col-\\nlection of methods (functions) in pandas. Table D.4 gives a selection of data inspection\\nmethods. See Chapter 1 for their practical use. The code below provides several examples ☞ 1\\nof useful methods. The apply method allows one to apply general functions to columns\\nor rows of a DataFrame. These operations do not change the data. The loc method allows\\nfor accessing elements (or ranges) in a data frame and acts similar to the slicing operation\\nfor lists and arrays, with the difference that the “stop” value is included , as illustrated in\\nthe code below.\\nimport numpy as np\\nimport pandas as pd\\nages = [6,3,5,6,5,8,0,3]\\nnp.random.seed(123)\\ndf = pd.DataFrame(np.random.randn(3,4), index = list (\\'abc\\'),\\ncolumns = list (\\'ABCD \\'))\\nprint (df)\\ndf1 = df.loc[\"b\":\"c\",\"B\":\"C\"] # create a partial data frame\\nprint (df1)\\nmeanA = df[ \\'A\\'].mean() # mean of \\'A\\' column\\nprint (\\'mean of column A = {} \\'.format (meanA))\\nexpA = df[ \\'A\\'].apply (np.exp) # exp of all elements in \\'A\\' column\\nprint (expA)\\nA B C D\\na -1.085631 0.997345 0.282978 -1.506295\\nb -0.578600 1.651437 -2.426679 -0.428913\\nc 1.265936 -0.866740 -0.678886 -0.094709\\nB C\\nb 1.651437 -2.426679\\nc -0.866740 -0.678886\\nmean of column A = -0.13276486552118785\\na 0.337689\\nb 0.560683\\nc 3.546412\\nName: A, dtype: float64\\nThe groupby method of a DataFrameobject is useful for summarizing and displaying\\nthe data in manipulated ways. It groups data according to one or more specified columns,\\nsuch that methods such as count and mean can be applied to the grouped data.\\ndf = pd.DataFrame({ \\'W\\':[\\'a\\',\\'a\\',\\'b\\',\\'a\\',\\'a\\',\\'b\\'],\\n\\'X\\':np.random.rand(6),\\n\\'Y\\':[\\'c\\',\\'d\\',\\'d\\',\\'d\\',\\'c\\',\\'c\\'], \\'Z\\':np.random.rand(6)})\\nprint (df)\\nW X Y Z\\n0 a 0.993329 c 0.641084\\n1 a 0.925746 d 0.428412\\n2 b 0.266772 d 0.460665\\n3 a 0.201974 d 0.261879\\n4 a 0.529505 c 0.503112'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 507, 'page_label': '490'}, page_content=\"490 Pandas\\nTable D.4: Useful pandas methods for data inspection.\\ncolumns Column names.\\ncount Counts number of non-NA cells.\\ncrosstab Cross-tabulate two or more categories.\\ndescribe Summary statistics.\\ndtypes Data types for each column.\\nhead Display the top rows of a DataFrame.\\ngroupby Group data by column(s).\\ninfo Display information about the DataFrame.\\nloc Access a group or rows or columns.\\nmean Column/row mean.\\nplot Plot of columns.\\nstd Column/row standard deviation.\\nsum Returns column/row sum.\\ntail Display the bottom rows of a DataFrame.\\nvalue_counts Counts of different non-null values.\\nvar Variance.\\n5 b 0.006231 c 0.849683\\nprint (df.groupby( 'W').mean())\\nX Z\\nW\\na 0.662639 0.458622\\nb 0.136502 0.655174\\nprint (df.groupby([ 'W', 'Y']).mean())\\nX Z\\nW Y\\na c 0.761417 0.572098\\nd 0.563860 0.345145\\nb c 0.006231 0.849683\\nd 0.266772 0.460665\\nTo allow for multiple functions to be calculated at once, the agg method can be used.\\nIt can take a list, dictionary, or string of functions.\\nprint (df.groupby( 'W').agg([ sum ,np.mean]))\\nX Z\\nsum mean sum mean\\nW\\na 2.650555 0.662639 1.834487 0.458622\\nb 0.273003 0.136502 1.310348 0.655174\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 508, 'page_label': '491'}, page_content=\"Python Primer 491\\nD.12.4 Plotting\\nThe plot method of a DataFrame makes plots of a DataFrame using Matplotlib. Different\\ntypes of plot can be accessed via the kind = 'str' construction, where str is one of\\nline (default), bar, hist, box, kde, and several more. Finer control, such as modifying\\nthe font, is obtained by using matplotlib directly. The following code produces the line\\nand box plots in Figure D.4.\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib\\ndf = pd.DataFrame({ 'normal ':np.random.randn(100),\\n'Uniform ':np.random.uniform(0,1,100)})\\nfont = { 'family ' : 'serif ', 'size ' : 14} #set font\\nmatplotlib.rc( 'font ', **font) # change font\\ndf.plot() # line plot (default)\\ndf.plot(kind = 'box') # box plot\\nmatplotlib.pyplot.show() #render plots\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\n2\\n0\\n2\\nNormal\\nUniform\\nNormal\\n Uniform\\n2\\n0\\n2\\nFigure D.4: A line and box plot using the plotmethod of DataFrame.\\nD.13 Scikit-learn\\nScikit-learn is an open-source machine learning and data science library for Python. The\\nlibrary includes a range of algorithms relating to the chapters in this book. It is widely\\nused due to its simplicity and its breadth. The module name is sklearn. Below is a brief\\nintroduction into modeling the data with sklearn. The full documentation can be found\\nat\\nhttps://scikit-learn.org/.\\nD.13.1 Partitioning the Data\\nRandomly partitioning the data in order to test the model may be achieved easily with\\nsklearn’s function train_test_split. For example, suppose that the training data is\\ndescribed by the matrix Xof explanatory variables and the vector yof responses. Then the\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 509, 'page_label': '492'}, page_content=\"492 Scikit-learn\\nfollowing code splits the data set into training and testing sets, with the testing set being\\nhalf of the total set.\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\ntest_size = 0.5)\\nAs an example, the following code generates a synthetic data set and splits it into\\nequally-sized training and test sets.\\nsyndat.py\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nnp.random.seed(1234)\\nX=np.pi*(2*np.random.random(size=(400,2))-1)\\ny=(np.cos(X[:,0])*np.sin(X[:,1])>=0)\\nX_train , X_test , y_train , y_test = train_test_split(X, y,\\ntest_size=0.5)\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\nax.scatter(X_train[y_train==0,0],X_train[y_train==0,1], c= 'g',\\nmarker= 'o',alpha=0.5)\\nax.scatter(X_train[y_train==1,0],X_train[y_train==1,1], c= 'b',\\nmarker= 'o',alpha=0.5)\\nax.scatter(X_test[y_test==0,0],X_test[y_test==0,1], c= 'g',\\nmarker= 's',alpha=0.5)\\nax.scatter(X_test[y_test==1,0],X_test[y_test==1,1], c= 'b',\\nmarker= 's',alpha=0.5)\\nplt.savefig( 'sklearntraintest.pdf ',format ='pdf')\\nplt.show()\\nD.13.2 Standardization\\nIn some instances it may be necessary to standardize the data. This may be done in\\nsklearn with scaling methods such asMinMaxScaler or StandardScaler. Scaling may\\nimprove the convergence of gradient-based estimators and is useful when visualizing data\\non vastly different scales. For example, suppose that Xis our explanatory data (e.g., stored\\nas a numpy array), and we wish to standardize such that each value lies between 0 and 1.\\nfrom sklearn import preprocessing\\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\\nx_scaled = min_max_scaler.fit_transform(X)\\n# equivalent to:\\nx_scaled = (X - X. min (axis=0)) / (X. max (axis=0) - X. min (axis=0))\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 510, 'page_label': '493'}, page_content=\"Python Primer 493\\n3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\nFigure D.5: Example training (circles) and test (squares) set for two class classification.\\nExplanatory variables are the (x,y) coordinates, classes are zero (green) or one (blue).\\nD.13.3 Fitting and Prediction\\nOnce the data has been partitioned and standardized if necessary, the data may be fitted to\\na statistical model, e.g., a classification or regression model. For example, continuing with\\nour data from above, the following fits a model to the data and predicts the responses for\\nthe test set.\\nfrom sklearn.someSubpackage import someClassifier\\nclf = someClassifier() # choose appropriate classifier\\nclf.fit(X_train, y_train) # fit the data\\ny_prediction = clf.predict(X_test) # predict\\nSpecific classifiers for logistic regression, naïve Bayes, linear and quadratic discrimin-\\nant analysis, K-nearest neighbors, and support vector machines are given in Section 7.8.\\n☞ 277\\nD.13.4 Testing the Model\\nOnce the model has made its prediction we may test its e ffectiveness, using relevant met-\\nrics. For example, for classification we may wish to produce the confusion matrix for the\\ntest data. The following code does this for the data shown in Figure D.5, using a support\\nvector machine classifier.\\nfrom sklearn import svm\\nclf = svm.SVC(kernel = 'rbf')\\nclf.fit(X_train , y_train)\\ny_prediction = clf.predict(X_test)\"),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 511, 'page_label': '494'}, page_content='494 System Calls, URL Access, and Speed-Up\\nfrom sklearn.metrics import confusion_matrix\\nprint (confusion_matrix(y_test , y_prediction))\\n[[102 12]\\n[ 1 85]]\\nD.14 System Calls, URL Access, and Speed-Up\\nOperating system commands (whether in Windows, MacOS, or Linux) for creating dir-\\nectories, copying or removing files, or executing programs from the system shell can be\\nissued from within Python by using the package os. Another useful package is requests\\nwhich enables direct downloads of files and webpages from URLs. The following Python\\nscript uses both. It also illustrates a simple example of exception handling in Python.\\nmisc.py\\nimport os\\nimport requests\\nfor c in \"123456\":\\ntry : # if it does not yet exist\\nos.mkdir(\"MyDir\"+ c) # make a directory\\nexcept : # otherwise\\npass # do nothing\\nuname = \"https://github.com/DSML-book/Programs/tree/master/\\nAppendices/Python Primer/\"\\nfname = \"ataleof2cities.txt\"\\nr = requests.get(uname + fname)\\nprint (r.text)\\nopen (\\'MyDir1/ato2c.txt \\', \\'wb\\').write(r.content) #write to a file\\n# bytes mode is important here\\nThe package numba can significantly speed up calculations via smart compilation. First\\nrun the following code.\\njitex.py\\nimport timeit\\nimport numpy as np\\nfrom numba import jit\\nn = 10**8\\n#@jit\\ndef myfun(s,n):\\nfor i in range (1,n):\\ns = s+ 1/i\\nreturn s\\nstart = timeit.time.clock()\\nprint (\"Euler \\'s constant is approximately {:9.8f}\". format ('),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 512, 'page_label': '495'}, page_content='Python Primer 495\\nmyfun(0,n) - np.log(n)))\\nend = timeit.time.clock()\\nprint (\"elapsed time: {:3.2f} seconds\". format (end-start))\\nEuler \\'s constant is approximately 0.57721566\\nelapsed time: 5.72 seconds\\nNow remove the # character before the @ character in the code above, in order to\\nactivate the “just in time” compiler. This gives a 15-fold speedup:\\nEuler \\'s constant is approximately 0.57721566\\nelapsed time: 0.39 seconds\\nFurther Reading\\nTo learn Python, we recommend [82] and [110]. However, as Python is constantly evolving,\\nthe most up-to-date references will be available from the Internet.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 513, 'page_label': '496'}, page_content='496 System Calls, URL Access, and Speed-Up'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 514, 'page_label': '497'}, page_content='BIBLIOGRAPHY\\n[1] S. C. Ahalt, A. K. Krishnamurthy, P. Chen, and D. E. Melton. Competitive learning\\nalgorithms for vector quantization. Neural Networks, 3:277–290, 1990.\\n[2] H. Akaike. A new look at the statistical model identification. IEEE Transactions on\\nAutomatic Control, 19(6):716–723, 1974.\\n[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Math-\\nematical Society, 68:337–404, 1950.\\n[4] D. Arthur and S. Vassilvitskii. K-means ++: The advantages of careful seeding.\\nIn Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Al-\\ngorithms, pages 1027–1035, Philadelphia, 2007. Society for Industrial and Applied\\nMathematics.\\n[5] S. Asmussen and P. W. Glynn. Stochastic Simulation: Algorithms and Analysis .\\nSpringer, New York, 2007.\\n[6] R. G. Bartle. The Elements of Integration and Lebesgue Measure . John Wiley &\\nSons, Hoboken, 1995.\\n[7] D. Bates and D. Watts. Nonlinear Regression Analysis and Its Applications . John\\nWiley & Sons, Hoboken, 1988.\\n[8] J. O. Berger. Statistical Decision Theory and Bayesian Analysis . Springer, New\\nYork, second edition, 1985.\\n[9] J. Bezdek. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum\\nPress, New York, 1981.\\n[10] P. J. Bickel and K. A. Doksum. Mathematical Statistics, volume I. Pearson Prentice\\nHall, Upper Saddle River, second edition, 2007.\\n497'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 515, 'page_label': '498'}, page_content='498 Bibliography\\n[11] P. Billingsley. Probability and Measure . John Wiley & Sons, New York, third\\nedition, 1995.\\n[12] C. M. Bishop. Pattern Recognition and Machine Learning . Springer, New York,\\n2006.\\n[13] P. T. Boggs and R. H. Byrd. Adaptive, limited-memory BFGS algorithms for un-\\nconstrained optimization. SIAM Journal on Optimization, 29(2):1282–1299, 2019.\\n[14] Z. I. Botev, J. F. Grotowski, and D. P. Kroese. Kernel density estimation via di ffu-\\nsion. Annals of Statistics, 38(5):2916–2957, 2010.\\n[15] Z. I. Botev and D. P. Kroese. Global likelihood optimization via the cross-entropy\\nmethod, with an application to mixture models. In R. G. Ingalls, M. D. Rossetti,\\nJ. S. Smith, and B. A. Peters, editors, Proceedings of the 2004 Winter Simulation\\nConference, pages 529–535, Washington, DC, December 2004.\\n[16] Z. I. Botev, D. P. Kroese, R. Y . Rubinstein, and P. L’Ecuyer. The cross-entropy\\nmethod for optimization. In V . Govindaraju and C.R. Rao, editors,Machine Learn-\\ning: Theory and Applications , volume 31 of Handbook of Statistics , pages 35–59.\\nElsevier, 2013.\\n[17] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and\\nstatistical learning via the alternating direction method of multipliers. Foundations\\nand Trends in Machine Learning, 3:1–122, 2010.\\n[18] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,\\nCambridge, 2004. Seventh printing with corrections, 2009.\\n[19] R. A. Boyles. On the convergence of the EM algorithm. Journal of the Royal\\nStatistical Society, Series B, 45(1):47–50, 1983.\\n[20] L. Breiman. Classification and Regression Trees. CRC Press, Boca Raton, 1987.\\n[21] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.\\n[22] L. Breiman. Heuristics of instability and stabilization in model selection. Annals of\\nStatistics, 24(6):2350–2383, 12 1996.\\n[23] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.\\n[24] F. Cao, D.-Z. Du, B. Gao, P.-J. Wan, and P. M. Pardalos. Minimax problems in\\ncombinatorial optimization. In D.-Z. Du and P. M. Pardalos, editors, Minimax and\\nApplications, pages 269–292. Kluwer, Dordrecht, 1995.\\n[25] G. Casella and R. L. Berger. Statistical Inference. Duxbury Press, Pacific Grove,\\nsecond edition, 2001.\\n[26] K. L. Chung. A Course in Probability Theory. Academic Press, New York, second\\nedition, 1974.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 516, 'page_label': '499'}, page_content='Bibliography 499\\n[27] E. Cinlar. Introduction to Stochastic Processes . Prentice Hall, Englewood Cli ffs,\\n1975.\\n[28] T. M. Cover and J. A. Thomas. Elements of Information Theory . John Wiley &\\nSons, New York, 1991.\\n[29] J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. Reorthogonalization and\\nstable algorithms for updating the Gram-Schmidt QR factorization. Mathematics of\\nComputation, 30(136):772–795, 1976.\\n[30] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y . Rubinstein. A tutorial on the\\ncross-entropy method. Annals of Operations Research, 134(1):19–67, 2005.\\n[31] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-\\nplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1 –\\n38, 1977.\\n[32] L. Devroye. Non-Uniform Random Variate Generation. Springer, New York, 1986.\\n[33] N. R. Draper and H. Smith. Applied Regression Analysis. John Wiley & Sons, New\\nYork, third edition, 1998.\\n[34] Q. Duan and D. P. Kroese. Splitting for optimization. Computers & Operations\\nResearch, 73:119–131, 2016.\\n[35] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons,\\nNew York, 2001.\\n[36] B. Efron and T. J. Hastie. Computer Age Statistical Inference: Algorithms, Evidence,\\nand Data Science. Cambridge University Press, Cambridge, 2016.\\n[37] B. Efron and R. Tibshirani. An Introduction to the Bootstrap . Chapman & Hall,\\nNew York, 1994.\\n[38] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters ,\\n27(8):861–874, June 2006.\\n[39] W. Feller. An Introduction to Probability Theory and Its Applications , volume I.\\nJohn Wiley & Sons, Hoboken, second edition, 1970.\\n[40] J. C. Ferreira and V . A. Menegatto. Eigenvalues of integral operators defined by\\nsmooth positive definite kernels. Integral Equations and Operator Theory, 64:61–\\n81, 2009.\\n[41] N. I. Fisher and P. K. Sen, editors. The Collected Works of Wassily Hoe ffding.\\nSpringer, New York, 1994.\\n[42] G. S. Fishman. Monte Carlo: Concepts, Algorithms and Applications . Springer,\\nNew York, 1996.\\n[43] R. Fletcher. Practical Methods of Optimization . John Wiley & Sons, New York,\\n1987.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 517, 'page_label': '500'}, page_content='500 Bibliography\\n[44] Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning\\nand an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.\\n[45] J. H. Friedman. Greedy function approximation: A gradient boosting machine. An-\\nnals of Statistics, 29:1189–1232, 2000.\\n[46] A. Gelman. Bayesian Data Analysis. Chapman & Hall, New York, second edition,\\n2004.\\n[47] A. Gelman and J. Hall. Data Analysis Using Regression and Multilevel/Hierarchical\\nModels. Cambridge University Press, Cambridge, 2006.\\n[48] S. Geman and D. Geman. Stochastic relaxation, Gibbs distribution and the Bayesian\\nrestoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence, 6(6):721–741, 1984.\\n[49] J. E. Gentle. Random Number Generation and Monte Carlo Methods . Springer,\\nNew York, second edition, 2003.\\n[50] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in\\nPractice. Chapman & Hall, New York, 1996.\\n[51] P. Glasserman. Monte Carlo Methods in Financial Engineering . Springer, New\\nYork, 2004.\\n[52] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University\\nPress, Baltimore, fourth edition, 2013.\\n[53] I. Goodfellow, Y . Bengio, and A. Courville.Deep Learning. MIT Press, Cambridge,\\n2016.\\n[54] G. R. Grimmett and D. R. Stirzaker. Probability and Random Processes. Oxford\\nUniversity Press, third edition, 2001.\\n[55] T. J. Hastie, R. J. Tibshirani, and J. H. Friedman. The Elements of Statistical Learn-\\ning: Data mining, Inference, and Prediction. Springer, New York, 2009.\\n[56] T. J. Hastie, R. J. Tibshirani, and M. Wainwright.Statistical Learning with Sparsity:\\nThe Lasso and Generalizations. CRC Press, Boca Raton, 2015.\\n[57] J.-B. Hiriart-Urruty and C. Lemarèchal. Fundamentals of Convex Analysis .\\nSpringer, New York, 2001.\\n[58] W. Hock and K. Schittkowski. Test Examples for Nonlinear Programming Codes.\\nSpringer, New York, 1981.\\n[59] J. E. Kelley, Jr. The cutting-plane method for solving convex programs. Journal of\\nthe Society for Industrial and Applied Mathematics, 8(4):703–712, 1960.\\n[60] A. K. Jain. Fundamentals of Digital Image Processing. Prentice Hall, Englewood\\nCliffs, 1989.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 518, 'page_label': '501'}, page_content='Bibliography 501\\n[61] O. Kallenberg. Foundations of Modern Probability. Springer, New York, second\\nedition, 2002.\\n[62] A. Karalic. Linear regression in regression tree leaves. In Proceedings of ECAI-92,\\npages 440–441, Hoboken, 1992. John Wiley & Sons.\\n[63] C. Kaynak. Methods of combining multiple classifiers and their applications to\\nhandwritten digit recognition. Master’s thesis, Institute of Graduate Studies in Sci-\\nence and Engineering, Bogazici University, 1995.\\n[64] T. Keilath and A. H. Sayed, editors. Fast Reliable Algorithms for Matrices with\\nStructure. SIAM, Pennsylvania, 1999.\\n[65] C. Nussbaumer Knaflic. Storytelling with Data: A Data Visualization Guide for\\nBusiness Professionals. John Wiley & Sons, Hoboken, 2015.\\n[66] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-\\nniques - Adaptive Computation and Machine Learning. The MIT Press, Cambridge,\\n2009.\\n[67] A. N. Kolmogorov and S. V . Fomin. Elements of the Theory of Functions and\\nFunctional Analysis. Dover Publications, Mineola, 1999.\\n[68] D. P. Kroese, T. Brereton, T. Taimre, and Z. I. Botev. Why the Monte Carlo method\\nis so important today. Wiley Interdisciplinary Reviews: Computational Statistics ,\\n6(6):386–392, 2014.\\n[69] D. P. Kroese and J. C. C. Chan. Statistical Modeling and Computation . Springer,\\n2014.\\n[70] D. P. Kroese, S. Porotsky, and R. Y . Rubinstein. The cross-entropy method for\\ncontinuous multi-extremal optimization. Methodology and Computing in Applied\\nProbability, 8(3):383–407, 2006.\\n[71] D. P. Kroese, T. Taimre, and Z. I. Botev. Handbook of Monte Carlo Methods. John\\nWiley & Sons, New York, 2011.\\n[72] H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms\\nand Applications. Springer, New York, second edition, 2003.\\n[73] P. Lafaye de Micheaux, R. Drouilhet, and B. Liquet. The R Software: Fundamentals\\nof Programming and Statistical Analysis. Springer, New York, 2014.\\n[74] R. J. Larsen and M. L. Marx. An Introduction to Mathematical Statistics and Its\\nApplications. Prentice Hall, New York, third edition, 2001.\\n[75] A. M. Law and W. D. Kelton. Simulation Modeling and Analysis . McGraw-Hill,\\nNew York, third edition, 2000.\\n[76] P. L’Ecuyer. A unified view of IPA, SF, and LR gradient estimation techniques.\\nManagement Science, 36:1364–1383, 1990.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 519, 'page_label': '502'}, page_content='502 Bibliography\\n[77] P. L’Ecuyer. Good parameters and implementations for combined multiple recursive\\nrandom number generators. Operations Research, 47(1):159 – 164, 1999.\\n[78] E. L. Lehmann and G. Casella. Theory of Point Estimation. Springer, New York,\\nsecond edition, 1998.\\n[79] T. G. Lewis and W. H. Payne. Generalized feedback shift register pseudorandom\\nnumber algorithm. Journal of the ACM, 20(3):456–468, 1973.\\n[80] R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. John Wiley\\n& Sons, Hoboken, second edition, 2002.\\n[81] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale\\noptimization. Mathematical Programming, 45(1-3):503–528, 1989.\\n[82] M. Lutz. Learning Python. O’Reilly, fifth edition, 2013.\\n[83] M. Matsumoto and T. Nishimura. Mersenne twister: A 623-dimensionally\\nequidistributed uniform pseudo-random number generator. ACM Transactions on\\nModeling and Computer Simulation, 8(1):3–30, 1998.\\n[84] W. McKinney. Python for Data Analysis . O’Reilly Media, Inc., second edition,\\n2017.\\n[85] G. J. McLachlan and T. Krishnan. The EM Algorithm and Extensions. John Wiley\\n& Sons, Hoboken, second edition, 2008.\\n[86] G. J. McLachlan and D. Peel. Finite Mixture Models. John Wiley & Sons, New\\nYork, 2000.\\n[87] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller.\\nEquations of state calculations by fast computing machines. Journal of Chemical\\nPhysics, 21(6):1087–1092, 1953.\\n[88] C. A. Micchelli, Y . Xu, and H. Zhang. Universal kernels. Journal of Machine\\nLearning Research, 7:2651–2667, 2006.\\n[89] Z. Michalewicz. Genetic Algorithms + Data Structures = Evolution Programs.\\nSpringer, New York, third edition, 1996.\\n[90] J. F. Monahan. Numerical Methods of Statistics. Cambridge University Press, Lon-\\ndon, 2010.\\n[91] T. A. Mroz. The sensitivity of an empirical model of married women’s hours of\\nwork to economic and statistical assumptions. Econometrica, 55(4):765–799, 1987.\\n[92] K. P. Murphy. Machine Learning: A Probabilistic Perspective . The MIT Press,\\nCambridge, 2012.\\n[93] J. Neyman and E. Pearson. On the problem of the most e fficient tests of statistical\\nhypotheses. Philosophical Transactions of the Royal Society of London, Series A ,\\n231:289–337, 1933.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 520, 'page_label': '503'}, page_content='Bibliography 503\\n[94] M. A. Nielsen. Neural Networks and Deep Learning , volume 25. Determination\\nPress, 2015.\\n[95] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook. Technical University of\\nDenmark, 2008.\\n[96] J. R. Quinlan. Learning with continuous classes. In A. Adams and L. Sterling,\\neditors, Proceedings AI’92, pages 343–348, Singapore, 1992. World Scientific.\\n[97] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning.\\nMIT Press, Cambridge, 2006.\\n[98] B. D. Ripley. Stochastic Simulation. John Wiley & Sons, New York, 1987.\\n[99] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, New York,\\nsecond edition, 2004.\\n[100] S. M. Ross. Simulation. Academic Press, New York, third edition, 2002.\\n[101] S. M. Ross. A First Course in Probability. Prentice Hall, Englewood Cliffs, seventh\\nedition, 2005.\\n[102] R. Y . Rubinstein. The cross-entropy method for combinatorial and continuous op-\\ntimization. Methodology and Computing in Applied Probability, 2:127–190, 1999.\\n[103] R. Y . Rubinstein and D. P. Kroese.The Cross-Entropy Method: A Unified Approach\\nto Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning .\\nSpringer, New York, 2004.\\n[104] R. Y . Rubinstein and D. P. Kroese. Simulation and the Monte Carlo Method . John\\nWiley & Sons, New York, third edition, 2017.\\n[105] S. Ruder. An overview of gradient descent optimization algorithms. arXiv:\\n1609.04747, 2016.\\n[106] W. Rudin. Functional Analysis. McGraw–Hill, Singapore, second edition, 1991.\\n[107] D. Salomon. Data Compression: The Complete Reference . Springer, New York,\\n2000.\\n[108] G. A. F. Seber and A. J. Lee. Linear Regression Analysis . John Wiley & Sons,\\nHoboken, second edition, 2003.\\n[109] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From The-\\nory to Algorithms. Cambridge University Press, Cambridge, 2014.\\n[110] Z. A. Shaw. Learning Python 3 the Hard Way. Addison–Wesley, Boston, 2017.\\n[111] Y . Shen, S. Kiatsupaibul, Z. B. Zabinsky, and R. L. Smith. An analytically de-\\nrived cooling schedule for simulated annealing. Journal of Global Optimization ,\\n38(2):333–365, 2007.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 521, 'page_label': '504'}, page_content='504 Bibliography\\n[112] N. Z. Shor. Minimization Methods for Non-differentiable Functions. Springer, Ber-\\nlin, 1985.\\n[113] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman &\\nHall, New York, 1986.\\n[114] J. S. Simono ff. Smoothing Methods in Statistics. Springer, New York, 2012.\\n[115] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York,\\n2008.\\n[116] G. Strang. Introduction to Linear Algebra. Wellesley–Cambridge Press, Cambridge,\\nfifth edition, 2016.\\n[117] G. Strang. Linear Algebra and Learning from Data . Wellesley–Cambridge Press,\\nCambridge, 2019.\\n[118] W. N. Street, W. H. Wolberg, and O. L. Mangasarian. Nuclear feature extraction for\\nbreast tumor diagnosis. In IS&T/SPIE 1993 International Symposium on Electronic\\nImaging: Science and Technology, San Jose, CA, pages 861–870, 1993.\\n[119] V . M. Tikhomirov. On the representation of continuous functions of several variables\\nas superpositions of continuous functions of one variable and addition. In Selected\\nWorks of A. N. Kolmogorov, pages 383–387. Springer, Berlin, 1991.\\n[120] S. van Buuren. Flexible Imputation of Missing Data . CRC Press, Boca Raton,\\nsecond edition, 2018.\\n[121] V . N. Vapnik.The Nature of Statistical Learning Theory. Springer, New York, 1995.\\n[122] V . N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative fre-\\nquencies of events to their probabilities. Theory of Probability and Its Applications,\\n16(2):264–280, 1971.\\n[123] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, 1990.\\n[124] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer,\\n2010.\\n[125] A. Webb. Statistical Pattern Recognition. Arnold, London, 1999.\\n[126] H. Wendland. Scattered Data Approximation. Cambridge University Press, Cam-\\nbridge, 2005.\\n[127] D. Williams. Probability with Martingales . Cambridge University Press, Cam-\\nbridge, 1991.\\n[128] C. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of\\nStatistics, 11(1):95–103, 1983.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 522, 'page_label': '505'}, page_content='INDEX\\nA\\nacceptance probability, 78–80, 97\\nacceptance–rejection method, 73, 78\\naccuracy (classification–), 254\\nactivation function, 204, 325\\nAdaBoost, 317–320\\nAdaGrad, 340\\nAdam method, 340, 348\\nadjoint operation, 361\\naffine transformation, 405, 435\\nagglomerative clustering, 147\\nAkaike information criterion, 126, 176,\\n177\\nalgebraic multiplicity, 363\\naligned arrays (Python), 482\\nalmost sure convergence, 440\\nalternating direction method of\\nmultipliers, 220, 416\\nalternative hypothesis, 458\\nanaconda (Python), 463\\nanalysis of variance (ANOV A), 183, 184,\\n196, 209\\nannealing schedule, 97\\napproximation error, 32–34, 184\\napproximation–estimation tradeoff, 32,\\n42, 323\\nArmijo inexact line search, 409\\nassignment operator (Python), 467\\nattributes (Python), 466\\nauxiliary variable methods, 128\\naxioms of Kolmogorov, 421\\nB\\nback-propagation, 332\\nbackward elimination, 201\\nbackward substitution, 370\\nbagged estimator, 306\\nbagging, 305, 307, 310\\nbalance equations (Markov chains), 78,\\n79, 452\\nbandwidth, 131, 134, 225\\nbarplot, 9\\nbarrier function, 417\\nBarzilai–Borwein formulas, 335, 413\\nbasis\\nof a vector space, 355\\northogonal –, 361\\nBayes\\nempirical, 241\\nerror rate, 252\\nfactor, 58\\nnaïve –, 258\\noptimal decision rule, 258\\nBayes’ rule, 48, 49, 428, 454\\nBayesian information criterion, 55\\nBayesian statistics, 48, 50, 454\\nBernoulli distribution, 425, 457\\nBessel distribution, 164, 226\\n505'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 523, 'page_label': '506'}, page_content='506 Index\\nbeta distribution, 53, 425\\nbias of an estimator, 454\\nbias vector (deep learning), 326\\nbias–variance tradeoff, 35, 305\\nbinomial distribution, 425\\nBoltzmann distribution, 96\\nbootstrap aggregation, see bagging\\nbootstrap method, 88, 306\\nbounded mapping, 389\\nboxplot, 10, 14\\nbroadcasting (Python), 482\\nBroyden’s family, 411\\nBroyden–Fletcher–Goldfarb–Shanno\\n(BFGS) updating, 267, 339, 411\\nburn-in period, 78\\nC\\ncategorical variable, 3, 177, 178, 191,\\n192, 251, 299\\nCauchy sequence, 246, 385\\nCauchy–Schwarz inequality, 223, 246,\\n389, 412\\ncentral difference estimator, 106\\ncentral limit theorem, 447, 458\\nmultivariate, 448\\ncentroid, 144\\nchain rule for differentiation, 401\\ncharacteristic function, 225–227, 246,\\n392, 441\\ncharacteristic polynomial, 363\\nChebyshev’s inequality, 444\\nchi-squared distribution, 436, 439\\nCholesky decomposition, 70, 154, 247,\\n264, 373\\ncirculant matrix, 381, 393\\nclass (Python), 474\\nclassification, 20, 251–286\\nhierarchical, 256\\nmultilabel, 256\\nclassifier, 21, 251\\ncoefficient of determination, 181, 195\\nadjusted, 181\\ncoefficient profiles, 221\\ncombinatorial optimization, 402\\ncomma separated values (CSV), 2\\ncommon random numbers, 106, 120\\ncomplete Hilbert space, 224, 384\\ncomplete vector space, 216\\ncomplete convergence, 443\\ncomplete-data\\nlikelihood, 128\\nlog-likelihood, 138\\ncomposition of functions, 400\\nconcave function, 404, 407\\nconditional\\ndistribution, 431\\nexpectation, 431\\npdf, 74, 431\\nprobability, 428\\nconfidence interval, 85, 89, 94, 186, 457\\nBayesian, 52\\nbootstrap, 89\\nconfidence region, 457\\nconfusion matrix, 253, 254\\nconstrained optimization, 403\\ncontext management (Python), 477\\ncontinuous mapping, 389\\ncontinuous optimization, 402\\ncontrol variable, 92\\nconvergence\\nalmost sure, 440\\nin Lp norm, 442\\nin distribution, 440\\nin probability, 439\\nsure, 439\\nconvex\\nfunction, 63, 220, 403\\nprogram, 405–408\\nset, 43, 403\\nconvolution, 380, 392\\nconvolution neural network, 330\\nCook’s distance, 212\\ncooling factor, 97\\ncorrelation coefficient, 71, 429\\ncost-complexity\\nmeasure, 303\\npruning, 303\\ncountable sample space, 422\\ncovariance, 429\\nmatrix, 46, 70, 430–432, 435, 436\\nproperties, 429\\ncoverage probability, 457'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 524, 'page_label': '507'}, page_content='Index 507\\ncredible\\ninterval, 52\\nregion, 52\\ncritical\\nregion, 458\\nvalue, 458\\ncross tabulate, 7\\ncross-entropy\\nmethod, 100, 111\\nrisk, 54, 122, 125\\nin-sample, 176\\ntraining loss, 123\\ncross-validation, 38\\nleave-one-out, 40, 173\\nlinear model, 174\\ncrude Monte Carlo, 85\\ncubic spline, 236\\ncumulative distribution function (cdf),\\n72, 423\\njoint, 427\\ncycle, 81\\nD\\nDavidon–Fletcher–Powell updating, 353,\\n412\\ndecision tree, 288\\ndeep learning, 331\\ndegrees of freedom, 181\\ndendrogram, 148\\ndensity, 385\\ndependent variable, 168\\nderivatives\\nmultidimensional, 398\\npartial, 397\\ndesign matrix, 179\\ndetailed balance equations, 453\\ndeterminant of a matrix, 357\\ndiagonal matrix, 357\\ndiagonalizable, 364\\ndictionary (Python), 473\\ndigamma function, 127, 163\\ndimension, 355\\ndirect sum, 217\\ndirectional derivative, 404\\ndiscrete\\ndistribution, 423\\nFourier transform, 392\\noptimization, 402\\nprobability space, 422\\nsample space, 422\\nuniform distribution, 425\\ndiscriminant analysis, 259\\ndistribution\\nBernoulli, 425\\nBessel, 226\\nbeta, 53, 425\\nbinomial, 425\\nchi-squared, 436, 439\\ndiscrete, 423\\ndiscrete uniform, 425\\nexponential, 425\\nextreme value, 115\\nF, 439\\ngamma, 425\\nGaussian, see normal\\ngeometric, 425\\ninverse-gamma, 51, 83\\njoint, 427\\nmultivariate normal, 46, 435\\nnoncentral χ2, 437\\nnormal, 45, 425, 434\\nPareto, 425\\nPoisson, 425\\nprobability, 422, 427\\nStudent’s t, 439\\nuniform, 425\\nWeibull, 425\\ndivisive clustering, 148\\ndot notation (Python), 466\\ndual optimization problem, 407–408\\nE\\nearly stopping, 50, 250\\nefficiency\\nof estimators, 454\\nof acceptance–rejection, 72\\neigen-decomposition, 364\\neigenvalue, 363\\neigenvector, 363\\nelementary event, 422\\nelite sample, 100\\nempirical'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 525, 'page_label': '508'}, page_content='508 Index\\nBayes, 241\\ncdf, 11, 76\\ndistribution, 131\\nentropy impurity, 292\\nepoch (deep learning), 350\\nequilikely principle, 422\\nergodic Markov chain, 452\\nerror of the first and second kind, 459\\nestimate, 454\\nestimator, 454\\nbias of, 454\\ncontrol variable, 92\\nefficiency of, 454\\nunbiased, 454\\nEuclidean norm, 360\\nevaluation functional, 223, 245\\nevent, 421\\nelementary, 422\\nindependent, 428\\nexact match ratio, 256\\nexchangeable variables, 41\\nexpectation, 426\\nconditional, 431\\nproperties, 429, 431\\nvector, 46, 430, 432, 435\\nexpectation–maximization (EM)\\nalgorithm, 128, 137, 209\\nexpected generalization risk, 24\\nexpected optimism, 36\\nexplanatory variable, 22, 168\\nexponential distribution, 425\\nextreme value distribution, 115\\nF\\nfactor, 3, 177\\nfalse negative, 254\\nfalse positive, 254\\nfast Fourier transform, 394\\nFβ score, 255\\nF distribution, 183, 198, 424, 439\\nfeasible region, 403\\nfeature, 1, 20\\nimportance, 311\\nmap, 189, 216, 224, 230, 243, 274\\nfeed-forward network, 326\\nfeedback shift register, 69\\nfinite difference method, 107, 114\\nfinite-dimensional distributions, 427\\nFisher information matrix, 124\\nFisher’s scoring method, 127\\nfolds (cross-validation), 38\\nforward selection, 201\\nforward substitution, 370\\nFourier expansion, 386\\nFourier transform, 391\\ndiscrete, 392\\nfrequentist statistics, 453\\nfull rank matrix, 28\\nfunction (Python), 468\\nfunction space, 384\\nfunction, Ck, 403\\nfunctional, 389\\nfunctions of random variables, 431\\nG\\ngamma\\ndistribution, 425\\nfunction, 424\\nGauss–Markov inequality, 60\\nGauss–Newton search direction, 414\\nGaussian distribution, see normal\\ndistribution\\nGaussian kernel, 225\\nGaussian kernel density estimate, 131\\nGaussian process, 71, 238\\nGaussian rule of thumb, 134\\ngeneralization risk, 23, 86\\ngeneralized inverse-gamma distribution,\\n163\\ngeneralized linear model, 204\\ngeometric cooling, 97\\ngeometric distribution, 425\\ngeometric multiplicity, 363\\nGibbs pdf, 97\\nGibbs sampler, 81, 83, 84\\nrandom, 82\\nrandom order, 82\\nreversible, 82\\nGini impurity, 292\\nglobal balance equations, 452\\nglobal minimizer, 402\\ngradient, 397, 403'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 526, 'page_label': '509'}, page_content='Index 509\\nboosting, 316\\ndescent, 412\\nGram matrix, 218, 222, 270\\nGram–Schmidt procedure, 375\\nH\\nHamming distance, 142\\nHermite polynomials, 389\\nHermitian matrix, 362, 365\\nHessian matrix, 124, 398, 403, 404\\nhidden layer, 325\\nhierarchical classification, 256\\nHilbert matrix, 33\\ninverse, 61\\nHilbert space, 215, 385\\nisomorphism, 247\\nhinge loss, 269\\nhistogram, 10\\nHoeffding’s inequality, 63\\nhomotopy paths, 221\\nhyperparameters, 51, 241\\nhypothesis testing, 458\\nI\\nimmutable (Python), 464\\nimportance sampling, 93–96\\nimproper prior, 51, 83\\nin-sample risk, 36\\nincremental effects, 179\\nindependence\\nof event, 428\\nof random variables, 429\\nindependence sampler, 79\\nindependent and identically distributed\\n(iid), 429, 446, 454\\nindicator, 11\\nindicator feature, 178\\nindicator loss, 251\\ninfinitesimal perturbation analysis, 114\\ninformation matrix equality, 124\\ninheritance (Python), 475\\ninitial distribution (Markov chain), 452\\ninner product, 360\\ninstance (Python), 474\\nintegration\\nMonte Carlo, 86\\ninteraction, 179, 193\\ninterior-point method, 418\\ninterval estimate, see confidence interval\\ninverse\\ndiscrete Fourier transform, 393\\nFourier transform, 391\\nmatrix, 370\\ninverse-gamma distribution, 51, 83\\ninverse-transform method, 72\\nirreducible risk, 32\\niterable (Python), 472\\niterative reweighted least squares, 213,\\n350\\niterator (Python), 472\\nJ\\nJacobi\\nmatrix of, 409, 433\\nJensen’s inequality, 63\\njoint\\ncdf, 427\\npdf, 427\\njointly normal, see multivariate normal\\njointly normal distribution, see\\nmultivariate normal distribution\\nK\\nKarush–Kuhn–Tucker (KKT) conditions,\\n407, 408\\nkernel density estimation, 131, 135, 226,\\n330\\nkernel trick, 231\\nKiefer–Wolfowitz algorithm, 107\\nK-nearest neighbors method, 268\\nKolmogorov axioms, 421\\nKullback–Leibler divergence, 42, 100,\\n128, 351\\nL\\nLagrange\\ndual program, 407\\nfunction, 406\\nmethod, 406–407\\nmultiplier, 406\\nLagrangian, 406, 416\\npenalty, 416\\nLaguerre polynomials, 388\\nLance–Williams update, 150'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 527, 'page_label': '510'}, page_content='510 Index\\nLaplace’s approximation, 450\\nlasso (regression), 220\\nlatent variable methods, see auxiliary\\nvariable methods\\nlaw of large numbers, 67, 446, 458\\nlaw of total probability, 428\\nlearner, 22, 168\\nlearning rate, 335, 409\\nleast-squares\\niterative reweighted, 213\\nnonlinear, 190, 336, 414\\nordinary, 27, 47, 171, 191, 211, 378\\nregularized, 172, 235, 376\\nleave-one-out cross-validation, 40, 173\\nleft pseudo-inverse, 360\\nleft-eigenvector, 365\\nLegendre polynomials, 387\\nlength preserving transformation, 361\\nlength of a vector, 360\\nlevel set, 103\\nLevenberg–Marquardt search direction,\\n415\\nleverage, 173\\nLevinson–Durbin, 71, 382\\nlikelihood, 43, 49, 123, 456\\ncomplete-data, 128\\nlog-, 136, 456\\noptimization, 137\\nratio, 93\\nlimited memory BFGS, 337\\nlimiting pdf, 452\\nlimiting pdf (Markov chain), 452\\nline search, 409\\nlinear\\ndiscriminant function, 260\\nkernel, 224, 271\\nmapping, 389\\nmodel, 44, 211, 212\\nprogram, 406\\nsubspace, 362\\ntransformation, 356, 431\\nlinearly independent, 355\\nlink function, 204\\nlinkage, 148\\nmatrix, 150\\nlist comprehension (Python), 473\\nlocal balance equations, see detailed\\nbalance equations\\nlocal minimizer, 402\\nlocal/global minimum, 402\\nlog-likelihood, 456\\nlog-odds ratio, 266\\nlogarithmic efficiency, 118\\nlogistic distribution, 205\\nlogistic regression, 205\\nlong-run average reward, 89\\nloss function, 20\\nloss matrix, 253\\nM\\nM-estimator, 448\\nManhattan distance, 142\\nmarginal distribution, 427, 436\\nMarkov chain, 74, 78, 80, 83, 451\\nergodic, 452\\nreversible, 452\\nsimulation of, 75\\nMarkov chain Monte Carlo, 78\\nMarkov property, 74, 451\\nMatérn kernel, 226\\nmatplotlib (Python), 483–486\\nmatrix, 356\\nblockwise inverse, 370\\ncovariance, 70, 436\\ndeterminant, 357\\ndiagonal —, 357\\ninverse, 357\\nof Jacobi, 398, 409, 414, 433\\npseudo-inverse, 360\\nsparse, 379\\nToeplitz, 379\\ntrace, 357\\ntranspose, 357\\nmatrix multiplication (Python), 482\\nmax-cut problem, 151\\nmaximum a posteriori, 53\\nmaximum distance, 142\\nmaximum likelihood estimation, 43, 47,\\n100, 127, 136, 137, 456\\nmean integrated squared error, 133\\nmean squared error, 32, 88, 454\\nmeasure, 385'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 528, 'page_label': '511'}, page_content='Index 511\\nMersenne twister, 69\\nmethod (Python), 466\\nmethod of moments, 455\\nMetropolis–Hastings algorithm, 78, 81\\nminibatch, 336\\nminimax\\nequality, 408\\nproblem, 408\\nminimization, 411\\nminimizer, 402\\nminimum\\nglobal, 402\\nlocal, 402\\nmisclassification error, 253\\nmisclassification impurity, 292\\nmixture density, 135\\nmodel, 40\\nevidence, 55\\nlinear, 211, 212\\nmatrix, 44, 170, 174\\nmultiple linear regression, 169\\nnormal linear, 174, 182, 183, 438\\nregression, 191\\nresponse surface, 189\\nsimple linear regression, 187\\nmodified Bessel function of the second\\nkind, 164, 226\\nmodule (Python), 470\\nmodulo 2 generators, 69\\nmodulus, 69\\nmoment\\ngenerating function, 427, 436\\nsample-, 455\\nmomentum method, 341\\nMonte Carlo\\nintegration, 86\\nsampling, 68–85\\nsimulation, 67\\nMoore–Penrose pseudo-inverse, 360\\nmulti-logit, 266\\nmulti-output linear regression, 214\\nnonlinear, 329\\nmultilabel classification, 256\\nmultiple linear regression, 169\\nmultiple-recursive generator, 69\\nmultiplier\\nLagrange, 406\\nmultivariate\\ncentral limit theorem, 448\\nnormal distribution, 45–47, 435\\nmutable (Python), 464\\nN\\nnaïve Bayes, 258\\nnamespace (Python), 470\\nnested models, 59, 180\\nnetwork architecture, 330\\nnetwork depth, 327\\nnetwork width, 327\\nneural networks, 323\\nNewton’s method, 127, 206, 213, 337,\\n409\\n— for root-finding, 409\\nquasi —, 337\\nNeyman–Pearson approach, 459\\nnoisy optimization, 106\\nnominal distribution, 93\\nnoncentral χ2 distribution, 437\\nnorm, 384, 389\\nnormal distribution, 46, 425, 434, 435\\nnormal equations, 28\\nnormal linear model, 47, 174, 182, 183,\\n438\\nnormal matrix, 365\\nnormal method (bootstrap), 89\\nnormal model, 45\\nBayesian, 50, 51, 83\\nnormal updating (cross-entropy), 101\\nnull hypothesis, 458\\nnull space, 363\\nO\\nobject (Python), 464\\nobjective function, 402, 403, 407, 415\\nOccam’s razor, 173\\noperator, 389\\noperator (Python), 465\\noptimal decision boundary, 270\\noptimization\\ncombinatorial, 402\\nconstrained, 403\\ncontinuous, 402\\nunconstrained, 403'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 529, 'page_label': '512'}, page_content='512 Index\\nordinary least-squares, 27\\northogonal\\nbasis, 361\\ncomplement, 362\\nmatrix, 361, 382\\npolynomial, 388\\nprojection, 362\\nvector, 360\\northonormal, 361\\nbasis, 386\\nsystem, 385\\nout-of-bag, 307\\noverfitting, 23, 35, 141, 172, 216, 237,\\n289, 293, 300, 314\\noverloading (Python), 468\\nP\\np-norm, 220, 408\\nP-value, 195, 459\\npandas (Python), 2, 486–491\\nPareto distribution, 425\\nParseval’s formula, 392\\npartial derivative, 397\\npartition, 428\\npeaks function, 233\\nPearson’s height data, 207\\npenalty function, 415, 419\\nexact, 416\\npercentile, 7\\npercentile method (bootstrap), 89, 91\\npermutation matrix, 368\\nPlancherel’s theorem, 392\\nPLU decomposition, 368\\npointwise squared bias, 35\\npointwise variance, 35\\nPoisson distribution, 425\\npolynomial kernel, 230\\npolynomial regression model, 26\\npositive definite\\nmatrix, 403\\npositive semidefinite\\nfunction, 222\\nmatrix, 367, 404\\nposterior\\npdf, 49\\npredictive density, 50\\nprecision, 255\\npredicted residual, 173\\n— sum of squares (PRESS), 173\\nprediction function, 20\\nprediction interval, 186\\npredictive mean, 240\\npredictor, 168\\nprimal optimization problem, 407\\nprincipal axes, 154\\nprincipal component analysis (PCA),\\n153, 155\\nprincipal components, 154\\nprior\\nimproper, 83\\npdf, 49\\npredictive density, 50\\nuninformative, 50\\nprobability\\ndensity function (pdf), 424\\ndensity function (pdf), joint, 427\\ndistribution, 422, 427\\nmass function, 424\\nmeasure, 421\\nspace, 422\\nproduct rule, 74, 428, 452\\nprojected subgradient method, 107\\nprojection matrix, 27, 173, 211, 265,\\n362, 438\\nprojection pursuit, 350\\nproposal (MCMC), 78\\npseudo-inverse, 28, 211, 360, 378\\nPythagoras’ theorem, 180, 181, 183, 231,\\n361\\nQ\\nquadratic discriminant function, 260\\nquadratic program, 406\\nqualitative variable, 3\\nquantile, 52, 85\\nquantile–quantile plot, 199\\nquantitative variable, 3\\nquartile, 7\\nquasi-Newton method, 337, 411\\nquasi-random point set, 233\\nquotient rule for differentiation, 160'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 530, 'page_label': '513'}, page_content='Index 513\\nR\\nradial basis function (rbf) kernel, 225,\\n276\\nrandom\\nexperiment, 421\\nnumber generator, 68\\nnumbers (Python), 483\\nsample\\nsee iid sample, 454\\nvariable, 422\\nvector, 427, 431\\ncovariance of, 430\\nexpectation of, 430\\nwalk sampler, 80\\nrange (Python), 472\\nrank, 28, 356\\nrarity parameter (cross-entropy), 100\\nratio estimator, 89\\nread_csv (Python), 2\\nrecall, 255\\nreference (Python), 467\\nregional prediction functions, 288\\nregression, 20, 167\\nfunction, 21\\nline, 169\\nmodel, 191\\nsimple linear, 181\\nregularization, 216, 217\\npaths, 221\\nregularization parameter, 217\\nregularizer, 217\\nrelative error (estimated), 85\\nrelative time variance product, 454\\nrenewal reward process, 89\\nrepresentational capacity, 323\\nrepresenter of evaluation, 222\\nreproducing kernel Hilbert space\\n(RKHS), 222\\nreproducing property, 222\\nresampling, 76, 88\\nresidual squared error, 171\\nresidual sum of squares, 171\\nresiduals, 171, 173\\nresponse surface model, 189\\nresponse variable, 20, 168\\nreverse Markov chain, 452\\nreversibility, 452\\nreversible Gibbs sampler, 82\\nridge regression, 216, 217\\nRiemann–Lebesgue lemma, 391\\nright pseudo-inverse, 360\\nrisk, 20, 167\\nRobbins–Monro algorithm, 107\\nroot finding, 408\\nR2, see coefficient of determination\\nS\\nsaddle point, 403\\nproblem, 408\\nsample\\nmean, 7, 85, 455\\nmedian, 7\\nquantile, 7\\nrange, 8\\nspace, 421\\ncountable, 422\\ndiscrete, 422\\nstandard deviation, 8, 455\\nvariance, 8, 89, 455\\nsaturation, 333\\nSavage–Dickey density ratio, 59\\nscale-mixture, 164\\nscatterplot, 13\\nscikit-learn (Python), 491–494\\nscore function, 43, 123\\nmethod, 114\\nsecant condition, 411\\nsemi-simple matrix, 364\\nsequence object (Python), 472\\nset (Python), 473\\nshear operation, 359\\nSherman–Morrison\\nformula, 174, 247, 248, 371\\nrecursion, 372, 373, 414\\nsignificance level, 459\\nsimple linear regression, 169, 187\\nsimulated annealing, 96, 97\\nsinc kernel, 226\\nsingular value, 377, 378\\nsingular value decomposition, 154, 376\\nslack variable, 417\\nSlater’s condition, 408'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 531, 'page_label': '514'}, page_content='514 Index\\nslice (Python), 3, 465\\nsmoothing parameter, 100\\nsoftmax function, 267, 329\\nsource vectors, 143\\nsparse matrix, 379\\nspecificity, 255\\nspectral representation, 377\\nsphere the data, 264\\nsplitting for continuous optimization,\\n103\\nsplitting rule, 289\\nsquared-error loss, 167\\nstandard basis, 356\\nstandard deviation, 426\\nsample-, 455\\nstandard error (estimated), 85\\nstandard normal distribution, 434\\nstandardization, 435\\nstationary point, 403\\nstatistical (estimation) error, 32, 95\\nstatistical test\\none-sided –, 458\\ntwo-sided –, 458\\nstatistics\\nBayesian, 454\\nfrequentist, 453\\nsteepest descent, 331, 412\\nstep-size parameter γ, 314\\nstochastic approximation, 106, 336\\nstochastic confidence interval, 457\\nstochastic counterpart, 107\\nstochastic gradient descent, 336, 350\\nstochastic process, 427\\nstrict feasibility, 408\\nstrong duality, 408\\nStudent’s t distribution, 183, 424, 439\\nmultivariate, 162, 164, 226\\nstudentized residual, 212\\nstumps, 319\\nsubgradient, 404\\nsubgradient method, 107\\nsum rule, 422\\nsupervised learning, 22\\nsupport vectors, 271\\nSylvester equation, 379\\nsystematic Gibbs sampler, 82\\nT\\ntables\\ncounts, 6\\nfrequency, 6\\nmargins, 7\\ntarget distribution, 78\\nTaylor’s theorem\\nmultidimensional, 400\\ntest\\nloss, 24\\nsample, 24\\nstatistic, 458\\ntheta KDE, 134\\ntime-homogeneous, 452\\nTobit regression, 209\\nToeplitz matrix, 379\\ntotal sum of squares, 181\\ntower property of expectation, 431\\ntrace of a matrix, 357\\ntraining loss, 23\\ntraining set, 21\\ntransformation\\nof random variables, 431, 433\\nrule, 95, 432\\ntransition\\ndensity, 74, 452\\ngraph, 75\\ntranspose of a matrix, 356, 357\\ntree branch, 301\\ntrue negative, 254\\ntrue positive, 254\\ntrust region, 409\\ntype (Python), 466\\ntype I and type II errors, 459\\nU\\nunbiased, 60\\nunbiased estimator, 454\\nunconstrained optimization, 403\\nuniform distribution, 425\\nunion bound, 422\\nunitary matrix, 362\\nuniversal approximation property, 226\\nunsupervised learning, 22\\nV\\nvalidation set, 25, 303'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 532, 'page_label': '515'}, page_content='Index 515\\nVandermonde matrix, 29, 393\\nVapnik–Chernovenkis bound, 63\\nvariance, 426, 430\\nproperties, 429\\nsample, 89, 455\\nsample-, 455\\nvector quantization, 143\\nvector space, 355\\nbasis, 355\\ndimension, 355\\nV oronoi tessellation, 143\\nW\\nweak derivative, 114\\nweak duality, 407\\nweak learners, 313\\nWeibull distribution, 425\\nweight matrix (deep learning), 326\\nWolfe dual program, 407\\nWoodbury identity, 248, 352, 371, 399')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text_documents = loader_pdf.load()\n",
    "pdf_text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c78ad105-51be-42da-9572-489cfb6784d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263c5afb-7979-4245-94fc-9815f164adb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-22T15:41:45+10:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-08-22T15:41:45+10:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 4.16.0 (1.40.25)', 'source': 'DSML.pdf', 'total_pages': 533, 'page': 13, 'page_label': 'xviii'}, page_content='xviii Notation\\n• We employ color to emphasize that certain words refer to a dataset, function, or\\npackage in Python. All code is written in typewriter font. To be compatible with past\\nnotation choices, we introduced a special blue symbol X for the model (design) matrix of\\na linear model.\\n• Important notation such as T, g, g∗ is often defined in a mnemonic way, such as Tfor\\n“training”, g for “guess”, g∗for the “star” (that is, optimal) guess, and ℓfor “loss”.\\n• We will occasionally use a Bayesian notation convention in which the same symbol is\\nused to denote different (conditional) probability densities. In particular, instead of writing\\nfX(x) and fX |Y (x |y) for the probability density function (pdf) of X and the conditional pdf\\nof X given Y, we simply write f (x) and f (x |y). This particular style of notation can be of\\ngreat descriptive value, despite its apparent ambiguity.\\nGeneral font/notation rules\\nx scalar\\nx vector\\nX random vector\\nX matrix\\nX set\\nbx estimate or approximation\\nx∗ optimal\\nx average\\nCommon mathematical symbols\\n∀ for all\\n∃ there exists\\n∝ is proportional to\\n⊥ is perpendicular to\\n∼ is distributed as\\niid\\n∼, ∼iid are independent and identically distributed as\\napprox.\\n∼ is approximately distributed as\\n∇f gradient of f\\n∇2 f Hessian of f\\nf ∈Cp f has continuous derivatives of order p\\n≈ is approximately\\n≃ is asymptotically\\n≪ is much smaller than\\n⊕ direct sum')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text_documents[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4433a9-0ee9-431f-92b1-78b318406dff",
   "metadata": {},
   "source": [
    "## Web based loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09c5b1fe-0703-4cd2-ab09-770ee04f5e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x246602d8290>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(web_path = (\"https://en.wikipedia.org/wiki/Data_science\"))\n",
    "\n",
    "#can give multiple websites\n",
    "web_loader = WebBaseLoader(web_paths = (\"https://en.wikipedia.org/wiki/Data_science\", ))\n",
    "web_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7189aa0b-cb00-4f79-a995-e8a040bf0fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Data_science', 'title': 'Data science - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nData science - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nFoundations\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nEtymology\\n\\n\\n\\n\\nToggle Etymology subsection\\n\\n\\n\\n\\n\\n2.1\\nEarly usage\\n\\n\\n\\n\\n\\n\\n\\n\\n2.2\\nModern usage\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nData science and data analysis\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nCloud computing for data science\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nEthical consideration in data science\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nData science\\n\\n\\n\\n50 languages\\n\\n\\n\\n\\nالعربيةAzərbaycancaবাংলাБългарскиCatalàČeštinaDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어Հայերենहिन्दीIdoBahasa IndonesiaIsiZuluItalianoעבריתಕನ್ನಡҚазақшаLatviešuМакедонскиBahasa Melayuမြန်မာဘာသာNederlands日本語Norsk bokmålਪੰਜਾਬੀPolskiPortuguêsRuna SimiРусскийShqipSimple EnglishسنڌيСрпски / srpskiSuomiதமிழ்ไทยTürkçeУкраїнськаاردوTiếng Việt粵語中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia CommonsWikibooksWikiversityWiktionaryWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nField of study to extract knowledge from data\\nNot to be confused with information science or computer science.\\n\\n\\nThe existence of Comet NEOWISE (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the Wide-field Infrared Survey Explorer.\\nData science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.[2]\\nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.[9]\\n\\n\\nFoundations[edit]\\nData science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.[11]\\nVasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[12] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[13] Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[14]\\n\\nEtymology[edit]\\nEarly usage[edit]\\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[14] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.\\xa0F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[15] Later, attendees at a 1992 statistics symposium at the University of Montpellier\\xa0 II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[16][17]\\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C.\\xa0F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[18] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[17]\\n\\nModern usage[edit]\\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[19] a catchphrase that was picked up even by major-city newspapers like the New York Times[20] and the Boston Globe.[21] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[22]\\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[23] In 2014, the American Statistical Association\\'s Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[24]\\nThe professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[25] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[26]\\n\\nData science and data analysis[edit]\\nExample for the usefulness of exploratory data analysis as demonstrated using the Datasaurus dozen data set\\nData science is at the intersection of mathematics, computer science and domain expertise.\\nData analysis typically involves working with structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.[27]\\nData science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning.[28][29]\\n\\nCloud computing for data science[edit]\\nA cloud-based architecture for enabling big data analytics. Data flows from various sources, such as personal computers, laptops, and smart phones, through cloud services for processing and analysis, finally leading to various big data applications.\\nCloud computing can offer access to large amounts of computational power and storage.[30] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[31]\\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times.[32]\\n\\nEthical consideration in data science[edit]\\nData science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [33][34]\\nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[35][36]\\n\\nSee also[edit]\\nPython (programming language)\\nR (programming language)\\nData engineering\\nBig data\\nMachine learning\\nBioinformatics\\nAstroinformatics\\nTopological data analysis\\nList of open-source data science software\\nReferences[edit]\\n\\n\\n^ Donoho, David (2017). \"50 Years of Data Science\". Journal of Computational and Graphical Statistics. 26 (4): 745–766. doi:10.1080/10618600.2017.1384734. S2CID\\xa0114558008.\\n\\n^ Dhar, V. (2013). \"Data science and prediction\". Communications of the ACM. 56 (12): 64–73. doi:10.1145/2500499. S2CID\\xa06107147. Archived from the original on 9 November 2014. Retrieved 2 September 2015.\\n\\n^ Danyluk, A.; Leidig, P. (2021). Computing Competencies for Undergraduate Data Science Curricula (PDF). ACM Data Science Task Force Final Report (Report).\\n\\n^ Mike, Koby; Hazzan, Orit (20 January 2023). \"What is Data Science?\". Communications of the ACM. 66 (2): 12–13. doi:10.1145/3575663. ISSN\\xa00001-0782.\\n\\n^ Hayashi, Chikio (1 January 1998). \"What is Data Science\\xa0? Fundamental Concepts and a Heuristic Example\". In Hayashi, Chikio; Yajima, Keiji; Bock, Hans-Hermann; Ohsumi, Noboru; Tanaka, Yutaka; Baba, Yasumasa (eds.). Data Science, Classification, and Related Methods. Studies in Classification, Data Analysis, and Knowledge Organization. Springer Japan. pp.\\xa040–51. doi:10.1007/978-4-431-65950-1_3. ISBN\\xa09784431702085.\\n\\n^ a b c Cao, Longbing (29 June 2017). \"Data Science: A Comprehensive Overview\". ACM Computing Surveys. 50 (3): 43:1–43:42. arXiv:2007.03606. doi:10.1145/3076253. ISSN\\xa00360-0300. S2CID\\xa0207595944.\\n\\n^ Tony Hey; Stewart Tansley; Kristin Michele Tolle (2009). The Fourth Paradigm: Data-intensive Scientific Discovery. Microsoft Research. ISBN\\xa0978-0-9825442-0-4. Archived from the original on 20 March 2017.\\n\\n^ Bell, G.; Hey, T.; Szalay, A. (2009). \"Computer Science: Beyond the Data Deluge\". Science. 323 (5919): 1297–1298. doi:10.1126/science.1170411. ISSN\\xa00036-8075. PMID\\xa019265007. S2CID\\xa09743327.\\n\\n^ Davenport, Thomas H.; Patil, D. J. (October 2012). \"Data Scientist: The Sexiest Job of the 21st Century\". Harvard Business Review. 90 (10): 70–76, 128. PMID\\xa023074866. Retrieved 18 January 2016.\\n\\n^ Emmert-Streib, Frank; Dehmer, Matthias (2018). \"Defining data science by a data-driven quantification of the community\". Machine Learning and Knowledge Extraction. 1: 235–251. doi:10.3390/make1010015.\\n\\n^ \"1. Introduction: What Is Data Science?\". Doing Data Science [Book]. O’Reilly. Retrieved 3 April 2020.\\n\\n^ Vasant Dhar (1 December 2013). \"Data science and prediction\". Communications of the ACM. 56 (12): 64–73. doi:10.1145/2500499. S2CID\\xa06107147.\\n\\n^ \"Statistics is the least important part of data science «\\xa0Statistical Modeling, Causal Inference, and Social Science\". statmodeling.stat.columbia.edu. Retrieved 3 April 2020.\\n\\n^ a b Donoho, David (18 September 2015). \"50 years of Data Science\" (PDF). Retrieved 2 April 2020.\\n\\n^ Wu, C. F. Jeff (1986). \"Future directions of statistical research in China: a historical perspective\" (PDF). Application of Statistics and Management. 1: 1–7. Retrieved 29 November 2020.\\n\\n^ Escoufier, Yves; Hayashi, Chikio; Fichet, Bernard, eds. (1995). Data science and its applications. Tokyo: Academic Press/Harcourt Brace. ISBN\\xa00-12-241770-4. OCLC\\xa0489990740.\\n\\n^ a b Murtagh, Fionn; Devlin, Keith (2018). \"The Development of Data Science: Implications for Education, Employment, Research, and the Data Revolution for Sustainable Development\". Big Data and Cognitive Computing. 2 (2): 14. doi:10.3390/bdcc2020014.\\n\\n^ Wu, C.\\xa0F. Jeff. \"Statistics=Data Science?\" (PDF). Retrieved 2 April 2020.\\n\\n^ Davenport, Thomas (1 October 2012). \"Data Scientist: The Sexiest Job of the 21st Century\". Harvard Business Review. Retrieved 10 October 2022.\\n\\n^ Miller, Claire (4 April 2013). \"Data Science: The Numbers of Our Lives\". New York Times. New York City. Retrieved 10 October 2022.\\n\\n^ Borchers, Callum (11 November 2015). \"Behind the scenes of the \\'sexiest job of the 21st century\\'\". Boston Globe. Boston. Retrieved 10 October 2022.\\n\\n^ Davenport, Thomas (15 July 2022). \"Is Data Scientist Still the Sexiest Job of the 21st Century?\". Harvard Business Review. Retrieved 10 October 2022.\\n\\n^ Gupta, Shanti (11 December 2015). \"William S. Cleveland\". Retrieved 2 April 2020.\\n\\n^ Talley, Jill (1 June 2016). \"ASA Expands Scope, Outreach to Foster Growth, Collaboration in Data Science\". Amstat News. American Statistical Association.. In 2013 the first European Conference on Data Analysis (ECDA2013) started in Luxembourg the process which founded the European Association for Data Science (EuADS) www.euads.org in Luxembourg in 2015. \\n\\n^ Davenport, Thomas H.; Patil, D. J. (1 October 2012). \"Data Scientist: The Sexiest Job of the 21st Century\". Harvard Business Review. No.\\xa0October 2012. ISSN\\xa00017-8012. Retrieved 3 April 2020.\\n\\n^ \"US NSF – NSB-05-40, Long-Lived Digital Data Collections Enabling Research and Education in the 21st Century\". www.nsf.gov. Retrieved 3 April 2020.\\n\\n^ James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (29 September 2017). An Introduction to Statistical Learning: with Applications in R. Springer.\\n\\n^ Provost, Foster; Tom Fawcett (1 August 2013). \"Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\". O\\'Reilly Media, Inc.\\n\\n^ Han, Kamber; Pei (2011). Data Mining: Concepts and Techniques. ISBN\\xa09780123814791.\\n\\n^ Hashem, Ibrahim Abaker Targio; Yaqoob, Ibrar; Anuar, Nor Badrul; Mokhtar, Salimah; Gani, Abdullah; Ullah Khan, Samee (2015). \"The rise of \"big data\" on cloud computing: Review and open research issues\". Information Systems. 47: 98–115. doi:10.1016/j.is.2014.07.006.\\n\\n^ Qiu, Junfei; Wu, Qihui; Ding, Guoru; Xu, Yuhua; Feng, Shuo (2016). \"A survey of machine learning for big data processing\". EURASIP Journal on Advances in Signal Processing. 2016 (1). doi:10.1186/s13634-016-0355-x. ISSN\\xa01687-6180.\\n\\n^ Armbrust, Michael; Xin, Reynold S.; Lian, Cheng; Huai, Yin; Liu, Davies; Bradley, Joseph K.; Meng, Xiangrui; Kaftan, Tomer; Franklin, Michael J.; Ghodsi, Ali; Zaharia, Matei (27 May 2015). \"Spark SQL: Relational Data Processing in Spark\". Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM. pp.\\xa01383–1394. doi:10.1145/2723372.2742797. ISBN\\xa0978-1-4503-2758-9.\\n\\n^ Floridi, Luciano; Taddeo, Mariarosaria (28 December 2016). \"What is data ethics?\". Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 374 (2083): 20160360. Bibcode:2016RSPTA.37460360F. doi:10.1098/rsta.2016.0360. ISSN\\xa01364-503X. PMC\\xa05124072. PMID\\xa028336805.\\n\\n^ Mittelstadt, Brent Daniel; Floridi, Luciano (2016). \"The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts\". Science and Engineering Ethics. 22 (2): 303–341. doi:10.1007/s11948-015-9652-2. ISSN\\xa01353-3452. PMID\\xa026002496.\\n\\n^ Barocas, Solon; Selbst, Andrew D (2016). \"Big Data\\'s Disparate Impact\". California Law Review. doi:10.15779/Z38BG31 – via Berkeley Law Library Catalog.\\n\\n^ Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind (14 April 2017). \"Semantics derived automatically from language corpora contain human-like biases\". Science. 356 (6334): 183–186. arXiv:1608.07187. Bibcode:2017Sci...356..183C. doi:10.1126/science.aal4230. ISSN\\xa00036-8075.\\n\\n\\nvteData\\nAcquisition\\nAugmentation\\nAnalysis\\nAnonymization\\nArchaeology\\nBig\\nCleansing\\nCollection\\nCompression\\nCorruption\\nCuration\\nDegradation\\nEcosystem\\nEditing\\nETL/ELT\\nExtract\\nTransform\\nLoad\\nEthics\\nFarming\\nFormat management\\nFusion\\nGovernance\\nCooperatives\\nInfrastructure\\nIntegration\\nIntegrity\\nLibrary\\nLineage\\nLoss\\nManagement\\nMigration\\nMining\\nPhilanthropy\\nPre-processing\\nPreservation\\nProcessing\\nProtection (privacy)\\nPublishing\\nOpen data\\nRecovery\\nReduction\\nRetention\\nQuality\\nScience\\nScraping\\nScrubbing\\nSecurity\\nSharing\\nStewardship\\nStorage\\nSynchronization\\nTopological data analysis\\nType\\nValidation\\nWarehouse\\nWrangling/munging\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Data_science&oldid=1278396049\"\\nCategories: Information scienceComputer occupationsComputational fields of studyData analysisData scienceHidden categories: Articles with short descriptionShort description is different from WikidataUse dmy dates from August 2023\\n\\n\\n\\n\\n\\n\\n This page was last edited on 2 March 2025, at 06:17\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nData science\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n50 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0fddb-f8d9-4bd4-b893-6412280645dc",
   "metadata": {},
   "source": [
    "### Want to retrive specifc classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0962b44a-b550-4078-b3a0-d93340f42650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2465ee84c50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use beautiful soap\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "web_loader = WebBaseLoader(web_path = (\"https://www.geeksforgeeks.org/data-science/\"), \n",
    "                          bs_kwargs = dict(parse_only = bs4.SoupStrainer( class_ = (\"header-main__left-list-item notification-button\"))))\n",
    "\n",
    "web_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6aaddc5-f5fd-47d7-b2c8-3dbdf24b33a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/data-science/'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                                        You're all caught up!!\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e75f5-9884-4916-959f-ec710a17d444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
